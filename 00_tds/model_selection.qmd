---
title: "Model Selection"
subtitle: ""
date: last-modified
sidebar: auto
number-sections: false
toc: true
author:
  - Jumbong Junior 

categories: []
tags: ["Model Selection"]
title-block-banner: false
bibliography: references.bib
format: 
  html: 
    mainfont: Times New Roman
    fontsize: 1.1em

jupyter: python3
notice: |
    @wasserman2004all 
---



**Introduction**

Reducing the number of variables in a regression model is not merely a technical exercise; it is a strategic choice that must be guided by the objectives of the analysis. In a previous work, we demonstrated how simple tools, such as correlation analysis or the Variance Inflation Factor (VIF), can already shrink a dataset with hundreds of predictors into a far more compact model. Yet, even after this initial reduction, models often still contain too many variables to work effectively. A smaller model with fewer predictors offers several advantages: it may yield better predictions than a larger model, it is more parsimonious—hence easier to interpret—and it often generalizes better. As more variables are added, the model’s bias decreases but its variance increases. This is the essence of the bias–variance trade-off: too few variables lead to high bias (underfitting), whereas too many lead to high variance (overfitting). Good predictive performance requires a balance between the two.

This is where variable selection and dimension reduction methods come into play. There are multiple approaches to address this challenge. Some methods inherently handle multicollinearity, such as Principal Component Analysis (PCA) combined with linear regression, or sparse regression techniques like the Lasso. Others—such as traditional model selection methods—require explicit treatment of multicollinearity, for example by monitoring the VIF. In model selection, two core problems arise: (i) assigning a score to each model that reflects, in some sense, how “good” the model is, and (ii) searching through the set of candidate models to identify the one with the best score.

Importantly, the choice of scoring criterion depends on the fundamental objective of the regression. In linear regression, three main objectives can be distinguished:

1. **Parameter estimation** – obtaining stable and precise coefficient estimates.
2. **Variable selection** – identifying the predictors whose coefficients are truly nonzero and significant.
3. **Prediction** – forecasting future values of the response variable as accurately as possible.

A model chosen to minimize the mean squared error of the coefficients (parameter estimation) is not necessarily the best for identifying the most relevant variables (variable selection) or for maximizing predictive accuracy (prediction). In this article, we focus on model selection methods. We first review the different scoring criteria used for model selection, and then present the strategies that allow us to navigate the space of possible models and select the most relevant variables according to the goal of the regression. The first section introduces the framework of model selection. The second section discusses the different scoring criteria used to evaluate models.


# Framework of this paper.

In this section, we provide a brief overview of the linear regression model. We begin by describing the dataset, including the number of observations and the number of covariates. We then introduce the model itself and outline the assumptions made about the data.

We assume that we have a dataset with $n$ observations and $p$ covariates. The response variable is denoted by $Y$ is continuous and the covariates are denoted by $X_1, \ldots, X_p$. We assume that the relationship between the response variable and the covariates is linear, that is:
$$
Y_i = \beta_0 + \sum_{j=1}^{p} \beta_j X_{ij} + \epsilon_i
$$

for $i = 1, \ldots, n$, where $\beta_0$ is the intercept, $\beta_j$ is the coefficient of the $j$-th covariate, and $\epsilon_i$ is the error term. We assume that the error term is independent and identically distributed (i.i.d.) with mean zero and variance $\sigma^2$. 


# Scoring models

In model selection, the first challenge is to assign a score to each model, where a model is defined by a particular subset of covariates. This section explains how models can be scored.

Let us first discuss the problem of scoring models. Let $S \subset \{1, \ldots, p\}$ and let $\mathcal{X}_S = \{X_j : j \in S\}$ denote a subset of the covariates. Let $\beta_S$ denote the coefficients of the corresponding set of covariates and let $\hat{\beta}_S$ denote the least squares estimate of $\beta_S$. Also, let $X_S$ denote the $X$ matrix for this subset of covariates and define $\hat{r}_S(x) = \hat{\beta}_{0,S} + \sum_{j \in S} \hat{\beta}_j x_j,$ to be the estimated regression function. The predicted values from model $S$ are denoted by $\hat{Y}_i(S) = \hat{r}_S(X_i)$. The $\textbf{prediction risk}$ is defined to be

$$
\begin{equation}
R(S) = \sum_{i=1}^{n} \mathbb{E}\left(\hat{Y}_i(S) - Y_i^*\right)^2
\end{equation}
$$

where $Y_i^*$ is the future observation of $Y_i$ at the covariate $X_i$. 
The goal of model selection is to find the subset $S$ that makes the prediction risk $R(S)$ small.

With data, we cannot compute the prediction risk $R(S)$ directly. In this situation, we generally use its estimate $\hat{R}(S)$ based on the available data. The estimation of the prediction risk are used as our scoring criteria. 

The naive estimate of the prediction risk that we can use is : the $\textbf{training error}$, which is defined as

$$
\hat{R_{tr}}(S) = \sum_{i=1}^{n} \left(\hat{Y}_i(S) - Y_i\right)^2
$$

where $Y_i$ is the observed value of the response variable for the $i$-th observation.

However, the training error is very biased as an estimate of the prediction risk. It is always smaller than the prediction risk $R(S)$. In fact,

$$
bias(\hat{R_{tr}}(S)) = \mathbb{E}\left(\hat{R_{tr}}(S)\right) - R(S) = -2\sum_{i=1}^{n} \mathbb{Cov}\left(\hat{Y}_i(S), Y_i\right)
$$

What explains this biais is that the data is used twice: once to fit the model and once to compute the training error. When we fit a complex model, with many parameters, the covariance $\mathbb{Cov}\left(\hat{Y}_i(S), Y_i\right)$ will be large and the bias of the training error will get worse. This is why we need to use a more reliable estimate of the prediction risk. 

## Mallow's $C_p$ statistic

The Mallow's $C_p$ statistic is a popular method for model selection. It is defined as:
$$
\hat{R}(S) = \hat{R_{tr}}(S) + 2|S| \hat{\sigma}^2
$$

where $|S|$ is the number of terms in $S$ and $\hat{\sigma}^2$ is the estimated of $\sigma^2$, the variance of the error term obtained from the full model with all variables($k$). It is a measure of the training error plus a biais correction. The first term measures the fit of the model to the data, while the second term measures the complexity of the model. More the model is complex, more the second term will be large and the Mallow's $C_p$ statistic will be large. The goal is to find the model that minimizes the Mallow's $C_p$ statistic.
The Mallow's $C_p$ statistic is seen as a trade-off between the fit of the model and its complexity. Thus find a good model involves trading off fit and complexity.


## Likelihood and penalization

The approach below to estimate the prediction risk is based on the maximum likelihood estimation of the parameters. 
In the hypothesis that the error term is normally distributed, the likelihood function is given by:
$$
\begin{align*}
\mathcal{l}(Y, \beta, \sigma^2)
&= \log l(Y, \beta, \sigma^2) \\
&= -\frac{n}{2} \log(2\pi) - \frac{1}{2\sigma^2} \sum_{i=1}^{n} (Y_i - X_i\beta)^2 - \frac{n}{2} \log(\sigma^2).
\end{align*}
$$

If you compute the maximum likelihood estimate of the parameters $\beta$ and $\sigma^2$, for the model $S$, that have $|S|$ variables, you will get respectively :
$\hat{\beta(S)_{MV}} = \hat{\beta(S)_{MCO}}$ and $\hat{\sigma(S)^2_{MV}} = \frac{1}{n}\sum_{i=1}^{n} (Y_i - \hat{Y}_i(S))^2$.

The log-likelihood of the model for the model $S$ which has $|S|$ variables is then given by:
$$
\mathcal{l(S)} = -\frac{n}{2}(1 + \log(2\pi)) - \frac{n}{2} \log \frac{\sum_{i=1}^{n} (Y_i - \hat{Y}_i(S))^2}{n}
$$

Choosing the model that maximizes the log-likelihood is equivalent to choosing the model that have the smallest residual sum of squares (RSS), that is:
$$
\hat{R}(S) = \frac{1}{n}\sum_{i=1}^{n} (Y_i - \hat{Y}_i(S))^2
$$

In other to minimize a criterion, we work with the opposite of the log-likelihood and the criteria will be generally defined as:
$$
- 2\mathcal{l}(S) + 2|S| f(n)  
$$

where $f(n)$ is a function of penalization that depends on the sample size $n$. This relation allows us to define the AIC and BIC criteria, which are defined below.

### Akaike Information Criterion (AIC)

Another method for model selection is the Akaike Information Criterion (AIC). The idea behind the AIC is to find a model that minimizes the information loss.  The idea is to choose S to minimize the AIC criterion, which is defined as:
$$
\text{AIC}(S) =  - 2\mathbfcal{l}_S + 2|S|
$$
where $\mathbfcal{l}_S$ is the log-likelihood of the model $S$ evaluated at the maximum likelihood estimates of the parameters. Here $f(n) = 2$.

This can be thought of goodness of fit plus a complexity. When we want to choose two models, we will prefer the one with the lower AIC. 


### Bayesian Information Criterion (BIC)

The Bayesian Information Criterion (BIC) is another method for model selection. It is similar to the AIC, and BIC is defined as:
$$
\text{BIC}(S) = -2\mathbfcal{l}_S + 2|S| \frac{1}{2} \log(n)
$$

where $\mathbfcal{l}_S$ is the log-likelihood of the model $S$ evaluated at the maximum likelihood estimates of the parameters. 
We call it Bayesian Information Criterion because it can be derived from a Bayesian perspective. In fact let $S = \{S_1, \ldots, S_m\}$ denoted a set of models. When we assigns a prior probability to each model $S_i$ as $\pi(S_i) = \frac{1}{m}$, the posterior probability of the model $S_i$ given the data is proportional to the likelihood of the model $S_i$ given the data, that is:
$$
\pi(S_i | \text{data}) \propto \frac{e^{-\frac{1}{2} \text{BIC}(S_i)}}{\sum_{j=1}^{m} e^{-\frac{1}{2} \text{BIC}(S_j)}}
$$

Hence, choosing the model that minimizes the BIC is equivalent to choosing the model with the highest posterior probability given the data. It also has a interpretation in terms of description length. It puts a more severe penalty for complexity than the AIC, which is why it is often preferred when the sample size is large. In fact, by definition, $f(n) = \frac{1}{2} \log(n)$. Ainsi, more the sample size $n$ is large, more the penalty is lower when we compare with the penalty of the AIC. However, this penalty is generally greater than 2 (When $n > 7$), donc the BIC tends to select smaller models than the AIC. The use of this criterion is similar to the use of the AIC, so when we want to choose two models, we will prefer the one with the lower BIC. 

## Leave-One-Out Cross-Validation (LOOCV) and k-Fold Cross-Validation

Yet another method for model selection is leave-one-out cross-validation (LOOCV). In this case, the risk estimator is given by:

$$
\hat{R}_{LOOCV}(S) = \sum_{i=1}^{n} \left(\hat{Y}_{-i}(S) - Y_{(i)}\right)^2
$$

Where \hat{Y}_{-i}(S) is the prediction for $Y_i$ using the model $S$ fitted on all data except the $i$-th observation, and $Y_{(i)}$ is the $i$-th observation of the response variable. It can be shown that 
$$
\hat{R}_{LOOCV}(S) = \sum_{i=1}^{n} (\frac{\hat{Y}_{i}(S) - Y_{i}}{1 - h_{ii}(S)})^2
$$
where $h_{ii}(S)$ is  the $i$-th diagonal element of the hat matrix $H_S = X_S(X_S^TX_S)^{-1}X_S^T$ for the model $S$.

Thus, one need not actually drop each observation and refit the model. A generalization of LOOCV is k-fold cross-validation. 

## K-Fold Cross-Validation


In this approach, the data is divided into $k$ groups, or *folds* (commonly $k = 5$ or $k = 10$). One fold is omitted, and the model is fitted using the remaining $k-1$ folds. The fitted model is then used to predict the responses for the omitted fold. The risk is estimated as

$$
\sum_i \left( Y_i - \hat{Y}_i \right)^2,
$$

where the sum is taken over the data points in the omitted fold. This process is repeated for each of the $k$ folds, and the average of the $k$ risk estimates is taken as the overall risk estimate.

This method is particularly appropriate when the primary objective of the regression is **prediction**. In this context, alternative performance measures such as the **Mean Absolute Error (MAE)** or the **Root Mean Squared Error (RMSE)** can also be used to evaluate the model’s predictive performance.


## Other criteria


In the literature, in addition to the criteria presented above, there are other measures that can be used for model selection. For example, the adjusted $R^2$ is a widely used criterion, defined as

$$
\text{Adjusted } R_a^2(S) = 1 - \frac{n - 1}{n - |S| - 1} \left( 1 - R^2(S) \right).
$$

Another option is to use nested model tests, such as the **F-test**. The F-test compares two nested models—specifically, a model $S_1$ whose covariates form a subset of those in a larger model $S_2$. The null hypothesis states that the additional variables in $S_2$ do not improve the fit of the model compared to $S_1$.

The methods presented above primarily address two key objectives of linear regression: **parameter estimation** and **variable selection**.



# Selection Procedure


Once models can be scored, the next step is to search either the entire space of possible models or a selected subset to find the one with the best score. With $k$ covariates, there are $2^k - 1$ possible models—a number that quickly becomes prohibitive for large $k$ (for example, more than one million when $k = 20$). In such cases, exhaustive search is impractical, and heuristic methods are preferred. Broadly, model selection strategies fall into two categories: **exhaustive search** and **stepwise search**.



## Exhaustive Search

This approach evaluates every possible model and selects the one with the best score. Feasible only for small $k$, as computation becomes prohibitive for large numbers of covariates.

## Stepwise Search

The methods presented here aim to identify a *local optimum*—that is, a model that performs better than its neighboring models. Such methods are generally recommended only when exhaustive search is not feasible (for example, when both $n$ and $p$ are large).

### Forward Stepwise Selection

In this approach, we first select a scoring criterion (AIC, BIC, Mallows’ $C_p$, etc.). We start with an empty model and, at each step, add the variable that provides the greatest improvement in the criterion. This process continues until no variable improves the score or all variables are included in the model.

### Backward Stepwise Selection

Here, we also begin by selecting a scoring criterion (AIC, BIC, Mallows’ $C_p$, etc.). We start with the full model containing all variables and, at each step, remove the variable whose removal most improves the criterion. We continue removing variables one at a time until no further improvement is possible or only the essential variables remain.

### Stepwise Selection (Mixed Method)

In the mixed approach, we again begin by selecting a scoring criterion (AIC, BIC, Mallows’ $C_p$, etc.). We start with an empty model and add variables one at a time, as in forward selection, until no additional variable improves the score. Then, we proceed as in backward selection, removing variables one at a time if doing so improves the criterion. This process stops when no further improvement can be made or all variables are included in the model.




# Application

In practice, before use the model selection techniques, we need to ensure that the covariates are not highly correlated. To do that, we will use the technique below.

- The first step is to remove the covariates that are not relevant to the response variable using expert approch, treatment of missing values, etc.

- Next, define a threshold for the correlation between covariates and the response variable. For example, we can set the threshold to 0.6. This will help us to remove the covariates that are not correlated with the response variable. We will not use this technique here to let enough covariates to selection techniques.

- Next, define a threshold for the correlation between covariates. For example, we can set the threshold to 0.7. Compute the correlation matrix of the covariates. If two covariates have a correlation greater than the threshold, we will consider them as highly correlated and return the one with the highest correlation with the response variable or the one with has the most interpretation metier.

- Next, we will compute the variance inflation factor (VIF) for all remaining covariates. And if the VIF of a covariate is greater than 5 or 10, we will consider it as highly correlated with the other covariates and remove it from the model. 

- Finally, we will use the model selection techniques discussed above to select the best model. We will use the Mallow's $C_p$ to score the models and select the one with the best score and backward stepwise selection to select variables. We will implement a stepwise selection procedure that will allow us to select the best model based on all criteria discussed above and the backward or forward stepwise selection strategy.

## Presentation of the dataset


We use the Communities dataset from the UCI Machine Learning Repository, which contains socio-economic and demographic information about communities in the United States. It includes over 100 variables, with the response variable being the number of violent crimes per population (`violentCrimesPerPop`). We apply the model selection techniques discussed above to identify the variables most strongly associated with the response.



## Handling Missing Values

In this analysis, we remove all rows containing missing values. An alternative approach is to drop variables with a high proportion of missing values (e.g., more than 10%) and then determine whether the remaining missing values are Missing At Random (MAR), applying an appropriate imputation method if needed. Here, however, we remove all incomplete rows. After this step, the dataset contains no missing values and includes 103 variables, comprising the response variable `violentCrimesPerPop` and the covariates.
## Selection of Relevant Variables Using Expert Judgment

We apply expert knowledge to assess the relevance of each variable and determine whether its correlation with the response variable is meaningful. This involves consultation between the statistician and domain experts to understand the context and importance of each variable. For this dataset, we remove `communityname` for simplicity, as it is a categorical variable with many levels, and `fold`, which is used only for cross-validation. This leaves 101 variables, including the response variable `violentCrimesPerPop` and the covariates.

## Reducing Covariates Using a Correlation Threshold of 0.6

To further reduce the number of covariates, we compute the correlation matrix for all covariates and the response variable. When multiple covariates are highly correlated with each other (correlation greater than 0.6), we retain only the one with the highest correlation to the response. This approach reduces the number of covariates while mitigating multicollinearity.

After applying this and the previous steps, the dataset is reduced to 19 covariates with the VIF below 5. These steps are documented in detail in my article [Feature Selection](https://juniorjumbong.github.io/personal-website/00_tds/feature_selection.html).

Table: Variance Inflation Factors (compact)

| Variable              | VIF      |
|:----------------------|---------:|
| pctWFarmSelf          | 1.329818 |
| indianPerCap          | 1.067939 |
| AsianPerCap           | 1.221415 |
| PctEmplManu           | 1.700146 |
| PctEmplProfServ       | 1.734482 |
| PctKids2Par           | 2.442528 |
| PctWorkMom            | 1.232448 |
| PctImmigRec10         | 1.668404 |
| PctVacantBoarded      | 1.828584 |
| PctVacMore6Mos        | 1.816510 |
| MedYrHousBuilt        | 2.587958 |
| PctWOFullPlumb        | 1.475350 |
| MedRentPctHousInc     | 1.462474 |
| MedOwnCostPctIncNoMtg | 1.452386 |
| PctSameHouse85        | 2.382161 |
| LandArea              | 1.289480 |
| PopDens               | 2.360121 |
| PctUsePubTrans        | 2.127730 |
| LemasPctOfficDrugUn   | 1.308731 |



## Model Selection with Backward Stepwise Selection

With 19 variables, the total number of possible models is $524,288$, which may be computationally infeasible for some systems. To reduce the number of models to evaluate, we use a stepwise selection procedure. We implement a function, `stepwise_selection`, that selects the most relevant variables based on a chosen selection criterion and method (forward, backward, or mixed). In this example, we use Mallows’ $C_p$ as the selection criterion and apply both forward and backward stepwise selection methods. 

### Result of the backward stepwise selection using Mallow's $C_p$ criterion

Using Mallows’ $C_p$ criterion within the backward stepwise selection procedure, we first remove the variable `pctWFarmSelf` because its exclusion reduces the criterion to $C_p = 41.74$, which is lower than that of the full model. Next, we remove `PctWOFullPlumb`, whose removal decreases $C_p$ to $41.69895$. Finally, we remove `indianPerCap`, resulting in a further reduction to $C_p = 41.66073$. In total, three variables are eliminated from the model.

### Result of the forward stepwise selection using Mallow's $C_p$ criterion

This method is generally recommended for model selection when the number of variables is large. Because forward stepwise selection starts with an empty model and adds variables one at a time, it is less computationally intensive than backward stepwise selection. Using this approach, we select exactly the same variables as in the backward stepwise selection presented above. The graphic below shows the variables that are successively added to the model, along with the corresponding Mallow's $C_p$ values. The first variable added is `PctKids2Par`, followed by `PctWorkMom`, `LandArea`, and so on, until the final model is reached with a $C_p$ value of $41.66073$.

```{python}
# Lire les noms de colonnes à partir du fichier .names
with open("communities.names", "r") as f:
    lines = f.readlines()

# Extraire les noms à partir des lignes définissant les attributs
columns = []
start_extracting = False
for line in lines:
    if line.startswith("@attribute"):
        parts = line.split()
        if len(parts) > 1:
            columns.append(parts[1])
    elif "attribute information:" in line.lower():
        start_extracting = True
    elif start_extracting and line.strip() and not line.startswith('@'):
        # handle lines like: "1. state: continuous"
        if ':' in line:
            name = line.split(':')[0].strip().split()[-1]
            columns.append(name)

# Supprimer les doublons tout en conservant l'ordre
from collections import OrderedDict
columns = list(OrderedDict.fromkeys(columns))

# Vérifie que ça correspond au nombre de colonnes du fichier data
print(f"{len(columns)} colonnes uniques")

```

```{python}
import pandas as pd

# Charger le fichier communities.data
df = pd.read_csv("communities.data", header=None, names=columns, na_values='?')

# Vérifier la forme
print(df.shape)
df.head()
```


```{python}
df['fold'].value_counts()  # Voir combien de points dans chaque fold

```


```{python}
# Liste des colonnes sans aucune valeur manquante
cols_without_na = df.columns[df.notna().all()].tolist()

print(f"{len(cols_without_na)} colonnes sans NA :")

print(cols_without_na)
df_clean = df.dropna(axis=1)
print(len(df_clean.columns))
df_clean.to_csv("communities_clean.csv", index=False)
```

```{python}
# Extract response variable and covariates
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

df = pd.read_csv("communities_clean.csv")
response = 'ViolentCrimesPerPop'
covariates = [col for col in df.columns if col not in ['fold', response, 'state', 'county', 'community', 'communityname', 'NumStreet']]


spearman_corr = df[covariates + [response]].corr(method='spearman')
plt.figure(figsize=(12, 10))
sns.heatmap(spearman_corr, annot=True, cmap="coolwarm", fmt=".2f", linewidths=0.5)
plt.title("Correlation Matrix Heatmap")
plt.show()
```


```{python}
# Step 2: Correlation of each variable with response R
spearman_corr_with_R = spearman_corr[response].drop(response)  # exclude R-R

# Step 3: Identify pairs of covariates with strong inter-correlation (e.g., > 0.9)
strong_pairs = []
threshold = 0.6
covariates = spearman_corr_with_R.index

for i, var1 in enumerate(covariates):
    for var2 in covariates[i+1:]:
        if abs(spearman_corr.loc[var1, var2]) > threshold:
            strong_pairs.append((var1, var2))

# Step 4: From each correlated pair, keep only the variable most correlated with R
to_keep = set()
to_discard = set()

for var1, var2 in strong_pairs:
    if abs(spearman_corr_with_R[var1]) >= abs(spearman_corr_with_R[var2]):
        to_keep.add(var1)
        to_discard.add(var2)
    else:
        to_keep.add(var2)
        to_discard.add(var1)

# Final selection: all covariates excluding the ones to discard due to redundancy
final_selected_variables = [var for var in covariates if var not in to_discard]

final_selected_variables
print(f"Number of selected variables: {len(final_selected_variables)}")
```



```{python}
from statsmodels.stats.outliers_influence import variance_inflation_factor
from statsmodels.tools.tools import add_constant
from sklearn.preprocessing import StandardScaler
import numpy as np

X = df[final_selected_variables]  

X_with_const = add_constant(X)  

vif_data = pd.DataFrame()
vif_data["variable"] = X_with_const.columns
vif_data["VIF"] = [variance_inflation_factor(X_with_const.values, i)
                   for i in range(X_with_const.shape[1])]

vif_data = vif_data[vif_data["variable"] != "const"]

print(vif_data)

```

```{python}
import pandas as pd
import numpy as np
import statsmodels.api as sm
import matplotlib.pyplot as plt
from collections import OrderedDict

def stepwise_selection(df, target, strategy='forward', metric='AIC', verbose=True):
    """
    Variable selection using stepwise regression (forward or backward) and AIC/BIC criterion.
    
    Parameters
    ----------
    df : pandas.DataFrame
        DataFrame containing the predictors and the target column.
    target : str
        Name of the target column.
    strategy : str
        'forward' or 'backward'.
    metric : str
        'AIC' or 'BIC'.
    verbose : bool
        Whether to print the steps.
    
    Returns
    -------
    selected_vars : list
        List of variables selected in the final model.
    best_model : statsmodels.regression.linear_model.RegressionResultsWrapper
        Fitted OLS model with selected variables.
    history : list of dict
        Steps taken (variables, metric).
    """
    # Vérification des NaN
    if df.isnull().values.any():
        raise ValueError("Des valeurs manquantes sont présentes dans le DataFrame. Veuillez les gérer avant d'appliquer la sélection.")
    
    X = df.drop(columns=[target])
    y = df[target]
    variables = list(X.columns)
    history = []
    
    # Initialisation
    if strategy == 'forward':
        selected = []
        remaining = variables.copy()
    elif strategy == 'backward':
        selected = variables.copy()
        remaining = []
    else:
        raise ValueError("strategy must be 'forward' or 'backward'")
    
    current_score = np.inf
    best_new_score = np.inf
    
    step = 0
    while True:
        step += 1
        scores_with_candidates = []
        
        if strategy == 'forward':
            # Tester chaque variable qui n'est pas dans 'selected'
            candidates = [var for var in remaining if var not in selected]
            for candidate in candidates:
                vars_to_test = selected + [candidate]
                X_train = sm.add_constant(X[vars_to_test])
                model = sm.OLS(y, X_train).fit()
                if metric == 'AIC':
                    score = model.aic
                elif metric == 'BIC':
                    score = model.bic
                scores_with_candidates.append((score, candidate, vars_to_test))
            if not scores_with_candidates:
                break
            # Chercher la variable qui améliore le plus la métrique
            scores_with_candidates.sort()
            best_new_score, best_candidate, vars_to_test = scores_with_candidates[0]
            
            if verbose:
                print(f"\nÉtape {step}:")
                print("Candidats testés:", [(v, round(s, 2)) for s, v, _ in scores_with_candidates])
                print(f"Meilleure variable à ajouter : {best_candidate} (score={round(best_new_score,2)})")
            
            # Critère d'arrêt
            if len(selected) == 0 or best_new_score < current_score - 1e-6:
                selected.append(best_candidate)
                remaining.remove(best_candidate)
                current_score = best_new_score
                history.append({
                    'step': step,
                    'selected': selected.copy(),
                    'score': current_score,
                    'modified': best_candidate
                })
            else:
                if verbose:
                    print("Plus aucune variable n'améliore le score.")
                break
        
        elif strategy == 'backward':
            # Tester la suppression de chaque variable restante
            if len(selected) == 0:
                break
            scores_with_candidates = []
            for candidate in selected:
                vars_to_test = [var for var in selected if var != candidate]
                if len(vars_to_test) == 0:
                    continue
                X_train = sm.add_constant(X[vars_to_test])
                model = sm.OLS(y, X_train).fit()
                if metric == 'AIC':
                    score = model.aic
                elif metric == 'BIC':
                    score = model.bic
                scores_with_candidates.append((score, candidate, vars_to_test))
            if not scores_with_candidates:
                break
            # Chercher la variable dont la suppression améliore le plus la métrique
            scores_with_candidates.sort()
            best_new_score, worst_candidate, vars_to_test = scores_with_candidates[0]
            
            if verbose:
                print(f"\nÉtape {step}:")
                print("Suppressions testées:", [(v, round(s, 2)) for s, v, _ in scores_with_candidates])
                print(f"Meilleure variable à retirer : {worst_candidate} (score={round(best_new_score,2)})")
            
            # Critère d'arrêt
            if best_new_score < current_score - 1e-6:
                selected.remove(worst_candidate)
                current_score = best_new_score
                history.append({
                    'step': step,
                    'selected': selected.copy(),
                    'score': current_score,
                    'modified': worst_candidate
                })
            else:
                if verbose:
                    print("Aucune suppression n'améliore le score.")
                break
    
    # Fit final model
    X_final = sm.add_constant(X[selected])
    best_model = sm.OLS(y, X_final).fit()
    if verbose:
        print("\nVariables sélectionnées :", selected)
        print(f"Score final ({metric}): {round(best_model.aic if metric=='AIC' else best_model.bic,2)}")
    
    
    return selected, best_model, history

# Graphique de l'historique des scores

```


```{python}
import matplotlib.pyplot as plt

import matplotlib.pyplot as plt

def plot_stepwise_crosses(history, all_vars, metric="AIC", title=None):
    """
    Affiche le graphique stepwise type heatmap à croix :
    - X : variables explicatives modifiées à au moins une étape (ordre d'apparition)
    - Y : score (AIC/BIC) à chaque étape (de l'historique)
    - Croix noire : variable modifiée à chaque étape
    - Vide ailleurs
    - Courbe du score
    """
    n_steps = len(history)
    scores = [h['score'] for h in history]
    
    # Extraire la liste ordonnée des variables effectivement modifiées
    modified_vars = []
    for h in history:
        var = h['modified']
        if var not in modified_vars and var is not None:
            modified_vars.append(var)
    
    n_mod_vars = len(modified_vars)
    
    # Construction des positions X pour les croix (selon modified_vars)
    mod_pos = [modified_vars.index(h['modified']) if h['modified'] in modified_vars else None for h in history]

    fig, ax = plt.subplots(figsize=(min(1.3 * n_mod_vars, 14), 6))
    # Placer la croix noire à chaque étape
    for i, x in enumerate(mod_pos):
        if x is not None:
            ax.scatter(x, scores[i], color='black', marker='x', s=100, zorder=3)
    # Tracer la courbe du score
    ax.plot(range(n_steps), scores, color='gray', alpha=0.7, linewidth=2, zorder=1)
    # Axe X : labels verticaux, police réduite (uniquement variables modifiées)
    ax.set_xticks(range(n_mod_vars))
    ax.set_xticklabels(modified_vars, rotation=90, fontsize=10)
    ax.set_xlabel("Variables modifiées")
    ax.set_ylabel(metric)
    ax.set_title(title or f"Stepwise ({metric}) – Variables modifiées à chaque étape")
    ax.grid(True, axis='y', alpha=0.2)
    plt.tight_layout()
    plt.show()


```

```{python}

df_for_stepwise = df[final_selected_variables + [response]].dropna()



```

```{python}
selected_vars, best_model, history = stepwise_selection(
    df=df_for_stepwise,
    target='ViolentCrimesPerPop',
    strategy='backward',    # Ou 'backward'
    metric='AIC',          # Ou 'BIC'
    verbose=True
)
print(best_model.summary())

```

```{python}
all_vars = [col for col in df.columns if col != 'target']
plot_stepwise_crosses(history, all_vars, metric="AIC", title="Sélection de variables – Stepwise")

```


```{python}
import numpy as np
import statsmodels.api as sm

def compute_score(y, X, vars_to_test, metric, full_model_mse=None):
    X_train = sm.add_constant(X[vars_to_test])
    model = sm.OLS(y, X_train).fit()
    n = len(y)
    p = len(vars_to_test) + 1  # +1 pour la constante

    if metric == 'AIC':
        return model.aic

    elif metric == 'BIC':
        return model.bic

    elif metric == 'Cp':
        if full_model_mse is None:
            raise ValueError("full_model_mse doit être fourni pour calculer Cp Mallows.")
        rss = sum(model.resid ** 2)
        return rss + 2 * p * full_model_mse

    elif metric == 'R2_adj':
        return -model.rsquared_adj  # négatif pour maximiser

    else:
        raise ValueError("Métrique inconnue. Utilisez 'AIC', 'BIC', 'Cp' ou 'R2_adj'.")

def get_best_candidate(y, X, selected, candidates, metric, strategy, full_model_mse=None):
    scores_with_candidates = []
    for candidate in candidates:
        vars_to_test = selected + [candidate] if strategy == 'forward' else [var for var in selected if var != candidate]
        score = compute_score(y, X, vars_to_test, metric, full_model_mse)
        scores_with_candidates.append((score, candidate, vars_to_test))

    scores_with_candidates.sort()
    print("Suppressions testées:", [(v, round(s, 2)) for s, v, _ in scores_with_candidates])
    return scores_with_candidates[0] if scores_with_candidates else (None, None, None)

def stepwise_selection(df, target, strategy='forward', metric='AIC', verbose=True):
    if df.isnull().values.any():
        raise ValueError("Des valeurs manquantes sont présentes dans le DataFrame.")

    X = df.drop(columns=[target])
    y = df[target]
    variables = list(X.columns)

    selected = [] if strategy == 'forward' else variables.copy()
    remaining = variables.copy() if strategy == 'forward' else []

    # Calcul préalable du MSE du modèle complet pour Cp Mallows
    if metric == 'Cp':
        X_full = sm.add_constant(X)
        full_model = sm.OLS(y, X_full).fit()
        full_model_mse = sum(full_model.resid ** 2) / (len(y) - len(variables) - 1)
    else:
        full_model_mse = None

    current_score = np.inf
    history = []
    step = 0

    while True:
        step += 1
        candidates = remaining if strategy == 'forward' else selected
        best_score, best_candidate, vars_to_test = get_best_candidate(y, X, selected, candidates, metric, strategy, full_model_mse)

        if best_candidate is None:
            if verbose:
                print("Aucun candidat disponible.")
            break

        if verbose:
            action = "ajouter" if strategy == 'forward' else "retirer"
            print(f"\nÉtape {step}: Meilleure variable à {action} : {best_candidate} (score={round(best_score,5)})")


        improvement = best_score < current_score - 1e-6

        if improvement:
            if strategy == 'forward':
                selected.append(best_candidate)
                remaining.remove(best_candidate)
            else:
                selected.remove(best_candidate)

            current_score = best_score
            history.append({
                'step': step,
                'selected': selected.copy(),
                'score': current_score,
                'modified': best_candidate
            })
        else:
            if verbose:
                print("Aucune amélioration supplémentaire du score.")
            break

    X_final = sm.add_constant(X[selected])
    best_model = sm.OLS(y, X_final).fit()

    if verbose:
        print("\nVariables sélectionnées :", selected)
        final_score = best_model.aic if metric == 'AIC' else best_model.bic
        if metric == 'Cp':
            final_score = compute_score(y, X, selected, metric, full_model_mse)
        elif metric == 'R2_adj':
            final_score = -compute_score(y, X, selected, metric)
        print(f"Score final ({metric}): {round(final_score,5)}")

    return selected, best_model, history

```


```{python}
selected_vars, best_model, history = stepwise_selection(
    df=df_for_stepwise,
    target='ViolentCrimesPerPop',
    strategy='backward',    # Ou 'backward'
    metric='Cp',           # Ou 'AIC', 'BIC', 'R2_adj'
    verbose=True
)
print(best_model.summary())
plot_stepwise_crosses(history, df_for_stepwise.columns.tolist(), metric="Cp", title="Sélection de variables – Stepwise avec Cp Mallows")
```


```{python}
# R2_adj
selected_vars, best_model, history = stepwise_selection(
    df=df_for_stepwise,
    target='ViolentCrimesPerPop',
    strategy='forward',    # Ou 'backward'
    metric='Cp',      # Ou 'AIC', 'BIC', 'Cp'
    verbose=True
)
print(best_model.summary())
plot_stepwise_crosses(history, df_for_stepwise.columns.tolist(), metric="Cp", title="Sélection de variables – Stepwise avec Cp ajusté")
```