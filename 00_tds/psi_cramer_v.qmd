---
title: "Representiveness"
subtitle: ""
date: last-modified
sidebar: auto
number-sections: false
toc: true
author:
  - Jumbong Junior 

categories: []
tags: ["Population Stability Index (PSI)", "Cramér's V"]
title-block-banner: false
bibliography: references.bib
format: 
  html: 
    mainfont: Times New Roman
    fontsize: 1.1em

jupyter: python3
notice: |
    @wasserman2004all 
---

It has happened to me more than once, in both academic and professional projects, to ask myself a question that seems simple on the surface but never really is in practice: *"Are the data I'm using truly representative of what my model will face in the real world?"*

I first ran into this challenge while working on a credit model. Like many practitioners, I had learned to build predictive models by splitting data into training, test, and sometimes out-of-sample sets. But I quickly discovered that the real challenge wasn't just achieving good training performance; it was ensuring those results would hold when the model encountered a different database, a different scope, or even a different customer population.

In reality, **comparing two distributions** isn't an academic or theoretical exercise: it's a critical concern throughout a credit model's entire lifecycle.

* During design, it helps verify that the training sample represents the broader database.
* In production, it serves to monitor population drift over time.
* During regulatory validation, it provides essential evidence for demonstrating model robustness and generalizability.

To address this need, credit risk practitioners have several tools at their disposal. Some are visual and offer intuitive insights (histograms, boxplots, density plots). Others are more statistical, allowing us to quantify differences and make the analysis more objective. Among these, two indicators prove particularly useful and complementary:

* The **Population Stability Index (PSI)**, commonly used to measure drift between two distributions,
* And **Cramér's V**, which quantifies the strength of association between two categorical variables and helps evaluate whether compared populations share the same underlying structure.

This article centers on these two indicators. My goal is twofold: first, to demonstrate **how and why** comparing distributions is essential in credit scoring; second, to illustrate through **practical examples** how PSI and Cramér's V can help answer questions about data representativeness across different scopes.

## Article Outline

1. **Comparing distributions: a key challenge throughout a credit model's lifecycle** (illustrating when this comparison becomes crucial)
2. **Practical examples**: when distribution comparison raises questions about representativeness
3. **Defining representativeness and its implications** in credit risk
4. **Evaluating representativeness with statistical tools**: introducing PSI and Cramér's V, with definitions and interpretations
5. **Practical application** using a concrete example to highlight their complementary nature


## I. Comparing Distributions: A Key Challenge Throughout the Lifecycle of a Credit Model

From a statistician’s perspective, the lifecycle of a credit model can be summed up in three main stages. 

**The construction phase**: this is where it all begins. You gather the data, clean it, split it into training, test, and out-of-time samples, estimate the parameters, and carefully document every decision. You ensure that the test and the out-of-time samples are representative of the training data.

**The application phase**: once the model is built, it must be confronted with reality. And here a crucial question arises: do the new datasets truly resemble the ones used during construction? If not, much of the previous work may quickly lose its value.

**The monitoring phase, or backtesting**: over time, populations evolve. The model must therefore be regularly challenged. Do its predictions remain valid? Is the representativeness of the target portfolio still ensured?

At each of these stages, the same recurring question plays like a refrain:
**Is the model still representative of the population it is supposed to score?**


### Practical Illustrations

To make this more concrete, let me share two situations I’ve encountered—or could easily have encountered.

**Case 1: A change in scope**

Imagine a bank developing a scoring model for small businesses. The model performs well and is recognized internally. Encouraged by this success, the leadership decides to extend its use to large corporations. But here every statistician would pause to ask: do the characteristic variables of large corporations follow the same distributions as those of small businesses? If not, the model risks becoming fragile.

**Case 2: A banking merger**

Now consider Bank A, equipped with a proven model to assess client default risk. It merges with Bank B and seeks to harmonize its tools. The challenge: Bank B operates in a different economic environment, with a portfolio of clients that may not share the same structure. Before transferring the model, the distributions of key variables across the two portfolios must be compared. Once again, representativeness is at stake.


## II. Comparing Distributions to Assess Representativeness Between Datasets

These examples show that behind every strategic decision; whether changing scope, merging models, or tracking their evolution over time—there lies the same underlying question: **are the variable distributions sufficiently similar to guarantee the model’s robustness?**

To answer it, statisticians can rely on a wide range of tools. Some are visual and intuitive:

* overlaying histograms to instantly spot divergent densities,
* using boxplots to compare medians and spreads,
* plotting cumulative distribution functions or estimated densities to get a global view.

Others are statistical and more formalized:

* the **Kolmogorov-Smirnov test**, which measures the maximum gap between two cumulative distributions,
* the **Chi-square test**, well suited for categorical variables,
* the **Anderson-Darling test**, particularly sensitive to differences in the tails.

These methods form a solid foundation, and Matteo Courthoud offers a clear and comprehensive overview of them in his article *[How to Compare Two (or More) Distributions](https://towardsdatascience.com/how-to-compare-two-or-more-distributions-9b06ee4d30bf/)*.

But in credit risk, I’ve found that two indicators deserve special attention because they are simple, easy to use, and they address representativeness issues very directly:

* the **Population Stability Index (PSI)**, widely used to detect distribution shifts over time, and
* **Cramér’s V**, which measures the strength of association between two categorical variables and helps assess the coherence between populations.

The next part of this article will focus on these two tools, showing how they complement classical approaches and illustrating their use with a concrete example.


## III. Two Indicators to Assess Representativeness: PSI and Cramér’s V

As I worked on comparing distributions, I quickly realized that classical methods—looking at cumulative distribution functions or densities—were not always enough. These approaches are valuable to statisticians, but in an operational setting they have two major limitations: they can be difficult to interpret quickly, and they are not easy to communicate to decision-makers who lack statistical training.

This is precisely where two indicators prove their worth: the **Population Stability Index (PSI)** and **Cramér’s V**. Both are simple to calculate, easy to interpret, and—most importantly—translate statistical differences into clear, actionable insights for risk and portfolio management teams.

---

### 1. The Population Stability Index (PSI)

The PSI has become a cornerstone in the credit industry. It measures the difference between two distributions of the same variable:

* for example, between the training dataset and a more recent application dataset,
* or between a reference dataset at time \$T\_0\$ and another at time \$T\_1\$.

In other words, the **PSI quantifies how much a population has drifted over time or across different scopes**.

Here’s how it works in practice:

* For a **categorical variable**, we compute the proportion of observations in each category for both datasets.
* For a **continuous variable**, we first **discretize it into bins**. In practice, deciles are often used to obtain a balanced distribution.

The PSI then compares, bin by bin, the proportions observed in the reference dataset versus the target dataset. The final indicator aggregates these differences using a logarithmic formula:

$$
PSI = \sum_{i=1}^{k} (p_i - q_i) \cdot \ln\!\left(\frac{p_i}{q_i}\right)
$$

where \$p\_i\$ and \$q\_i\$ represent the proportions in bin \$i\$ for the reference dataset and the target dataset, respectively.

The interpretation is highly intuitive:

* A smaller PSI means the two distributions are closer.
* A PSI of **0** means the distributions are identical.
* A very large PSI (tending toward infinity) means the two distributions are fundamentally different.

In practice, industry guidelines often use the following thresholds:

* **PSI < 0.1**: the population is stable,
* **0.1 ≤ PSI < 0.25**: the shift is noticeable—monitor closely,
* **PSI ≥ 0.25**: the shift is significant—the model may no longer be reliable.

## 2. Cramér’s V

When assessing the representativeness of a categorical variable (or a discretized continuous variable) between two datasets, a natural starting point is the **Chi-square test of independence**.

We build a contingency table crossing:

* the categories (modalities) of the variable of interest, and
* an indicator variable for dataset membership (Dataset 1 / Dataset 2).

The test is based on the following statistic:

$$
\chi^2 \;=\; \sum_{i=1}^{r}\sum_{j=1}^{c} \frac{(O_{ij} - E_{ij})^2}{E_{ij}}
$$

where \$O\_{ij}\$ are the observed counts and \$E\_{ij}\$ are the expected counts under the assumption of independence.

* **Null hypothesis \$H\_0\$**: the variable has the same distribution in both datasets (independence).
* **Alternative hypothesis \$H\_1\$**: the distributions differ.

If \$H\_0\$ is rejected, we conclude that the variable does not follow the same distribution across the two datasets.

However, the Chi-square test has a major limitation: it only provides a binary answer (reject / do not reject), and its power is highly sensitive to sample size. With very large datasets, even tiny differences can appear statistically significant.

To address this limitation, we use **Cramér’s V**, which rescales the Chi-square statistic to produce a normalized measure of association bounded between 0 and 1:

$$
V = \sqrt{\frac{\chi^2}{n \cdot \min(r-1,\,c-1)}}
$$

where \$n\$ is the total sample size, \$r\$ is the number of rows, and \$c\$ is the number of columns in the contingency table.

The interpretation is intuitive:

* \$V \approx 0\$ → the distributions are very similar; representativeness is strong.
* \$V\$ close to 1 → the difference between distributions is large; the datasets are structurally different.

Unlike the Chi-square test, which simply answers “yes” or “no,” Cramér’s V provides a graded measure of the strength of the difference. This allows us to assess whether the difference is negligible, moderate, or substantial.

