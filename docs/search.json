[
  {
    "objectID": "Meduim/RegressionLogistique/bias_sample.html",
    "href": "Meduim/RegressionLogistique/bias_sample.html",
    "title": "Sampling Bias and Class Imbalance in Maximum-likelihood Logistic Regression",
    "section": "",
    "text": "In this article, the impact of sampling bias (sample dataset distribution different from the population distribution) and class imbalance on logistic regression models is explored. We hypothesize that the predictive performance of a logistic regression model is related to the sampling bias associated with the data and it has a performance advantage when the data is balanced. The hypothesis is testing with two simulated datasets : a balanced dataset (50:50) and an imbalanced dataset (80:20). Each dataset will be sampled to produce samples with the following distribution : 50:50, 60:40, 70:30, 80:20, 90:10, 95:5, 99:1.\nThe performance of the logistic regression model will be evaluated using the Area Under the Curve (AUC), Area Under the Precision-Recall Curve (AU-PCR), Precision, Recall, and F1-score.\nMonte-Carlo simulations will be carried out to evaluate the distribution of the performance metrics for each of the samples and insure the robustness of the results.\nThis study gives three main results :\nA. The predicted probability using a maximum logistic regression (MLLR) model is closest to the true probability when the sample has the same class distribution as the original population. Therefore, in probabilistic modeling with MLLR, it is important to create a sample that matches the class distribution of the original population rather than ensuring equal class sampling, see Figure 3 and Figure 6.\nB. AUC measures how well probabilistic classifiers predict. It ranges from 0.5 (random) to 1 (perfect). AUC shows class separability regardless of class imbalance or sampling bias, see Figure 5 and Figure 8.\nC. We recommend AUC to evaluate class separability in probabilistic models. To analyse sampling biais as well as the difference in the true and predicted probabilities, AUC-PR, Recall, precision and f1-score can be used as indicator.\nThe protocol of this paper is as follows. First, we describe how to simulate data. Next, we present the methodology. Finally, we present the results."
  },
  {
    "objectID": "Meduim/RegressionLogistique/bias_sample.html#numerical-approach-to-determine-parameters-alpha-and-beta-knowing-the-proportion-of-y1.",
    "href": "Meduim/RegressionLogistique/bias_sample.html#numerical-approach-to-determine-parameters-alpha-and-beta-knowing-the-proportion-of-y1.",
    "title": "Sampling Bias and Class Imbalance in Maximum-likelihood Logistic Regression",
    "section": "1.1 Numerical Approach to determine parameters \\(\\alpha\\) and \\(\\beta\\) knowing the proportion of y=1.",
    "text": "1.1 Numerical Approach to determine parameters \\(\\alpha\\) and \\(\\beta\\) knowing the proportion of y=1.\nThe numerical approach consists to determine, for a given value of \\(\\alpha =-10\\), the value of \\(\\beta\\) that will allow to have a proportion of y=1 equal to 0.5 in the case of a balanced dataset and 0.2 in the case of an imbalanced dataset.\nThe optimization problem can be formulated as follows :\n\\[\n\\min_{\\beta} \\left( \\text{prop} - \\frac{1}{n}\\sum_{i=1}^{n} \\frac{\\exp(\\alpha + \\beta x_i)}{1 + \\exp(\\alpha + \\beta x_i)} \\right)^2\n\\]\nwhere \\(\\text{prop}\\) is the proportion of y=1 in the dataset, \\(x_i\\) is the predictor variable, and \\(n\\) is the number of observations.\nThe optimization problem can be solved using the scipy.optimize.minimize function with the Nelder-Mead method.\n\nimport numpy as np\nfrom scipy.optimize import minimize\n\n\n# Define the logistic function\ndef logistic_function(x, alpha, beta):\n    return 1 / (1 + np.exp(-(alpha + beta * x)))\n\n# Objective function: minimize the squared difference between mean(pi) and 0.2\n\ndef objective(alpha, prop, beta, n):\n    x = np.random.uniform(0, 10, n)  # Simulate x values\n    pi = logistic_function(x, alpha, beta)\n    return (np.mean(pi) - 0.2)**2  # Target mean(pi) = 0.2\n\n# Initial guesses for alpha and beta\n\ninitial_params = [0]\n\n# Optimize alpha and beta\nresult = minimize(lambda params: objective(-10, 0.2, params, 50000), initial_params, method='Nelder-Mead')\n\n# Get optimized alpha and beta\n\nbeta_opt = result.x\nprint(f\"Optimized alpha: {-10}, beta: {beta_opt}\")\n\n# Generate x and simulate y\nx = np.random.uniform(0, 10, 1000)\npi = logistic_function(x, -10, beta_opt)\ny = (np.random.uniform(0, 1, 1000) &lt; pi).astype(int)\n\n# Verify proportions\ny_mean = np.mean(y)\nprint(f\"Proportion of y=1: {y_mean:.2f}, y=0: {1-y_mean:.2f}\")\n\nOptimized alpha: -10, beta: [1.23775]\nProportion of y=1: 0.20, y=0: 0.80"
  },
  {
    "objectID": "Meduim/RegressionLogistique/bias_sample.html#simulated-data-generation-with-alpha--10",
    "href": "Meduim/RegressionLogistique/bias_sample.html#simulated-data-generation-with-alpha--10",
    "title": "Sampling Bias and Class Imbalance in Maximum-likelihood Logistic Regression",
    "section": "Simulated Data Generation with \\(\\alpha = -10\\)",
    "text": "Simulated Data Generation with \\(\\alpha = -10\\)\nLet’s consider two cases :\n\nCase A : A balanced dataset with 50:50 distribution of y=0 and y=1.\nCase B : An imbalanced dataset with 80:20 distribution of y=0 and y=1.\n\nThe code below generates the data for the two cases and plots the proportion of y=1 as a function of beta.\nThe graph Figure 1, both the datasets have a total of 50,000 events, with the Case A dataset having a class distribution of about 50:50 and Case B dataset having a class distribution of about 80:20.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Inputs : n_events, alpha, beta, random_state\n# Outputs : x, y, prop_y0, prop_y1\n# Objective : Simulate data from a logistic model with given alpha, beta.\n\ndef simulate_case_data(n_events, alpha, beta, random_state=42):\n    \"\"\"\n    Simulate data from a logistic model with given alpha, beta.\n    \n    x ~ Uniform(0, 10), y ~ Bernoulli(pi(x)), \n    where pi(x) = exp(alpha + beta*x) / (1 + exp(alpha + beta*x)).\n\n    Parameters\n    ----------\n    n_events : int\n        Number of observations (events) to generate.\n    alpha : float\n        Intercept (alpha) for the logistic function.\n    beta : float\n        Slope (beta) for the logistic function.\n    random_state : int\n        Seed for reproducibility.\n\n    Returns\n    -------\n    x : np.ndarray of shape (n_events,)\n        Predictor values sampled from Uniform(0,10).\n    y : np.ndarray of shape (n_events,)\n        Binary outcomes (0 or 1) from Bernoulli trials.\n    prop_y0 : float\n        Proportion of y==0 in the dataset.\n    prop_y1 : float\n        Proportion of y==1 in the dataset.\n    \"\"\"\n    np.random.seed(random_state)\n    \n    # 1) Draw x from Uniform(0,10)\n    x = np.random.uniform(0, 10, size=n_events)\n    \n    # 2) Compute pi(x, alpha, beta)\n    logit = alpha + beta*x\n    pi = np.exp(logit) / (1.0 + np.exp(logit))\n    \n    # 3) Generate y via Bernoulli(pi)\n    dummy = np.random.rand(n_events)\n    y = (dummy &lt; pi).astype(int)\n    \n    # 4) Calculate proportions of 0 and 1\n    prop_y0 = np.mean(y == 0)\n    prop_y1 = np.mean(y == 1)\n    \n    return x, y, prop_y0, prop_y1\n\n# ---------------- Example usage ----------------\n\n\n# Case A: alpha=-10, beta=2 --&gt; expected ~50:50 distribution\nxA, yA, p0_A, p1_A = simulate_case_data(\n    n_events=50000,\n    alpha=-10,\n    beta=2,\n    random_state=42\n)\n\n# Case B: alpha=-10, beta=3.85 --&gt; expected ~80:20 distribution\nxB, yB, p0_B, p1_B = simulate_case_data(\n    n_events=50000,\n    alpha=-10,\n    beta=beta_opt,\n    random_state=42\n)\n\n# Verify proportions\n\n# Suppose p0_A, p1_A, p0_B, p1_B are already defined\n# e.g., p0_A = 0.50; p1_A = 0.50; p0_B = 0.80; p1_B = 0.20\n\nfig, axes = plt.subplots(1, 2, figsize=(5, 3))  # 1 ligne, 2 colonnes\n\ncolors = ['royalblue', 'darkorange']  # Couleurs distinctes pour y=0 et y=1\n\n# -------- LEFT SUBPLOT: Case A -----------\nax1 = axes[0]\nbar_container_A = ax1.bar(['y=0', 'y=1'], [p0_A, p1_A], color=colors)\nax1.set_title('Case A')\nax1.set_xlabel('Classe')\nax1.set_ylabel('Proportion')\nax1.set_ylim([0, 1])  # Echelle de 0 à 1\nax1.bar_label(bar_container_A, fmt='%.2f')\n\n# -------- RIGHT SUBPLOT: Case B -----------\nax2 = axes[1]\nbar_container_B = ax2.bar(['y=0', 'y=1'], [p0_B, p1_B], color=colors)\nax2.set_title('Case B')\nax2.set_xlabel('Classe')\nax2.set_ylabel('Proportion')\nax2.set_ylim([0, 1])\nax2.bar_label(bar_container_B, fmt='%.2f')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\nFigure 1: Simulated data from logistic model with alpha=-10, beta=2 and alpha=-10, beta=beta_opt\n\n\n\n\n\nThe theoretical properties of the simulated datasets are presented in Figure 2. The left subplot shows the probability of y=1 as a function of x for Case A and Case B. The right subplot shows the logit function as a function of x for Case A and Case B. The logit function is given by \\(\\alpha + \\beta x\\).\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef logistic(x, alpha, beta):\n    return np.exp(alpha + beta*x) / (1.0 + np.exp(alpha + beta*x))\n\nx_values = np.linspace(0, 10, 50000)\n\n# Case A\nalpha_A, beta_A = -10, 2\npi_A = logistic(x_values, alpha_A, beta_A)\nlogit_A = alpha_A + beta_A * x_values\n\n# Case B\nalpha_B, beta_B = -10, beta_opt\npi_B = logistic(x_values, alpha_B, beta_B)\nlogit_B = alpha_B + beta_B * x_values\n\nfig, axes = plt.subplots(2, 2, figsize=(5, 5))  # Taille un peu plus grande\n\n# (a) Probability vs. x for Case A\naxes[0, 0].plot(x_values, pi_A, color='b', label='Probability')\naxes[0, 0].set_title('Case A: Probability vs. x')\naxes[0, 0].set_xlabel('x')\naxes[0, 0].set_ylabel(r'$\\pi(x,\\alpha,\\beta)$')\naxes[0, 0].axhline(y=0.5, color='k', linestyle='--', label='y=0.5')\naxes[0, 0].axvline(x=5, color='gray', linestyle='--', label='x=5')\naxes[0, 0].set_ylim([0,1])  # Probabilité entre 0 et 1\naxes[0, 0].legend(loc='best')\n\n# (b) Logit vs. x for Case A\naxes[0, 1].plot(x_values, logit_A, color='b', label='Logit')\naxes[0, 1].set_title('Case A: Logit vs. x')\naxes[0, 1].set_xlabel('x')\naxes[0, 1].set_ylabel(r'$\\alpha + \\beta x$')\naxes[0, 1].legend(loc='best')\n\n# (c) Probability vs. x for Case B\naxes[1, 0].plot(x_values, pi_B, color='r', label='Probability')\naxes[1, 0].set_title('Case B: Probability vs. x')\naxes[1, 0].set_xlabel('x')\naxes[1, 0].set_ylabel(r'$\\pi(x,\\alpha,\\beta)$')\naxes[1, 0].axhline(y=0.2, color='k', linestyle='--', label='y=0.2')\naxes[1, 0].axvline(x=5, color='gray', linestyle='--', label='x=5')\naxes[1, 0].set_ylim([0,1])\naxes[1, 0].legend(loc='best')\n\n# (d) Logit vs. x for Case B\naxes[1, 1].plot(x_values, logit_B, color='r', label='Logit')\naxes[1, 1].set_title('Case B: Logit vs. x')\naxes[1, 1].set_xlabel('x')\naxes[1, 1].set_ylabel(r'$\\alpha + \\beta x$')\naxes[1, 1].legend(loc='best')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\nFigure 2: Logit vs. x for Case A and Case B"
  },
  {
    "objectID": "Meduim/RegressionLogistique/bias_sample.html#case-a-balanced-dataset",
    "href": "Meduim/RegressionLogistique/bias_sample.html#case-a-balanced-dataset",
    "title": "Sampling Bias and Class Imbalance in Maximum-likelihood Logistic Regression",
    "section": "Case A : Balanced Dataset",
    "text": "Case A : Balanced Dataset\nThe Case A dataset has a balanced class distribution of 50:50. This will be the true distribution of the dataset. Eight random samples will be extracted from the Case A population with varying class distributions of 50:50, 60:40, 70:30, 80:20, 90:10, 95:5, and 99:1. A sample with a class distributiion of 60:40 from the Case A is referred as \\(A_{60:40}\\). The sample size for each of the eight samples is determined by fixing the lenght of the majority class (class 0) at 5000. In order word, for the \\(A_{60:40}\\) sample, the number of observations in class 0 is 5000 and the number of observations in class 1 is :\n\\[\n\\text{Number of observations in class 1} = \\frac{40}{60} \\times 5000 = 3333\n\\]\n\nEight sub-samples generated from Case A.\nThe code below generates the eight sub-samples from the Case A dataset with varying class distributions.\n\ndef create_subsample_fixed_majority(\n    X, y, \n    fraction_class0=0.6,  # e.g., 0.6 =&gt; 60:40\n    majority_class0_size=5000,\n    random_state=42\n):\n    \"\"\"\n    Extract a subsample where the number of class-0 = majority_class0_size,\n    and overall fraction of class-0 is fraction_class0.\n    \n    Returns X_sub, y_sub.\n    \"\"\"\n    np.random.seed(random_state)\n    \n    # Indices of class 0 and 1 in the population\n    idx_0 = np.where(y == 0)[0]\n    idx_1 = np.where(y == 1)[0]\n    \n    # We fix #class0 = 5000\n    n0 = majority_class0_size\n    \n    # fraction_class0 = n0 / (n0 + n1) =&gt; n1 = n0 * (1 - p)/p\n    p = fraction_class0\n    n1 = int(round(n0 * (1 - p) / p))\n    \n    chosen_0 = np.random.choice(idx_0, size=n0, replace=False)\n    chosen_1 = np.random.choice(idx_1, size=n1, replace=False)\n    \n    chosen_indices = np.concatenate([chosen_0, chosen_1])\n    np.random.shuffle(chosen_indices)\n    \n    return X[chosen_indices], y[chosen_indices]\n\nThe code below gives examples of generating the eight sub-samples from the Case A dataset with varying class distributions. Figure 3 shows the distribution of the dependent variable y for each of the eight sub-samples.\n\n# Module : Generation\n# Inputs : fraction_class0 = [0.5, 0.6, 0.7, 0.8, 0.9, 0.95, 0.99], create_subsample_fixed_majority.\n# Ourputs : A dictionnarie with keys in fraction_class0 and values a tuple (X_sub, y_sub).\n\nfractions_class0 = [0.50, 0.60, 0.70, 0.80, 0.90, 0.95, 0.98, 0.99]\n\nsamples_A = {}\nfor frac0 in fractions_class0:\n    X_sub, y_sub = create_subsample_fixed_majority(\n        xA, yA, \n        fraction_class0=frac0, \n        majority_class0_size=5000,\n        random_state=42\n    )\n    # Store or process each sample\n    samples_A[frac0] = (X_sub, y_sub)\n\n\nffig, axes = plt.subplots(4, 2, figsize=(6, 8))\naxes = axes.ravel()  # on a maintenant 8 sous-graphiques\n\n# 1) Déterminer la fréquence max pour fixer une échelle cohérente\nall_counts = [np.bincount(y_sub) for _, (_, y_sub) in samples_A.items()]\nglobal_max_count = max(counts.max() for counts in all_counts)\n\nfor i, (label, (X_sub, y_sub)) in enumerate(samples_A.items()):\n    ax = axes[i]\n    \n    # 2) Histogramme sur 2 bins =&gt; classes 0 et 1\n    ax.hist(y_sub, bins=[-0.5, 0.5, 1.5],  # histogramme \"catégoriel\"\n            color='steelblue', edgecolor='black', alpha=0.7)\n    \n    # 3) Titre + proportion de y=1\n    mean_y = np.mean(y_sub)\n    ax.set_title(f\"{label}:{mean_y:.2f}\", fontsize=9)\n    \n    # 4) Ajuster l’axe X pour forcer l’affichage (0,1)\n    ax.set_xticks([0, 1])\n    ax.set_xticklabels(['0', '1'], fontsize=8)\n    \n    # 5) Limiter l’axe Y pour comparer visuellement entre les sous-figures\n    ax.set_ylim(0, global_max_count)\n    \n    # 6) N’afficher “Frequency” que sur la première colonne\n    #    pour éviter la répétition\n    if i % 2 == 0:\n        ax.set_ylabel('Frequency', fontsize=9)\n    else:\n        ax.set_ylabel('')\n    \n    # 7) Ajouter un label X plus discret\n    ax.set_xlabel('y', fontsize=9)\n    \n    # 8) Afficher le count exact sur chacune des barres\n    counts = np.bincount(y_sub)\n    for j, c in enumerate(counts):\n        ax.text(j, c + 0.5, str(c), ha='center', va='bottom', fontsize=8)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\nFigure 3: Distribution of y for Case A sub-samples\n\n\n\n\n\nA maximum-likelihood logistic regression model will be fitted to each of the eight sub-samples. A plot of the true \\(p(x)\\) versus the estimated \\(p(x)\\) is presented in Figure 4.\n\nimport numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm\n\n# Inputs : samples_A, fractions_class0\n# Outputs : fig\n\n\n\n\n# For illustration, let's create a 2 x 4 grid to show all eight ratio-samples\nfig, axes = plt.subplots(nrows=4, ncols=2, figsize=(6,8))\naxes = axes.ravel()  # flatten into 1D array [ax0, ax1, ..., ax7]\n\n# Hardcode alpha=-10, beta=2 for \"true\" logistic in Case A\nALPHA_TRUE = -10\nBETA_TRUE  = 2\n\nfor i, (label, (X_sub, y_sub)) in enumerate(samples_A.items()):\n    ax = axes[i]\n    \n    # 1) Put the data into a DataFrame for convenience\n    df_sub = pd.DataFrame({\n        'X': X_sub,       # predictor\n        'y': y_sub        # binary outcome\n    })\n    \n    # 2) Add a constant column for the intercept in statsmodels\n    df_sub = sm.add_constant(df_sub, has_constant='add')  \n    # Now df_sub has columns ['const', 'X', 'y']\n    \n    # 3) Fit the logistic model\n    model = sm.Logit(df_sub['y'], df_sub[['const', 'X']])\n    results = model.fit(disp=False)  # disp=False to suppress output\n    \n    # 4) Predict the fitted probability\n    df_sub['pi_pred'] = results.predict(df_sub[['const', 'X']])\n    \n    # 5) Compute the \"true\" pi for comparison\n    df_sub['pi_true'] = logistic(df_sub['X'].values, alpha=ALPHA_TRUE, beta=BETA_TRUE)\n    \n    # 6) Plot pi_true vs. pi_pred\n    ax.scatter(df_sub['pi_true'], df_sub['pi_pred'], \n               alpha=0.3, s=10, color='blue', edgecolors='none')\n    # Add a diagonal line for reference\n    ax.plot([0, 1], [0, 1], color='black', linestyle='--')\n    # 7) Decorate the subplot\n    ax.set_xlabel(\"True pi(x)\")\n    ax.set_ylabel(\"Predicted pi_hat(x)\")\n    ax.set_title(f\"{label}:{1-label:.2f}\")  # e.g., \"Case A60:40\"\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\nFigure 4: True vs. estimated p(x) for Case A sub-samples\n\n\n\n\n\nFigure 4 shows that the sample (Case \\(A_{50:50}\\)) with no class imbalance and sampling bias has the best fit between the true and estimated probabilities. As the class imbalance increases from 60:40 to 99:1, the fit between the true and estimated probabilities deteriorates. To have more confidence in the results, the distribution of the performance metrics for each of the eight sub-samples using monte-carlo simulations will be carried out.\n\n\nDistribution of the performance metrics for the eight sub-samples from Case A with monte-carlo simulations.\nWe perform 1000 monte-carlo simulations for each of the eight sub-samples from Case A. For each simulation, we fit a maximum-likelihood logistic regression model and compute the performance metrics : AUC, AU-PCR, Precision, Recall, F1-score. In order words, in the end of this exercise, we will have 1000 values for each of the performance metrics for each of the eight sub-samples.\nThe code below gives the distribution of the performance metrics for each of the eight sub-samples from Case A with monte-carlo simulations.\n\nfrom sklearn.metrics import (\n    roc_auc_score,\n    precision_score,\n    recall_score,\n    f1_score,\n    average_precision_score\n)\n\ndef evaluate_model_performance(y_true, y_proba, threshold=0.5):\n    \"\"\"\n    Given true labels and predicted probabilities, compute AUC, AU-PRC,\n    Precision, Recall, and F1 at a chosen threshold.\n    \"\"\"\n    # 1) AUC (ROC)\n    auc = roc_auc_score(y_true, y_proba)\n    \n    # 2) AU-PRC (average precision)\n    auprc = average_precision_score(y_true, y_proba)\n    \n    # 3) Convert probas -&gt; hard predictions\n    y_pred = (y_proba &gt;= threshold).astype(int)\n    \n    # 4) Precision, Recall, F1\n    prec = precision_score(y_true, y_pred, zero_division=0)\n    rec  = recall_score(y_true, y_pred, zero_division=0)\n    f1   = f1_score(y_true, y_pred, zero_division=0)\n    \n    return auc, auprc, prec, rec, f1\n\nfrom sklearn.model_selection import train_test_split\n\nMC_RUNS = 1000\nSAMPLE_SIZE = 5000  # e.g., majority class size if using a fixed majority approach\nratios = [0.50, 0.60, 0.70, 0.80, 0.90, 0.95, 0.98, 0.99]\nresults_list = []\n\n#for r, (X_sub, y_sub) in samples_A.items():\nfor r in ratios:\n    for mc_i in range(MC_RUNS):\n        \n        \n        # 2) Split the subsample into train/test\n        #    stratify ensures class distribution is preserved\n        X_sub, y_sub = create_subsample_fixed_majority(\n          xA, yA,\n          fraction_class0=r,\n          majority_class0_size=SAMPLE_SIZE,\n          random_state=None\n      )\n\n        X_train, X_test, y_train, y_test = train_test_split(\n            X_sub,\n            y_sub,\n            test_size=0.3,\n            random_state=42,\n            stratify=y_sub\n        )\n        \n        # Prepare DataFrame for the training set\n        df_train = pd.DataFrame({'X': X_train, 'y': y_train})\n        df_train = sm.add_constant(df_train, prepend=True, has_constant='add')  \n        # =&gt; columns: ['const', 'X', 'y']\n        \n        # 3) Fit logistic regression on the TRAIN portion\n        logit_model = sm.Logit(df_train['y'], df_train[['const', 'X']])\n        result = logit_model.fit(disp=False)\n        \n        # 4) Predict probabilities on the TEST portion\n        df_test = pd.DataFrame({'X': X_test})\n        df_test = sm.add_constant(df_test, prepend=True, has_constant='add')\n        \n        y_proba_test = result.predict(df_test[['const', 'X']])\n        \n        # 5) Evaluate performance metrics on the TEST set\n        auc, auprc, prec, rec, f1 = evaluate_model_performance(y_test, y_proba_test, threshold=0.5)\n        \n        # 6) Store results\n        results_list.append({\n            'ratio_0': r,\n            'auc': auc,\n            'auprc': auprc,\n            'precision': prec,\n            'recall': rec,\n            'f1': f1\n        })\n\n# Convert collected results to a DataFrame\ndf_results_A = pd.DataFrame(results_list)\n# Compute the mean performance metrics for each ratio\ndf_results_A.groupby('ratio_0').mean()\n\n\n\n\n\n\n\n\nauc\nauprc\nprecision\nrecall\nf1\n\n\nratio_0\n\n\n\n\n\n\n\n\n\n0.50\n0.984082\n0.984669\n0.930641\n0.930979\n0.930783\n\n\n0.60\n0.984122\n0.978174\n0.924246\n0.908146\n0.916079\n\n\n0.70\n0.983972\n0.968164\n0.914901\n0.882000\n0.898063\n\n\n0.80\n0.984141\n0.952157\n0.903439\n0.842323\n0.871622\n\n\n0.90\n0.984098\n0.916591\n0.897012\n0.777216\n0.832299\n\n\n0.95\n0.984102\n0.871136\n0.885726\n0.710380\n0.787117\n\n\n0.98\n0.984290\n0.799221\n0.866131\n0.618839\n0.717647\n\n\n0.99\n0.983755\n0.737946\n0.863657\n0.552467\n0.663418\n\n\n\n\n\n\n\nThe mean of the performance metrics for each of the eight sub-samples from Case A is presented in Figure 5.\n\n# Module : Plotting\n# Inputs : df_results_A\n# Outputs : fig showing the performance metrics vs the ratios\n# Objective : Group data by ratio_0 and plot the performance metrics.\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\ndf_grouped_A = df_results_A.groupby('ratio_0').mean().reset_index()\nmetrics = ['auc', 'auprc', 'precision', 'recall', 'f1']\ncolours = ['blue', 'orange', 'green', 'red', 'purple']\n\nplt.figure(figsize=(6, 5))\n\nfor metric, colour in zip(metrics, colours):\n    plt.plot(df_grouped_A['ratio_0'], df_grouped_A[metric], label=metric, color=colour, marker='o')\n\n# Improve readability with grid and styling\nplt.xlabel(\"Rate of Y = 0\", fontsize=12, weight='bold')\nplt.ylabel(\"Mean of Performance Metrics\", fontsize=12, weight='bold')\nplt.title(\"Mean of Performance Metrics vs Ratios\", fontsize=14, weight='bold')\nplt.xticks(fontsize=10)\nplt.yticks(fontsize=10)\nplt.grid(False)\nplt.legend(title=\"Metrics\", fontsize=10, title_fontsize=12, loc=\"best\")\nplt.tight_layout()\n\nplt.show()\n\n\n\n\n\n\n\nFigure 5: Performance metrics Vs. Ratios for Case A\n\n\n\n\n\nAs the class imbalance and sampling bias increase, the performance metrics deteriorate except for the AUC metric. It seems that the AUC metric is not sensitive to class imbalance and sampling bias."
  },
  {
    "objectID": "Meduim/RegressionLogistique/bias_sample.html#case-b-imbalanced-dataset",
    "href": "Meduim/RegressionLogistique/bias_sample.html#case-b-imbalanced-dataset",
    "title": "Sampling Bias and Class Imbalance in Maximum-likelihood Logistic Regression",
    "section": "Case B : Imbalanced Dataset",
    "text": "Case B : Imbalanced Dataset\nThe Case B dataset has an imbalanced class distribution of 80:20. This will be the true distribution of the dataset. Eight random samples will be extracted from the Case B population with varying class distributions of 50:50, 60:40, 70:30, 80:20, 90:10, 95:5, and 99:1.\n\nEight sub-samples generated from Case B.\nThe code below generates the eight sub-samples from the Case B dataset with varying class distributions. Figure 6 shows the distribution of the dependent variable y for each of the eight sub-samples.\n\n# Module : Generation\n# Inputs : fraction_class0 = [0.5, 0.6, 0.7, 0.8, 0.9, 0.95, 0.99], create_subsample_fixed_majority.\n# Ourputs : A dictionnarie with keys in fraction_class0 and values a tuple (X_sub, y_sub).\n\nfractions_class0 = [0.50, 0.60, 0.70, 0.80, 0.90, 0.95, 0.98, 0.99]\n\nsamples_B = {}\nfor frac0 in fractions_class0:\n    X_sub, y_sub = create_subsample_fixed_majority(\n        xB, yB, \n        fraction_class0=frac0, \n        majority_class0_size=5000,\n        random_state=42\n    )\n    # Store or process each sample\n    samples_B[frac0] = (X_sub, y_sub)\n\n\nfig, axes = plt.subplots(4, 2, figsize=(6, 8))\naxes = axes.ravel()  # on a maintenant 8 sous-graphiques\n\n# 1) Déterminer la fréquence max pour fixer une échelle cohérente\nall_counts = [np.bincount(y_sub) for _, (_, y_sub) in samples_B.items()]\nglobal_max_count = max(counts.max() for counts in all_counts)\n\nfor i, (label, (X_sub, y_sub)) in enumerate(samples_B.items()):\n    ax = axes[i]\n    \n    # 2) Histogramme sur 2 bins =&gt; classes 0 et 1\n    ax.hist(y_sub, bins=[-0.5, 0.5, 1.5],  # histogramme \"catégoriel\"\n            color='steelblue', edgecolor='black', alpha=0.7)\n    \n    # 3) Titre + proportion de y=1\n    mean_y = np.mean(y_sub)\n    ax.set_title(f\"{label}:{mean_y:.2f}\", fontsize=9)\n    \n    # 4) Ajuster l’axe X pour forcer l’affichage (0,1)\n    ax.set_xticks([0, 1])\n    ax.set_xticklabels(['0', '1'], fontsize=8)\n    \n    # 5) Limiter l’axe Y pour comparer visuellement entre les sous-figures\n    ax.set_ylim(0, global_max_count)\n    \n    # 6) N’afficher “Frequency” que sur la première colonne\n    #    pour éviter la répétition\n    if i % 2 == 0:\n        ax.set_ylabel('Frequency', fontsize=9)\n    else:\n        ax.set_ylabel('')\n    \n    # 7) Ajouter un label X plus discret\n    ax.set_xlabel('y', fontsize=9)\n    \n    # 8) Afficher le count exact sur chacune des barres\n    counts = np.bincount(y_sub)\n    for j, c in enumerate(counts):\n        ax.text(j, c + 0.5, str(c), ha='center', va='bottom', fontsize=8)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\nFigure 6: Distribution of y for Case B sub-samples\n\n\n\n\n\nA maximum-likelihood logistic regression model will be fitted to each of the eight sub-samples. A plot of the true \\(p(x)\\) versus the estimated \\(p(x)\\) is presented in Figure 7.\n\nimport numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm\n\n# Inputs : samples_B, fractions_class0\n# Outputs : fig\n\n\n\n\n# For illustration, let's create a 2 x 4 grid to show all eight ratio-samples\nfig, axes = plt.subplots(nrows=4, ncols=2, figsize=(6,8))\naxes = axes.ravel()  # flatten into 1D array [ax0, ax1, ..., ax7]\n\n# Hardcode alpha=-10, beta=2 for \"true\" logistic in Case A\nALPHA_TRUE = -10\nBETA_TRUE  = beta_opt\nprint(BETA_TRUE)\n\nfor i, (label, (X_sub, y_sub)) in enumerate(samples_B.items()):\n    ax = axes[i]\n    \n    # 1) Put the data into a DataFrame for convenience\n    df_sub = pd.DataFrame({\n        'X': X_sub,       # predictor\n        'y': y_sub        # binary outcome\n    })\n    \n    # 2) Add a constant column for the intercept in statsmodels\n    df_sub = sm.add_constant(df_sub, has_constant='add')  \n    # Now df_sub has columns ['const', 'X', 'y']\n    \n    # 3) Fit the logistic model\n    model = sm.Logit(df_sub['y'], df_sub[['const', 'X']])\n    results = model.fit(disp=False)  # disp=False to suppress output\n    \n    # 4) Predict the fitted probability\n    df_sub['pi_pred'] = results.predict(df_sub[['const', 'X']])\n    \n    # 5) Compute the \"true\" pi for comparison\n    df_sub['pi_true'] = logistic(df_sub['X'].values, alpha=ALPHA_TRUE, beta=BETA_TRUE)\n    \n    \n    # 6) Plot pi_true vs. pi_pred\n    ax.scatter(df_sub['pi_true'], df_sub['pi_pred'], \n               alpha=0.3, s=10, color='blue', edgecolors='none')\n    # Add a diagonal line for reference\n    ax.plot([0, 1], [0, 1], color='black', linestyle='--')\n    # 7) Decorate the subplot\n    ax.set_xlabel(\"True pi(x)\")\n    ax.set_ylabel(\"Predicted pi_hat(x)\")\n    ax.set_title(f\"{label}:{1-label:.2f}\")  # e.g., \"Case A60:40\"\n\nplt.tight_layout()\nplt.show()\n\n[1.23775]\n\n\n\n\n\n\n\n\nFigure 7: True vs. estimated p(x) for Case B sub-samples\n\n\n\n\n\nHere, it is evident from Figure 7 that the sample (Case \\(B_{50:50}\\)) with balance class no longer has the best fit between the true and estimated probabilities.\nThe sample (Case \\(B_{80:20}\\)) that performs the best does not have the sampling bias because it that case, the class distribution of the sample (80:20) is equal to the class distribution of the population (80:20). Furthermore, as the sample bias increases, the maximum-likelihood logistic regression model’s highly under- or overpredicts the probability.\nWhen the distribution of the minority in the sample is less than the distribution of the minority in the population, the model underpredicts the probability. Conversely, when the distribution of the minority in the sample is greater than the distribution of the minority in the population, the model overpredicts the probability.\n\n\nDistribution of the performance metrics for the eight sub-samples from Case B with monte-carlo simulations.\nWe perform 1000 monte-carlo simulations for each of the eight sub-samples from Case B. For each simulation, we fit a maximum-likelihood logistic regression model and compute the performance metrics : AUC, AU-PCR, Precision, Recall, F1-score. In order words, in the end of this exercise, we will have 1000 values for each of the performance metrics for each of the eight sub-samples.\nThe code below gives the distribution of the performance metrics for each of the eight sub-samples from Case B with monte-carlo simulations.\n\nC_RUNS = 1000\nSAMPLE_SIZE = 5000  # e.g., majority class size if using a fixed majority approach\nratios = [0.50, 0.60, 0.70, 0.80, 0.90, 0.95, 0.98, 0.99]\nresults_list = []\n\n#for r, (X_sub, y_sub) in samples_B.items():\nfor r in ratios:\n    # 1) Create a random subsample\n    #    Use None or vary random_state so each iteration is unique\n    for mc_i in range(MC_RUNS):\n      X_sub, y_sub = create_subsample_fixed_majority(\n          xB, yB,\n          fraction_class0=r,\n          majority_class0_size=SAMPLE_SIZE,\n          random_state=None\n      )\n      \n      # 2) Split the subsample into train/test\n      #    stratify ensures class distribution is preserved\n      X_train, X_test, y_train, y_test = train_test_split(\n          X_sub,\n          y_sub,\n          test_size=0.3,\n          random_state=42,\n          stratify=y_sub\n      )\n      \n      # Prepare DataFrame for the training set\n      df_train = pd.DataFrame({'X': X_train, 'y': y_train})\n      df_train = sm.add_constant(df_train, prepend=True, has_constant='add')  \n      # =&gt; columns: ['const', 'X', 'y']\n      \n      # 3) Fit logistic regression on the TRAIN portion\n      logit_model = sm.Logit(df_train['y'], df_train[['const', 'X']])\n      result = logit_model.fit(disp=False)\n      \n      # 4) Predict probabilities on the TEST portion\n      df_test = pd.DataFrame({'X': X_test})\n      df_test = sm.add_constant(df_test, prepend=True, has_constant='add')\n      \n      y_proba_test = result.predict(df_test[['const', 'X']])\n      \n      # 5) Evaluate performance metrics on the TEST set\n      auc, auprc, prec, rec, f1 = evaluate_model_performance(y_test, y_proba_test, threshold=0.5)\n      \n      # 6) Store results\n      results_list.append({\n          'ratio_0': r,\n          'auc': auc,\n          'auprc': auprc,\n          'precision': prec,\n          'recall': rec,\n          'f1': f1\n      })\n\n# Convert collected results to a DataFrame\ndf_results_B = pd.DataFrame(results_list)\n# Compute the mean performance metrics for each ratio\ndf_results_B.groupby('ratio_0').mean()\n\n\n\n\n\n\n\n\nauc\nauprc\nprecision\nrecall\nf1\n\n\nratio_0\n\n\n\n\n\n\n\n\n\n0.50\n0.946381\n0.933513\n0.857843\n0.911224\n0.883693\n\n\n0.60\n0.945966\n0.904535\n0.826579\n0.870215\n0.847776\n\n\n0.70\n0.946137\n0.863102\n0.791813\n0.812588\n0.801942\n\n\n0.80\n0.946245\n0.794082\n0.751109\n0.724216\n0.737141\n\n\n0.90\n0.945992\n0.652381\n0.672349\n0.527701\n0.590304\n\n\n0.95\n0.946195\n0.499444\n0.613992\n0.301671\n0.400875\n\n\n0.98\n0.946513\n0.319746\n0.114888\n0.014935\n0.025306\n\n\n0.99\n0.946483\n0.218801\n0.000000\n0.000000\n0.000000\n\n\n\n\n\n\n\nThe mean of the performance metrics for each of the eight sub-samples from Case B is presented in Figure 8.\n\n# Module : Plotting\n# Inputs : df_results_B\n# Objective : Group data by ratio_0 and plot the performance metrics.\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndf_grouped_B = df_results_B.groupby('ratio_0').mean().reset_index()\n\nplt.figure(figsize=(6, 6))\n\nfor metric, colour in zip(metrics, colours):\n    plt.plot(df_grouped_B['ratio_0'], df_grouped_B[metric], label=metric, color=colour, marker='o')\n\n# Improve readability with grid and styling\n\n# Add vertical lines at 0.8.\nplt.axvline(x=0.8, color='black', linestyle='--', linewidth=1.0)\nplt.xlabel(\"Ratio of Y=0\", fontsize=12, weight='bold')\nplt.ylabel(\"Mean of Performance Metrics\", fontsize=12, weight='bold')\nplt.title(\"Mean of Performance Metrics vs Ratios\", fontsize=14, weight='bold')\nplt.xticks(fontsize=10)\nplt.yticks(fontsize=10)\n#plt.grid(False, which='both', linestyle='--', linewidth=0.5, alpha=0.7)\nplt.legend(title=\"Metrics\", fontsize=10, title_fontsize=12, loc=\"best\")\nplt.tight_layout()\n\nplt.show()\n\n\n\n\n\n\n\nFigure 8: Performance metrics Vs. Ratios for Case B\n\n\n\n\n\nWhen the distribution of the minority in the sample is less than the distribution of the minority in the population, the performance metrics deteriorate considerably; when the distribution of the minority in the sample is greater than the distribution of the minority in the population, the performance metrics improve. Similar to Case A, there is not a significant change in the AUC metric values due to class imbalance and sampling bias.\nNext, it can be interesting to compare the performance measures from the samples that have the best performance in Case A (Case \\(A_{50:50}\\)) and Case B (Case \\(B_{80:20}\\)). In case, the sample \\(A_{50:50}\\) that performs the best has no sampling bias and class imbalance, while the sample \\(B_{80:20}\\) that performs the best has no sampling bias but has class imbalance. From these comparisons, it can be concluded that the performance of maximum-likelihood logistic regression is more sensitive to sampling bias than class imbalance."
  },
  {
    "objectID": "index_gdr.html",
    "href": "index_gdr.html",
    "title": "Spearman Correlation Matrix",
    "section": "",
    "text": "Compte tenu de ma spécialisation en gestion des risques, les contenus que je prévois de partager dans cette section dédiée à la 3ème année (3A) porteront principalement sur les enseignements spécifiques à cette spécialisation. Je m’efforcerai de rendre ces partages aussi pertinents et enrichissants que possible, en espérant qu’ils serviront de guide précieux pour ceux qui suivront une voie similaire."
  },
  {
    "objectID": "index_gdr.html#the-prediction-risk",
    "href": "index_gdr.html#the-prediction-risk",
    "title": "Spearman Correlation Matrix",
    "section": "The prediction risk",
    "text": "The prediction risk\nThe risk prediction is the expected loss between the predicted value and the true value on new, unseen data. In formula form, it can be expressed as: \\[\n\\text{R(X)} = \\mathbb{E}[(Y - \\hat{Y})^2] = \\mathbb{E}[(Y - \\mathbb{text{E}}(Y|X))^2]\n\\] where \\(Y\\) is the true value, \\(\\hat{Y}\\) is the predicted value, and \\(X\\) is the set of covariates used for prediction.\n\nMay 28, 2025"
  },
  {
    "objectID": "index_gdr.html#observed-default-rate-odr",
    "href": "index_gdr.html#observed-default-rate-odr",
    "title": "Spearman Correlation Matrix",
    "section": "Observed Default Rate (ODR)",
    "text": "Observed Default Rate (ODR)\nHow can we estimate the observed default rate (ODR) of a portfolio of loans?\nThis is example of a database where we want to estimate the ODR and the yearly ODR as the the average of the quarterly ODRs.\n\nimport pandas as pd\nimport numpy as np\n\n\n# Étape 1 : Création de la table \"frequentist_PD\"\n# Simulons 6 trimestres de données pour 2 clusters (A et B)\nclusters = ['A'] * 6 + ['B'] * 6\ndates = pd.date_range('2020-01-01', periods=6, freq='QE').tolist() * 2\nodrQ = [0.020, 0.025, 0.030, 0.028, 0.027, 0.029,\n        0.030, 0.032, 0.031, 0.033, 0.035, 0.034]\n\nfrequentist_PD = pd.DataFrame({\n    'cluster': clusters,\n    'hpm_arret': dates,\n    'odrQ': odrQ\n})\n\n# On ajoute des colonnes d'index pour simuler \"obs\"\nfrequentist_PD['obs'] = frequentist_PD.groupby('cluster').cumcount()\n\n# On trie pour garantir l'ordre temporel par cluster\nfrequentist_PD = frequentist_PD.sort_values(['cluster', 'hpm_arret']).reset_index(drop=True)\n\nfrequentist_PD\n\n\n\n\n\n\n\n\ncluster\nhpm_arret\nodrQ\nobs\n\n\n\n\n0\nA\n2020-03-31\n0.020\n0\n\n\n1\nA\n2020-06-30\n0.025\n1\n\n\n2\nA\n2020-09-30\n0.030\n2\n\n\n3\nA\n2020-12-31\n0.028\n3\n\n\n4\nA\n2021-03-31\n0.027\n4\n\n\n5\nA\n2021-06-30\n0.029\n5\n\n\n6\nB\n2020-03-31\n0.030\n0\n\n\n7\nB\n2020-06-30\n0.032\n1\n\n\n8\nB\n2020-09-30\n0.031\n2\n\n\n9\nB\n2020-12-31\n0.033\n3\n\n\n10\nB\n2021-03-31\n0.035\n4\n\n\n11\nB\n2021-06-30\n0.034\n5\n\n\n\n\n\n\n\n\n# Étape 2 : Simuler les frequentist_PD_2_j (décalages pour j = i+1, i = 1, 2, 3)\n\n# On crée un dictionnaire pour stocker les décalages\nfrequentist_PD_2 = {}\n\n# On prépare une copie de base à laquelle on ajoutera les colonnes décalées\nbase = frequentist_PD.copy()\n\n# Pour chaque i (1 to 3), on génère le j = i + 1 et on décale les colonnes\nfor i in range(1, 4):\n    j = i + 1\n    shifted = base.copy()\n    shifted['obs'] = shifted['obs'] - i  # décale l'observation comme point=obs+i en SAS\n    shifted = shifted.rename(columns={\n        'cluster': f'cluster{j}',\n        'hpm_arret': f'hpm_arret{j}',\n        'odrQ': f'odrQ{j}'\n    })\n    frequentist_PD_2[j] = shifted[['obs', f'cluster{j}', f'hpm_arret{j}', f'odrQ{j}']]\n\n# On merge progressivement pour simuler les frequentist_PD_3_j\nmerged = base.copy()\nfor j in range(2, 5):\n    merged = pd.merge(merged, frequentist_PD_2[j], on='obs', how='left')\n\nmerged\n\n\n\n\n\n\n\n\ncluster\nhpm_arret\nodrQ\nobs\ncluster2\nhpm_arret2\nodrQ2\ncluster3\nhpm_arret3\nodrQ3\ncluster4\nhpm_arret4\nodrQ4\n\n\n\n\n0\nA\n2020-03-31\n0.020\n0\nA\n2020-06-30\n0.025\nA\n2020-09-30\n0.030\nA\n2020-12-31\n0.028\n\n\n1\nA\n2020-03-31\n0.020\n0\nA\n2020-06-30\n0.025\nA\n2020-09-30\n0.030\nB\n2020-12-31\n0.033\n\n\n2\nA\n2020-03-31\n0.020\n0\nA\n2020-06-30\n0.025\nB\n2020-09-30\n0.031\nA\n2020-12-31\n0.028\n\n\n3\nA\n2020-03-31\n0.020\n0\nA\n2020-06-30\n0.025\nB\n2020-09-30\n0.031\nB\n2020-12-31\n0.033\n\n\n4\nA\n2020-03-31\n0.020\n0\nB\n2020-06-30\n0.032\nA\n2020-09-30\n0.030\nA\n2020-12-31\n0.028\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n57\nB\n2020-12-31\n0.033\n3\nB\n2021-03-31\n0.035\nA\n2021-06-30\n0.029\nNaN\nNaT\nNaN\n\n\n58\nB\n2020-12-31\n0.033\n3\nB\n2021-03-31\n0.035\nB\n2021-06-30\n0.034\nNaN\nNaT\nNaN\n\n\n59\nB\n2021-03-31\n0.035\n4\nA\n2021-06-30\n0.029\nNaN\nNaT\nNaN\nNaN\nNaT\nNaN\n\n\n60\nB\n2021-03-31\n0.035\n4\nB\n2021-06-30\n0.034\nNaN\nNaT\nNaN\nNaN\nNaT\nNaN\n\n\n61\nB\n2021-06-30\n0.034\n5\nNaN\nNaT\nNaN\nNaN\nNaT\nNaN\nNaN\nNaT\nNaN\n\n\n\n\n62 rows × 13 columns\n\n\n\nThe SAS script is given below:\n%macro test1;\n\n  %do i=1 %to 3 %by 1;\n    %let j = %sysevalf(&i.+1);\n\n    /* Crée une table contenant les colonnes odrQ décalées */\n    data frequentist_PD_2_&j.;\n      obs1 = 1;\n      do while (obs1 &lt; nobs);\n        set frequentist_PD nobs=nobs;\n        obs&j. = obs1 + &i.;\n\n        set frequentist_PD (\n          rename=(\n            cluster=cluster&j.\n            lb=lb&j.\n            ub=ub&j.\n            hpm_arret=hpm_arret&j.\n            def=defQ&j.\n            n=nQ&j.\n            odrQ=odrQ&j.\n          )\n        ) point=obs&j.;\n\n        output;\n        keep cluster lb ub hpm_arret def defQ&j. n nQ&j. odrQ odrQ&j.;\n        obs1 + 1;\n      end;\n    run;\n\n    /* Initialisation au premier tour */\n    %if &i. = 1 %then %do;\n      data frequentist_PD_3_&j.;\n        set frequentist_PD_2_&j.;\n      run;\n    %end;\n\n    /* Jointure gauche avec la table précédente pour empiler les odrQ */\n    %else %do;\n      proc sql;\n        create table frequentist_PD_3_&j. as\n        select a.*,\n               b.defQ&j.,\n               b.nQ&j.,\n               b.odrQ&j.\n        from frequentist_PD_3_&i. as a\n        left join frequentist_PD_2_&j. as b\n          on a.cluster = b.cluster\n          and a.hpm_arret = b.hpm_arret;\n      quit;\n    %end;\n\n  %end;\n\n  /* Résultat final : table contenant odrQ, odrQ2, odrQ3, odrQ4 */\n  data frequentist_PD_4;\n    set frequentist_PD_3_&j.;\n    \n    /* Calcul du ODR annuel glissant comme moyenne des 4 trimestres */\n    odrY = mean(of odrQ odrQ2 odrQ3 odrQ4);\n  run;\n\n  /* Décale hpm_arret de 9 mois (équivalent intnx) */\n  data frequentist_PD_4;\n    set frequentist_PD_4;\n    hpm_arret = intnx('month', hpm_arret, 9, 'same');\n    if year(hpm_arret) = 2019 then delete; /* Exclusion des années trop anciennes */\n  run;\n\n  /* Sauvegarde finale */\n  data frequentist_PD_5;\n    set frequentist_PD_4;\n  run;\n\n%mend;\n\n/* Exécution de la macro */\n%test1;\n\nMay 27, 2025"
  },
  {
    "objectID": "index_gdr.html#model-selection",
    "href": "index_gdr.html#model-selection",
    "title": "Spearman Correlation Matrix",
    "section": "Model Selection",
    "text": "Model Selection\nWe have a data with many covariates. But we want to include only the covariates that are relevant to the response variable. This allows us to have a parsimonious model, with fewer covariates, which is easier to interpret and to use for prediction.\nGenerally, when we add more covariates to a model, the bias of the model decreases, but the variance increases.\n\nMay 26, 2025"
  },
  {
    "objectID": "index_gdr.html#the-adulterous-woman",
    "href": "index_gdr.html#the-adulterous-woman",
    "title": "Spearman Correlation Matrix",
    "section": "The adulterous woman",
    "text": "The adulterous woman\nA woman was caught in adultery. The Pharisees brought her to Jesus and asked him what should be done with her. They said that according the law of Moses, she should be stoned to death. Jesus replied, “Let anyone among you who is without sin be the first to throw a stone at her.”\nI like this story because it shows that Jesus is a feminist.\n\nMay 25, 2025"
  },
  {
    "objectID": "index_gdr.html#isaac-asimovs-three-laws-of-robotics",
    "href": "index_gdr.html#isaac-asimovs-three-laws-of-robotics",
    "title": "Spearman Correlation Matrix",
    "section": "Isaac Asimov’s “Three Laws of Robotics”",
    "text": "Isaac Asimov’s “Three Laws of Robotics”\n\nA robot may not injure a human being or, through inaction, allow a human being to come to harm.\nA robot must obey the orders given it by human beings except where such orders would conflict with the First Law.\nA robot must protect its own existence as long as such protection does not conflict with the First or Second Law.\n\n\nMay 24, 2025"
  },
  {
    "objectID": "index_gdr.html#density-probability-function-using-plotly",
    "href": "index_gdr.html#density-probability-function-using-plotly",
    "title": "Spearman Correlation Matrix",
    "section": "Density probability function using plotly",
    "text": "Density probability function using plotly\n\nimport pandas as pd\nimport numpy as np\n\n# Simuler les données\ndf = pd.DataFrame({\n    '2012': np.random.randn(200),\n    '2013': np.random.randn(200) + 1\n})\n\n# Calculer les statistiques\nstats_df = pd.DataFrame({\n    'min': df.min(),\n    'mean': df.mean(),\n    'median': df.median(),\n    'max': df.max(),\n    'std': df.std()\n})\n\n# Affichage\nprint(stats_df)\n\n           min     mean    median       max       std\n2012 -2.399943  0.07926  0.075151  2.206767  0.880948\n2013 -1.076208  1.02229  0.963917  3.607347  0.920831\n\n\n\nimport plotly.express as px\nimport pandas as pd\nimport numpy as np\n\n# Simuler les données\ndf = pd.DataFrame({\n    '2012': np.random.randn(200),\n    '2013': np.random.randn(200) + 1\n})\n\n# Convertir au format long\ndf_long = df.melt(var_name='Year', value_name='Value')\n\n# Créer le boxplot\nfig = px.box(df_long, x='Year', y='Value', points=False, title=\"Boxplot for 2012 and 2013\", color='Year',\n             labels={'Year': 'Year', 'Value': 'Values'})\nfig.show()\n\n        \n        \n        \n\n\n                            \n                                            \n\n\n\nimport numpy as np\nimport pandas as pd\nimport plotly.graph_objects as go\nfrom scipy.stats import gaussian_kde\n\n# Example data (you can replace this with your real df[\"age\"])\nage = np.array([15, 16, 16, 17, 18, 19, 20, 21, 22, 22, 23, 24, 25, 25, 26, 27, 28])\n\n# Remove NA if needed\nage = age[~np.isnan(age)]\n\n# KDE estimate\nkde = gaussian_kde(age)\n\n# Define range of x values\nx_vals = np.linspace(age.min(), age.max(), 200)\ny_vals = kde(x_vals)\n\n# Plot using Plotly\nfig = go.Figure()\nfig.add_trace(go.Scatter(x=x_vals, y=y_vals, mode='lines', name='Density', line=dict(width=2)))\nfig.update_layout(title=\"Density of age\", xaxis_title=\"Age\", yaxis_title=\"Density\")\nfig.show()"
  },
  {
    "objectID": "index_gdr.html#exploring-variable-relationships-in-python",
    "href": "index_gdr.html#exploring-variable-relationships-in-python",
    "title": "Spearman Correlation Matrix",
    "section": "Exploring Variable Relationships in Python",
    "text": "Exploring Variable Relationships in Python\nThe graphic analysis is a tool to understand the relationship between the covariates and the response variable. It is very important when we want to perform a linear regression.\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\nfrom statsmodels.api import OLS, add_constant\n\n# Load the dataset\ndf = pd.read_csv('data/Multiple_Regression_Dataset.csv')\ndf.head()\n\n\n\n\n\n\n\n\nR\nAge\nS\nEd\nEx0\nEx1\nLF\nM\nN\nNW\nU1\nU2\nW\nX\n\n\n\n\n0\n79.1\n151\n1\n91\n58\n56\n510\n950\n33\n301\n108\n41\n394\n261\n\n\n1\n163.5\n143\n0\n113\n103\n95\n583\n1012\n13\n102\n96\n36\n557\n194\n\n\n2\n57.8\n142\n1\n89\n45\n44\n533\n969\n18\n219\n94\n33\n318\n250\n\n\n3\n196.9\n136\n0\n121\n149\n141\n577\n994\n157\n80\n102\n39\n673\n167\n\n\n4\n123.4\n141\n0\n121\n109\n101\n591\n985\n18\n30\n91\n20\n578\n174\n\n\n\n\n\n\n\n\n# Create a new figure\n\n# Extract response variable and covariates\nresponse = 'R'\ncovariates = [col for col in df.columns if col != response]\n\nfig, axes = plt.subplots(nrows=4, ncols=4, figsize=(5, 5), sharex=False, sharey=True)\naxes = axes.flatten()\n\n# Plot boxplot for binary variable 'S'\nsns.boxplot(data=df, x='S', y='R', ax=axes[0])\naxes[0].set_title('Boxplot of R by S')\naxes[0].set_xlabel('S')\naxes[0].set_ylabel('R')\n\n# Plot regression lines for all other covariates\nplot_index = 1\nfor cov in covariates:\n    if cov != 'S':\n        sns.regplot(data=df, x=cov, y='R', ax=axes[plot_index], scatter=True, line_kws={\"color\": \"red\"})\n        axes[plot_index].set_title(f'{cov} vs R')\n        axes[plot_index].set_xlabel(cov)\n        axes[plot_index].set_ylabel('R')\n        plot_index += 1\n\n# Hide unused subplots\nfor i in range(plot_index, len(axes)):\n    fig.delaxes(axes[i])\n\nfig.tight_layout()\nplt.show()"
  },
  {
    "objectID": "index_gdr.html#when-several-variables-are-correlated",
    "href": "index_gdr.html#when-several-variables-are-correlated",
    "title": "Spearman Correlation Matrix",
    "section": "When several variables are correlated",
    "text": "When several variables are correlated\nWhen several variables are correlated with each other, we keep only one of them. The one the most correlated with the response variable.\n\n# Step 2: Correlation of each variable with response R\nspearman_corr_with_R = spearman_corr['R'].drop('R')  # exclude R-R\n\n# Step 3: Identify pairs of covariates with strong inter-correlation (e.g., &gt; 0.9)\nstrong_pairs = []\nthreshold = 0.6\ncovariates = spearman_corr_with_R.index\n\nfor i, var1 in enumerate(covariates):\n    for var2 in covariates[i+1:]:\n        if abs(spearman_corr.loc[var1, var2]) &gt; threshold:\n            strong_pairs.append((var1, var2))\n\n# Step 4: From each correlated pair, keep only the variable most correlated with R\nto_keep = set()\nto_discard = set()\n\nfor var1, var2 in strong_pairs:\n    if abs(spearman_corr_with_R[var1]) &gt;= abs(spearman_corr_with_R[var2]):\n        to_keep.add(var1)\n        to_discard.add(var2)\n    else:\n        to_keep.add(var2)\n        to_discard.add(var1)\n\n# Final selection: all covariates excluding the ones to discard due to redundancy\nfinal_selected_variables = [var for var in covariates if var not in to_discard]\n\nfinal_selected_variables\n\n['Ex1', 'LF', 'M', 'N', 'NW', 'U2']"
  },
  {
    "objectID": "index_gdr.html#fit-a-linear-regression-model-on-a-six-variables-after-standardization-not-split-data-into-train-and-test",
    "href": "index_gdr.html#fit-a-linear-regression-model-on-a-six-variables-after-standardization-not-split-data-into-train-and-test",
    "title": "Spearman Correlation Matrix",
    "section": "Fit a linear regression model on a six variables after standardization not split data into train and test",
    "text": "Fit a linear regression model on a six variables after standardization not split data into train and test\n\n# Variables\nX = df[final_selected_variables]\ny = df['R']\n\n# Standardisation des variables explicatives\nscaler = StandardScaler()\nX_scaled_vars = scaler.fit_transform(X)\n\n# ➕ Remettre les noms des colonnes (après standardisation)\nX_scaled_df = pd.DataFrame(X_scaled_vars, columns=final_selected_variables)\n\n# ➕ Ajouter l'intercept (constante)\nX_scaled_df = add_constant(X_scaled_df)\n\n# Régression avec noms conservés\nmodel = OLS(y, X_scaled_df).fit()\nprint(model.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      R   R-squared:                       0.568\nModel:                            OLS   Adj. R-squared:                  0.503\nMethod:                 Least Squares   F-statistic:                     8.773\nDate:                Sat, 31 May 2025   Prob (F-statistic):           4.07e-06\nTime:                        16:51:47   Log-Likelihood:                -218.24\nNo. Observations:                  47   AIC:                             450.5\nDf Residuals:                      40   BIC:                             463.4\nDf Model:                           6                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst         90.5085      3.975     22.767      0.000      82.474      98.543\nEx1           25.3077      5.008      5.054      0.000      15.187      35.429\nLF             4.9155      5.631      0.873      0.388      -6.465      16.296\nM              9.9027      5.681      1.743      0.089      -1.579      21.384\nN              2.6733      5.647      0.473      0.639      -8.741      14.087\nNW            11.1950      4.432      2.526      0.016       2.238      20.152\nU2             3.1268      4.928      0.634      0.529      -6.834      13.088\n==============================================================================\nOmnibus:                        1.619   Durbin-Watson:                   2.089\nProb(Omnibus):                  0.445   Jarque-Bera (JB):                1.126\nSkew:                           0.045   Prob(JB):                        0.569\nKurtosis:                       2.247   Cond. No.                         3.06\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
  },
  {
    "objectID": "index_gdr.html#dont-trust-chatgpt",
    "href": "index_gdr.html#dont-trust-chatgpt",
    "title": "Spearman Correlation Matrix",
    "section": "Don’t Trust ChatGPT",
    "text": "Don’t Trust ChatGPT\nEach year, we see rapid advancements in artificial language tools, especially from companies like OpenAI. These models are evolving fast. Today, we can generate images with ease, solve complex problems in computer science, mathematics, physics, and more. But can we really trust these tools?\nI don’t think so.\nToday, I was writing an article about regression to the mean. The equation is given by:\n\n\\(\\mathbb{E}(Y|X) = \\alpha + \\beta X\\)\n\nMy goal was to explain the meaning of the regression coefficient and offer an intuitive technique to predict the value of \\(Y\\) given a value of \\(X\\). I assumed that the variances of \\(X\\) and \\(Y\\) were both equal to one, and that \\(X\\) was centered (i.e., had mean zero). I then stated that under these conditions, the slope \\(\\beta\\) is equal to the correlation between \\(X\\) and \\(Y\\).\nBut ChatGPT told me this was incorrect. It insisted that this only holds true if \\(Y\\) is also centered.\nYet, we know that if both \\(X\\) and \\(Y\\) are scaled to have unit variance, then the slope of the regression is indeed equal to the correlation between them—even if \\(Y\\) is not centered. That’s a basic identity in linear regression under standardized variables.\nSo while these AI tools can be helpful, they’re not always right. You still need to think critically, check the math, and trust your own understanding.\n\nMay 11, 2025"
  },
  {
    "objectID": "index_gdr.html#sports-bring-us-together",
    "href": "index_gdr.html#sports-bring-us-together",
    "title": "Spearman Correlation Matrix",
    "section": "Sports Bring Us Together",
    "text": "Sports Bring Us Together\nToday, like most days, I went for a run and stopped at the park to do some exercises—push-ups, a few pull-ups, and some jump rope.\nEvery time, I find it amazing. I meet people from all walks of life—different backgrounds, all genders, all ages. Some look wealthy, others look like they’re struggling. Some seem strong and healthy, others clearly carry the weight of illness. There are professionals and amateurs. Some are trying sports for the very first time—and probably won’t be back. Others stick with it for a while before giving up. And then, of course, there are the regulars who keep showing up.\nBut what stays constant is the atmosphere. It’s always welcoming. When you greet someone, they greet you back—with kindness. That simple act of saying hello when you arrive creates a sense of safety. You never know what might happen out there, but that greeting—it’s a small sign of trust, of connection. It reminds me of Marcel Mauss’s idea of the gift and counter-gift: you offer a smile, and in return, you receive not just a smile, but a sense of protection.\n\nMay 11, 2025"
  },
  {
    "objectID": "index_gdr.html#why-certain-couples-marry-and-others-do-not",
    "href": "index_gdr.html#why-certain-couples-marry-and-others-do-not",
    "title": "Spearman Correlation Matrix",
    "section": "Why certain couples marry and others do not?",
    "text": "Why certain couples marry and others do not?\nToday, saturday, May 10 2025, I attended the wedding of a friend–a girl I met in high school. We were in the same grade from ninth grade all the way through senior year.\nShe was smart, almost among the top students in our class. She was attentive, displined, serious and hardworking. She rarely laughted out loud, but she always had a warm smile. Given her academic achievements and her work ethic, I expected her to pursue advanced studies and build a successful career. And that’s exactly what happened–she went on to study at ENSAE, one of the most prestigious schools in France, and she is now an actuary.\nNow she is married too, and I hope she will find the same success in her marriage as she did in her studies and career. But this made me wonder: why do some people marry and others do not? What are the factors that influence the decision to marry?\nAt her wedding, I began to reflect on this question using the story of my friend as a starting point. First, she and her husband come from similar social backgrounds—which I believe played a stabilizing role in their relationship.\nThen, there was distance. After her master’s, she earned a scholarship to continue her studies at ENSAE and moved to Paris, while her husband remained in Cameroon. They had to maintain a long-distance relationship for two years and waited until the third year to get married. Despite the distance, they managed to stay connected and committed.\nLastly, their social circle made a real difference. The groom repeatedly thanked his friends during the ceremony for supporting their relationship. He said they reassured his bride during moments of doubt and helped keep them grounded and hopeful.\nBy the end of the ceremony, I had identified three key factors that seemed to have supported their union: shared social status, the ability to overcome physical distance, and the strength of their support network. I believe there are many other factors yet to discover, and I’m curious to explore what else might influence the decision to marry.\n\nMay 10, 2025"
  },
  {
    "objectID": "index_gdr.html#regression-to-the-mean",
    "href": "index_gdr.html#regression-to-the-mean",
    "title": "Spearman Correlation Matrix",
    "section": "Regression to the mean",
    "text": "Regression to the mean\nRegression to the mean was discovered and named by Sir Francis Galton in the 19th century. It refers to the phenomenon where extreme observations tend to be followed by more moderate ones, and moving closer to the average.\nThis concept is often misunderstood, and interpreted in terms of causality. Take this proposition: &gt; “Highly intelligent women tend to marry men who are less intelligent than they are.” What is the explanation?\nSome may think of highly intelligent women wanting to avoid the competition equally intelligent men, or being forced to compromise in their choice of partner because intelligent men do not want to compete with intelligent women. This explanation is wrong.\nThe correlation between the intelligence scores of spouces is less than perfect. If the correlation between the intelligence scores of spouses is not perfect( and if men and women on average do not differ in intelligence), then it is mathematically inevitable that the higly intelligent women will be married to men who are on overage less intelligent than they are.\n\nMay 9, 2025"
  },
  {
    "objectID": "index_gdr.html#should-you-invest-save-or-keep-your-money-under-the-mattress",
    "href": "index_gdr.html#should-you-invest-save-or-keep-your-money-under-the-mattress",
    "title": "Spearman Correlation Matrix",
    "section": "Should You Invest, Save, or Keep Your Money Under the Mattress?",
    "text": "Should You Invest, Save, or Keep Your Money Under the Mattress?\nA quick read of the bible can help you to answer this question. I recommend reading Mathieu 25:14, the parable of the servants and the master’s reward.\nIn this parable, a master gives three servants different amounts of money to manage during his absence. The first servant receives five talents, the second two talents, and the third one talent. The first two servants invest their money and double it, while the third servant hides his talent in the ground.\nWhen the master returned from his journey, the first two servants presented their profits. The master was proud of them and rewarded them. But the third servant explained he knew that his master was hard and demanding. Because he was afraid of losing the money, he hid the talent in the ground. The master was angry and disappointed, and he took the talent. He told him that if he was so afraid, the least he could have done was put the money in a bank to earn interest.\nThis parable teaches us that the most important to get money is to invest it wisely. If you are afraid of losing your money, you should at least put it in a bank to earn interest. Keep money under a pillow is not a good idea, and should be used as the last resort.\n\nMay 8, 2025"
  },
  {
    "objectID": "index_gdr.html#how-can-we-choose-to-modeling-time-series-using-ardl",
    "href": "index_gdr.html#how-can-we-choose-to-modeling-time-series-using-ardl",
    "title": "Spearman Correlation Matrix",
    "section": "How can we choose to modeling time series using ARDL?",
    "text": "How can we choose to modeling time series using ARDL?\nWhat ultimately determines the choice of a model is the data. This assumes that we’ve already carried out a preliminary analysis to understand the nature of the risk, define its scope, and identify the relevant risk factors.\nThat’s why it’s misguided to say, for example, that we should—or should not—use an ARDL model to analyze a time series. What truly matters is ensuring that the residuals of the model are not autocorrelated.\n\nMay 7, 2025"
  },
  {
    "objectID": "index_gdr.html#what-drives-stock-prices",
    "href": "index_gdr.html#what-drives-stock-prices",
    "title": "Spearman Correlation Matrix",
    "section": "What drives stock prices?",
    "text": "What drives stock prices?\nIt’s been about a week since I invested in Eutelsat stock at €3.59 through my PEA (Plan d’Épargne en Actions). When I woke up on Monday, May 5th, I was surprised to see that the stock had jumped to €4.20. Suddenly, it started dropping, and I was tempted to sell. Fortunately, I didn’t—because by the end of the day, the stock had climbed to over €4.65.\nThat’s a 29.5% increase relative to my investment:\n\\[\n\\frac{4.65 - 3.59}{3.59} \\times 100 ≈ 29.5\\%\n\\]\nNaturally, I started wondering what could have caused such a sharp rise. I looked it up on Google and checked the news on TV—but found nothing.\nLater that afternoon—maybe by chance, maybe because I had been actively searching—I stumbled upon an article in Les Échos. It mentioned that Eutelsat’s CEO, Eva Berneke, had been replaced by Jean-François Fallacher, the former CEO of Orange.\nThat was likely one reason for the stock’s spike. Another possible explanation is the French government’s involvement with the company. In the context of European and French national defense, Eutelsat is seen as a strategic alternative to Starlink, SpaceX’s satellite internet service.\nIt’s incredibly hard to predict stock prices.\nOn a similar note, I also noticed that oil prices had dropped recently. And again, there’s a potential explanation: the number of oil producers within OPEC has increased by three. This rise in oil production leads to a greater supply, which in turn explains the drop in oil prices.\n\nMay 6, 2025"
  },
  {
    "objectID": "index_gdr.html#we-are-champions-the-champions-the-first-trophy-of-harry-kane-in-his-career.",
    "href": "index_gdr.html#we-are-champions-the-champions-the-first-trophy-of-harry-kane-in-his-career.",
    "title": "Spearman Correlation Matrix",
    "section": "We are champions the champions : The first trophy of Harry Kane in his career.",
    "text": "We are champions the champions : The first trophy of Harry Kane in his career.\nYesterday, after Bayer Leverkusen’s draw, Bayern Munich officially clinched the Bundesliga title. This marks the very first trophy of Harry Kane’s career.\nOne question comes to mind: Is this title more meaningful to Harry Kane than it is to other Bayern players like Thomas Müller or Manuel Neuer, who’ve already won countless trophies?I want to take it even further: Is Kane happier about this title than a die-hard Bayern Munich fan might be? To answer those questions properly, we’d have to consider several factors—but let me just share my opinion.\nIt’s true that after a long, demanding season, winning a title is always a great source of satisfaction for any player. But for a club like Bayern Munich, winning the Bundesliga is important, yes—but it’s also expected. The fans and the club’s leadership invest a lot of money in top-tier talent to win the Champions League. So when you’re a Bayern player, lifting the Bundesliga trophy isn’t enough—you need to win the Champions League to feel truly fulfilled.\nNow, for someone like Harry Kane, who’s never won a single trophy in his career and who’s been through a lot of disappointment and heartbreak—with Tottenham and even with England—this title must feel incredibly rewarding. Let’s not forget: he lost the Champions League final in 2019 with Tottenham against Liverpool, and the Euro 2020 final with England against Italy. People even started saying he was cursed. Let’s hope he fully enjoys this title—and that it’s just the beginning of many more to come with Bayern Munich.\n\nMay 5, 2025"
  },
  {
    "objectID": "index_gdr.html#warren-buffett-retires-at-94",
    "href": "index_gdr.html#warren-buffett-retires-at-94",
    "title": "Spearman Correlation Matrix",
    "section": "Warren Buffett retires at 94",
    "text": "Warren Buffett retires at 94\nThe most famous investor in the world, Warren Buffett, has announced his retirement at the age of 94 in the end of the year. He has announced this decision during the annual meeting of Berkshire Hathaway, the company he founded in 1965.\nWhat explains his success? He is known for his long-term investment strategy, which focuses on buying and holding quality companies. He has also been a strong advocate of value investing, which involves looking for undervalued stocks with strong fundamentals.\nDo all these elements fully explain his success? I don’t think so. To explain his success, we also need to take luck into account. He wrote a book, and many others have written about him. But not many people have achieved the kind of success he has. That’s why I want to propose a model to predict Warren Buffett’s success, which will be formulated as follows:\nSuccess = f(strategy, long-term investment, value investing, …) + luck\n\nMay 4, 2025"
  },
  {
    "objectID": "index_gdr.html#diagnostic-de-performance-énergétique-dpe",
    "href": "index_gdr.html#diagnostic-de-performance-énergétique-dpe",
    "title": "Spearman Correlation Matrix",
    "section": "Diagnostic de performance énergétique (DPE)",
    "text": "Diagnostic de performance énergétique (DPE)\nLe DPE renseigne sur les performances énergétiques et environnementales d’un logement et d’un bâtiment, en évaluant ses émissions de gaz à effet de serre (GHG).\nLe DPE contient les informations suivantes : - Les informations sur les caractéristiques du bâtiment telles que la surface, les orientations, les mûrs, les fenêtres, les matériaux, etc. - Les informations sur les équipements du logement tels que le chauffage, la climatisation, l’eau chaude sanitaire, la ventilation, etc.\nLe contenu et les modalités du DPE sont réglementés. Ainsi, les données sur les DPE peuvent être utilisées comme facteur de risque ESG (environnemental, social et de gouvernance).\nPour plus d’informations, veuillez consulter le site de l’ademe ici\n\nMay 3, 2025"
  },
  {
    "objectID": "index_gdr.html#aimé-césaire-inspired-by-the-bible",
    "href": "index_gdr.html#aimé-césaire-inspired-by-the-bible",
    "title": "Spearman Correlation Matrix",
    "section": "Aimé Césaire inspired by the Bible",
    "text": "Aimé Césaire inspired by the Bible\nWhen we take time to reflect on the condition of the oppressed, the poor, and the suffering, we see that the Bible has inspired many well-known writers—among them, Aimé Césaire.\nIn Proverbs 31:8–9, it is written: “Speak up for those who cannot speak for themselves, for the rights of all who are abandoned.Speak up and judge fairly; defend the rights of the poor and needy.”\nWith this in mind, we can believe that Aimé Césaire—a powerful French poet—was deeply influenced by this text when he wrote: “My mouth will be the mouth of those who have no mouth, my voice, the freedom of those who sink into the dungeons of despair.”\n\nMay 2, 2025"
  },
  {
    "objectID": "index_gdr.html#la-clé-lamine-yamal",
    "href": "index_gdr.html#la-clé-lamine-yamal",
    "title": "Spearman Correlation Matrix",
    "section": "La clé Lamine Yamal",
    "text": "La clé Lamine Yamal\nHier soir, le barça affrontait l’Inter Milan dans le cadre du match aller de la demi-finale de la Ligue des champions. Ce match opposait la meilleure attaque de la compétition, le barça, à la meilleure défense, l’Inter Milan. On s’attendait donc à un match difficile et fermé avec peu de buts. Cependant, on a assisté à spectable incroyable, magnifique et inoubliable.\nL’Inter Milan a ouvert le score grâce à un but superbe de Marcus Thuram au tout début du match. L’inter de Milan a même doublé la mise grâce à un but de Dunfries. On s’est dit en ce moment que c’est fini pour le barça. L’Inter Milan a commencé à défendre, à mettre le bus. Mais il a fallu que Yamal trouve la clé pour ouvrir le cadenas mis en place par l’Inter.\nAprès avoir effacé Henrikh Mkhitaryan avec une facilité déconcertante, le joyau de la Masia a fixé Alessandro Bastoni puis décoché un remarquable tir du pied gauche qui est aller fracasser le poteau droit de Yann Sommer avant de franchir la ligne. On a vu en lui du Ronaldinho, du Neymar et même du Messi. On se pose même la question de savoir si c’est le nouveau Messi du Barça. Ce qui est sûr, c’est que Lamine Yamal est un génie, comme l’a dit son entraîneur, ancien entraîneur du Bayern Munich, Hans-Dieter Flick.\nIl est difficile de croire mais cet enfant sort de l’adolescence, mais il a une grande maturité. Il n’a que 17 ans et ce match est son 100e match avec le Barça. Il est le plus jeune joueur de l’histoire a avoir marqué en demi-finale de la Ligue des champions, dépassant ainsi le record d’un certain Kylian Mbappé.\nGrâce à lui, le Barça a pu arracher un match nul 3-3. Nous sommes impatients de voir Yamal briller lors du match retour à Milan."
  },
  {
    "objectID": "index_gdr.html#le-livre-des-réponses",
    "href": "index_gdr.html#le-livre-des-réponses",
    "title": "Spearman Correlation Matrix",
    "section": "Le livre des réponses",
    "text": "Le livre des réponses\nJ’adore la bible parce que tu peux y trouver des réponses à toutes tes questions. Par exemple, si tu est quelqu’un qui travaille beaucoup et dort peu, si ton entourage te dit qu’il faut dormir, que le sommeil est réparateur. Si tu es fatigué par eux, tu peux leur répondre que d’après Proverbes 20:13, “N’aime pas le sommeil, tu risquerais de t’appauvrir. Garde les yeux ouverts et tu seras rassasié de pain.”\nLire la bible, notamment le livre des proverbes ou de l’Ecclésiaste, écrit par Salomon, te donnera de l’intelligence et de la sagesse."
  },
  {
    "objectID": "index_gdr.html#categories",
    "href": "index_gdr.html#categories",
    "title": "Spearman Correlation Matrix",
    "section": "Categories",
    "text": "Categories\nHow tall is Junior? If Junior is 1.5m. Your answer is a function of his age. He is very tall if I tell you that he is 6 years old. Very short if he is 20 years old. Your brain automatically returns the relevant norm, which allows you to make a quick decision.\nWe are also able to match intensity across categories and answer the question: “How expensive is a restaurant that matches Junior’s height?”\nOur world is broken into categories for which we have a norm. And those norms allow us to make quick decisions."
  },
  {
    "objectID": "index_gdr.html#what-evokes-you-the-100-days-of-donald-trump",
    "href": "index_gdr.html#what-evokes-you-the-100-days-of-donald-trump",
    "title": "Spearman Correlation Matrix",
    "section": "What evokes you the 100 days of Donald Trump?",
    "text": "What evokes you the 100 days of Donald Trump?\nEven if Trump sticks to his program, the first 100 days have nonetheless been marked by a major disruption in the global economy, which has had a negative impact on various markets, including Wall Street, the European market, and the Asian market.\nFurthermore, we were shocked by his stances on the war in Ukraine, his attack on the Chairman of the Federal Reserve, Jerome Powell, as well as the increase in tariffs — especially in the context of the trade war with China.\n\nMay 1, 2025"
  },
  {
    "objectID": "index_gdr.html#la-puissance-de-la-parole.",
    "href": "index_gdr.html#la-puissance-de-la-parole.",
    "title": "Spearman Correlation Matrix",
    "section": "La puissance de la parole.",
    "text": "La puissance de la parole.\nLa bible met en évidence la puissance de la parole. Par la parole, Dieu à créé le monde. Pour des non croyants, ceci peut être ridicule. Je pense qu’ils conviennent avec moi que ceux qui maitrisent la parole ont un pouvoir. Ils peuvent séduire, enchanter, persuader, parfois ils peuvent même manipuler.\nDans la bible, notamment dans prophète, l’auteur préconise l’usage adéquat de la parole. Il dit que la langue, qui permet de parler, a un pouvoir de vie et de mort; ceux qui aiment parler en goûteront les fruits. Ensuite, il conseille de refléchir avant de parler. Celui qui répond avant d’avoir écouté fait preuve de folie et se couvre de honte.\nPlusieurs écrivaints ont souligné cette importance de la parole, notamment les mots. Jean Paul Sartre, un écrivain français, disait que “Les mots sont des pistolets chargés.”\n\nApril 30, 2025"
  },
  {
    "objectID": "Meduim/RegressionLogistique/HosmerL.html",
    "href": "Meduim/RegressionLogistique/HosmerL.html",
    "title": "Evaluation of the Logistic Regression : The Hosmer-Lemeshow test",
    "section": "",
    "text": "Model performance is commonly evaluated based on two main criteria. Discrimination and calibration. The discrimination describes the ability of the model to assign higher probabilities of the outcomes to those observations that actually experience the outcome. A recognized metric for assessing a model’s discrimination is the area under the receiver operating characteristic (ROC) curve. Calibration or fit, on the other hand, captures how accurately the predicted probabilities is close to the actual occurence of the outcome. Several tests and graphical methods have been proposed to assess the calibration of a model, which is often referred to as “goodness of fit.” Among the goodness of fit tests, the Hosmer-Lemeshow (HL) test is the most widely applied approach (Nattino, 2020).\nCentral idea of the test : The main idea of the test is to divide the observations into groups and compute a chi2 statistic that reflects the overall mismatch between the observed number of events and the expected number of events in each group-outcome category.\nAs with most goodness of fit tests, the HL test is designed to decide between a null hypothesis of perfect fit, where the probabilities assumed by the model are hypothesized to coincide with the real probabilities, and a general alternative hypothesis of nonperfect fit. Let’s present the framework of the test.\nThe value of the test statistic is : \\[\nHL  = \\sum_{g=1}^{G} \\frac{(O_{D,k} - E_{D,k})^2}{E_{D,k}} + \\frac{(O_{ND,k} - E_{ND,k})^2}{E_{ND,k}}\n\\]\nWhere :\n\n\\(O_{D,k}\\) and \\(E_{D,k}\\) are respectively the number of observed events (default for example) and the number of expected events in the group k.\n\\(O_{ND,k}\\) and \\(E_{ND,k}\\) are respectively the number of observed non-events (non-default for example) and the number of expected non-events in the group k.\nG is the number of groups (typically 10)."
  },
  {
    "objectID": "Meduim/RegressionLogistique/HosmerL.html#logistic-regression-model",
    "href": "Meduim/RegressionLogistique/HosmerL.html#logistic-regression-model",
    "title": "Evaluation of the Logistic Regression : The Hosmer-Lemeshow test",
    "section": "Logistic Regression Model",
    "text": "Logistic Regression Model\n\nLet Y be the dependent variable: \\(Y = \\begin{cases}\n1 & \\text{if default} \\\\\n0 & \\text{if not default}\n\\end{cases}\\)\nLet \\(( X_1, \\ldots, X_p )\\) represent the set of p variables.\nThe logistic regression equation can be written as: \\[\n\\text{logit}(P(Y = 1 \\mid X)) = \\beta_0 + \\beta_1 X_1 + \\ldots + \\beta_p X_p\n\\]\nSolving for P: \\[\nP(Y = 1 \\mid X) = \\frac{1}{1 + \\exp{-(\\beta_0 + \\beta_1 X_1 + \\ldots + \\beta_p X_p)}}\n\\]\nHere, $_0, _1, , _p $ are parameters to be estimated."
  },
  {
    "objectID": "Meduim/RegressionLogistique/HosmerL.html#computing-probabilities",
    "href": "Meduim/RegressionLogistique/HosmerL.html#computing-probabilities",
    "title": "Evaluation of the Logistic Regression : The Hosmer-Lemeshow test",
    "section": "Computing Probabilities",
    "text": "Computing Probabilities\nFor a dataset with N individuals:\n\nFor each individual i, compute the probability of success: \\[\nP_i = P(Y_i = 1 \\mid X_1^i, \\ldots, X_p^i) = \\frac{1}{1 + \\exp{-(\\beta_0 + \\beta_1 X_1^i + \\ldots + \\beta_p X_p^i)}}\n\\]\n\\(P_i\\) represents the expected probability for individual i .\nCreate a table of individuals with their observed outcomes Y and predicted probabilities \\(P_i\\).\n\n\nExample Table\nAfter computing \\(P_i\\) for all individuals, the results can be summarized in a table:\n\n\n\nIndividual\nEvent (\\(Y\\))\n\\(P_i\\)\n\n\n\n\n1\n1\n0.8\n\n\n2\n0\n0.2\n\n\n3\n1\n0.9\n\n\n4\n0\n0.1\n\n\n…\n…\n…\n\n\nN\n1\n0.95\n\n\n\nIf the logistic regression fits well, the predicted probability \\(P_i\\) for each individual should align closely with the observed outcome Y Specifically:\n\nWhen Y=1 (the event occurred), \\(P_i\\) should be close to 1, reflecting high confidence in predicting the event.\nWhen Y = 0 (the event did not occur), \\(P_i\\) should be close to 0, reflecting high confidence in predicting non-occurrence.\n\n\n\nPerforming the Hosmer-Lemeshow Test\nAfter this stage, it is not difficult to carry out the Hosmer-Lemeshow test. What is necessary is ordering and grouping individuals. The Hosmer-Lemeshow test can be performed by dividing the predicted probabilities (Pi) into deciles (10 groups based on percentile ranks) and then computing the Chi-square statistic that compares the predicted to the observed frequencies (Hyeoun-AE, 2013).\nThe value of the Hosmer-Lemeshow statistic is given by:\n\\[\nH = \\sum_{g=1}^{G} \\frac{(O_g - E_g)^2}{E_g} + \\frac{(n_g - O_g - (n_g - E_g))^2}{n_g - E_g}\n\\]\nWhere:\n\nG : Number of groups (typically 10).\n\\(O_g\\) : Observed number of events in group g.\n\\(E_g\\) : Expected number of events in group g or the sum of predicted probabilities for the group g (\\(\\sum_{i =1}^{n_g}P_i\\)).\n\\(n_g\\) : Total number of individuals in group g.\n\n\n\nExample of Computing Each Element in One Group\nTo illustrate how the statistic is computed for a single group g :\n\nSuppose the group contains \\(n_g = 100\\) individuals.\nOut of these, \\(O_g = 20\\) individuals experienced the event (e.g., default).\nThe sum of predicted probabilities for the group is \\(E_g = 18.5\\).\n\nUsing the formula:\n\\[\nH_g = \\frac{(O_g - E_g)^2}{E_g} + \\frac{(n_g - O_g - (n_g - E_g))^2}{n_g - E_g}\n\\]\nSubstitute the values:\n\nFirst term: \\(\\frac{(20 - 18.5)^2}{18.5}\\)\nSecond term: \\(\\frac{(100 - 20 - (100 - 18.5))^2}{100 - 18.5}\\)\n\nCalculate each term and sum them to obtain the contribution of group g to the overall Hosmer-Lemeshow statistic.\n\n\nInterpreting the Hosmer-Lemeshow Test\nUnder the null hypothesis (the observed default numbers correspond to the expected default numbers), the test statistic asymptotically follows the $^2 $ distribution with G - 2 degrees of freedom, where G is the number of groups.\n\nIf the p-value is higher than 5%, it indicates a small statistic and thus a limited gap between observed defaults and expected defaults, suggesting a good model fit.\nIf the p-value is lower than 5%, it indicates a significant discrepancy between observed and expected values, suggesting a poor model fit.\n\nCaution: Hosmer and Lemeshow recommend avoiding the use of this test when there is a small number of observations (less than 400), as the test may yield unreliable results."
  },
  {
    "objectID": "Others/number_obs_for_model.html",
    "href": "Others/number_obs_for_model.html",
    "title": "Optimal Number of default for constructing a model",
    "section": "",
    "text": "A common step before building a model is to determine the optimal number of observations required. In credit scoring, it’s not only essential to know the total number of observations, but also the optimal number of defaults needed to develop a meaningful and reliable model.\nIn the case of a low-default portfolio (LDP), defaults are rare, making it difficult to assess whether constructing a model is even possible. Rather than building a model blindly and relying on luck for acceptable performance, it is crucial to first determine the minimum number of defaults required to justify model development.\nThis determination must satisfy both statistical and regulatory constraints. From a statistical perspective, the sample must be representative, with a distribution that reflects the underlying population. On the regulatory side, authorities typically require that a rating model include at least seven distinct grades (e.g., AAA, AA, A, BBB, BB, B, C).\nFailing to meet these conditions can lead to regulatory sanctions, delays due to corrective actions, and financial costs for the institution.\nThis brings us to the question, What is the optimal number of defaults required to build a seven-grade rating model for a low-default portfolio that satisfies both statistical rigor and regulatory expectations, while ensuring heterogeneity between rating grades?"
  },
  {
    "objectID": "Others/number_obs_for_model.html#introduction",
    "href": "Others/number_obs_for_model.html#introduction",
    "title": "Optimal Number of default for constructing a model",
    "section": "",
    "text": "A common step before building a model is to determine the optimal number of observations required. In credit scoring, it’s not only essential to know the total number of observations, but also the optimal number of defaults needed to develop a meaningful and reliable model.\nIn the case of a low-default portfolio (LDP), defaults are rare, making it difficult to assess whether constructing a model is even possible. Rather than building a model blindly and relying on luck for acceptable performance, it is crucial to first determine the minimum number of defaults required to justify model development.\nThis determination must satisfy both statistical and regulatory constraints. From a statistical perspective, the sample must be representative, with a distribution that reflects the underlying population. On the regulatory side, authorities typically require that a rating model include at least seven distinct grades (e.g., AAA, AA, A, BBB, BB, B, C).\nFailing to meet these conditions can lead to regulatory sanctions, delays due to corrective actions, and financial costs for the institution.\nThis brings us to the question, What is the optimal number of defaults required to build a seven-grade rating model for a low-default portfolio that satisfies both statistical rigor and regulatory expectations, while ensuring heterogeneity between rating grades?"
  },
  {
    "objectID": "Others/number_obs_for_model.html#which-data-should-be-used",
    "href": "Others/number_obs_for_model.html#which-data-should-be-used",
    "title": "Optimal Number of default for constructing a model",
    "section": "1. Which data should be used ?",
    "text": "1. Which data should be used ?\nIt is important to define the perimeter over which data will be collected. This is done by considering several criteria. If we consider a portfolio made up of large corporations, we can define the perimeter based on company size—for example, a turnover above a certain threshold (100 million euros), the region (Europe, North America, etc.), or the sector (agriculture, industry, services, etc.).\nWhat characterizes large companies is that they rarely default—that is, they generally meet their financial obligations to creditors. As a result, if we assign them ratings on a seven-grade scale (AAA, AA, A, BBB, BB, B, C), very few companies would receive a C rating. Most companies would fall within the intermediate grades (A, BBB, BB), and only a few would receive the highest rating (AAA). Therefore, the distribution of ratings tends to resemble a normal distribution.\nGoing forward, we will assume that our portfolio consists of large corporations and that the rating scale follows a normal distribution.\nIf you’re interested in reproducing this work, you can use a portfolio of your choice and define your own distribution of ratings across the grades."
  },
  {
    "objectID": "Others/number_obs_for_model.html#statistical-contraints-heterogeneity-between-grades.",
    "href": "Others/number_obs_for_model.html#statistical-contraints-heterogeneity-between-grades.",
    "title": "Optimal Number of default for constructing a model",
    "section": "2. Statistical contraints : heterogeneity between grades.",
    "text": "2. Statistical contraints : heterogeneity between grades.\nTo effectively assess the credit risk of counterparties, it’s important that the rating scale is both homogeneous within each category and heterogeneous across categories. In other words, observations within the same rating grade should share similar characteristics — meaning they represent the same level of risk — while observations in different rating grades should reflect distinct risk profiles.\nWe won’t address within-grade homogeneity in this post.\nTo ensure heterogeneity across rating categories, we’ll use the binomial test to compare the proportions of observations assigned to each grade. The difference in proportions will be evaluated under the assumption that it follows a normal distribution with a mean of zero — which serves as the null hypothesis.\nBy dividing the difference by its standard deviation, we obtain a standardized value that follows the standard normal distribution. \\[\nZ = \\frac{p_1 - p_2}{\\sqrt{\\frac{p_1(1-p_1)}{n_1} + \\frac{p_2(1-p_2)}{n_2}}}.\n\\] Where \\(p_1\\) and \\(p_2\\) are the proportions of observations in the two rating categories being compared, and \\(n_1\\) and \\(n_2\\) are the number of observations in each category."
  },
  {
    "objectID": "Others/number_obs_for_model.html#methodology",
    "href": "Others/number_obs_for_model.html#methodology",
    "title": "Optimal Number of default for constructing a model",
    "section": "Methodology",
    "text": "Methodology\nThis section outlines the procedure used to determine the optimal number of defaults when constructing a default model with seven rating grades.\n\nStep 1: Estimating the Number of Observations per Grade\nFirst, we need to determine how many observations fall into each rating grade. Given a total number of observations \\(N\\) and assuming a seven-grade scale (AAAA, AA, A, BBB, BB, B, C), we compute the number of observations per grade using the normal distribution. That means calculating the probability associated with each grade and multiplying it by \\(N\\).\nThere are many ways to assign probabilities across grades, but we choose this method because it is deterministic and replicable. Other approaches, including random sampling techniques, could be used as well. In our case, the number of observations for each grade is generated based on the following methodology:\n\nComputing the probability density function (PDF) of the normal distribution for each grade.\n\nFor the first grade (AAAA), the probability is:\n\n\\[\nP(\\text{AAAA}) = \\frac{F(\\text{AAAA} + \\epsilon)}{2\\epsilon},\n\\]\nwhere \\(F\\) is the cumulative distribution function (CDF) of the normal distribution and \\(\\epsilon \\to 0\\). We assume \\(F(\\text{AAAA} - \\epsilon) = 0\\).\n\nFor middle grades (AA, A, BBB, BB, B), the probability is calculated as:\n\n\\[\nP(\\text{Grade}) = \\frac{F(\\text{Grade} + \\epsilon) - F(\\text{Grade} - \\epsilon)}{2\\epsilon},\n\\]\n\nFor the last grade (C), since \\(F(\\text{C} + \\epsilon) = 1\\), the probability becomes:\n\n\\[\nP(\\text{C}) = \\frac{1 - F(\\text{C} - \\epsilon)}{2\\epsilon}.\n\\]\nThe number of observations in each grade is then computed as:\n\n\\[\nN_{\\text{grade}} = N \\cdot P(\\text{grade}).\n\\]\n\n\nStep 2: Estimating the Number of Defaults per Grade\nNext, we determine the number of defaults for each grade while ensuring heterogeneity between them. In the context of low-default portfolios (LDP), the highest rating (AAAA) is expected to have very few defaults. So, we begin by fixing the number of defaults for grade AAAA at 1.\nTo compute the number of defaults for grade \\(i+1\\) based on grade \\(i\\), we follow this approach:\n\n1. Ensuring Heterogeneity Between Grades\nThe two grades must be heterogeneous, meaning the null hypothesis of the binomial test (that the default rates are equal) must be rejected at a significance level \\(\\alpha\\). This leads to the following statistical condition:\n\\[\nP(Z = \\frac{|p_i - p_{i+1}|}{\\sqrt{\\frac{p_i(1 - p_i)}{N_i} + \\frac{p_{i+1}(1 - p_{i+1})}{N_{i+1}}}} \\geq Z_\\alpha) = \\alpha,\n\\]\nwhere \\(p_i\\) and \\(p_{i+1}\\) are the default rates for grades \\(i\\) and \\(i+1\\), and \\(N_i\\), \\(N_{i+1}\\) are the respective numbers of observations. \\(Z_\\alpha = \\Phi^{-1}(1 - \\alpha/2)\\), where \\(\\Phi^{-1}\\) is the inverse of the cumulative distribution function of the standard normal distribution.\nis the critical value of the standard normal distribution for the chosen confidence level.\nWe can rewrite this condition using the pooled default rate:\n\\[\np = \\frac{N_i p_i + N_{i+1} p_{i+1}}{N_i + N_{i+1}}.\n\\]\nThen the \\(Z\\)-statistic becomes:\n\\[\nP(Z = \\frac{|p_i - p_{i+1}|}{\\sqrt{p(1 - p)\\left(\\frac{1}{N_i} + \\frac{1}{N_{i+1}}\\right)}} \\geq Z_\\alpha ) = \\alpha,\n\\]\nNow, let \\(e = p_{i+1} - p_i\\). Then:\n\\[\np_{i+1} = p_i + e.\n\\]\n\n\n2. Computing the Number of Defaults for Grade \\(i+1\\)\nOnce \\(p_{i+1}\\) is known, the number of defaults for grade \\(i+1\\) is:\n\\[\nn_{i+1} = p_{i+1} \\cdot N_{i+1}.\n\\]\nTo find the value of the optimal \\(e\\) that satisfies the heterogeneity condition, we solve the equation:\n\\[\nZ = \\frac{e}{\\sqrt{p(1 - p)\\left(\\frac{1}{N_i} + \\frac{1}{N_{i+1}}\\right)}} = Z_\\alpha,\n\\]\nThis leads to a second-degree equation of the form:\n\\[\nae^2 + be + c = 0.\n\\]\nwith coefficients:\n\\[\na = -\\left[\\left(\\frac{N_{i+1}}{N_i + N_{i+1}}\\right)^2 + \\frac{1}{Z_\\alpha^2\\left(\\frac{1}{N_i} + \\frac{1}{N_{i+1}}\\right)}\\right],\n\\]\n\\[\nb = (1 - 2p_i)\\left(\\frac{N_{i+1}}{N_i + N_{i+1}}\\right),\n\\]\n\\[\nc = p_i(1 - p_i),\n\\]\nSince \\(a &lt; 0\\) and \\(c &gt; 0\\), the quadratic equation has two real roots — one negative and one positive. We select the positive solution:\n\\[\ne = \\frac{-b - \\sqrt{b^2 - 4ac}}{2a}.\n\\]\n\n\nimport openpyxl\nimport numpy as np\nimport pandas as pd\nfrom scipy.stats import norm\nfrom IPython.display import Markdown\nfrom tabulate import tabulate\nimport seaborn as sns\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "Others/number_obs_for_model.html#results",
    "href": "Others/number_obs_for_model.html#results",
    "title": "Optimal Number of default for constructing a model",
    "section": "Results",
    "text": "Results\nTo implement the method, we use Python. The number of defaults for the highest grade (AAAA) is fixed at 1. A simulation is then conducted by varying the total number of observations from 1,000 to 10,000 in steps of 1,000. The optimal number of defaults is defined as the average number of defaults across all simulations.\n\nSimulating the Distribution of Observations per Grade\nThe code below simulates how observations are distributed across rating grades based on the normal distribution. We assume a mean of 4 (corresponding to grade BBB) and a standard deviation of 1. The number of observations in each grade is determined by multiplying the total number of observations by the probability density function (PDF) of the normal distribution centered at each grade (see Figure 1).\n\ndef generate_obs_pdf(N, mu, sigma,espilon=0.5):\n    grades = ['AAAA', 'AA', 'A', 'BBB', 'BB', 'B', 'C']\n    grades_positions = np.arange(1,len(grades)+1)\n    prob = np.zeros(len(grades))\n    prob[0] = norm.cdf(grades_positions[0] + espilon, mu, sigma) \n    prob[6] = 1 - norm.cdf(grades_positions[6] - espilon, mu, sigma)\n    for i in range(1, len(grades)-1):\n        prob[i] = norm.cdf(grades_positions[i] + espilon, mu, sigma) - norm.cdf(grades_positions[i] - espilon, mu, sigma)\n\n    obs_count = (prob * N).round().astype(int)\n    count_int = np.floor(obs_count).astype(int)\n    remainder = N - count_int.sum()\n    fractional_part = obs_count - count_int\n    sorted_indices = np.argsort(-fractional_part)\n    for i in range(remainder):\n        count_int[sorted_indices[i]] += 1\n\n    obs_count = pd.Series(obs_count, index=grades)\n    return obs_count\n# Define the parameters\nN = 5001\nmu = 4\nsigma = 1\n# Generate the number of observations per grade\nobs_count = generate_obs_pdf(N, mu, sigma)\n\n\n# Plot the results and add the number of observations in each grade\nplt.figure(figsize=(8, 5))\nsns.barplot(x=obs_count.index, y=obs_count.values)\nplt.title(f'Number of Observations per Grade (N={N}, mu={mu}, sigma={sigma})')\nfor i, v in enumerate(obs_count.values):\n    plt.text(i, v + 0.5, str(v), ha='center', va='bottom')\nplt.xlabel('Grade')\nplt.ylabel('Number of Observations')\nplt.xticks(rotation=45)\nplt.show()\n\n\n\n\n\n\n\nFigure 1: Distribution of Observations per Grade\n\n\n\n\n\n\n\nEstimating the Number of Defaults per Grade\nThe following code estimates the number of defaults for each grade, given the total number of observations, while ensuring heterogeneity between consecutive grades. This is done using the binomial test at a significance level \\(\\alpha = 0.05\\) (see Table 1).\n\n\n\nTable 1: Number of Defaults per Grade\n\n\ndef generate_defaut_per_grade(N, mu, sigma, alpha):\n    df_obs = generate_obs_pdf(N, mu, sigma)\n    grades = df_obs.index\n    n = df_obs.values\n\n    # Initialisation des vecteurs\n    z_alpha = norm.ppf(alpha/2)\n    k = len(n)\n    d_rate = np.zeros(k)\n    ecart = np.zeros(k)\n    p_moy = np.zeros(k)\n    stats = np.zeros(k)\n    p_vals = np.zeros(k)\n    n_def = np.zeros(k, dtype=int)\n\n    # Conditions initiales\n    n_def[0] = 1\n    d_rate[0] = 1 / n[0]\n\n    for i in range(1, k):\n        n1, n2 = n[i-1], n[i]\n        prev_rate = d_rate[i-1]\n        frac = n2 / (n1 + n2)\n\n        # Coefficients quadratiques\n        a = - (frac**2 + 1 / (z_alpha**2 * (1/n1 + 1/n2)))\n        b = (1 - 2 * prev_rate) * frac\n        c = prev_rate * (1 - prev_rate)\n        delta = b**2 - 4*a*c\n\n        if delta &lt; 0:\n            raise ValueError(f\"No real solution at i={i} (delta &lt; 0)\")\n\n        ecart[i] = (-b - np.sqrt(delta)) / (2 * a)\n        d_rate[i] = prev_rate + ecart[i]\n        n_def[i] = int(round(d_rate[i] * n[i]))\n        p_moy[i] = prev_rate + ecart[i] * frac\n\n        var = p_moy[i] * (1 - p_moy[i]) * (1/n1 + 1/n2)\n        stats[i] = ecart[i] / np.sqrt(var)\n        p_vals[i] = 2 * (1 - norm.cdf(abs(stats[i])))\n\n    return pd.DataFrame({\n        'Grade': grades,\n        'nb obs': n,\n        '#defaut': n_def,\n        'pct defaut': d_rate,\n        'écart pct defaut': ecart,\n        'proba moyenne': p_moy,\n        'statistic': stats,\n        'p-value': p_vals\n    })\n\n# Exemple d'appel\nN = 5001\nmu = 4\nsigma = 1\nalpha = 0.05\ndf_defaut = generate_defaut_per_grade(N, mu, sigma, alpha)\n# Display the results\nprint(tabulate(df_defaut, headers='keys', tablefmt='psql', showindex=False))\ndf_defaut.to_excel('df_defaut.xlsx', index=False)\n\n+---------+----------+-----------+--------------+--------------------+-----------------+-------------+-----------+\n| Grade   |   nb obs |   #defaut |   pct defaut |   écart pct defaut |   proba moyenne |   statistic |   p-value |\n|---------+----------+-----------+--------------+--------------------+-----------------+-------------+-----------|\n| AAAA    |       31 |         1 |    0.0322581 |          0         |        0        |     0       |      0    |\n| AA      |      303 |        50 |    0.165283  |          0.133025  |        0.152936 |     1.95996 |      0.05 |\n| A       |     1209 |       261 |    0.216208  |          0.0509256 |        0.206003 |     1.95996 |      0.05 |\n| BBB     |     1915 |       472 |    0.246731  |          0.0305224 |        0.234918 |     1.95996 |      0.05 |\n| BB      |     1209 |       336 |    0.278268  |          0.0315377 |        0.258936 |     1.95996 |      0.05 |\n| B       |      303 |       102 |    0.335389  |          0.0571204 |        0.289715 |     1.95996 |      0.05 |\n| C       |       31 |        16 |    0.511876  |          0.176487  |        0.351769 |     1.95996 |      0.05 |\n+---------+----------+-----------+--------------+--------------------+-----------------+-------------+-----------+"
  },
  {
    "objectID": "Others/number_obs_for_model.html#simulating-the-optimal-number-of-defaults",
    "href": "Others/number_obs_for_model.html#simulating-the-optimal-number-of-defaults",
    "title": "Optimal Number of default for constructing a model",
    "section": "Simulating the Optimal Number of Defaults",
    "text": "Simulating the Optimal Number of Defaults\nThis final simulation estimates the optimal number of defaults by computing the average number of defaults over multiple runs. The number of observations varies from 5,000 to 1,000,000 in increments of 5,000(see Figure 2).\n\nplt.figure(figsize=(8, 5))\n\nN_values = np.arange(5000, 1000000, 5000)\noptimal_defaults = [generate_defaut_per_grade(N, mu, sigma, alpha)['#defaut'].sum() for N in N_values]\nmean_defaults = np.mean(optimal_defaults)\nplt.plot(N_values, optimal_defaults, marker='o')\nplt.axhline(mean_defaults, color='r', linestyle='--', label=f'Mean Defaults: {mean_defaults:.2f}')\n\n########################################\n# Ajout de texte pour afficher la valeur de la moyenne\n##########################################\nplt.text(N_values[-1], mean_defaults, f'Mean: {mean_defaults:.2f}', color='red', fontsize=10, ha='left', va='center')\nplt.title('Optimal Number of Defaults vs. Number of Observations')\nplt.xlabel('Number of Observations')\nplt.ylabel('Optimal Number of Defaults')\nplt.xticks(rotation=45)\nplt.legend()\nplt.grid(False)\nplt.show()\n\n\n\n\n\n\n\nFigure 2: Optimal Number of Defaults vs. Number of Observations\n\n\n\n\n\n\nConclusion\nThis study addressed a fundamental challenge in credit risk modeling: determining the optimal number of defaults required to develop a reliable and regulatory-compliant rating model, particularly for low-default portfolios (LDPs). Building such models without sufficient defaults not only compromises statistical robustness but also risks non-compliance with regulatory expectations, especially the requirement of a seven-grade rating scale. Through a structured and reproducible methodology based on the normal distribution of credit grades and binomial statistical tests, the research demonstrated how one can estimate the number of observations and defaults per grade necessary to ensure heterogeneity between rating categories. A key insight from the simulation was that, even with increasing total observations, the number of defaults required stabilizes - highlighting that data quantity alone does not compensate for data quality or risk signal strength. The implications of these findings are significant for risk managers and model validators. Institutions should not only focus on collecting more data but also ensure they meet a minimum threshold of defaults (estimated at 1,419) to build a statistically valid model. This threshold acts as a benchmark for initiating model development, validating its structure, and anticipating regulatory scrutiny. Nevertheless, the approach is not without limitations. The model assumes a normal distribution of credit grades and does not explore within-grade homogeneity or other sources of model uncertainty such as macroeconomic shocks, structural breaks, or portfolio shifts. Additionally, the focus on large corporate portfolios may limit generalizability to other segments like SMEs or retail. Future research could refine the simulation under alternative rating distributions.In conclusion, this research provides a quantitative foundation for institutions managing low-default portfolios to assess their readiness for model development. It encourages a shift from ad hoc modeling toward data-driven thresholds, reinforcing both statistical credibility and regulatory alignment in credit risk modeling."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Meduim Publications",
    "section": "",
    "text": "This website serves as the treasury of my intellectual voyage, housing the jewels of my published articles on Medium, the budding seeds of works in progress, and the pillars of knowledge acquired during my third year in the Risk Management program at the National School of Statistics and Information Analysis in Rennes.\n\n\nMeduim Publications\nMedium is an online publishing platform for sharing ideas, stories, and knowledge through articles written by individuals and organizations.\n\nCredit Scoring modellling approach.\n\nDefinition of default and construction of the database.\nOutliers Identification and Treatment\nMissing data analysis\nDatabase splitting\nPre-selection of explanatory variables\nMethodological Approach To Merge Modalities of qualitative variables\nMethodological Approach To Discretize Quantitative variables\nMonotony and Stability\nOversampling\nLogistic Regression\n\nDefinition\nFitting The logistic Regression\nEvaluation of the logistic Regression\n\nAnalysis of the statistical significance of the model\nThe overall fit of the model : Likelihhod ratio test\nStatistical significance of individual regression coefficients: Likelihood ratio test and Z statistic\nHosmer-Lemeshow test\n\nContribution of variables\n\nModel performances\n\nAccuracy Ratio or Gini\nstability test\n\n\nTimes Series\nTowards data science (TDS) :\n\nProportional Odds Model for Ordinal Logistic Regression\nSpurious Regression in Time Series\nMultiple Linear Regression Analysis\nFourier Transformation\n\nOthers :\n\nSampling Bias and Class Imbalance in Maximum-likelihood Logistic Regression\nNumber of optimal defaults in credit rating\n\n\n\n\nLearning SAS\n\nBackground :\n\nPrerequisites\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "LearningSas/prerequisites.html",
    "href": "LearningSas/prerequisites.html",
    "title": "Learning SAS",
    "section": "",
    "text": "This document is a compilation of what is essential to know about SAS programming. The are two notions that are fondamental to understand before diving into the SaS programming language. These are the data step and the proc step.\nThe data step consists to create a dataset and to manipulate it. The proc step is used to analyze the data.\nBefore starting, let’s talk about the libname.\n\n\nThe libname statement is used to assign a library reference name to a physical location. The example below shows how to assign the reference name Inputs and outputs to the physical location /home/u63691422/EPG1V2/data and /home/u63691422/EPG1V2/Jumbong_Training/Outputs respectively.\nTlibname Inputs \"/home/u63691422/EPG1V2/data\" ;\nlibname Outputs \"/home/u63691422/EPG1V2/Jumbong_Training/Outputs\";"
  },
  {
    "objectID": "LearningSas/prerequisites.html#operator-summary-table",
    "href": "LearningSas/prerequisites.html#operator-summary-table",
    "title": "Learning SAS",
    "section": "Operator summary table",
    "text": "Operator summary table\n\nTable of comparison operators\n\n\n\nOperator\nSymbol\nMeaning\n\n\n\n\nLT\n&lt;\nLess than\n\n\nGT\n&gt;\nGreater than\n\n\nLE\n&lt;=\nLess than or equal to\n\n\nGE\n&gt;=\nGreater than or equal to\n\n\nEQ\n=\nEqual to\n\n\nNE\n^=\nNot equal to\n\n\nIN\nIN\nIn a list of values\n\n\n\n\n\nLogical operators table\n\n\n\nOperator\nSymbol\nMeaning\n\n\n\n\nAND\n&\nLogical AND\n\n\nOR\n!\nLogical OR\n\n\nNOT\n~\nLogical NOT\n\n\n\n\n\nArithmetic operators table\n\n\n\nOperator\nSymbol\nMeaning\n\n\n\n\nADD\n+\nAddition\n\n\nSUB\n-\nSubtraction\n\n\nMUL\n*\nMultiplication\n\n\nDIV\n/\nDivision\n\n\nPOW\n**\nExponentiation\n\n\nMOD\n%\nModulus\n\n\nMax\n&lt;&gt;\nMaximum\n\n\nMin\n&gt;&lt;\nMinimum\n\n\n\n\n\nOperator of concatenation\nThe concatenation operator is used to concatenate two or more character strings. The concatenation operator is represented by two vertical bars (||)."
  },
  {
    "objectID": "LearningSas/prerequisites.html#variables-selection",
    "href": "LearningSas/prerequisites.html#variables-selection",
    "title": "Learning SAS",
    "section": "Variables Selection",
    "text": "Variables Selection\nIn order to select only some variables from the data source, the keep statement is used.\nDATA Outputs.mydata;\n    SET Inputs.mydata;\n    KEEP var1 var2 var3;\nRUN;"
  },
  {
    "objectID": "LearningSas/prerequisites.html#observation-filtration",
    "href": "LearningSas/prerequisites.html#observation-filtration",
    "title": "Learning SAS",
    "section": "Observation filtration",
    "text": "Observation filtration\n\nIf it is important to extract only observations m and n from the data source, the obs statement is used.\n\nDATA Outputs.mydata;\n    SET Inputs.mydata (firstobs=m obs=n);\nRUN;\n\nIn order to select observations that meet certain conditions, the WHERE statement is used.\n\nDATA Outputs.mydata;\n    SET Inputs.mydata;\n    WHERE CONDITION;\nRUN;"
  },
  {
    "objectID": "LearningSas/prerequisites.html#cumulative-sum-by-group-of-a-variable.",
    "href": "LearningSas/prerequisites.html#cumulative-sum-by-group-of-a-variable.",
    "title": "Learning SAS",
    "section": "Cumulative sum by group of a variable.",
    "text": "Cumulative sum by group of a variable.\n\nproc sort data=INPUTS.class_update out=class_sorted;\n   by Sex;\nrun;\n\ndata output;\n   set class_sorted(keep=Sex Age);\n   by Sex;\n   retain s_age;\n   if first.Sex then s_age = Age; /* Réinitialiser pour chaque groupe */\n   else s_age + Age; /* Cumul des âges */\n  \n   if last.Sex then output;\nrun;\n\nRUN;\n\nThis code compute the cumulative sum of the variable Age by group of the variable and then take the end last element of each group which corresponds to the cumulative sum of the variable Age by group.\n\n## Conactenation and control concatenation\n\n```default\nDATA Outputs.mydata; \n    SET table1 table2;\nRUN;\nDATA Outputs.mydata; \n    SET table1 table2;\n    BY ID;\nRUN;"
  },
  {
    "objectID": "LearningSas/prerequisites.html#compute-the-frequency-of-a-variable-in-a-dataset-and-the-frequency-of-missing-values-of-the-variable.",
    "href": "LearningSas/prerequisites.html#compute-the-frequency-of-a-variable-in-a-dataset-and-the-frequency-of-missing-values-of-the-variable.",
    "title": "Learning SAS",
    "section": "Compute the frequency of a variable in a dataset and the frequency of missing values of the variable.",
    "text": "Compute the frequency of a variable in a dataset and the frequency of missing values of the variable.\nPROC FREQ data = Outputs.mydata ;\n    table variable/missing;\nRUN;"
  },
  {
    "objectID": "LearningSas/prerequisites.html#frequency-on-each-sheet-of-an-excel-file",
    "href": "LearningSas/prerequisites.html#frequency-on-each-sheet-of-an-excel-file",
    "title": "Learning SAS",
    "section": "Frequency on each sheet of an Excel file",
    "text": "Frequency on each sheet of an Excel file\n\n/* Module : Macro \nInput : \n    - data : dataset\n    - list_var : macro variable containing the list of variables to compute the frequency\n    - path : path to save the excel file\nOutput :\n    - Excel file containing the frequency of the variables\n*/\n\n%macro export_freq(data, list_var, path);\n    %let nbvar = %sysfunc(countw(&list_var));\n\n    %do i = 1 %to &nbvar;\n        %let var = %scan(&list_var, &i);\n        PROC FREQ data = &data ;\n            tables &var /out = freq&var missing;\n        RUN;\n\n        PROC EXPORT data = freq&var\n            outfile = \"&path\"\n            dbms = xlsx replace;\n            sheet = \"&var\";\n        RUN;\n    %end;\n%mend export_freq;\n\n%export_freq(Outputs.mydata, var1 var2 var3, /home/u63691422/EPG1V2/Jumbong_Training/Outputs/freq.xlsx);"
  },
  {
    "objectID": "LearningSas/prerequisites.html#if-and-else-if-statement",
    "href": "LearningSas/prerequisites.html#if-and-else-if-statement",
    "title": "Learning SAS",
    "section": "if and else if statement",
    "text": "if and else if statement\ndata Outputs.mydata;\n    set;\n    format methode_agregee $20.;\n    if methode_eng_agregee_1 in(\"IRB\") or methode_eng_agregee_2 in(\"IRB\") then methode_agregee = \"IRB\";\n    else if methode_eng_agregee_1 in(\"PD\") or methode_eng_agregee_2 in(\"PD\") then methode_agregee = \"PD\";\n    else if methode_eng_agregee_1 in(\"LGD\") or methode_eng_agregee_2 in(\"LGD\") then methode_agregee = \"LGD\";\n    else methode_agregee = \"Non renseigné\";\nrun;"
  },
  {
    "objectID": "LearningSas/prerequisites.html#organizing-your-workspace-effectively",
    "href": "LearningSas/prerequisites.html#organizing-your-workspace-effectively",
    "title": "Learning SAS",
    "section": "“Organizing Your Workspace Effectively”",
    "text": "“Organizing Your Workspace Effectively”\nTo work efficiently, it is essential to structure your workspace properly. For a well-organized project, create a main directory named after the project. This directory should include at least three subfolders .\n\nInput (00_Inputs): Stores all raw data and input files.\n\nOutput (01_Codes): Contains all generated data and results.\n\nCodes (03_Outputs): Holds all scripts and working files, possibly with additional subfolders.\n\nEach file of each subfolder should resolve a specific task. Example : - The initialisation for all libraries should be in the file : 00_Libname_Init.sas - The file which contains all macros should be named : 00_Macros.sas"
  },
  {
    "objectID": "00_tds/model_selection.html",
    "href": "00_tds/model_selection.html",
    "title": "Model Selection",
    "section": "",
    "text": "Introduction\nWe will discuss the problem that may arise in multiple regression in practice. We have data with many variables but we may not want to include all of them in the model. We may not want to include all of them because some variables may be irrelevat, or because a smaller model with fewer variables has several advantages: It might give better predictions than a larger model and it is more parimonious [simple] then easier to interpret.In fact, as you add more variables, the bias of the model decreases, but the variance increases. This is known as the bias-variance trade-off. Too few variables yield high bias [this called underfitting].Too many covariates yields high variance [this called overfitting]. Good prediction requires a balance between bias and variance.\nMême après avoir utilisé une approche experte, [ou éliminer les rédondances entre les variables fortement corrélés, ou les variables qui ont un vif élévé dans le cadre de certains modèles de régressions] pour select relevant variables, il est possible que nous ayons encore trop de variables. A smaller model with fewer variables has several advantages: It might give better predictions than a larger model and it is more parimonious [simple] then easier to interpret. If we take the example of regression, as you add more variables, the bias of the model decreases, but the variance increases. This is known as the bias-variance trade-off. Too few variables yield high bias [this called underfitting].Too many covariates yields high variance [this called overfitting]. Good prediction requires a balance between bias and variance.\nC’est dans ce contexte que les mèthodes de sélection de variables sont utiles. There exists two main approaches to variable selection that we will explore : the first method consists to minimize the risk prediction and the second method assumes some subset of the \\(\\beta\\) coefficients are exactly equal to 0 and tries to find the true model, that is, the smallest sub-model consisting of nonzero \\(\\beta\\) terms.\n\n\n\n\n\n Back to topReferences\n\nWasserman, Larry. 2004. All of Statistics: A Concise Course in Statistical Inference. Springer Science & Business Media."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "Jumbong Junior\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "Meduim/RegressionLogistique/MissForest.html",
    "href": "Meduim/RegressionLogistique/MissForest.html",
    "title": "Missing data treatment and imputation in python",
    "section": "",
    "text": "Introduction\n# Importation des bibliothèques nécessaires import pandas as pd import numpy as np from sklearn.experimental import enable_iterative_imputer # Activation de l’IterativeImputer from sklearn.impute import IterativeImputer from sklearn.ensemble import RandomForestRegressor\n\n\nDéfinition de la fonction\ndef miss_forest_application_fold_mmsa(data_base_train, data_base_test, list_raw_ratios): # Assurez-vous que les valeurs manquantes sont bien définies comme np.nan # data_base_train = data_base_train.replace({None: np.nan, ‘NA’: np.nan, ‘null’: np.nan}) #data_base_test = data_base_test.replace({None: np.nan, ‘NA’: np.nan, ‘null’: np.nan})\n# Préparation des données d'entraînement\nbase_raw_ratios_train = data_base_train[list_raw_ratios].copy()\n\n# Imputation sur la base d'entraînement\nimputer = IterativeImputer(\n    estimator=RandomForestRegressor(n_estimators=100, random_state=42),\n    max_iter=10,\n    random_state=42\n)\nimp_train = pd.DataFrame(\n    imputer.fit_transform(base_raw_ratios_train),\n    columns=list_raw_ratios,\n    index=base_raw_ratios_train.index\n)\n\n# Remplacement des valeurs imputées dans la base d'entraînement originale\ndata_base_train.update(imp_train)\n\n# Imputation sur la base de test\nbase_raw_ratios_test = data_base_test[list_raw_ratios].copy()\nimp_test = pd.DataFrame(\n    imputer.transform(base_raw_ratios_test),\n    columns=list_raw_ratios,\n    index=base_raw_ratios_test.index\n)\n\n# Remplacement des valeurs imputées dans la base de test originale\ndata_base_test.update(imp_test)\n\n# Calcul des erreurs (facultatif)\nerrors = abs(imp_train - base_raw_ratios_train)\nrmse_train = pd.DataFrame({\n    \"ratios\": list_raw_ratios,\n    \"RMSE\": np.sqrt((errors**2).mean(axis=0))\n})\n\nprint(f\" The error is given by {errors}\")\n\n# Sélection des variables à garder ou à supprimer\nvars_to_keep = rmse_train.loc[rmse_train[\"RMSE\"] &lt; 1000, \"ratios\"]\nvars_to_drop = rmse_train.loc[rmse_train[\"RMSE\"] &gt; 0.5, \"ratios\"]\n\n# Filtrage des variables dans les bases d'entraînement et de test\nfinal_train = data_base_train.loc[:, ~data_base_train.columns.isin(vars_to_drop)]\nfinal_test = data_base_test.loc[:, data_base_test.columns.isin(vars_to_keep)]\n\n# Résultats finaux\nreturn {\n    \"final_train\": final_train,\n    \"final_test\": final_test\n}\n\n\nExemple de DataFrame\ndata_train = pd.DataFrame({ “A”: [1, 2, np.nan, 4], “B”: [np.nan, 2, 3, 4], “C”: [5, 6, 7, np.nan] })\ndata_test = pd.DataFrame({ “A”: [np.nan, 2, 3], “B”: [1, np.nan, 3], “C”: [4, 5, np.nan] })\nlist_vars = [“A”, “B”, “C”] results = miss_forest_application_fold_mmsa(data_train, data_test, list_vars)\nresults[“final_train”], results[“final_test”]\n\n```{python}\n\nimport pandas as pd\nimport numpy as np\nimport sys\nimport sklearn.neighbors._base\nsys.modules['sklearn.neighbors.base'] = sklearn.neighbors._base\nfrom missingpy import MissForest\n\n\n# No module named sklearn.neighbors.base\n\n# Sample DataFrame with missing values\ndata = {\n    'A': [1, 2, np.nan, 4],\n    'B': [np.nan, 2, 3, 4],\n    'C': ['cat', np.nan, 'dog', 'cat'],\n    'D': [1.5, 2.5, np.nan, 4.5]\n}\ndf = pd.DataFrame(data)\n\nprint(\"Original DataFrame:\")\nprint(df)\n\n# Convert categorical column to numerical codes (if needed)\ndf['C'] = df['C'].astype('category').cat.codes.replace(-1, np.nan)\n\n# Instantiate MissForest\nimputer = MissForest(random_state=42)\n\n# Impute missing values\nimputed_array = imputer.fit_transform(df.values)\n\n# Convert back to a DataFrame\ndf_imputed = pd.DataFrame(imputed_array, columns=df.columns)\n\n# Convert categorical columns back to original categories\ndf_imputed['C'] = df_imputed['C'].round().astype(int).map({0: 'cat', 1: 'dog'})\n\nprint(\"\\nDataFrame After Imputation:\")\nprint(df_imputed)\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "Others/spurious_reg.html",
    "href": "Others/spurious_reg.html",
    "title": "Linear Regression in Time Series: Sources of Spurious Regression",
    "section": "",
    "text": "It is common when analyzing the relationship between a dependent time series and several independent time series, to use the regression model. In their well know paper, Granger and Newbold (1974) found several articles in the literature, presenting regression models with apparently high goodness of fit, measured by the coefficient of determination, \\(R^2\\), but with very low durbin-watson statistics.\nWhat is particularly surprising is that almost all econometric textbook warns about the danger of autocorrelated errors, yet this issue persist in many published papers. Granger and Newbold (1974) identified several examples. For instance, they found published equations with \\(R^2 = 0.997\\) and the the Durbin-Watson statistic (d) equal to 0.53. The most extreme, the found is an equation with \\(R^2 = 0.999\\) and \\(d = 0.093\\).\nThese clear examples of what is called spurious regression, where the results look statistically impressive but are in fact misleading, falsely suggesting a strong relationship between the variables when no such relationship exists.\nThis honestly made me laugh because, during my internships, I saw many colleagues using regression models for time series data, evaluating performance purely based on the \\(R^2\\), especially when it was high (close to 1), along with metrics like the Mean Squared Error (MSE) or the Mean Absolute Error (MAE), without taking into account the autocorrelation of the residuals.\nWhen I came across this article, I realized how common this issue is. That is why I wanted to show you how crucial to always check the autocorrelation of the residuals when performing a regression analysis with time series data.\n\n\n\nOur post provides :\n\nA detailed explanation of the results from Granger and Newbold (1974).\nA Python simulation replicating the key results presented in their article.\n\n\n\n\nThe classic regression model tests assume independent data. However, in the case of time series, observations are not independent — they are autocorrelated, and sometimes we even observe what’s called serial correlation, which is the correlation between successive values of the series. For example, today’s GDP is strongly correlated with the GDP of the previous quarter. This autocorrelation of the data can lead to correlation in the errors, which influence the results of the regression analysis.\nThere are three main consequences of autocorrelated errors in regression analysis:\n\nEstimation of the coefficients of the model is inefficient.\nForecasts based on the regression equation are sub-optimal.\nThe hypothesis tests of the coefficients are invalid.\n\nThe first two points are well documented in the literature. However, Granger and Newbold (1974) focused specifically on the third point, showing that it’s possible to obtain very high \\(R^2\\) values, even though the models have no real economic meaning — what we call spurious regressions.\nTo support their analysis, the authors first present some key results from time series analysis. Then, they explain how nonsense regressions can occur, and finally, they run simulations to demonstrate that if the variables follow a random walk (or close to it), and if you include variables in the regression that shouldn’t be there, then it becomes the rule rather than the exception to obtain misleading results\nTo walk you through this paper, the next section will introduce the random walk and the ARIMA(0,1,1) process. In section 3, we will explain how Granger and Newbold (1974) describe the emergence of nonsense regressions, with examples illustrated in section 4. Finally, we’ll show how to avoid spurious regressions when working with time series data."
  },
  {
    "objectID": "Others/spurious_reg.html#motivation",
    "href": "Others/spurious_reg.html#motivation",
    "title": "Linear Regression in Time Series: Sources of Spurious Regression",
    "section": "",
    "text": "It is common when analyzing the relationship between a dependent time series and several independent time series, to use the regression model. In their well know paper, Granger and Newbold (1974) found several articles in the literature, presenting regression models with apparently high goodness of fit, measured by the coefficient of determination, \\(R^2\\), but with very low durbin-watson statistics.\nWhat is particularly surprising is that almost all econometric textbook warns about the danger of autocorrelated errors, yet this issue persist in many published papers. Granger and Newbold (1974) identified several examples. For instance, they found published equations with \\(R^2 = 0.997\\) and the the Durbin-Watson statistic (d) equal to 0.53. The most extreme, the found is an equation with \\(R^2 = 0.999\\) and \\(d = 0.093\\).\nThese clear examples of what is called spurious regression, where the results look statistically impressive but are in fact misleading, falsely suggesting a strong relationship between the variables when no such relationship exists.\nThis honestly made me laugh because, during my internships, I saw many colleagues using regression models for time series data, evaluating performance purely based on the \\(R^2\\), especially when it was high (close to 1), along with metrics like the Mean Squared Error (MSE) or the Mean Absolute Error (MAE), without taking into account the autocorrelation of the residuals.\nWhen I came across this article, I realized how common this issue is. That is why I wanted to show you how crucial to always check the autocorrelation of the residuals when performing a regression analysis with time series data."
  },
  {
    "objectID": "Others/spurious_reg.html#contribution",
    "href": "Others/spurious_reg.html#contribution",
    "title": "Linear Regression in Time Series: Sources of Spurious Regression",
    "section": "",
    "text": "Our post provides :\n\nA detailed explanation of the results from Granger and Newbold (1974).\nA Python simulation replicating the key results presented in their article."
  },
  {
    "objectID": "Others/spurious_reg.html#objectives",
    "href": "Others/spurious_reg.html#objectives",
    "title": "Linear Regression in Time Series: Sources of Spurious Regression",
    "section": "",
    "text": "The classic regression model tests assume independent data. However, in the case of time series, observations are not independent — they are autocorrelated, and sometimes we even observe what’s called serial correlation, which is the correlation between successive values of the series. For example, today’s GDP is strongly correlated with the GDP of the previous quarter. This autocorrelation of the data can lead to correlation in the errors, which influence the results of the regression analysis.\nThere are three main consequences of autocorrelated errors in regression analysis:\n\nEstimation of the coefficients of the model is inefficient.\nForecasts based on the regression equation are sub-optimal.\nThe hypothesis tests of the coefficients are invalid.\n\nThe first two points are well documented in the literature. However, Granger and Newbold (1974) focused specifically on the third point, showing that it’s possible to obtain very high \\(R^2\\) values, even though the models have no real economic meaning — what we call spurious regressions.\nTo support their analysis, the authors first present some key results from time series analysis. Then, they explain how nonsense regressions can occur, and finally, they run simulations to demonstrate that if the variables follow a random walk (or close to it), and if you include variables in the regression that shouldn’t be there, then it becomes the rule rather than the exception to obtain misleading results\nTo walk you through this paper, the next section will introduce the random walk and the ARIMA(0,1,1) process. In section 3, we will explain how Granger and Newbold (1974) describe the emergence of nonsense regressions, with examples illustrated in section 4. Finally, we’ll show how to avoid spurious regressions when working with time series data."
  },
  {
    "objectID": "Others/spurious_reg.html#random-walk",
    "href": "Others/spurious_reg.html#random-walk",
    "title": "Linear Regression in Time Series: Sources of Spurious Regression",
    "section": "Random Walk",
    "text": "Random Walk\nLet \\(X_t\\) be a time series. We say that \\(X_t\\) follows a random walk if its representation is given by:\n\\[\nX_t = X_{t-1} + \\epsilon_t\n\\tag{1}\\]\nwhere \\(\\epsilon_t\\) is a white noise. It can be writen as a sum of white noise :\n\\[\nX_t =  X_0 + \\sum_{i=1}^{t} \\epsilon_i\n\\tag{2}\\]\nwhere \\(X_0\\) is the initial value of the series.\nThe random walk is a non-stationary time series. In fact, if we take the Variance of \\(X_t\\), we have:\n\\[\nV(X_t) = t\\sigma^2\n\\tag{3}\\]\nwhich is increasing with time.\nThis model have been found to represent well certain price series, particularly in speculative markets.\nFor many others series,\n\\[\nX_t - X_{t-1} = \\epsilon_t - \\theta \\epsilon_{t-1}\n\\tag{4}\\]\nhas been found to provide a good representation.\nThose non-stationary series are often employed as bench-marks against which the forecasting performance of other models is judged."
  },
  {
    "objectID": "Others/spurious_reg.html#arima011-process",
    "href": "Others/spurious_reg.html#arima011-process",
    "title": "Linear Regression in Time Series: Sources of Spurious Regression",
    "section": "ARIMA(0,1,1) Process",
    "text": "ARIMA(0,1,1) Process\nThe ARIMA(0,1,1) process is given by:\n\\[\nX_t = X_{t-1} + \\epsilon_t - \\theta \\epsilon_{t-1}\n\\tag{5}\\]\nwhere \\(\\epsilon_t\\) is a white noise. The ARIMA(0,1,1) process is non-stationary. It can be written as a sum of independent random walk and white noise :\n\\[\nX_t =  X_0 + random walk + white noise\n\\tag{6}\\]\nIn the next section, we will see how Nonsense Regression can arise when we regress a dependent series on independent series that follow a random walk."
  },
  {
    "objectID": "00_tds/fourrier_transformation.html",
    "href": "00_tds/fourrier_transformation.html",
    "title": "Fourrier Transformation",
    "section": "",
    "text": "The primary goal of the Fourier transformation is to analyze signals by decomposing them into their constituent frequencies. It was Joseph Fourier who first recognized the significance of spectral decomposition of a signal. Indeed, he demonstrated that any periodic signal can be decomposed into a finite sum of sinusoidal signals with constant frequencies and amplitudes. The finite set of these frequencies constitutes the spectrum of the signal.\nBeyond spectral analysis, the Fourier transformation is employed in solving partial differential equations, evaluating integrals, and computing sums of series. It has applications in various fields, including physics, engineering, and signal processing. In recent years, it has found new uses in finance, such as estimating financial asset volatility, which We will cover in future posts.\nIn this article, we explore the numerical computation of the Fourier transform for both real and complex functions. Drawing on the work of Balac (2011), who proposed a quadrature-based approach, we illustrate how this method can be applied. We also demonstrate how the Fast Fourier Transform (FFT) algorithm (for more details see Cooley and Tukey 1965) enables efficient computation of the discrete Fourier transform, making it well-suited for calculating the Fourier transform of integrable functions and the Fourier coefficients of periodic functions.\nThis article was motivated by the realization that the numerical methods implemented in Python—such as [scipy.fft and numpy.fft] do not directly compute the Fourier transform of a function. This observation had already been made by Balac (2011) in the context of MATLAB.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.fft import fft, fftfreq, fftshift\n\n# Define the function f(t) = exp(-pi * t^2)\ndef f(t):\n    return np.exp(-np.pi * t**2)\n\n# Parameters\nN = 1024\nT = 1.0 / 64\nt = np.linspace(-N/2*T, N/2*T, N, endpoint=False)\ny = f(t)\n\n# FFT with scipy\nyf_scipy = fftshift(fft(y)) * T\nxf = fftshift(fftfreq(N, T))\nFT_exact = np.exp(-np.pi * xf**2)\n\n# FFT with numpy\nyf_numpy = np.fft.fftshift(np.fft.fft(y)) * T\nxf_numpy = np.fft.fftshift(np.fft.fftfreq(N, T))\n\n# Plot with subplot_mosaic\nfig, axs = plt.subplot_mosaic([[\"scipy\", \"numpy\"]], figsize=(14, 5), layout=\"constrained\", sharey=True)\n\n# Scipy FFT\naxs[\"scipy\"].plot(xf, FT_exact, 'k-', linewidth=1.5, label='Exact FT')\naxs[\"scipy\"].plot(xf, np.real(yf_scipy), 'r--', linewidth=1, label='FFT (scipy)')\naxs[\"scipy\"].set_xlim(-6, 6)\naxs[\"scipy\"].set_ylim(-1, 1)\naxs[\"scipy\"].set_title(\"Scipy FFT\")\naxs[\"scipy\"].set_xlabel(\"Frequency\")\naxs[\"scipy\"].set_ylabel(\"Amplitude\")\naxs[\"scipy\"].legend()\naxs[\"scipy\"].grid(False)\n\n# NumPy FFT\naxs[\"numpy\"].plot(xf_numpy, FT_exact, 'k-', linewidth=1.5, label='Exact FT')\naxs[\"numpy\"].plot(xf_numpy, np.real(yf_numpy), 'b--', linewidth=1, label='FFT (numpy)')\naxs[\"numpy\"].set_xlim(-6, 6)\naxs[\"numpy\"].set_title(\"NumPy FFT\")\naxs[\"numpy\"].set_xlabel(\"Frequency\")\naxs[\"numpy\"].legend()\naxs[\"numpy\"].grid(False)\n\nplt.suptitle(\"Comparison of FFT Implementations vs. Exact Fourier Transform\", fontsize=14)\nplt.show()\n\n\n\n\n\n\n\n\n\n\nWe must first define the space in which the function we want to compute the Fourier transform is defined and the caracteristics they must satisfy so that the Fourier transform exists. These functions are defined in the space of integrable functions, denoted by \\(L^1(\\mathbb{R,K})\\) which are functions \\(f:\\mathbb{R} \\to \\mathbb{K}\\) [where \\(\\mathbb{K}\\) is either \\(\\mathbb{R}\\) or \\(\\mathbb{C}\\)]. These functions are integrable in the sense of Lebesgue, meaning that the integral of their absolute value is finite: \\[\n\\int_{-\\infty}^{+\\infty} |f(t)| dt &lt; +\\infty.\n\\]\nSo for a function \\(f\\) to be in \\(L^1(\\mathbb{R,K})\\), the product \\(f(t) \\cdot e^{-2i\\pi\\nu t}\\) is integrable for all \\(\\nu \\in \\mathbb{R}\\). And for all \\(\\nu \\in \\mathbb{R}\\), the function \\(\\hat{f}\\) [notée egalement \\(F(f)\\)] defined by \\[\n\\hat{f}(\\nu) = \\int_{-\\infty}^{+\\infty}\nf(x) e^{-2i\\pi\\nu t} dt\n\\]\nis well-defined and is called the Fourier transform of \\(f\\).\nWe deduce in this formula that a Fourier transform is a complex-valued function, and it is a linear operator. We can also deduce others properties of the Fourier transform in particular for a function \\(f\\) in \\(L^1(\\mathbb{R,R})\\), une fonction à valeurs réelles :\n\nTranslation property: If \\(f\\) is in \\(L^1(\\mathbb{R,K})\\), \\(t_0 \\in \\mathbb{R}\\), and \\(h : t \\in \\mathbb{R} \\mapsto f(t - t_0)\\), we have \\(h \\in L^1(\\mathbb{R,K})\\) and \\(\\hat{h}(\\nu) = e^{-2i\\pi\\nu t_0} \\hat{f}(\\nu)\\).\nHomothety property: If \\(f\\) is in \\(L^1(\\mathbb{R,K})\\) and \\(a \\in \\mathbb{R}\\), then the function \\(g : t \\in \\mathbb{R} \\mapsto f(at)\\) is in \\(L^1(\\mathbb{R,K})\\) and \\(\\hat{g}(\\nu) = \\frac{1}{|a|} \\hat{f}\\left(\\frac{\\nu}{a}\\right)\\).\nModulation property: If \\(f\\) is in \\(L^1(\\mathbb{R,K})\\) and \\(\\nu_0 \\in \\mathbb{R}\\), and \\(h : t \\in \\mathbb{R} \\mapsto f(t) e^{2i\\pi\\nu_0 t}\\), we have \\(h \\in L^1(\\mathbb{R,K})\\) and \\(\\hat{h}(\\nu) = \\hat{f}(\\nu - \\nu_0)\\).\nIf \\(f \\in L^1(\\mathbb{R,R})\\) and est pair, then \\(\\hat{f}\\) is also real-valued and the real part of \\(\\hat{f}\\) is also an even function.\nIf \\(f\\) is in \\(L^1(\\mathbb{R,R})\\) and is odd, then \\(\\hat{f}\\) is est une fonction imaginaire pure and the imaginary part of \\(\\hat{f}\\) is also an odd function.\n\nFor some functions, the Fourier transform can be computed analytically. For example, for the function \\(f : t \\in \\mathbb{R} \\mapsto \\mathbb{1}_{[-\\frac{a}{2}, \\frac{a}{2}]}(t)\\), the Fourier transform is given by: \\[\n\\hat{f} : \\nu \\in \\mathbb{R} \\mapsto a sinc(a \\pi \\nu)\n\\] where \\(sinc(t) = \\frac{\\sin(t)}{t}\\) is the sinc function.\nHowever, for many functions, the Fourier transform cannot be computed analytically. In such cases, we can use numerical methods to approximate the Fourier transform. We will explore these numerical dans la suite of this article.\n\n\n\nThe Fourier transform of a function \\(f\\) is defined as an integral over the entire real line. However, for the functions that are integral in the sense of lebesgue and that have a practical applications tend to 0 as \\(|t| \\to +\\infty\\). And we can approximate the Fourier transform by integrating over a finite interval \\([-T, T]\\). If the lenght of the interval is large enough, or if the function decays quickly when t tends to infinity, this approximation will be accurate.\n\\[\n\\hat{f}(\\nu) \\approx \\int_{-T}^{T} f(t) e^{-2i\\pi\\nu t} dt\n\\]\nIn his article, Balac (2011) va plus loin en montrant qu’approximating the Fourier transform met en oeuvre trois outils mathématiques :\n\nLes séries de Fourier dans le cadre d’un signal périodique.\nLa transformée de Fourier dans le cadre d’un signal non périodique.\nLa transformée de Fourier discrète dans le cadre d’un signal discret.\n\nAnd for each of these tools, computing the Fourier transform revient à calculer l’intgrale below: \\[\n\\hat{f}(\\nu) \\approx \\int_{-\\frac{T}{2}}^{\\frac{T}{2}} f(t) e^{-2i\\pi\\nu t} dt\n\\] I recommends reading his article for more details on these three tools. Then we will focus on the numerical computation of the Fourier transform using quadrature methods, which is a numerical integration technique.\n\n\n\nWe show that to compute the Fourier transform of a function \\(f\\) consists to approximating it by the integral below in the interval \\([-\\frac{T}{2}, \\frac{T}{2}]\\): \\[\n\\underbrace{\n  \\int_{-\\infty}^{+\\infty} f(t)\\, e^{-2\\pi i \\nu t} \\, dt\n}_{=\\,\\hat{f}(\\nu)}\n\\approx\n\\underbrace{\n  \\int_{-\\frac{T}{2}}^{\\frac{T}{2}} f(t)\\, e^{-2\\pi i \\nu t} \\, dt\n}_{=\\,\\tilde{S}(\\nu)}\n\\]\nwhere T is a large enough number such that the integral converges. Une valeur approchée of the integral \\(\\tilde{S}(\\nu) = \\int_{-\\frac{T}{2}}^{\\frac{T}{2}} f(t)\\, e^{-2\\pi i \\nu t} \\, dt\\) can be computed using quadrature methods. In the next section, we will approximate the integral using the quadrature method of rectangles à gauche.\n\n\n\nFor computing the integral \\(\\tilde{S}(\\nu) = \\int_{-\\frac{T}{2}}^{\\frac{T}{2}} f(t)\\, e^{-2\\pi i \\nu t} \\, dt\\), using the quadrature method of rectangles à gauche, we must respect the following steps: 1. Discretization of the Interval: We discretize the interval \\([-\\frac{T}{2}, \\frac{T}{2}]\\) into \\(N\\) uniform subintervals of length \\(h_t = \\frac{T}{N}\\). The points in the interval are given by:\n\\[\nt_k = -\\frac{T}{2} + k \\cdot h_t, \\quad k = 0, 1, \\ldots, N-1.\n\\]\n\nApproximation of the Integral: Using the Chasles relation, we can approximate the integral \\(\\tilde{S}(\\nu)\\) as follows:\n\n\\[\n\\tilde{S}(\\nu) = \\int_{-\\frac{T}{2}}^{\\frac{T}{2}} f(t)\\, e^{-2\\pi i \\nu t} \\, dt = \\sum_{k=0}^{N-1}  \\int_{t_k}^{t_{k+1}} f(t)\\, e^{-2\\pi i \\nu t} \\, dt.\n\\]\nBy taking into account that we have \\(t_{k+1} - t_k = h_t\\), and \\(t_k = -\\frac{T}{2} + k \\cdot h_t = T(\\frac{k}{N} - \\frac{1}{2})\\), we can rewrite the integral as: \\[\n\\tilde{S}(\\nu) = \\sum_{k=0}^{N-1} f(t_k) e^{-2\\pi i \\nu t_k} h_t.\n\\] 3. Final Formula: The final formula for the approximation of the Fourier transform is given by:\n\\[\n\\boxed{\n\\forall \\nu \\in \\mathbb{R} \\quad\n\\underbrace{\n\\int_{-\\frac{T}{2}}^{\\frac{T}{2}} f(t)\\, e^{-2\\pi i \\nu t} \\, dt\n}_{=\\,\\tilde{S}(\\nu)}\n\\approx\n\\underbrace{\n\\frac{T}{n} e^{i \\pi \\nu T} \\sum_{k=0}^{n-1} f_k\\, e^{-2 i \\pi \\nu T k / n}\n}_{=\\,\\tilde{S}_n(\\nu)}\n\\quad \\text{où } f_k = f\\left( \\frac{2k - n}{2n} T \\right).\n}\n\\]\n\n\nThe function tfquad below implements the quadrature method of rectangles à gauche to compute the Fourier transform of a function f at a given frequency nu.\n\nimport numpy as np\n\ndef tfquad(f, nu, n, T):\n    \"\"\"\n    Computes the Fourier transform of a function f at frequency nu\n    using left Riemann sum quadrature over the interval [-T/2, T/2].\n\n    Parameters:\n    ----------\n    f : callable\n        The function to transform. Must accept a NumPy array as input.\n    nu : float\n        The frequency at which to evaluate the Fourier transform.\n    n : int\n        Number of quadrature points.\n    T : float\n        Width of the time window [-T/2, T/2].\n\n    Returns:\n    -------\n    tfnu : complex\n        Approximated value of the Fourier transform at frequency nu.\n    \"\"\"\n    k = np.arange(n)\n    t_k = (k / n - 0.5) * T\n    weights = np.exp(-2j * np.pi * nu * T * k / n)\n    prefactor = (T / n) * np.exp(1j * np.pi * nu * T)\n\n    return prefactor * np.sum(f(t_k) * weights)\n\nWe can also use the function scipy’s quad functionnfor defining the Fourier transform of a function f at a given frequency nu. The function tf_integral below implements this approach. It uses numerical integration to compute the Fourier transform of a function f over the interval \\([-T/2, T/2]\\).\n\nfrom scipy.integrate import quad\n\ndef tf_integral(f, nu, T):\n    \"\"\"Compute FT of f at frequency nu over [-T/2, T/2] using scipy quad.\"\"\"\n    real_part = quad(lambda t: np.real(f(t) * np.exp(-2j * np.pi * nu * t)), -T/2, T/2)[0]\n    imag_part = quad(lambda t: np.imag(f(t) * np.exp(-2j * np.pi * nu * t)), -T/2, T/2)[0]\n    return real_part + 1j * imag_part\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# ----- Function Definitions -----\n\ndef f(t):\n    \"\"\"Indicator function on [-1, 1].\"\"\"\n    return np.where(np.abs(t) &lt;= 1, 1.0, 0.0)\n\ndef exact_fourier_transform(nu):\n    \"\"\"Analytical FT of the indicator function over [-1, 1].\"\"\"\n    # f̂(ν) = ∫_{-1}^{1} e^{-2πiνt} dt = 2 * sinc(2ν)\n    return 2 * np.sinc(2 * nu)\n\n\n# ----- Computation -----\n\nT = 2.0\nn = 32\nnu_vals = np.linspace(-6, 6, 500)\nexact_vals = exact_fourier_transform(nu_vals)\napprox_vals = np.array([tfquad(f, nu, n, T) for nu in nu_vals])\n\n\nfig, axs = plt.subplot_mosaic([[\"quad\", \"tfquad\"]], figsize=(12, 4), layout=\"constrained\")\n\n# Plot using scipy.integrate.quad\naxs[\"quad\"].plot(nu_vals, np.real(exact_vals), 'b-', linewidth=2, label=r'$\\hat{f}$ (quad)')\naxs[\"quad\"].plot(nu_vals, np.real(approx_vals), 'r--', linewidth=1.5, label=r'approximation $\\hat{S}_n$')\naxs[\"quad\"].set_title(\"TF avec scipy.integrate.quad\")\naxs[\"quad\"].set_xlabel(r'$\\nu$')\naxs[\"quad\"].set_ylabel('Amplitude')\naxs[\"quad\"].grid(False)\naxs[\"quad\"].legend()\naxs[\"quad\"].set_ylim(-0.5, 2.1)\n\n# Plot using tfquad implementation\naxs[\"tfquad\"].plot(nu_vals, np.real(exact_vals), 'b-', linewidth=2, label=r'$\\hat{f}$ (exact)')\naxs[\"tfquad\"].plot(nu_vals, np.real(approx_vals), 'r--', linewidth=1.5, label=r'approximation $\\hat{S}_n$')\naxs[\"tfquad\"].set_title(\"TF avec tfquad (rectangles)\")\naxs[\"tfquad\"].set_xlabel(r'$\\nu$')\naxs[\"tfquad\"].grid(False)\naxs[\"tfquad\"].legend()\naxs[\"tfquad\"].set_ylim(-0.5, 2.1)\n\nplt.show()"
  },
  {
    "objectID": "00_tds/fourrier_transformation.html#definition-and-properties-of-the-fourier-transform",
    "href": "00_tds/fourrier_transformation.html#definition-and-properties-of-the-fourier-transform",
    "title": "Fourrier Transformation",
    "section": "",
    "text": "We must first define the space in which the function we want to compute the Fourier transform is defined and the caracteristics they must satisfy so that the Fourier transform exists. These functions are defined in the space of integrable functions, denoted by \\(L^1(\\mathbb{R,K})\\) which are functions \\(f:\\mathbb{R} \\to \\mathbb{K}\\) [where \\(\\mathbb{K}\\) is either \\(\\mathbb{R}\\) or \\(\\mathbb{C}\\)]. These functions are integrable in the sense of Lebesgue, meaning that the integral of their absolute value is finite: \\[\n\\int_{-\\infty}^{+\\infty} |f(t)| dt &lt; +\\infty.\n\\]\nSo for a function \\(f\\) to be in \\(L^1(\\mathbb{R,K})\\), the product \\(f(t) \\cdot e^{-2i\\pi\\nu t}\\) is integrable for all \\(\\nu \\in \\mathbb{R}\\). And for all \\(\\nu \\in \\mathbb{R}\\), the function \\(\\hat{f}\\) [notée egalement \\(F(f)\\)] defined by \\[\n\\hat{f}(\\nu) = \\int_{-\\infty}^{+\\infty}\nf(x) e^{-2i\\pi\\nu t} dt\n\\]\nis well-defined and is called the Fourier transform of \\(f\\).\nWe deduce in this formula that a Fourier transform is a complex-valued function, and it is a linear operator. We can also deduce others properties of the Fourier transform in particular for a function \\(f\\) in \\(L^1(\\mathbb{R,R})\\), une fonction à valeurs réelles :\n\nTranslation property: If \\(f\\) is in \\(L^1(\\mathbb{R,K})\\), \\(t_0 \\in \\mathbb{R}\\), and \\(h : t \\in \\mathbb{R} \\mapsto f(t - t_0)\\), we have \\(h \\in L^1(\\mathbb{R,K})\\) and \\(\\hat{h}(\\nu) = e^{-2i\\pi\\nu t_0} \\hat{f}(\\nu)\\).\nHomothety property: If \\(f\\) is in \\(L^1(\\mathbb{R,K})\\) and \\(a \\in \\mathbb{R}\\), then the function \\(g : t \\in \\mathbb{R} \\mapsto f(at)\\) is in \\(L^1(\\mathbb{R,K})\\) and \\(\\hat{g}(\\nu) = \\frac{1}{|a|} \\hat{f}\\left(\\frac{\\nu}{a}\\right)\\).\nModulation property: If \\(f\\) is in \\(L^1(\\mathbb{R,K})\\) and \\(\\nu_0 \\in \\mathbb{R}\\), and \\(h : t \\in \\mathbb{R} \\mapsto f(t) e^{2i\\pi\\nu_0 t}\\), we have \\(h \\in L^1(\\mathbb{R,K})\\) and \\(\\hat{h}(\\nu) = \\hat{f}(\\nu - \\nu_0)\\).\nIf \\(f \\in L^1(\\mathbb{R,R})\\) and est pair, then \\(\\hat{f}\\) is also real-valued and the real part of \\(\\hat{f}\\) is also an even function.\nIf \\(f\\) is in \\(L^1(\\mathbb{R,R})\\) and is odd, then \\(\\hat{f}\\) is est une fonction imaginaire pure and the imaginary part of \\(\\hat{f}\\) is also an odd function.\n\nFor some functions, the Fourier transform can be computed analytically. For example, for the function \\(f : t \\in \\mathbb{R} \\mapsto \\mathbb{1}_{[-\\frac{a}{2}, \\frac{a}{2}]}(t)\\), the Fourier transform is given by: \\[\n\\hat{f} : \\nu \\in \\mathbb{R} \\mapsto a sinc(a \\pi \\nu)\n\\] where \\(sinc(t) = \\frac{\\sin(t)}{t}\\) is the sinc function.\nHowever, for many functions, the Fourier transform cannot be computed analytically. In such cases, we can use numerical methods to approximate the Fourier transform. We will explore these numerical dans la suite of this article."
  },
  {
    "objectID": "00_tds/fourrier_transformation.html#how-to-approximate-the-fourier-transform",
    "href": "00_tds/fourrier_transformation.html#how-to-approximate-the-fourier-transform",
    "title": "Fourrier Transformation",
    "section": "",
    "text": "The Fourier transform of a function \\(f\\) is defined as an integral over the entire real line. However, for the functions that are integral in the sense of lebesgue and that have a practical applications tend to 0 as \\(|t| \\to +\\infty\\). And we can approximate the Fourier transform by integrating over a finite interval \\([-T, T]\\). If the lenght of the interval is large enough, or if the function decays quickly when t tends to infinity, this approximation will be accurate.\n\\[\n\\hat{f}(\\nu) \\approx \\int_{-T}^{T} f(t) e^{-2i\\pi\\nu t} dt\n\\]\nIn his article, Balac (2011) va plus loin en montrant qu’approximating the Fourier transform met en oeuvre trois outils mathématiques :\n\nLes séries de Fourier dans le cadre d’un signal périodique.\nLa transformée de Fourier dans le cadre d’un signal non périodique.\nLa transformée de Fourier discrète dans le cadre d’un signal discret.\n\nAnd for each of these tools, computing the Fourier transform revient à calculer l’intgrale below: \\[\n\\hat{f}(\\nu) \\approx \\int_{-\\frac{T}{2}}^{\\frac{T}{2}} f(t) e^{-2i\\pi\\nu t} dt\n\\] I recommends reading his article for more details on these three tools. Then we will focus on the numerical computation of the Fourier transform using quadrature methods, which is a numerical integration technique."
  },
  {
    "objectID": "00_tds/fourrier_transformation.html#numerical-computation-of-the-fourier-transform",
    "href": "00_tds/fourrier_transformation.html#numerical-computation-of-the-fourier-transform",
    "title": "Fourrier Transformation",
    "section": "",
    "text": "We show that to compute the Fourier transform of a function \\(f\\) consists to approximating it by the integral below in the interval \\([-\\frac{T}{2}, \\frac{T}{2}]\\): \\[\n\\underbrace{\n  \\int_{-\\infty}^{+\\infty} f(t)\\, e^{-2\\pi i \\nu t} \\, dt\n}_{=\\,\\hat{f}(\\nu)}\n\\approx\n\\underbrace{\n  \\int_{-\\frac{T}{2}}^{\\frac{T}{2}} f(t)\\, e^{-2\\pi i \\nu t} \\, dt\n}_{=\\,\\tilde{S}(\\nu)}\n\\]\nwhere T is a large enough number such that the integral converges. Une valeur approchée of the integral \\(\\tilde{S}(\\nu) = \\int_{-\\frac{T}{2}}^{\\frac{T}{2}} f(t)\\, e^{-2\\pi i \\nu t} \\, dt\\) can be computed using quadrature methods. In the next section, we will approximate the integral using the quadrature method of rectangles à gauche."
  },
  {
    "objectID": "00_tds/fourrier_transformation.html#quadrature-method-of-rectangles-à-gauche",
    "href": "00_tds/fourrier_transformation.html#quadrature-method-of-rectangles-à-gauche",
    "title": "Fourrier Transformation",
    "section": "",
    "text": "For computing the integral \\(\\tilde{S}(\\nu) = \\int_{-\\frac{T}{2}}^{\\frac{T}{2}} f(t)\\, e^{-2\\pi i \\nu t} \\, dt\\), using the quadrature method of rectangles à gauche, we must respect the following steps: 1. Discretization of the Interval: We discretize the interval \\([-\\frac{T}{2}, \\frac{T}{2}]\\) into \\(N\\) uniform subintervals of length \\(h_t = \\frac{T}{N}\\). The points in the interval are given by:\n\\[\nt_k = -\\frac{T}{2} + k \\cdot h_t, \\quad k = 0, 1, \\ldots, N-1.\n\\]\n\nApproximation of the Integral: Using the Chasles relation, we can approximate the integral \\(\\tilde{S}(\\nu)\\) as follows:\n\n\\[\n\\tilde{S}(\\nu) = \\int_{-\\frac{T}{2}}^{\\frac{T}{2}} f(t)\\, e^{-2\\pi i \\nu t} \\, dt = \\sum_{k=0}^{N-1}  \\int_{t_k}^{t_{k+1}} f(t)\\, e^{-2\\pi i \\nu t} \\, dt.\n\\]\nBy taking into account that we have \\(t_{k+1} - t_k = h_t\\), and \\(t_k = -\\frac{T}{2} + k \\cdot h_t = T(\\frac{k}{N} - \\frac{1}{2})\\), we can rewrite the integral as: \\[\n\\tilde{S}(\\nu) = \\sum_{k=0}^{N-1} f(t_k) e^{-2\\pi i \\nu t_k} h_t.\n\\] 3. Final Formula: The final formula for the approximation of the Fourier transform is given by:\n\\[\n\\boxed{\n\\forall \\nu \\in \\mathbb{R} \\quad\n\\underbrace{\n\\int_{-\\frac{T}{2}}^{\\frac{T}{2}} f(t)\\, e^{-2\\pi i \\nu t} \\, dt\n}_{=\\,\\tilde{S}(\\nu)}\n\\approx\n\\underbrace{\n\\frac{T}{n} e^{i \\pi \\nu T} \\sum_{k=0}^{n-1} f_k\\, e^{-2 i \\pi \\nu T k / n}\n}_{=\\,\\tilde{S}_n(\\nu)}\n\\quad \\text{où } f_k = f\\left( \\frac{2k - n}{2n} T \\right).\n}\n\\]\n\n\nThe function tfquad below implements the quadrature method of rectangles à gauche to compute the Fourier transform of a function f at a given frequency nu.\n\nimport numpy as np\n\ndef tfquad(f, nu, n, T):\n    \"\"\"\n    Computes the Fourier transform of a function f at frequency nu\n    using left Riemann sum quadrature over the interval [-T/2, T/2].\n\n    Parameters:\n    ----------\n    f : callable\n        The function to transform. Must accept a NumPy array as input.\n    nu : float\n        The frequency at which to evaluate the Fourier transform.\n    n : int\n        Number of quadrature points.\n    T : float\n        Width of the time window [-T/2, T/2].\n\n    Returns:\n    -------\n    tfnu : complex\n        Approximated value of the Fourier transform at frequency nu.\n    \"\"\"\n    k = np.arange(n)\n    t_k = (k / n - 0.5) * T\n    weights = np.exp(-2j * np.pi * nu * T * k / n)\n    prefactor = (T / n) * np.exp(1j * np.pi * nu * T)\n\n    return prefactor * np.sum(f(t_k) * weights)\n\nWe can also use the function scipy’s quad functionnfor defining the Fourier transform of a function f at a given frequency nu. The function tf_integral below implements this approach. It uses numerical integration to compute the Fourier transform of a function f over the interval \\([-T/2, T/2]\\).\n\nfrom scipy.integrate import quad\n\ndef tf_integral(f, nu, T):\n    \"\"\"Compute FT of f at frequency nu over [-T/2, T/2] using scipy quad.\"\"\"\n    real_part = quad(lambda t: np.real(f(t) * np.exp(-2j * np.pi * nu * t)), -T/2, T/2)[0]\n    imag_part = quad(lambda t: np.imag(f(t) * np.exp(-2j * np.pi * nu * t)), -T/2, T/2)[0]\n    return real_part + 1j * imag_part\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# ----- Function Definitions -----\n\ndef f(t):\n    \"\"\"Indicator function on [-1, 1].\"\"\"\n    return np.where(np.abs(t) &lt;= 1, 1.0, 0.0)\n\ndef exact_fourier_transform(nu):\n    \"\"\"Analytical FT of the indicator function over [-1, 1].\"\"\"\n    # f̂(ν) = ∫_{-1}^{1} e^{-2πiνt} dt = 2 * sinc(2ν)\n    return 2 * np.sinc(2 * nu)\n\n\n# ----- Computation -----\n\nT = 2.0\nn = 32\nnu_vals = np.linspace(-6, 6, 500)\nexact_vals = exact_fourier_transform(nu_vals)\napprox_vals = np.array([tfquad(f, nu, n, T) for nu in nu_vals])\n\n\nfig, axs = plt.subplot_mosaic([[\"quad\", \"tfquad\"]], figsize=(12, 4), layout=\"constrained\")\n\n# Plot using scipy.integrate.quad\naxs[\"quad\"].plot(nu_vals, np.real(exact_vals), 'b-', linewidth=2, label=r'$\\hat{f}$ (quad)')\naxs[\"quad\"].plot(nu_vals, np.real(approx_vals), 'r--', linewidth=1.5, label=r'approximation $\\hat{S}_n$')\naxs[\"quad\"].set_title(\"TF avec scipy.integrate.quad\")\naxs[\"quad\"].set_xlabel(r'$\\nu$')\naxs[\"quad\"].set_ylabel('Amplitude')\naxs[\"quad\"].grid(False)\naxs[\"quad\"].legend()\naxs[\"quad\"].set_ylim(-0.5, 2.1)\n\n# Plot using tfquad implementation\naxs[\"tfquad\"].plot(nu_vals, np.real(exact_vals), 'b-', linewidth=2, label=r'$\\hat{f}$ (exact)')\naxs[\"tfquad\"].plot(nu_vals, np.real(approx_vals), 'r--', linewidth=1.5, label=r'approximation $\\hat{S}_n$')\naxs[\"tfquad\"].set_title(\"TF avec tfquad (rectangles)\")\naxs[\"tfquad\"].set_xlabel(r'$\\nu$')\naxs[\"tfquad\"].grid(False)\naxs[\"tfquad\"].legend()\naxs[\"tfquad\"].set_ylim(-0.5, 2.1)\n\nplt.show()"
  },
  {
    "objectID": "00_tds/fourrier_transformation.html#characterization-of-the-approximation",
    "href": "00_tds/fourrier_transformation.html#characterization-of-the-approximation",
    "title": "Fourrier Transformation",
    "section": "",
    "text": "We observe that the transform \\(\\hat{f}\\) of the function \\(f\\) is oscillatory. This oscillatory nature is due to the complex exponential term \\(e^{-2\\pi i \\nu t}\\) in the integral.\nTo illustrate this, the figure below represents the function \\(f : t \\in \\mathbb{R} \\mapsto e^{-t^2} \\in \\mathbb{R}\\), ainsi que les parties réelle et imaginaire de sa transformée de Fourier \\(\\hat{f} : \\nu \\in \\mathbb{R} \\mapsto \\hat{f}(\\nu) \\in \\mathbb{C}\\), for \\(\\nu = \\frac{5}{2}\\). Bien que la fonction \\(f\\) soit lisse, on observe des fortes oscillations pour la fonction \\(\\hat{f}\\),.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nnu = 5 / 2\nt1 = np.linspace(-8, 8, 1000)\nt2 = np.linspace(-4, 4, 1000)\n\nf = lambda t: np.exp(-t**2)\nphi = lambda t: f(t) * np.exp(-2j * np.pi * nu * t)\n\nf_vals = f(t1)\nphi_vals = phi(t2)\n\n# Plot\nfig, axs = plt.subplots(1, 2, figsize=(10, 4))\n\naxs[0].plot(t1, f_vals, 'k', linewidth=2)\naxs[0].set_xlim(-8, 8)\naxs[0].set_ylim(0, 1)\naxs[0].set_title(r\"$f(t) = e^{-t^2}$\")\naxs[0].grid(True)\n\naxs[1].plot(t2, np.real(phi_vals), 'b', label=r\"$\\Re(\\phi)$\", linewidth=2)\naxs[1].plot(t2, np.imag(phi_vals), 'r', label=r\"$\\Im(\\phi)$\", linewidth=2)\naxs[1].set_xlim(-4, 4)\naxs[1].set_ylim(-1, 1)\naxs[1].set_title(r\"$\\phi(t) = f(t)e^{-2i\\pi\\nu t}$, $\\nu=5/2$\")\naxs[1].legend()\naxs[1].grid(True)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nCes fortes variations peuvent poser des problèmes pour l’approximation numérique de la transformée de Fourier en utilisant des méthodes de quadrature même lorsqu’on prend un nombre de points \\(n\\) assez grand. L’utilisation de l’agorithme de la transformée de Fourier rapide (FFT) est une solution pour surmonter ce problème."
  },
  {
    "objectID": "00_tds/fourrier_transformation.html#the-approximation-using-the-quadrature-method-of-rectangles-à-gauche-is-periodic.",
    "href": "00_tds/fourrier_transformation.html#the-approximation-using-the-quadrature-method-of-rectangles-à-gauche-is-periodic.",
    "title": "Fourrier Transformation",
    "section": "",
    "text": "On remarque que même si la fonction \\(f\\) n’est pas périodique, la transformée de Fourier \\(\\hat{f}\\) est périodique. En effet, la fonction \\(\\hat{S}_n\\) est périodique de période \\(T = \\frac{n}{T}\\) : \\[\n\\forall \\nu \\in \\mathbb{R} \\quad\n\\widehat{S}_n\\left(\\nu + \\frac{n}{T} \\right)\n= \\frac{T}{n} e^{i\\pi \\nu T} \\sum_{k=0}^{n-1} f_k \\, e^{-2i\\pi\\left(\\nu + \\frac{n}{T} \\right)T \\frac{k}{n}}\n= \\frac{T}{n} e^{i\\pi \\nu T} \\sum_{k=0}^{n-1} f_k \\, e^{-2i\\pi \\nu T \\frac{k}{n}} \\underbrace{e^{-2i\\pi k}}_{=1}\n\\]\n\\[\n\\phantom{\\forall \\nu \\in \\mathbb{R} \\quad\n\\widehat{S}_n\\left(\\nu + \\frac{n}{T} \\right)}\n= \\frac{T}{n} e^{i\\pi \\nu T} \\sum_{k=0}^{n-1} f_k \\, e^{-2i\\pi \\nu T \\frac{k}{n}} = \\widehat{S}_n(\\nu).\n\\]\nThis periodicity of \\(\\hat{S}_n\\) has pour consequence that it is not possible to compute the Fourier transform by the quadrature method for all frequencies \\(\\nu \\in \\mathbb{R}\\) when the parameter \\(T\\) and \\(n\\) are fixed. In fact, it is impossible to compute \\(\\hat{f}(\\nu)\\) when \\(\\nu  \\geq {\\nu}_{max}\\) or \\(\\nu \\leq -{\\nu}_{max}\\) where \\({\\nu}_{max} = \\frac{n}{T}\\) is the period of the Fourier transform \\(\\hat{S}_n\\). So in practice, for computing the Fourier transform of a function \\(f\\) for \\(\\nu\\) very large, we need to increase the parameter \\(T\\) or \\(n\\). In fact, by evaluating the error of the approximation of the Fourier transform \\(\\hat{f}\\) by the quadrature method of rectangles à gauche, we can see that we have a good approximation au point \\(\\nu\\) when this relation holds:\n\\[\n\\frac{\\nu T}{n} \\ll 1.\n\\]\nEpstein (2005) shows that when using the algorithm of the Fast Fourier Transform (FFT), we can correctly compute the Fourier transform of a function \\(f\\) for all frequencies \\(\\nu \\in \\mathbb{R}\\), even when \\(\\frac{\\nu T}{n}\\) is proche to 1 for toute fonction \\(f\\) continue par morceaux et à support borné."
  },
  {
    "objectID": "00_tds/fourrier_transformation.html#utilisation-the-lalgorithme-de-la-transformée-de-fourier-rapide-fft-pour-calculer-la-transformée-de-fourier-dune-fonction-f-en-un-point-nu.",
    "href": "00_tds/fourrier_transformation.html#utilisation-the-lalgorithme-de-la-transformée-de-fourier-rapide-fft-pour-calculer-la-transformée-de-fourier-dune-fonction-f-en-un-point-nu.",
    "title": "Fourrier Transformation",
    "section": "",
    "text": "Dans cette partie, nous considérons toujours que \\(\\hat{S}_n(\\nu)\\) est l’approximation de la transformée de Fourier \\(\\hat{f}(\\nu)\\) de la fonction \\(f\\) en un point \\(\\nu \\in [-\\frac{\\nu_{max}}{2}, \\frac{\\nu_{max}}{2}]\\) où \\(\\nu_{max} = \\frac{n}{T}\\) ie \\[\n\\hat{f}(\\nu) \\approx \\hat{S}_n(\\nu) = \\frac{T}{n} e^{i\\pi \\nu T} \\sum_{k=0}^{n-1} f_k\\, e^{-2 i \\pi \\nu T k / n}.\n\\]\nNous allons présenter l’algorithme de la transformée qui permet d’approximer \\(\\hat{f}(\\nu)\\). Nous n’allons pas détailler l’algorithme de la Transformée de Fourier Rapide (FFT) dans cet article. Balac (2011) provides a simplified explanation of the FFT algorithm. Pour plus de détails, je vous recommande de lire l’article de Cooley and Tukey (1965).\nCe qu’il faut retenir est que l’utilisation de l’algorithme de la Transformée de Fourier Rapide (FFT) pour calculer la transformée de Fourier d’une fonction \\(f\\) en un point \\(\\nu\\) se fait en deux étapes :\nLa première étape utilise le fait que Epstein (2005) shows that when \\(\\hat{S_n}(\\nu)\\) is evaluated at \\(\\nu = \\frac{j}{T}\\) for \\(j = 0, 1, \\ldots, n-1\\), we have the good approximation of the Fourier transform \\(\\hat{f}(\\nu)\\).\nDe plus nous savons que \\(\\hat{S_n}\\) est périodique. Et Cette périodicité fait jouer un rôle symétrique aux indices \\(j \\in \\{0, 1, \\ldots, n-1\\}\\) et \\(k \\in \\{-n/2, -n/2 + 1, \\ldots, -1\\}\\). En effet, les valeurs de la transformée de Fourier de f sur l’intervalle \\([-\\frac{\\nu_{max}}{2}, \\frac{\\nu_{max}}{2}]\\) peuvent être déduites des valeurs de \\(\\hat{S_n}\\) aux points \\(\\nu_j = \\frac{j}{T}\\) pour \\(j = 0, 1, \\ldots, n-1\\) comme suit :\n\\[\n\\widehat{S}_n(\\nu'_j) = \\frac{T}{n} (-1)^j \\sum_{k=0}^{n-1} f_k\\, e^{-2i\\pi j \\frac{k}{n}}\n=\n\\begin{cases}\n\\widehat{S}_n(\\nu_j) & \\text{si } j \\in \\left\\{0, \\dots, \\frac{n}{2} - 1 \\right\\} \\\\\n\\widehat{S}_n(\\nu_{j-n}) & \\text{si } j \\in \\left\\{ \\frac{n}{2}, \\dots, n-1 \\right\\}\n\\end{cases}\n\\]\n\\[\n\\text{où on a exploité la relation} \\quad\ne^{-2i\\pi j \\frac{k}{n}} = e^{-2i\\pi (j-n) \\frac{k}{n}} \\times \\underbrace{e^{-2i\\pi k}}_{=1}\n= e^{-2i\\pi (j-n) \\frac{k}{n}}\n\\quad \\text{pour } j \\in \\left\\{ \\frac{n}{2}, \\dots, n-1 \\right\\}.\n\\]\nCette relation nous montre que l’on peut calculer la transformée de Fourier \\(\\hat{S_n}(\\frac{j}{T})\\) pour \\(j = -\\frac{n}{2}, \\ldots, \\frac{n}{2} - 1\\). De plus, lorsque \\(n\\) est une puissance de 2, calcul est plus rapide. Ce procédé est appelé la Transformée de Fourier Rapide (FFT).\nSi nous récapitulons, nous avons montré que nous pouvons approximer la transformée de Fourier de la fonction \\(f\\) dans l’intervalle \\([-\\frac{T}{2}, \\frac{T}{2}]\\) en les fréquences \\(\\nu_j = \\frac{j}{T}\\) pour \\(j = -\\frac{n}{2}, \\ldots, \\frac{n}{2} - 1\\) avec \\(n = 2^m\\) pour un entier \\(m \\geq 0\\) en utilisant l’algorithme de la Transformée de Fourier Rapide (FFT) en procédant comme suit :\n\nCréer la suite finie F de valeurs \\(f(\\frac{2k - n}{2n} T)\\) pour \\(k = 0, 1, \\ldots, n-1\\).\nCalculer la transformée de Fourier discrète \\(\\hat{F}\\) de la suite F en utilisant l’algorithme de la Transformée de Fourier Rapide (FFT) qui est donnée par \\(\\sum_{k=0}^{n-1} f_k e^{-2i\\pi \\frac{jk}{n}}\\) pour \\(j = 0, 1, \\ldots, n-1\\).\nSymmétriser les valeurs de la seconde partie de \\(-\\frac{n}{2}, \\ldots, -1\\);\nMultiplier chacune des valeurs du tableau par \\(\\frac{T}{n} (-1)^{j-1}\\) où \\(j \\in \\{1, \\ldots, n\\}\\).\n\nOn dispose ainsi d’un tableau correspondant aux valeurs de la transformée de Fourier \\(\\hat{f}(\\nu_j)\\) pour \\(\\nu_j = \\frac{j}{T}\\) pour \\(j = -\\frac{n}{2}, \\ldots, \\frac{n}{2} - 1\\).\nDonc dans la première étape, consiste à discretiser l’intervalle \\([-\\frac{T}{2}, \\frac{T}{2}]\\) en \\(n\\) points \\(\\nu_j = \\frac{j}{T}\\) pour \\(j = 0, 1, \\ldots, n-1\\). En effet on a :\nLa fonction matlab tffft suivante calcule la transformée de Fourier en python d’une fonction donnée en mettant en œuvre les différentes étapes qui ont été détaillées précédemment.\n\nimport numpy as np\nfrom scipy.fft import fft, fftshift\n\ndef tffft(f, T, n):\n    \"\"\"\n    Calcule la transformée de Fourier approchée d'une fonction f à support dans [-T/2, T/2],\n    en utilisant l’algorithme FFT.\n\n    Paramètres\n    ----------\n    f : callable\n        Fonction à transformer (doit être vectorisable avec numpy).\n    T : float\n        Largeur de la fenêtre temporelle (intervalle [-T/2, T/2]).\n    n : int\n        Nombre de points de discrétisation (doit être une puissance de 2 pour FFT efficace).\n\n    Retours\n    -------\n    tf : np.ndarray\n        Valeurs approximées de la transformée de Fourier aux fréquences discrètes.\n    nu : np.ndarray\n        Fréquences discrètes correspondantes (de -n/(2T) à (n/2 - 1)/T).\n    \"\"\"\n    h = T / n\n    t = -0.5 * T + np.arange(n) * h  # noeuds temporels\n    F = f(t)                         # échantillonnage de f\n    tf = h * (-1) ** np.arange(n) * fftshift(fft(F))  # TF approximée\n    nu = -n / (2 * T) + np.arange(n) / T              # fréquences ν_j = j/T\n\n    return tf, nu, t\n\nOn illustre dans le programme ci-dessous le calcul de la transformée de Fourier d’une fonction donnée (une fonction de Gauss).\n\n# Paramètres\na = 10\nf = lambda t: np.exp(-a * t**2)\nT = 10\nn = 2**8  # 256\n\n# Calcul de la transformée de Fourier\ntf, nu, t = tffft(f, T, n)\n\n# Représentation graphique\nfig, axs = plt.subplots(1, 2, figsize=(10, 4))\n\naxs[0].plot(t, f(t), '-g', linewidth=3)\naxs[0].set_xlabel(\"temps\")\naxs[0].set_title(\"Fonction considérée\")\naxs[0].set_xlim(-6, 6)\naxs[0].set_ylim(-0.5, 1.1)\naxs[0].grid(True)\n\naxs[1].plot(nu, np.abs(tf), '-b', linewidth=3)\naxs[1].set_xlabel(\"fréquence\")\naxs[1].set_title(\"Transformée de Fourier\")\naxs[1].set_xlim(-15, 15)\naxs[1].set_ylim(-0.5, 1)\naxs[1].grid(True)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nDans ce cas, lorsque n est une puissance de 2 \\(\\hat{S_n}\\) est approximée rapidement. Et c’est l’algorithme qui permet d’estimer \\(\\hat{S_n}\\) au point \\(\\nu_j = \\frac{j}{T}\\) pour \\(j = 0, 1, \\ldots, n-1\\) qui est appelé la Transformée de Fourier Rapide (FFT)."
  },
  {
    "objectID": "00_tds/proportional_ordinal_regression.html",
    "href": "00_tds/proportional_ordinal_regression.html",
    "title": "Proportional Odds Model for Ordinal Logistic Regression",
    "section": "",
    "text": "The proportional odds model for ordinal logistic regression was first introduced by McCullagh (1980). This model extends binary logistic regression to situations where the dependent variable is ordinal—that is, it consists of ordered categorical values. The proportional odds model is built on several assumptions, including independence of observations, linearity of the log-odds, absence of multicollinearity among predictors, and, most importantly, the proportional odds assumption. This last assumption states that the regression coefficients are constant across all thresholds of the ordinal dependent variable. Ensuring the proportional odds assumption holds is crucial for the validity and interpretability of the model.\nA variety of methods have been proposed in the literature to assess model fit and, in particular, to test the proportional odds assumption. In this paper, we focus on two approaches developed by Brant in his article Brant (1990), “Assessing Proportionality in the Proportional Odds Model for Ordinal Logistic Regression.” We also demonstrate how to implement these techniques in Python, applying them to real-world data. Whether you come from a background in data science, machine learning, or statistics, this article aims to help your understand how to evaluate model fit in ordinal logistic regression.\nThis paper is organized into four main sections:"
  },
  {
    "objectID": "00_tds/proportional_ordinal_regression.html#introduction-to-the-proportional-odds-model",
    "href": "00_tds/proportional_ordinal_regression.html#introduction-to-the-proportional-odds-model",
    "title": "Proportional Odds Model for Ordinal Logistic Regression",
    "section": "Introduction to the Proportional Odds Model",
    "text": "Introduction to the Proportional Odds Model\nBefore presenting the model, we introduce the data structure. We assume we have \\(N\\) independent observations. Each observation is represented by a vector of \\(p\\) explanatory variables \\(X_i = (X_{i1}, X_{i2}, \\ldots, X_{ip})\\), along with a dependent or response variable \\(Y\\) that takes ordinal values from \\(1\\) to \\(K\\). The proportional odds model specifically models the cumulative distribution probabilities of the response variable \\(Y\\), defined as \\(\\gamma_j = P(Y \\leq j \\mid X_i)\\) for \\(j = 1, 2, \\dots, K-1\\), as functions of the explanatory variables \\(X_i\\). The model is formulated as follows:\n\\[\n\\text{logit}(\\gamma_j) = \\log\\left(\\frac{\\gamma_j}{1 - \\gamma_j}\\right) = \\theta_j - \\beta^\\top \\mathbf{X}\n\\tag{1}\\]\nWhere \\(\\theta_j\\) are the intercepts for each category j and respect the condition \\(\\theta_1 &lt; \\theta_2 &lt; ... &lt; \\theta_{K-1}\\), and \\(\\beta\\) is the vector of regression coefficients which are the same for all categories. We observe a monotonic trend in the coefficients \\(\\theta_j\\) across the categories of the response variable Y.\nThis model is also known as the grouped continuous model, as it can be derived by assuming the existence of a continuous latent variable \\(Y^*\\). This latent variable follows a linear regression model with conditional mean \\(\\eta = \\boldsymbol{\\beta}^{\\top} \\mathbf{X}\\), and it relates to the observed ordinal variable \\(Y\\) through thresholds \\(\\theta_j\\) defined as follows: \\[\ny^* = {\\beta}^{T}\\mathbf{X} + \\epsilon\n\\tag{2}\\]\nwhere \\(\\epsilon\\) is an error term (random noise), generally assumed to follow a standard logistic distribution in the proportional odds model.\nThe latent variable \\(Y^*\\) is unobserved and partitioned into intervals defined by thresholds \\(\\theta_1, \\theta_2, \\dots, \\theta_{K-1}\\), generating the observed ordinal variable \\(Y\\) as follows:\n\\[\nY = \\begin{cases}\n1 & \\text{if } Y^* \\leq \\theta_1 \\\\\n2 & \\text{if } \\theta_1 &lt; Y^* \\leq \\theta_2 \\\\\n\\vdots & \\\\\nK & \\text{if } Y^* &gt; \\theta_{K-1}\n\\end{cases}\n\\tag{3}\\]\nIn the next section, we introduce the various approaches proposed by Brant (1990) for assessing the proportional odds assumption. These methods evaluate whether the regression coefficients remain constant across the categories defined by the ordinal response variable."
  },
  {
    "objectID": "00_tds/proportional_ordinal_regression.html#assessing-the-proportional-odds-assumption-the-likelihood-ratio-test",
    "href": "00_tds/proportional_ordinal_regression.html#assessing-the-proportional-odds-assumption-the-likelihood-ratio-test",
    "title": "Proportional Odds Model for Ordinal Logistic Regression",
    "section": "Assessing the Proportional Odds Assumption: The Likelihood Ratio Test",
    "text": "Assessing the Proportional Odds Assumption: The Likelihood Ratio Test\nTo assess the proportional odds assumption in an ordinal logistic regression model, Brant (1990) proposes the use of the likelihood ratio test. This approach begins by fitting a less restrictive model in which the regression coefficients are allowed to vary across categories. This model is expressed as: \\[\n\\text{logit}(\\gamma_j) = \\theta_j - \\beta_j^\\top \\mathbf{X}\n\\tag{4}\\]\nwhere \\(\\beta_j\\) is the vector of regression coefficients for each category j. Here the coefficients \\(\\beta_j\\) are allowed to vary across categories, which means that the proportional odds assumption is not satisfied. We then use the conventionnel likelihood ratio test to assess the hypothesis : \\[\nH_0: \\beta_j = \\beta \\quad \\text{for all } j = 1, 2, \\ldots, K-1\n\\tag{5}\\]\nTo perform this test, we conduct a likelihood ratio test comparing the unconstrained (non-proportional or satured) model with the constrained (proportional odds or reduced) model.\nBefore proceeding further, we briefly recall how to use the likelihood ratio test in hypothesis testing. Suppose we want to evaluate the null hypothesis \\(H_0 : \\theta \\in \\Theta_0\\) against the alternative \\(H_1 : \\theta \\in \\Theta_1\\),\nThe likelihood ratio statistic is defined as: \\[\n\\lambda = 2 \\log\\left(\\frac{\\displaystyle\\sup_{\\theta \\in \\Theta}\\mathcal{L}(\\theta)}{\\displaystyle\\sup_{\\theta \\in \\Theta_0}\\mathcal{L}(\\theta)}\\right)\n= 2\\log\\left(\\frac{\\mathcal{L}(\\hat{\\theta})}{\\mathcal{L}(\\hat{\\theta}_0)}\\right),\n\\tag{6}\\]\nwhere \\(\\mathcal{L}(\\theta)\\) is the likelihood function, \\(\\hat{\\theta}\\) is the maximum likelihood estimate (MLE) under the full model, and \\(\\hat{\\theta}_0\\) is the MLE under the constrained model. The test statistic \\(\\lambda\\) follows a chi-square distribution with degrees of freedom equal to the difference in the number of parameters between the full and constrained models.\nHere, \\(\\hat{\\theta}\\) is the maximum likelihood estimate (MLE) under the full (unconstrained) model, and \\(\\hat{\\theta}_0\\) is the MLE under the constrained model where the proportional odds assumption holds. The test statistic \\(\\lambda\\) follows a chi-square distribution under the null hypothesis.\nIn a general setting, suppose the full parameter space is denoted by\n\\[\n\\Theta = (\\theta_1, \\theta_2, \\ldots, \\theta_q, \\ldots, \\theta_p),\n\\]\nand the restricted parameter space under the null hypothesis is\n\\[\n\\Theta_0 = (\\theta_1, \\theta_2, \\ldots, \\theta_q).\n\\]\n(Note: These parameters are generic and should not be confused with the \\(K - 1\\) thresholds or intercepts in the proportional odds model.), the likelihood ratio test statistic \\(\\lambda\\) follows a chi-square distribution with \\(p - q\\) degrees of freedom. Where \\(p\\) represents the total number of parameters in the full (unconstrained or “saturated”) model, while \\(K - 1\\) corresponds to the number of parameters in the reduced (restricted) model.\nNow, let us apply this approach to the ordinal logistic regression model with the proportional odds assumption. Assume that our response variable has \\(K\\) ordered categories and that we have \\(p\\) predictor variables. To use the likelihood ratio test to evaluate the proportional odds assumption, we need to compare two models:\n\n1. Unconstrained Model (non-proportional odds):\nThis model allows each outcome threshold to have its own set of regression coefficients, meaning that we do not assume the regression coefficients are equal across all thresholds. The model is defined as:\n\\[\n\\text{logit}(\\mathbb{P}(Y \\leq j \\mid \\mathbf{X})) = \\theta_j - \\boldsymbol{\\beta}_j^\\top \\mathbf{X}\n\\]\n\nThere are \\(K - 1\\) threshold (intercept) parameters: \\(\\theta_1, \\theta_2, \\ldots, \\theta_{K-1}\\)\nEach threshold has its own vector of slope coefficients \\({\\beta}_j\\) of dimension \\(p\\)\n\nThus, the total number of parameters in the unconstrained model is:\n\\[\n(K - 1) \\text{ thresholds} + (K - 1) \\times p \\text{ slopes} = (K - 1)(p + 1)\n\\]\n\n\n2. Proportional Odds Model:\nThis model assumes a single set of regression coefficients for all thresholds:\n\\[\n\\text{logit}(\\mathbb{P}(Y \\leq j \\mid \\mathbf{X})) = \\theta_j - {\\beta}^\\top \\mathbf{X}\n\\]\n\nThere are \\(K - 1\\) threshold parameters\nThere is one common slope vector \\({\\beta}\\) for all \\(j\\)\n\nThus, the total number of parameters in the proportional odds model is:\n\\[\n(K - 1) \\text{ thresholds} + p \\text{ slopes} = (K - 1) + p\n\\]\nThus, the likelihood ratio test statistic follows a chi-square distribution with degrees of freedom:\n\\[\n\\text{df} = [(K - 1) \\times (p+1)] - [(K - 1) + p] = (K - 2) \\times p\n\\tag{7}\\]\nThis test provides a formal way to assess whether the proportional odds assumption holds for the given data. At a significance level of 1%, 5%, or any other conventional threshold, the proportional odds assumption is rejected if the test statistic \\(\\lambda\\) exceeds the critical value from the chi-square distribution with \\((K - 2) \\times p\\) degrees of freedom.\nIn other words, we reject the null hypothesis\n\\[\nH_0 : {\\beta}_1 = {\\beta}_2 = \\cdots = {\\beta}_{K-1} = {\\beta},\n\\]\nwhich states that the regression coefficients are equal across all cumulative logits. This test has the advantage of being straightforward to implement and provides an overall assessment of the proportional odds assumption.\nIn the next section, we introduce the proportional odds test based on separate fits.\n\nAssessing the Proportional Odds Assumption: The Separate Fits Approach\n\nTo understand this part, you must understand the Mahalanobis distance and its properties. The Mahalanobis distance can be used to measure the dissimilarity between two vectors \\(x=(x_1, x_2, \\ldots, x_p)^\\top\\) and \\(y=(y_1, y_2, \\ldots, y_p)^\\top\\) in a multivariate space with the same distribution. It is defined as: \\[\nD_M(x, y) = \\sqrt{(x - y)^\\top \\Sigma^{-1} (x - y)}\n\\tag{8}\\]\nwhere \\(\\Sigma\\) is the covariance matrix of the distribution. The Mahalanobis distance is linked with the \\(\\chi^2\\) distribution, specifically, if \\(X \\sim N(\\mu, \\Sigma)\\) is a p-dimensional normal random vector, with the mean \\(\\mu\\) and covariance matrix \\(\\Sigma\\), then the Mahalanobis distance \\(D_M(X, \\mu)\\) follows a \\(\\chi^2\\) distribution with \\(p\\) degrees of freedom. This step is essential for understanding how to assess proportionality using separate fits. You will see why shortly.\nIn fact, the author notes that the natural approach to evaluating the proportional odds assumption is to fit a set of \\(K-1\\) binary logistic regression models (where \\(K\\) is the number of categories of the response variable), and then use the statistical properties of the estimated parameters to construct a test statistic for the proportional odds hypothesis.\nThe procedure is as follows:\nFirst, we construct separate binary logistic regression models for each threshold \\(j = 1, 2, \\ldots, K-1\\) of the ordinal response variable \\(Y\\). For each threshold \\(j\\), we define a binary variable \\(Z_j\\), which takes the value 1 if the observation exceeds threshold \\(j\\), and 0 otherwise. Specifically, we have: \\[\nZ_j = \\begin{cases}\n0 & \\text{if } Y &gt; j \\\\\n0 & \\text{if } Y \\leq j\n\\end{cases}\n\\tag{9}\\]\nWith the probaility, \\(\\pi_j = P(Z_j = 1 \\mid \\mathbf{X}) = 1 - \\gamma_j\\) satisfying the logistic regression model: \\[\n\\text{logit}(\\pi_j) = \\theta_j - \\beta_j^\\top \\mathbf{X}.\n\\tag{10}\\]\nThen, assessing the proportional odds assumption in this context involves testing the hypothesis that the regression coefficients \\(\\beta_j\\) are equal across all \\(K-1\\) models. This is equivalent to testing the hypothesis:\n\\[\nH_0 : \\beta_1 = \\beta_2 = \\cdots = \\beta_{K-1} = \\beta\n\\tag{11}\\]\nLet \\(\\hat{\\beta}_j\\) denote the maximum likelihood estimators of the regression coefficients for each binary model, and let \\(\\hat{\\beta} = (\\hat{\\beta}_1^\\top, \\hat{\\beta}_2^\\top, \\ldots, \\hat{\\beta}_{K-1}^\\top)^\\top\\) represent the global vector of estimators. This vector is asymptotically normally distributed, such that \\(\\mathbb{E}(\\hat{\\beta}_j) \\approx \\beta\\), with variance-covariance matrix \\(\\mathbb{V}(\\hat{\\beta}_j)\\). The general term of this matrix, \\(\\text{cov}(\\hat{\\beta}_j, \\hat{\\beta}_k)\\), needs to be determined and is given by:\n\\[\n\\widehat{V}(\\hat{\\boldsymbol{\\beta}}) =\n\\begin{bmatrix}\n\\text{Cov}(\\hat{\\boldsymbol{\\beta}}_1, \\hat{\\boldsymbol{\\beta}}_1) & \\text{Cov}(\\hat{\\boldsymbol{\\beta}}_1, \\hat{\\boldsymbol{\\beta}}_2) & \\cdots & \\text{Cov}(\\hat{\\boldsymbol{\\beta}}_1, \\hat{\\boldsymbol{\\beta}}_{K-1}) \\\\\n\\text{Cov}(\\hat{\\boldsymbol{\\beta}}_2, \\hat{\\boldsymbol{\\beta}}_1) & \\text{Cov}(\\hat{\\boldsymbol{\\beta}}_2, \\hat{\\boldsymbol{\\beta}}_2) & \\cdots & \\text{Cov}(\\hat{\\boldsymbol{\\beta}}_2, \\hat{\\boldsymbol{\\beta}}_{K-1}) \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\text{Cov}(\\hat{\\boldsymbol{\\beta}}_{K-1}, \\hat{\\boldsymbol{\\beta}}_1) & \\text{Cov}(\\hat{\\boldsymbol{\\beta}}_{K-1}, \\hat{\\boldsymbol{\\beta}}_2) & \\cdots & \\text{Cov}(\\hat{\\boldsymbol{\\beta}}_{K-1}, \\hat{\\boldsymbol{\\beta}}_{K-1})\n\\end{bmatrix}\n\\in \\mathbb{R}^{(K-1)p \\times (K-1)p}\n\\tag{12}\\]\nwhere \\(\\text{Cov}(\\hat{\\boldsymbol{\\beta}}_j, \\hat{\\boldsymbol{\\beta}}_k)\\) is the covariance between the estimated coefficients of the \\(j\\)-th and \\(k\\)-th binary models. To evaluate the proportional odds assumption, Brant constructs a matrix \\(\\mathbf{D}\\) that captures the differences between the coefficients \\(\\hat{\\beta}_j\\). Recall that each vector \\(\\hat{\\beta}_j\\) has dimension \\(p\\). The matrix \\(\\mathbf{D}\\) is defined as follows:\n\\[\n\\mathbf{D} =\n\\begin{bmatrix}\nI & -I & 0 & \\cdots & 0 \\\\\nI & 0 & -I & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\nI & 0 & 0 & \\cdots & -I \\\\\n\\end{bmatrix}\n\\in \\mathbb{R}^{(K-2)p \\times (K-1)p}\n\\tag{13}\\]\nwhere \\(I\\) is the identity matrix of size \\(p \\times p\\). The first row of the matrix D corresponds to the difference between the first and second coefficients, the second row corresponds to the difference between the second and third coefficients, and so on, until the last row which corresponds to the difference between the \\((K-2)\\)-th and \\((K-1)\\)-th coefficients. We can notice that the product \\(\\mathbf{D} \\hat{{\\beta}}\\) will yield a vector of differences between the coefficients \\(\\hat{\\beta_j}\\).\nOnce the matrix \\(\\mathbf{D}\\) is constructed, Brant defines the Wald statistic \\(X^2\\) to test the proportional odds assumption. This statistic can be interpreted as the Mahalanobis distance between the vector \\(\\mathbf{D} \\hat{\\boldsymbol{\\beta}}\\) and the zero vector. The Wald statistic is defined as follows:\n\\[\nX^2 = (\\mathbf{D} \\hat{{\\beta}})^\\top \\left[ \\mathbf{D} \\widehat{V}(\\hat{{\\beta}}) \\mathbf{D}^\\top \\right]^{-1} (\\mathbf{D} \\hat{{\\beta}})\n\\tag{14}\\]\nwhich will be asymptotically \\(\\chi^2\\) distributed with \\((K - 2)p\\) degrees of freedom under the null hypothesis. The challenging part here is to determine the variance-covariance matrix \\(\\widehat{V}(\\hat{\\beta})\\). In his article, Brant provides an explicit estimator for this variance-covariance matrix, which is based on the maximum likelihood estimators \\(\\hat{\\beta}_j\\) from each binary model.\nIn the following sections, we implement these approaches in Python, using the statsmodels package for the regressions and statistical tests."
  },
  {
    "objectID": "00_tds/proportional_ordinal_regression.html#example",
    "href": "00_tds/proportional_ordinal_regression.html#example",
    "title": "Proportional Odds Model for Ordinal Logistic Regression",
    "section": "Example",
    "text": "Example\nThe data for this example comes from the “Wine Quality” dataset, which contains information about red wine samples and their quality ratings. The dataset includes 1,599 observations and 12 variables. The target variable, “quality,” is ordinal and originally ranges from 3 to 8. To ensure enough observations in each group, we combine categories 3 and 4 into a single category (labeled 4), and categories 7 and 8 into a single category (labeled 7), so the response variable has four levels. We then handle outliers in the explanatory variables using the Interquartile Range (IQR) method. Finally, we select three predictors—volatile acidity, free sulfur dioxide, and total sulfur dioxide—to use in our ordinal logistic regression model, and we standardize these variables to have a mean of 0 and a standard deviation of 1.\nTables 1 and 2 present the results of the three binary logistic regression models and the proportional odds model, respectively. Several discrepancies can be seen in these tables, particularly in the “volatile acidity” coefficients. For instance, the difference in the “volatile acidity” coefficient between the first and second binary models is -0.280, while the difference between the second and third models is 0.361. These differences—especially when compared alongside the standard errors—suggest that the proportional odds assumption may not hold.\nTo assess the overall significance of the proportional odds assumption, we perform the likelihood ratio test, which yields a test statistic of \\(\\mathrm{LR} = 53.207\\) and a p-value of \\(1.066 \\times 10^{-9}\\) when compared to the chi-square distribution with 6 degrees of freedom. This result indicates that the proportional odds assumption is violated at the 5% significance level, suggesting that the model may not be appropriate for the data. We also use the separate fits approach to further investigate this assumption. The Wald test statistic is computed as \\(X^2 = 41.880\\), with a p-value of \\(1.232 \\times 10^{-7}\\), also based on the chi-square distribution with 6 degrees of freedom. This further confirms that the proportional odds assumption is violated at the 5% significance level."
  },
  {
    "objectID": "00_tds/proportional_ordinal_regression.html#conclusion",
    "href": "00_tds/proportional_ordinal_regression.html#conclusion",
    "title": "Proportional Odds Model for Ordinal Logistic Regression",
    "section": "Conclusion",
    "text": "Conclusion\nThis paper had two main goals: first, to illustrate how to test the proportional odds assumption in the context of ordinal logistic regression, and second, to encourage readers to explore Brant (1990)’s article for a deeper understanding of the topic.\nBrant’s work extends beyond assessing the proportional odds assumption—it also provides methods for evaluating the overall adequacy of the ordinal logistic regression model. For instance, he discusses how to test whether the latent variable \\(Y^*\\) truly follows a logistic distribution or whether an alternative link function might be more appropriate.\nIn this article, we focused on a global assessment of the proportional odds assumption, without investigating which specific coefficients may be responsible for any violations. Brant also addresses this finer-grained analysis, which is why we strongly encourage you to read his 1990 article in full.\nWe welcome any comments or suggestions. Happy reading!\n\nimport pandas as pd\n\ndata = pd.read_csv(\"winequality-red.csv\", sep=\";\")\ndata.head()\n\n# Repartition de la variable cible quality \n\ndata['quality'].value_counts(normalize=False).sort_index()\n\n# I want to regroup modalities 3, 4 and the modalities 7 and 8\ndata['quality'] = data['quality'].replace({3: 4, 8: 7})\ndata['quality'].value_counts(normalize=False).sort_index()\nprint(\"Number of observations:\", data.shape[0])\n\nNumber of observations: 1599\n\n\n\n# Traitons les outliers des variables privées de la variable cible quality par IQR.\n\ndef remove_outliers_iqr(df, column):\n    Q1 = df[column].quantile(0.25)\n    Q3 = df[column].quantile(0.75)\n    IQR = Q3 - Q1\n    lower_bound = Q1 - 1.5 * IQR\n    upper_bound = Q3 + 1.5 * IQR\n    return df[(df[column] &gt;= lower_bound) & (df[column] &lt;= upper_bound)]\nfor col in data.columns:\n    if col != 'quality':\n        data = remove_outliers_iqr(data, col)\n\n\nvar_names_without_quality = [col for col in data.columns if col != 'quality']\n\n##  Create the boxplot of each variable per group of quality\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.figure(figsize=(15, 10))\nfor i, var in enumerate(var_names_without_quality):\n    plt.subplot(3, 4, i + 1)\n    sns.boxplot(x='quality', y=var, data=data)\n    plt.title(f'Boxplot of {var} by quality')\n    plt.xlabel('Quality')\n    plt.ylabel(var)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n# Implement the ordered logistic regression to variables 'volatile acidity', 'free sulfur dioxide', and 'total sulfur dioxide'\nfrom statsmodels.miscmodels.ordinal_model import OrderedModel\nfrom sklearn.preprocessing import StandardScaler\nexplanatory_vars = ['volatile acidity', 'free sulfur dioxide', 'total sulfur dioxide']\n# Standardize the explanatory variables\ndata[explanatory_vars] = StandardScaler().fit_transform(data[explanatory_vars])\n\ndef fit_ordered_logistic_regression(data, response_var, explanatory_vars):\n    model = OrderedModel(\n        data[response_var],\n        data[explanatory_vars],\n        distr='logit'\n    )\n    result = model.fit(method='bfgs', disp=False)\n    return result\nresponse_var = 'quality'\n\nresult = fit_ordered_logistic_regression(data, response_var, explanatory_vars)\nprint(result.summary())\n# Compute the log-likelihood of the model\nlog_reduced = result.llf\nprint(f\"Log-likelihood of the reduced model: {log_reduced}\")\n\n                             OrderedModel Results                             \n==============================================================================\nDep. Variable:                quality   Log-Likelihood:                -1130.1\nModel:                   OrderedModel   AIC:                             2272.\nMethod:            Maximum Likelihood   BIC:                             2302.\nDate:                Tue, 10 Jun 2025                                         \nTime:                        23:03:11                                         \nNo. Observations:                1135                                         \nDf Residuals:                    1129                                         \nDf Model:                           3                                         \n========================================================================================\n                           coef    std err          z      P&gt;|z|      [0.025      0.975]\n----------------------------------------------------------------------------------------\nvolatile acidity        -0.7180      0.064    -11.302      0.000      -0.842      -0.593\nfree sulfur dioxide      0.3627      0.076      4.770      0.000       0.214       0.512\ntotal sulfur dioxide    -0.5903      0.080     -7.406      0.000      -0.747      -0.434\n4/5                     -3.8601      0.182    -21.153      0.000      -4.218      -3.502\n5/6                      1.3002      0.050     25.863      0.000       1.202       1.399\n6/7                      0.8830      0.042     20.948      0.000       0.800       0.966\n========================================================================================\nLog-likelihood of the reduced model: -1130.0713953351503\n\n\n\nnum_of_thresholds = len(result.params) - len(explanatory_vars)  # Number of thresholds is total params minus explanatory vars\nOrderedModel(\n        data[response_var],\n        data[explanatory_vars],\n        distr='logit'\n    ).transform_threshold_params(result.params[-num_of_thresholds:])\n\narray([       -inf, -3.86010874, -0.19012621,  2.2279648 ,         inf])\n\n\n\n# The likelihood ratio test\n# Compute the full multinomial model\nimport statsmodels.api as sm\n\ndata_sm = sm.add_constant(data[explanatory_vars])\nmodel_full = sm.MNLogit(data[response_var], data_sm)\nresult_full = model_full.fit(method='bfgs', disp=False)\n#summary\nprint(result_full.summary())\n# Commpute the log-likelihood of the full model\nlog_full = result_full.llf\nprint(f\"Log-likelihood of the full model: {log_full}\")\n\n# Compute the likelihood ratio statistic\n\nLR_statistic = 2 * (log_full - log_reduced)\nprint(f\"Likelihood Ratio Statistic: {LR_statistic}\")\n\n# Compute the degrees of freedom\ndf1 = (num_of_thresholds - 1) * len(explanatory_vars)\ndf2 = result_full.df_model - OrderedModel(\n        data[response_var],\n        data[explanatory_vars],\n        distr='logit'\n    ).fit().df_model\nprint(f\"Degrees of Freedom: {df1}\")\nprint(f\"Degrees of Freedom for the full model: {df2}\")\n\n# Compute the p-value\nfrom scipy.stats import chi2\nprint(\"The LR statistic :\", LR_statistic)\np_value = chi2.sf(LR_statistic, df1)\nprint(f\"P-value: {p_value}\")\nif p_value &lt; 0.05:\n    print(\"Reject the null hypothesis: The proportional odds assumption is violated.\")\nelse:\n    print(\"Fail to reject the null hypothesis: The proportional odds assumption holds.\")\n\n                          MNLogit Regression Results                          \n==============================================================================\nDep. Variable:                quality   No. Observations:                 1135\nModel:                        MNLogit   Df Residuals:                     1123\nMethod:                           MLE   Df Model:                            9\nDate:                Tue, 10 Jun 2025   Pseudo R-squ.:                  0.1079\nTime:                        23:03:15   Log-Likelihood:                -1103.5\nconverged:                      False   LL-Null:                       -1236.9\nCovariance Type:            nonrobust   LLR p-value:                 2.753e-52\n========================================================================================\n           quality=5       coef    std err          z      P&gt;|z|      [0.025      0.975]\n----------------------------------------------------------------------------------------\nconst                    3.2418      0.269     12.034      0.000       2.714       3.770\nvolatile acidity        -0.6541      0.180     -3.624      0.000      -1.008      -0.300\nfree sulfur dioxide      0.2494      0.323      0.772      0.440      -0.384       0.882\ntotal sulfur dioxide     0.6314      0.310      2.037      0.042       0.024       1.239\n----------------------------------------------------------------------------------------\n           quality=6       coef    std err          z      P&gt;|z|      [0.025      0.975]\n----------------------------------------------------------------------------------------\nconst                    3.2549      0.269     12.089      0.000       2.727       3.783\nvolatile acidity        -1.0838      0.184     -5.880      0.000      -1.445      -0.723\nfree sulfur dioxide      0.6269      0.325      1.930      0.054      -0.010       1.264\ntotal sulfur dioxide     0.0723      0.315      0.230      0.818      -0.544       0.689\n----------------------------------------------------------------------------------------\n           quality=7       coef    std err          z      P&gt;|z|      [0.025      0.975]\n----------------------------------------------------------------------------------------\nconst                    1.4139      0.302      4.684      0.000       0.822       2.006\nvolatile acidity        -1.8364      0.214     -8.596      0.000      -2.255      -1.418\nfree sulfur dioxide      1.0125      0.358      2.830      0.005       0.311       1.714\ntotal sulfur dioxide    -0.9086      0.389     -2.337      0.019      -1.671      -0.147\n========================================================================================\nLog-likelihood of the full model: -1103.467809036406\nLikelihood Ratio Statistic: 53.20717259748881\nDegrees of Freedom: 6\nDegrees of Freedom for the full model: 6.0\nThe LR statistic : 53.20717259748881\nP-value: 1.0658102529671109e-09\nReject the null hypothesis: The proportional odds assumption is violated.\n\n\n/Users/juniorjumbong/Desktop/personal-website/.venv/lib/python3.13/site-packages/statsmodels/base/model.py:607: ConvergenceWarning:\n\nMaximum Likelihood optimization failed to converge. Check mle_retvals\n\n/Users/juniorjumbong/Desktop/personal-website/.venv/lib/python3.13/site-packages/statsmodels/base/optimizer.py:737: RuntimeWarning:\n\nMaximum number of iterations has been exceeded.\n\n/Users/juniorjumbong/Desktop/personal-website/.venv/lib/python3.13/site-packages/statsmodels/base/model.py:607: ConvergenceWarning:\n\nMaximum Likelihood optimization failed to converge. Check mle_retvals\n\n\n\n\nimport numpy as np\nimport statsmodels.api as sm\nimport pandas as pd\n\ndef fit_binary_models(data, explanatory_vars, y):\n    \"\"\"\n    - data : DataFrame pandas original (doit contenir toutes les variables)\n    - explanatory_vars : liste des variables explicatives\n    - y : array-like, cible ordinale (n,) (ex: 4, 5, 6, 7)\n\n    Retourne :\n      - binary_models : liste d'objets Logit results (statsmodels)\n      - beta_hat : array (K-1, p+1) (coeffs incluant l'intercept)\n      - var_hat : liste de matrices (p+1, p+1) (variance-covariance complète)\n      - z_mat : DataFrame des variables binaires z_j (pour debug/inspection)\n      - thresholds : liste des seuils utilisés\n    \"\"\"\n    qualities = np.sort(np.unique(y))   # toutes les modalités, triées\n    thresholds = qualities[:-1]         # seuils pour les modèles binaires (K-1)\n    p = len(explanatory_vars)\n    n = len(y)\n    K_1 = len(thresholds)\n\n    binary_models = []\n    beta_hat = np.full((K_1, p+1), np.nan)\n    p_values_beta_hat = np.full((K_1, p+1), np.nan)  # pour les p-values\n    var_hat = []\n    z_mat = pd.DataFrame(index=np.arange(n))\n    X_with_const = sm.add_constant(data[explanatory_vars])\n\n    # Construction et estimation des modèles binaires pour chaque seuil\n    for j, t in enumerate(thresholds):\n        z_j = (y &gt; t).astype(int)\n        z_mat[f'z&gt;{t}'] = z_j\n        model = sm.Logit(z_j, X_with_const)\n        res = model.fit(disp=0)\n        binary_models.append(res)\n        beta_hat[j, :] = res.params.values           # Incluant intercept\n        p_values_beta_hat[j, :] = res.pvalues.values  # P-values des coefficients\n        var_hat.append(res.cov_params().values)      # Covariance complète (p+1, p+1)\n\n    return binary_models, beta_hat, X_with_const, var_hat, z_mat, thresholds\nbinary_models, beta_hat,X_with_const, var_hat, z_mat, thresholds = fit_binary_models(data, explanatory_vars, data[response_var])\n# Afficher les coefficients estimés\nprint(\"Estimated coefficients (beta_hat):\")\nprint(beta_hat)\n# Afficher les p-values des coefficients\nprint(\"P-values of coefficients (p_values_beta_hat):\")\nprint(X_with_const)\n# Afficher les seuils\nprint(\"Thresholds:\")\nprint(thresholds)   \nprint(\"z_mat (variables binaires créées) :\\n\", z_mat.head())\n\nEstimated coefficients (beta_hat):\n[[ 4.09606917 -0.88743434  0.63477387  0.20921617]\n [ 0.15729349 -0.60735704  0.4339553  -0.65663161]\n [-2.60302245 -0.9677302   0.60691768 -1.30246297]]\nP-values of coefficients (p_values_beta_hat):\n      const  volatile acidity  free sulfur dioxide  total sulfur dioxide\n0       1.0          1.080055            -0.441353             -0.282198\n1       1.0          2.173545             1.189601              1.058458\n2       1.0          1.444552             0.024634              0.530321\n3       1.0         -1.471421             0.257627              0.774077\n4       1.0          1.080055            -0.441353             -0.282198\n...     ...               ...                  ...                   ...\n1594    1.0          0.472561             2.005078              0.124061\n1595    1.0          0.168814             2.820555              0.408443\n1596    1.0         -0.074184             1.655588             -0.038443\n1597    1.0          0.745933             2.005078              0.124061\n1598    1.0         -1.289172             0.374124              0.042809\n\n[1135 rows x 4 columns]\nThresholds:\n[4 5 6]\nz_mat (variables binaires créées) :\n    z&gt;4  z&gt;5  z&gt;6\n0  1.0  0.0  0.0\n1  1.0  0.0  0.0\n2  1.0  0.0  0.0\n3  1.0  1.0  0.0\n4  1.0  0.0  0.0\n\n\n\ndef compute_pi_hat(binary_models, X_with_const):\n    \"\"\"\n    - binary_models : liste d'objets Logit results (statsmodels)\n    - X_with_const  : matrice (n, p+1) des variables explicatives AVEC constante\n\n    Retourne :\n      - pi_hat : array (n, K-1) des fitted values pour chaque modèle binaire\n    \"\"\"\n    n = X_with_const.shape[0]\n    K_1 = len(binary_models)\n    pi_hat = np.full((n, K_1), np.nan)\n    for m, model in enumerate(binary_models):\n        pi_hat[:, m] = model.predict(X_with_const)\n    return pi_hat\n\n# Supposons que tu as :\n# - binary_models (liste)\n# - X_with_const (matrice numpy (n, p+1) créée dans la fonction précédente)\n\npi_hat = compute_pi_hat(binary_models, X_with_const)\nprint(\"Shape de pi_hat :\", pi_hat.shape)  # (n, K-1)\nprint(\"Aperçu de pi_hat :\\n\", pi_hat[:5, :])\n\nShape de pi_hat : (1135, 3)\nAperçu de pi_hat :\n [[0.94258882 0.37638681 0.02796232]\n [0.95866233 0.20724576 0.00466477]\n [0.94982271 0.25776823 0.00922353]\n [0.99675485 0.65802083 0.11599334]\n [0.94258882 0.37638681 0.02796232]]\n\n\n\nimport numpy as np\n\ndef assemble_varBeta(pi_hat, X_with_const):\n    \"\"\"\n    Construit la matrice de variance-covariance globale varBeta pour les estimateurs des modèles binaires.\n    - pi_hat : array (n, K-1), chaque colonne = fitted proba du modèle binaire j\n    - X_with_const : array (n, p+1), matrice de design AVEC constante\n\n    Retourne :\n      - varBeta : array ((K-1)*p, (K-1)*p) [sans l'intercept]\n    \"\"\"\n    # Assure que tout est en numpy\n    if hasattr(X_with_const, 'values'):\n        X = X_with_const.values\n    else:\n        X = np.asarray(X_with_const)\n    n, p1 = X.shape  # p1 = p + 1 (avec intercept)\n    p = p1 - 1\n    K_1 = pi_hat.shape[1]\n\n    # Initialisation de la matrice globale\n    varBeta = np.zeros(((K_1)*p, (K_1)*p))\n\n    # Pour chaque bloc (j, l)\n    for j in range(K_1):\n        pi_j = pi_hat[:, j]\n        Wj = np.diag(pi_j * (1 - pi_j))\n        X_j = X\n        Xt = X_j.T\n\n        # Diagonale principale (variance de beta_j)\n        inv_XtWjX = np.linalg.pinv(Xt @ Wj @ X_j)\n        # On enlève la première ligne/colonne (intercept)\n        inv_XtWjX_no_const = inv_XtWjX[1:, 1:]\n\n        row_start = j * p\n        row_end = (j + 1) * p\n        varBeta[row_start:row_end, row_start:row_end] = inv_XtWjX_no_const\n\n        # Blocs hors diagonale (covariances entre beta_j et beta_l)\n        for l in range(j + 1, K_1):\n            pi_l = pi_hat[:, l]\n            Wml = np.diag(pi_l - pi_j * pi_l)\n            Wl = np.diag(pi_l * (1 - pi_l))\n            # Termes croisés\n            inv_XtWlX = np.linalg.pinv(Xt @ Wl @ X_j)\n            block_vars = (\n                inv_XtWjX @ (Xt @ Wml @ X_j) @ inv_XtWlX\n            )[1:, 1:]  # Retirer intercept\n            # Place les blocs (symétriques)\n            col_start = l * p\n            col_end = (l + 1) * p\n            varBeta[row_start:row_end, col_start:col_end] = block_vars\n            varBeta[col_start:col_end, row_start:row_end] = block_vars.T  # symétrie\n\n    return varBeta\n\nvarBeta = assemble_varBeta(pi_hat, X_with_const)\nprint(\"Shape de varBeta :\", varBeta.shape)  # ((K-1)*p, (K-1)*p)\nprint(\"Aperçu de varBeta :\\n\", varBeta[:5, :5])  # Afficher un aperçu\n\nShape de varBeta : (9, 9)\nAperçu de varBeta :\n [[ 2.87696584e-02  8.96840197e-04 -9.43834509e-04  1.80729535e-03\n   2.32011452e-04]\n [ 8.96840197e-04  1.09112963e-01 -5.78619412e-02  3.21424832e-04\n   3.10295243e-03]\n [-9.43834509e-04 -5.78619412e-02  7.73627725e-02 -4.69694598e-04\n  -1.48837502e-03]\n [ 1.80729535e-03  3.21424832e-04 -4.69694598e-04  4.61543407e-03\n   1.04759944e-04]\n [ 2.32011452e-04  3.10295243e-03 -1.48837502e-03  1.04759944e-04\n   7.48753786e-03]]\n\n\n\ndef fill_varBeta_diagonal(varBeta, var_hat):\n    K_1 = len(var_hat)\n    p = var_hat[0].shape[0] - 1  # -1 car on enlève l'intercept\n    for m in range(K_1):\n        block = var_hat[m][1:, 1:]  # enlève intercept\n        row_start = m * p\n        row_end = (m + 1) * p\n        varBeta[row_start:row_end, row_start:row_end] = block\n    return varBeta\n\n# betaStar : concaténation des coefficients sans intercept\nbetaStar = beta_hat[:, 1:].flatten()\n\n# Compléter les blocs diagonaux de varBeta\nvarBeta = fill_varBeta_diagonal(varBeta, var_hat)\nprint(\"Shape de varBeta après remplissage diagonal :\", varBeta.shape)  # ((K-1)*p, (K-1)*p)\nprint(\"Aperçu de varBeta après remplissage diagonal :\\n\", varBeta[:5, :5])  # Afficher un aperçu    \n\nShape de varBeta après remplissage diagonal : (9, 9)\nAperçu de varBeta après remplissage diagonal :\n [[ 2.87696584e-02  8.96840197e-04 -9.43834509e-04  1.80729535e-03\n   2.32011452e-04]\n [ 8.96840197e-04  1.09112963e-01 -5.78619412e-02  3.21424832e-04\n   3.10295243e-03]\n [-9.43834509e-04 -5.78619412e-02  7.73627725e-02 -4.69694598e-04\n  -1.48837502e-03]\n [ 1.80729535e-03  3.21424832e-04 -4.69694598e-04  4.61543407e-03\n   1.04759944e-04]\n [ 2.32011452e-04  3.10295243e-03 -1.48837502e-03  1.04759944e-04\n   7.48753786e-03]]\n\n\n\nimport numpy as np\n\ndef construct_D(K_1, p):\n    \"\"\"\n    Construit la matrice D de taille ((K-2)*p, (K-1)*p) pour le test de Wald.\n    K_1 : nombre de seuils (K-1)\n    p   : nombre de variables explicatives (hors intercept)\n    \"\"\"\n    D = np.zeros(((K_1-1)*p, K_1*p))\n    I = np.eye(p)\n    for i in range(K_1-1):  # i = 0 à K-2\n        for j in range(K_1):\n            if j == 0:\n                temp = I\n            elif j == i+1:\n                temp = -I\n            else:\n                temp = np.zeros((p, p))\n            col_start = j*p\n            col_end = (j+1)*p\n            row_start = i*p\n            row_end = (i+1)*p\n            D[row_start:row_end, col_start:col_end] += temp\n    return D\nD = construct_D(len(thresholds), len(explanatory_vars))\nprint(\"Shape de D :\", D.shape)  # ((K-2)*p, (K-1)*p)\nprint(\"Aperçu de D :\\n\", D[:5, :5])  # Afficher un aperçu\n\nShape de D : (6, 9)\nAperçu de D :\n [[ 1.  0.  0. -1.  0.]\n [ 0.  1.  0.  0. -1.]\n [ 0.  0.  1.  0.  0.]\n [ 1.  0.  0.  0.  0.]\n [ 0.  1.  0.  0.  0.]]\n\n\n\ndef wald_statistic(D, betaStar, varBeta):\n    \"\"\"\n    Calcule la statistique de Wald X^2 pour le test de proportionnalité.\n    \"\"\"\n    Db = D @ betaStar\n    V = D @ varBeta @ D.T\n    # Symétriser V pour stabilité\n    #V = 0.5 * (V + V.T)\n    # Utilise le pseudo-inverse par sécurité numérique\n    inv_V = np.linalg.inv(V)\n    X2 = float(Db.T @ inv_V @ Db)\n    return X2\n\n\n# Supposons que tu as K_1, p, betaStar, varBeta\nK_1 = len(binary_models)\np = len(explanatory_vars)  # Nombre de variables explicatives (hors intercept)\nD = construct_D(K_1, p)\nX2 = wald_statistic(D, betaStar, varBeta)\nddl = (K_1-1)*p\n\nfrom scipy.stats import chi2\npval = 1 - chi2.cdf(X2, ddl)\n\nprint(f\"Statistique X² = {X2:.4f}\")\nprint(f\"Degrés de liberté = {ddl}\")\nprint(f\"p-value = {pval:.4g}\")\n\nStatistique X² = 42.8803\nDegrés de liberté = 6\np-value = 1.232e-07"
  }
]