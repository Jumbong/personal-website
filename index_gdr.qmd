---
format: 
  html: 
    fontsize: 1.3em
---

<div class="box">
Vous trouverez ici certains exercices de la fiche de TD, qui vous permettront de mieux comprendre les concepts abordés par le professeur pendant les cours. N'hésitez pas à me contacter si vous avez des questions ou des suggestions. Je mettrai l'accent sur la rédaction des corrections.
</div>

# Espaces probabilisés

Je reviendrai ici sur l'exercice 5 que nous n'avons pas eu la possibilité de mettre oeuvre la simulation monte Carlo.

## Correction de l'exercice 5

Nous sélectionnons un entier au hasard parmi les N = 5,000 entiers qui sont dans l'intervalle [0, 4999]. 
Quelle est la probabilité qu’il soit divisible par 4,7 ou 10 ?

Modifier le code R vu en cours pour estimer cette probabilité par une simulation
de Monte-Carlo pour vérifier ce résultat. Vous utiliserez au moins 50,000
simulations.

Pour résoudre cette exercice, il faut d'abord définir l'espace probabilisé. 
L'espace probabilisé est défini par le triplet (Ω, F, P) où:


- Ω est l'ensemble des résultats possibles. Dans notre cas, Ω = {0, 1, 2, ..., 4999}.

- F est la σ-algèbre des événements. Ici, nous pouvons considérer les événements comme les sous-ensembles de Ω.

- P est la mesure de probabilité. Dans notre cas, chaque entier a une probabilité égale d'être sélectionné. On dit encore que la loi de probabilité est uniforme sur Ω.

Le cardinal de l'ensemble Ω est |Ω| = 5000.
La probabilité d'un élément spécifique dans Ω est donc $P({x}) = \frac{1}{5000}$ pour tout x dans Ω.

Après avoir défini l'espace probabilisé, nous pouvons maintenant définir l'événement A que nous voulons étudier. 
De ce fait, définissons les événements suivants:

- $A_4$ : l'événement que l'entier sélectionné est divisible par 4.
- $A_7$ : l'événement que l'entier sélectionné est divisible par 7.
- $A_{10}$ : l'événement que l'entier sélectionné est divisible par 10.
L'événement A que nous voulons étudier est l'union de ces trois événements:

$$
A = A_4 \cup A_7 \cup A_{10}
$$

Nous cherchons à calculer la probabilité de l'événement A, c'est-à-dire $P(A)$. Pour cela, nous allons utiliser la formule inclusion-exclusion:
$$
\begin{aligned}
\mathbb{P}(A_4 \cup A_7 \cup A_{10})
&= \mathbb{P}(A_4) + \mathbb{P}(A_7) + \mathbb{P}(A_{10}) \\
&\quad - \big\{\, \mathbb{P}(A_4 \cap A_7) + \mathbb{P}(A_4 \cap A_{10}) + \mathbb{P}(A_7 \cap A_{10}) \,\big\} \\
&\quad + \mathbb{P}(A_4 \cap A_7 \cap A_{10}).
\end{aligned}
$$

Calculons maintenant chaque terme de cette formule:
Pour calculer chaque probabilité, nous devons compter le nombre d'entiers dans Ω qui satisfont chaque condition $A_i$.
Et la probabilité de chaque événement est donnée par le rapport du nombre d'entiers satisfaisant la condition sur le cardinal de l'ensemble Ω : 

$$
P(A_i) = \frac{\text{nombre d'entiers satisfaisant } A_i}{5000}.
$$


- Pour $A_4$ : Les entiers divisibles par 4 sont 0, 4, 8, ..., 4996. Ainsi le nombre d'entiers divisibles par 4 est de la forme $4k$ et qui vérifient $0 \leq 4(k+1) \leq 5000 < 4(k+2)$, donc de cette inégalité, on déduit que $k$ vérifie l'inégalité sous dessous :

$$
k \leq \frac{4999}{4} < k+1.
$$

Donc le nombre d'entiers divisibles par 4 correspond à la partie entière de $\frac{4999}{4}$ plus 1 (pour inclure le 0), soit:
$$
\text{nombre d'entiers divisibles par 4} = \left\lfloor\frac{4999}{4}\right\rfloor + 1 = 1249 + 1 = 1250.
$$

Donc, la probabilité que l'entier sélectionné soit divisible par 4 est:
$$
P(A_4) = \frac{1250}{5000} = 0.25.
$$

- Pour $A_7$ : Les entiers divisibles par 7 sont 0, 7, 14, ..., 4996. En suivant le même raisonnement que pour $A_4$, nous trouvons:

$$
\text{nombre d'entiers divisibles par 7} = \left\lfloor\frac{4999}{7}\right\rfloor + 1 = 714 + 1 = 715.
$$

Donc, la probabilité que l'entier sélectionné soit divisible par 7 est:
$$
P(A_7) = \frac{715}{5000} = 0.143.
$$

- Pour $A_{10}$ : Les entiers divisibles par 10 sont 0, 10, 20, ..., 4990. En suivant le même raisonnement que pour $A_4$, nous trouvons:
$$
\text{nombre d'entiers divisibles par 10} = \left\lfloor\frac{4999}{10}\right\rfloor + 1 = 499 + 1 = 500.
$$

Donc, la probabilité que l'entier sélectionné soit divisible par 10 est:
$$
P(A_{10}) = \frac{500}{5000} = 0.1.
$$  

- Pour $A_4 \cap A_7$ : Les entiers qui sont divisibles par 4 et 7 sont ceux qui sont divisibles par 28 qui est le PPCM de 4 et 7. En suivant le même raisonnement que pour $A_4$, nous trouvons:
$$
\text{nombre d'entiers divisibles par 28} = \left\lfloor\frac{4999}{28}\right\rfloor + 1 = 178 + 1 = 179.
$$

Donc, la probabilité que l'entier sélectionné soit divisible par 4 et 7 est:

$$
P(A_4 \cap A_7) = \frac{179}{5000} = 0.0358.
$$

- Pour $A_4 \cap A_{10}$ : Les entiers qui sont divisibles par 4 et 10 sont ceux qui sont divisibles par 20 qui est le PPCM de 4 et 10. En suivant le même raisonnement que pour $A_4, nous trouvons:
$$
\text{nombre d'entiers divisibles par 20} = \left\lfloor\frac{4999}{20}\right\rfloor + 1 = 249 + 1 = 250.
$$

Donc, la probabilité que l'entier sélectionné soit divisible par 4 et 10 est:
$$
P(A_4 \cap A_{10}) = \frac{250}{5000} = 0.05.
$$

- Pour $A_7 \cap A_{10}$ : Les entiers qui sont divisibles par 7 et 10 sont ceux qui sont divisibles par 70 qui est le PPCM de 7 et 10. En suivant le même raisonnement que pour $A_4, nous trouvons:

$$
\text{nombre d'entiers divisibles par 70} = \left\lfloor\frac{4999}{70}\right\rfloor + 1 = 71 + 1 = 72.
$$

Donc, la probabilité que l'entier sélectionné soit divisible par 7 et 10 est:
$$
P(A_7 \cap A_{10}) = \frac{72}{5000} = 0.0144.
$$

- Pour $A_4 \cap A_7 \cap A_{10}$ : Les entiers qui sont divisibles par 4, 7 et 10 sont ceux qui sont divisibles par 140 qui est le PPCM de 4, 7 et 10. En suivant le même raisonnement que pour $A_4$, nous trouvons:

$$
\text{nombre d'entiers divisibles par 140} = \left\lfloor\frac{4999}{140}\right\rfloor + 1 = 35 + 1 = 36.
$$

Donc, la probabilité que l'entier sélectionné soit divisible par 4, 7 et 10 est:
$$
P(A_4 \cap A_7 \cap A_{10}) = \frac{36}{5000} = 0.0072.
$$  

En substituant ces valeurs dans la formule d'inclusion-exclusion, nous obtenons:
$$
\begin{aligned}
P(A)
&= P(A_4) + P(A_7) + P(A_{10}) \\
&\quad - \big\{\, P(A_4 \cap A_7) + P(A_4 \cap A_{10}) + P(A_7 \cap A_{10}) \,\big\} \\
&\quad + P(A_4 \cap A_7 \cap A_{10}) \\
&= 0.25 + 0.143 + 0.1 \\
&\quad - \big\{\, 0.0358 + 0.05 + 0.0144 \,\big\} \\
&\quad + 0.0072 \\
&= 0.493 - 0.1002 + 0.0072 \\
&= 0.4.
\end{aligned}
$$  

Donc, la probabilité qu'un entier sélectionné au hasard parmi les 5000 entiers soit divisible par 4, 7 ou 10 est de 0.4 ou 40%.

Maintenant, nous allons vérifier ce résultat par une simulation de Monte-Carlo en R avec au moins 50,000 simulations.
Une simulation de Monte-Carlo est une méthode statistique qui utilise des échantillons aléatoires pour estimer des propriétés mathématiques ou physiques comme des espérances, des intégrales ou des probabilités. Cette méthode fonctionne en générant un grand nombre de scénarios aléatoires et en observant les résultats pour obtenir une estimation statistique :

- On fixe le nombre de simulations, disons n = 50000.
- On initialise un compteur pour le nombre de succès (entiers divisibles par 4, 7 ou 10).
- Pour chaque simulation, on génère un entier aléatoire entre 0 et 4999.
- On vérifie si cet entier est divisible par 4, 7 ou 10. Si c'est le cas, on incrémente le compteur de succès.
- Après avoir effectué toutes les simulations, on calcule la probabilité estimée comme le ratio du nombre de succès sur le nombre total de simulations.

Voici un exemple de code R pour effectuer cette simulation de Monte-Carlo :

```{python}
# Monte Carlo in Python with running estimate and plot (matplotlib only)

import numpy as np
import matplotlib.pyplot as plt

# --- Parameters (mirror your R snippet) ---
seed = 123
N = 4999               # sample from 0..N (inclusive)
M = 50_000             # number of simulations

rng = np.random.default_rng(seed)
x = rng.integers(low=0, high=N+1, size=M)  # uniform integers in [0, N]

is_div = (x % 4 == 0) | (x % 7 == 0) | (x % 10 == 0)

# Final Monte Carlo estimate (same as R's mean(is_div))
p_hat = is_div.mean()

# Running estimates vs number of simulations
running_est = np.cumsum(is_div) / np.arange(1, M + 1)

# Mean of the running estimates (to draw an horizontal reference line)
mean_running = running_est.mean()

# --- Plot ---
plt.figure(figsize=(7.24, 4.07), dpi=100)  # ~724x407 px

plt.plot(np.arange(1, M + 1), running_est, linewidth=2)
plt.axhline(mean_running, linestyle="--", linewidth=2)
plt.xlabel("Nombre de simulations")
plt.ylabel("Estimation de la probabilité")
plt.title("Convergence de l’estimation Monte-Carlo", fontsize=14, weight="bold")
plt.suptitle(
    "Courbe de l’estimation cumulée; ligne horizontale = moyenne des estimations",
    fontsize=10, color="gray"
)

# Style cues similar to the provided seaborn example
ax = plt.gca()
for spine in ["top", "right"]:
    ax.spines[spine].set_visible(False)
plt.grid(False)

plt.tight_layout()
plt.show()

p_hat


```


::: {.byline}
<span class="date">May 28, 2025</span>
:::

# Rappels de la séance précédente

L'objectif de la prémière séance de TD était de consolider vos connaissances sur les espaces probabilisés et sur les probabilités conditionnelles.

Vous devez actuellement être capable de :

- Définir un espace probabilisé (Ω, F, P).

- Utiliser les propriétés des probabilités pour calculer des probabilités d'événements simples et composés.

- Vous devez maitriser les lois de Morgan :

  - La loi de Morgan pour l'union : $\overline{A \cup B} = \overline{A} \cap \overline{B}$
  - La loi de Morgan pour l'intersection : $\overline{A \cap B} = \overline{A} \cup \overline{B}$

- Appliquer la formule d'inclusion-exclusion pour calculer la probabilité de l'union de deux événements :

  $$
  P(A \cup B) = P(A) + P(B) - P(A \cap B)
  $$

- Appliquer la formule d'inclusion-exclusion pour calculer la probabilité de l'union de plusieurs événements :

  $$
  \begin{aligned}
  P\left(\bigcup_{i=1}^{n} A_i\right) &= \sum_{i=1}^{n} P(A_i) - \sum_{1 \leq i < j \leq n} P(A_i \cap A_j) \\
  &\quad + \sum_{1 \leq i < j < k \leq n} P(A_i \cap A_j \cap A_k) - \ldots + (-1)^{n+1} P(A_1 \cap A_2 \cap \ldots \cap A_n)
  \end{aligned}
  $$
- Calculer des probabilités conditionnelles en utilisant la formule de Bayes :

$$
  P(A|B) = \frac{P(A \cap B)}{P(B)}.
$$

ou bien encore :

$$
  P(A\cap B) = P(A|B) \cdot P(B).
$$

- Appliquer la loi des probabilités totales pour décomposer des probabilités complexes en utilisant des événements disjoints et exhaustifs :

Par exemple, si $C_1, C_2$ sont deux événements disjoints et exhaustifs, c'est-à-dire que $C_1 \cap C_2 = \emptyset$ et $C_1 \cup C_2 = \Omega$, alors pour tout événement A, on a :

$$
P(A) = P(A|C_1) \cdot P(C_1) + P(A|C_2) \cdot P(C_2).
$$

Et finalement, cela nous permet de calculer la probabilité de $C_1$ sachant A en utilisant la formule de Bayes :

$$
P(C_1|A) = \frac{P(A|C_1) \cdot P(C_1)}{P(A|C_1) \cdot P(C_1) + P(A|C_2) \cdot P(C_2)}.
$$


```{mermaid}
flowchart LR
  A[Espaces probabilisés] --> B(Variable aléatoire X)
  B --> C{type}
  C --> D[Discrète]
  C --> E[Continue]
  D --> F[Caractérisation de la loi de X]
  E --> G[Caractérisation de la loi de X]
```

# Variables aléatoires générales


## Exercice 1

Nous considérons l’expérience aléatoire consistant à lancer 3 fois une pièce de monnaie. Soit $X$ la variable aléatoire donnant le nombre de « Pile » obtenu.

1) Donner la loi de la variable aléatoire $X$.
2) Donner la fonction de répartition de $X$, et la représenter graphiquement.
3) Calculer l’espérance, la variance et le coefficient de variation de $X$.

## Correction de l'exercice 1

1) Nous considérons l'espace probabilisé $(\Omega, \mathcal{F}, P)$ associé à l'expérience aléatoire de lancer 3 fois une pièce de monnaie. L'ensemble des résultats possibles $\Omega$ est donné par :

$$
\Omega = \{PPP, PPF, PFP, PFF, FPP, FPF, FFP, FFF\} = \{P, F\}^3
$$

Nous munissons cet espace de la tribu $\mathcal{F}$ des parties de $\Omega$ et de la probabilité uniforme $P$.

La variable aléatoire $X$ est définie comme le nombre de « Pile » obtenus lors des 3 lancers. Ainsi, $X$ peut prendre les valeurs suivantes :
- $X = 0$ : lorsque le résultat est FFF
- $X = 1$ : lorsque le résultat est PFF, FPF, ou FFP
- $X = 2$ : lorsque le résultat est PPF, PFP, ou FPP
- $X = 3$ : lorsque le résultat est PPP

Cette variable est donc définie comme suit :


$$
X : \Omega^3 \longrightarrow \Omega' = \{0,1,2,3\}
$$

$$
(\omega_1, \omega_2, \omega_3) \longmapsto
\begin{cases}
0 & \text{si } (\omega_1, \omega_2, \omega_3) = (FFF),\\[4pt]
1 & \text{si } (\omega_1, \omega_2, \omega_3) \in \{(PFF),(FPF),(FFP)\},\\[4pt]
2 & \text{si } (\omega_1, \omega_2, \omega_3) \in \{(PPF),(PFP),(FPP)\},\\[4pt]
3 & \text{si } (\omega_1, \omega_2, \omega_3) = (PPP).
\end{cases}
$$

Comme la variable aléatoire X est **discrète**, il suffit de déterminer la probabilité $\mathbb{P}_X$ sur les singletons.

$$
\begin{aligned}
\mathbb{P}_X(0) &= \frac{1}{8},\
\mathbb{P}_X(1) &= \frac{3}{8},\
\mathbb{P}_X(2) &= \frac{3}{8},\
\mathbb{P}_X(3) &= \frac{1}{8}.
\end{aligned}
$$

Ainsi, ( X ) suit une **loi binomiale** :
$$
X \sim \mathcal{B}(n=3,, p=\tfrac{1}{2}).
$$



2. Fonction de répartition

À partir de la définition de la fonction de répartition :
$$
F_X(x) = \mathbb{P}(X \le x),
$$
on obtient :

$$
F_X(x) =
\begin{cases}
0 & \text{si } x < 0,\\[4pt]
\frac{1}{8} & \text{si } 0 \le x < 1,\\[4pt]
\frac{4}{8} = \frac{1}{2} & \text{si } 1 \le x < 2,\\[4pt]
\frac{7}{8} & \text{si } 2 \le x < 3,\\[4pt]
1 & \text{si } 3 \le x.
\end{cases}
$$

3. Espérance, variance et coefficient de variation

Pour calculer l'espérance, la variance et le coefficient de variation de la variable aléatoire $X$, nous allons utiliser la formule de transfert en temps discret :

$$
\mathbb{E}[g(X)] = \sum_{i} g(x_i) \cdot \mathbb{P}_X(x_i),
$$
où $g$ est une fonction mésurable.

Ainsi, pour l'espérance, nous avons :
$$
\mathbb{E}[X] = \sum_{x=0}^{3} x \cdot \mathbb{P}_X(x) = 0 \cdot \frac{1}{8} + 1 \cdot \frac{3}{8} + 2 \cdot \frac{3}{8} + 3 \cdot \frac{1}{8} = \frac{12}{8} = np =1.5.
$$

Pour la variance, nous utilisons la formule :

$$
\text{Var}(X) = \sum_{x=0}^{3} (x - \mathbb{E}[X])^2 \cdot \mathbb{P}_X(x) = (0 - 1.5)^2 \cdot \frac{1}{8} + (1 - 1.5)^2 \cdot \frac{3}{8} + (2 - 1.5)^2 \cdot \frac{3}{8} + (3 - 1.5)^2 \cdot \frac{1}{8} = \frac{6}{8} = np(1-p) = 0.75.
$$

Enfin, le coefficient de variation est donné par la formule :

$$
CV(X) = \frac{\sigma_X}{\mathbb{E}[X]} = \frac{\sqrt{\text{Var}(X)}}{\mathbb{E}[X]} = \frac{\sqrt{0.75}}{1.5} = \frac{\sqrt{3}}{3} \approx 0.577.
$$

# Exercice 2

Nous considérons l’expérience aléatoire consistant à jeter un dé jusqu’à obtenir un **« 6 »**. Soit $X$ la variable aléatoire donnant le nombre de jets.

1. Donner la loi de la variable aléatoire $X$.
2. Donner la fonction de répartition de $X$, et la représenter graphiquement.
3. Montrer que la fonction génératrice des moments de $X$ vaut :

   $$
   M_X(t) = \frac{pe^t}{1 - e^t(1 - p)},
   $$

   avec $p = \frac{1}{6}$.
    
4. En déduire l’espérance et la variance de $X$, puis son coefficient de variation.

## Correction de l'exercice 2

1) Loi de la variable aléatoire X

Nous considérons l'espace probabilisé $(\Omega^{\mathbb{N}}, \mathcal{P}(\Omega^{\mathbb{N}}), \mathbb{P})$ avec $\Omega = \{1,2,3,4,5,6\}$ muni de la tribu produit et de la probabilité uniforme $\mathbb{P}$. La variable aléatoire étudiée est

$$
X : \Omega^{\mathbb{N}} \longrightarrow \Omega' = \mathbb{N}^*
$$

$$
(\omega_1, \omega_2, \ldots) \longmapsto
\begin{cases}
1 & \text{si } \omega_1 = \{6\},\\[4pt]
2 & \text{si } \omega_1 \neq \{6\} \text{ et } \omega_2 = \{6\},\\[4pt]
& \cdots\\[4pt]
k & \text{si } \omega_1, \ldots, \omega_{k-1} \neq \{6\} \text{ et } \omega_k = \{6\},\\[4pt]
& \cdots
\end{cases}
$$

Pour tout $k \in \mathbb{N}^*$, on a :

$$
\mathbb{P}(X = k) = \mathbb{P}(\omega_1, \ldots, \omega_{k-1} \neq \{6\} \text{ et } \omega_k = \{6\})
$$

$$
= \prod_{i=1}^{k-1} \mathbb{P}(\omega_i \neq \{6\}) \times \mathbb{P}(\omega_k = \{6\})
$$

$$
= (1 - p)^{k-1}p,
$$

avec $p = \frac{1}{6}$ la probabilité d'obtenir un "6" lors d'un lancer. Il s'agit d'une distribution géométrique de paramètre $p$. 

2) Fonction de répartition

Soit $k \geq 1$. Nous avons :

$$
\mathbb{P}(X \leq k) = \sum_{j=1}^{k} (1-p)^{j-1}p
$$

$$
= p\sum_{j=0}^{k-1} (1-p)^{j} = p\frac{1-(1-p)^k}{1-(1-p)} = 1-(1-p)^k.
$$

A partir de la définition de la fonction de répartition, nous obtenons

$$
F_X(x) = 
\begin{cases}
0 & \text{si } x < 1,\\[4pt]
p & \text{si } 1 \leq x < 2,\\[4pt]
& \cdots,\\[4pt]
1-(1-p)^k & \text{si } k \leq x < k+1,\\[4pt]
& \cdots.
\end{cases}
$$

3) Fonction génératrice des moments

En utilisant la formule de transfert en temps discret, nous avons :
$$
M_X(t) = \int e^{tX} d\mathbb{P} = \sum_{x=1}^{\infty} e^{tx}\mathbb{P}_X(x)
$$

$$
= p\sum_{j=1}^{\infty} e^{tj}(1-p)^{j-1}
$$

$$
= pe^t\sum_{j=0}^{\infty} e^{tj}(1-p)^j
$$

$$
= pe^t\sum_{j=0}^{\infty} q^j \text{ avec } q = e^t(1-p)
$$

$$
= \frac{pe^t}{1-q} = \frac{pe^t}{1-e^t(1-p)}.
$$

La série converge si et seulement si $|q| < 1$, c'est-à-dire si $|e^t(1-p)| < 1$, soit $e^t < \frac{1}{1-p}$, ce qui donne $t < \ln\left(\frac{1}{1-p}\right) = -\ln(1-p)$.

4) Espérance, variance et coefficient de variation

Nous savons que la fonction $M_X$ est indéfiniment dérivable dans un voisinage de 0, et que $M'_X(0) = E(X)$ et $M''_X(0) = E(X^2)$. Nous avons après calcul

$$
M'_x(t) = \frac{pe^t}{\{1-e^t(1-p)\}^2},
$$

$$
M''_x(t) = \frac{pe^t\{1-(1-p)^2e^{2t}\}}{\{1-e^t(1-p)\}^4},
$$

dont nous déduisons que

$$
M'_x(0) = \frac{1}{p} \equiv E(X),
$$

$$
M''_x(0) = \frac{p\{1-(1-p)^2\}}{\{1-(1-p)\}^4} = \frac{2-p}{p^2} \equiv E(X^2),
$$

puis

$$
V(X) = \frac{1-p}{p^2},
$$

$$
CV(X) = \sqrt{1-p}.
$$

Numériquement, nous obtenons

$$
E(X) = 6, \quad V(X) = 30, \quad CV(X) = 0.91.
$$


## Exercice 3

Soit $X$ une variable aléatoire admettant un moment d'ordre 2. Nous notons $m = E(X)$ et $\sigma^2 = V(X)$. Soient $X_1, \ldots, X_n$ des variables aléatoires indépendantes et de même loi que $X$. Nous notons

$$
\overline{X}_n = \frac{1}{n}\sum_{i=1}^{n} X_i
$$

la moyenne empirique des $X_i$, $i = 1, \ldots, n$, et

$$
s_X^2 = \frac{1}{n-1}\sum_{i=1}^{n} (X_i - \overline{X}_n)^2
$$

la dispersion des $X_i$, $i = 1, \ldots, n$.

1) Montrer que $E(\overline{X}_n) = m$.

2) Montrer que $E(s_X^2) = \sigma^2$. Vous pourrez utiliser l'identité

$$
s_X^2 = \frac{1}{2n(n-1)} \sum_{i=1}^{n} \sum_{\substack{j=1\\j \neq i}}^{n} (X_i - X_j)^2.
$$

3) Utiliser ces résultats pour obtenir une approximation de Monte-Carlo de l'espérance, la variance et le coefficient de variation de la variable aléatoire $X$ de l'exercice 2.

- Démonstration de l'équivalence des formules de variance

Nous voulons démontrer que :

$$\frac{1}{n-1}\sum_{i=1}^{n}(X_i - \bar{X}_n)^2 = \frac{1}{2n(n-1)}\sum_{i=1}^{n}\sum_{\substack{j=1\\j\neq i}}^{n}(X_i - X_j)^2$$

- Étape 1 : Développement de la somme double

Commençons par développer le membre de droite :

$$\sum_{i=1}^{n}\sum_{\substack{j=1\\j\neq i}}^{n}(X_i - X_j)^2$$

Développons $(X_i - X_j)^2$ :

$$= \sum_{i=1}^{n}\sum_{\substack{j=1\\j\neq i}}^{n}(X_i^2 - 2X_iX_j + X_j^2)$$

$$= \sum_{i=1}^{n}\sum_{\substack{j=1\\j\neq i}}^{n}X_i^2 - 2\sum_{i=1}^{n}\sum_{\substack{j=1\\j\neq i}}^{n}X_iX_j + \sum_{i=1}^{n}\sum_{\substack{j=1\\j\neq i}}^{n}X_j^2$$

- Étape 2 : Simplification de chaque terme

Premier terme :
$$\sum_{i=1}^{n}\sum_{\substack{j=1\\j\neq i}}^{n}X_i^2 = \sum_{i=1}^{n}X_i^2 \cdot (n-1) = (n-1)\sum_{i=1}^{n}X_i^2$$

Car pour chaque $i$ fixé, $X_i^2$ est sommé $(n-1)$ fois (pour tous les $j \neq i$).

Troisième terme :
Par symétrie, le troisième terme donne le même résultat :
$$\sum_{i=1}^{n}\sum_{\substack{j=1\\j\neq i}}^{n}X_j^2 = (n-1)\sum_{j=1}^{n}X_j^2 = (n-1)\sum_{i=1}^{n}X_i^2$$

Deuxième terme :
$$\sum_{i=1}^{n}\sum_{\substack{j=1\\j\neq i}}^{n}X_iX_j$$

Pour chaque paire $(i,j)$ avec $i \neq j$, le produit $X_iX_j$ apparaît dans cette double somme. On peut réécrire :

$$= \sum_{i=1}^{n}X_i\sum_{\substack{j=1\\j\neq i}}^{n}X_j = \sum_{i=1}^{n}X_i\left(\sum_{j=1}^{n}X_j - X_i\right)$$

$$= \sum_{i=1}^{n}X_i \cdot n\bar{X}_n - \sum_{i=1}^{n}X_i^2$$

$$= n\bar{X}_n \cdot n\bar{X}_n - \sum_{i=1}^{n}X_i^2 = n^2\bar{X}_n^2 - \sum_{i=1}^{n}X_i^2$$

- Étape 3 : Assemblage

En rassemblant les trois termes :

$$\sum_{i=1}^{n}\sum_{\substack{j=1\\j\neq i}}^{n}(X_i - X_j)^2 = (n-1)\sum_{i=1}^{n}X_i^2 - 2\left(n^2\bar{X}_n^2 - \sum_{i=1}^{n}X_i^2\right) + (n-1)\sum_{i=1}^{n}X_i^2$$

$$= (n-1)\sum_{i=1}^{n}X_i^2 - 2n^2\bar{X}_n^2 + 2\sum_{i=1}^{n}X_i^2 + (n-1)\sum_{i=1}^{n}X_i^2$$

$$= 2(n-1)\sum_{i=1}^{n}X_i^2 + 2\sum_{i=1}^{n}X_i^2 - 2n^2\bar{X}_n^2$$

$$= 2n\sum_{i=1}^{n}X_i^2 - 2n^2\bar{X}_n^2$$

$$= 2n\left(\sum_{i=1}^{n}X_i^2 - n\bar{X}_n^2\right)$$

- Étape 4 : Lien avec la variance classique

Rappelons la formule classique de la variance :

$$\sum_{i=1}^{n}(X_i - \bar{X}_n)^2 = \sum_{i=1}^{n}(X_i^2 - 2X_i\bar{X}_n + \bar{X}_n^2)$$

$$= \sum_{i=1}^{n}X_i^2 - 2\bar{X}_n\sum_{i=1}^{n}X_i + n\bar{X}_n^2$$

$$= \sum_{i=1}^{n}X_i^2 - 2\bar{X}_n \cdot n\bar{X}_n + n\bar{X}_n^2$$

$$= \sum_{i=1}^{n}X_i^2 - n\bar{X}_n^2$$

- Étape 5 : Conclusion

D'après l'étape 3 :
$$
\sum_{i=1}^{n}\sum_{\substack{j=1\\j\neq i}}^{n}(X_i - X_j)^2 = 2n\left(\sum_{i=1}^{n}X_i^2 - n\bar{X}_n^2\right)
$$


D'après l'étape 4 :
$$
\sum_{i=1}^{n}(X_i - \bar{X}_n)^2 = \sum_{i=1}^{n}X_i^2 - n\bar{X}_n^2
$$

Donc :
$$
\sum_{i=1}^{n}\sum_{\substack{j=1\\j\neq i}}^{n}(X_i - X_j)^2 = 2n\sum_{i=1}^{n}(X_i - \bar{X}_n)^2
$$

En divisant par $2n(n-1)$ :

$$
\frac{1}{2n(n-1)}\sum_{i=1}^{n}\sum_{\substack{j=1\\j\neq i}}^{n}(X_i - X_j)^2 = \frac{1}{n-1}\sum_{i=1}^{n}(X_i - \bar{X}_n)^2
$$

## Correction de l'exercice 3

1) Calcul de $E(\overline{X}_n)$

Par linéarité de l'espérance, nous avons :

$$
E(\overline{X}_n) = E\left(\frac{1}{n}\sum_{i=1}^{n} X_i\right) = \frac{1}{n}\sum_{i=1}^{n} E(X_i) = \frac{1}{n} \cdot n \cdot m = m.
$$



2) Calcul de $E(s_X^2)$

En utilisant l'identité donnée, nous avons en utilisant la linéarité de l'espérance :

$$
E(s_X^2) = E\left(\frac{1}{2n(n-1)} \sum_{i=1}^{n} \sum_{\substack{j=1\\j \neq i}}^{n} (X_i - X_j)^2\right)
$$

$$
= \frac{1}{2n(n-1)} \sum_{i=1}^{n} \sum_{\substack{j=1\\j \neq i}}^{n} E\left((X_i - X_j)^2\right).
$$

Comme les $X_i$ sont indépendantes et de même loi, nous avons :

$$
= \frac{1}{2} E\left((X_1 - X_2)^2\right).
$$

D'autre part, nous avons :

$$
E\left((X_1 - X_2)^2\right) = E[{(X_1 - m) - (X_2 - m)}^2]
$$

$$
= E\left((X_1 - m)^2\right) + E\left((X_2 - m)^2\right) - 2E\left((X_1 - m)(X_2 - m)\right)
$$

$$
= \sigma^2 + \sigma^2 - 2 \cdot 0 = 2\sigma^2.
$$

Donc, nous obtenons :
$$
E(s_X^2) = \frac{1}{2} \cdot 2\sigma^2 = \sigma^2.
$$

3) Approximation de Monte-Carlo

Ce que nous avons montré aux points précédents, est que nous pouvons estimer l'espérance et la variance de la variable aléatoire $X$ en utilisant les statistiques d'échantillon $\overline{X}_n$ et $s_X^2$. C'est-à-dire en générant un grand nombre de réalisations indépendantes de $X$ et en calculant la moyenne empirique et la dispersion de ces réalisations.

Voici un exemple de code R pour effectuer cette approximation de Monte-Carlo pour la variable aléatoire $X$ de l'exercice 2 :


```default
# Monte Carlo approximation in R

# Répétition de l'expérience aléatoire
# Simulation du nombre de lancers nécessaires pour obtenir un 6

set.seed(360)
n <- 50000
ctr <- 0
simlist <- numeric(n)

while (ctr < n) {
  ctr <- ctr + 1
  
  # Valeur du lancer (initialisation)
  trial <- 0
  
  # Nombre de tentatives (initialisation)
  nb_tent <- 0
  
  while (trial != 6) {
    nb_tent <- nb_tent + 1
    trial <- sample(1:6, 1)
  }
  
  simlist[ctr] <- nb_tent
}

# Affichage des résultats
mean(simlist)
var(simlist)

# Statistiques supplémentaires
cat("\n=== Résultats de la simulation ===\n")
cat("Nombre de simulations:", n, "\n")
cat("Moyenne du nombre de lancers:", mean(simlist), "\n")
cat("Variance:", var(simlist), "\n")
cat("Écart-type:", sd(simlist), "\n")
cat("Minimum:", min(simlist), "\n")
cat("Maximum:", max(simlist), "\n")
cat("Médiane:", median(simlist), "\n")

# Histogramme
hist(simlist, 
     breaks = 30, 
     col = "lightblue", 
     border = "white",
     main = "Distribution du nombre de lancers pour obtenir un 6",
     xlab = "Nombre de lancers",
     ylab = "Fréquence",
     xlim = c(0, max(simlist)))

# Ajout de la moyenne théorique
abline(v = 6, col = "red", lwd = 2, lty = 2)
legend("topright", 
       legend = c("Moyenne observée", "Moyenne théorique = 6"),
       col = c("blue", "red"), 
       lty = c(1, 2), 
       lwd = 2)
```

```{python}

import random
import numpy as np
import matplotlib.pyplot as plt

# Répétition de l'expérience
random.seed(360)
n = 50000
simlist = []

for _ in range(n):
    trial = 0        # valeur du lancer
    nb_tent = 0      # nombre de tentatives
    
    while trial != 6:
        nb_tent += 1
        trial = random.randint(1, 6)  # équivalent de sample(1:6,1)
    
    simlist.append(nb_tent)

simlist = np.array(simlist)

# Résultats
print("\n=== Résultats de la simulation ===")
print("Nombre de simulations:", n)
print("Moyenne du nombre de lancers:", simlist.mean())
print("Variance:", simlist.var(ddof=1))
print("Écart-type:", simlist.std(ddof=1))
print("Minimum:", simlist.min())
print("Maximum:", simlist.max())
print("Médiane:", np.median(simlist))

# Histogramme
plt.figure(figsize=(8, 5))
plt.hist(simlist, bins=30, color="lightblue", edgecolor="white")
plt.title("Distribution du nombre de lancers pour obtenir un 6")
plt.xlabel("Nombre de lancers")
plt.ylabel("Fréquence")

# Ajout de la moyenne théorique
plt.axvline(6, color="red", linewidth=2, linestyle="--", label="Moyenne théorique = 6")
plt.axvline(simlist.mean(), color="blue", linewidth=2, linestyle="-", label="Moyenne observée")

plt.legend()
plt.show()

```



```default
# Utilisation de la distribution géométrique
# Alternative plus efficace à la simulation par boucle

set.seed(360)
n <- 50000
p <- 1/6
ctr <- 0
simlist <- numeric(n)

while (ctr < n) {
  ctr <- ctr + 1
  
  # Génération d'une réalisation d'une loi géométrique
  # Attention, rgeom(n,p) donne le nombre d'échecs avant
  # le premier succès
  simlist[ctr] <- rgeom(1, p) + 1
}

# Affichage des résultats
mean(simlist)
var(simlist)

# Statistiques détaillées
cat("\n=== Résultats de la simulation (méthode géométrique) ===\n")
cat("Nombre de simulations:", n, "\n")
cat("Probabilité de succès (p):", p, "\n")
cat("Moyenne observée:", mean(simlist), "\n")
cat("Moyenne théorique:", 1/p, "\n")
cat("Variance observée:", var(simlist), "\n")
cat("Variance théorique:", (1-p)/p^2, "\n")
cat("Écart-type observé:", sd(simlist), "\n")
cat("Médiane:", median(simlist), "\n")

# Comparaison graphique
par(mfrow = c(1, 2))

# Histogramme
hist(simlist, 
     breaks = 50, 
     col = "lightgreen", 
     border = "white",
     main = "Distribution du nombre de lancers\n(méthode rgeom)",
     xlab = "Nombre de lancers",
     ylab = "Fréquence",
     probability = TRUE)

# Ajout de la moyenne
abline(v = mean(simlist), col = "blue", lwd = 2)
abline(v = 1/p, col = "red", lwd = 2, lty = 2)
legend("topright", 
       legend = c("Moyenne obs.", "Moyenne théo."),
       col = c("blue", "red"), 
       lty = c(1, 2), 
       lwd = 2,
       cex = 0.8)

# QQ-plot pour vérifier la distribution
qqplot(qgeom(ppoints(n), p) + 1, simlist,
       main = "QQ-Plot : Théorique vs Observé",
       xlab = "Quantiles théoriques (Géométrique)",
       ylab = "Quantiles observés",
       col = "darkgreen",
       pch = 20,
       cex = 0.5)
abline(0, 1, col = "red", lwd = 2)

par(mfrow = c(1, 1))

# Méthode encore plus efficace (vectorisée)
cat("\n=== Méthode vectorisée (plus rapide) ===\n")
set.seed(360)
simlist_vec <- rgeom(n, p) + 1
cat("Moyenne:", mean(simlist_vec), "\n")
cat("Variance:", var(simlist_vec), "\n")
```



```{python}
import numpy as np
import matplotlib.pyplot as plt
import scipy.stats as st

# Paramètres
np.random.seed(360)
n = 50000
p = 1/6

# La loi géométrique de R donne "nombre d'échecs avant succès"
# => équivalent python : st.geom(p).rvs() renvoie déjà nb_tentatives
# MAIS Python compte à partir de 1, donc c'est ce qu’on veut !
simlist = st.geom(p).rvs(size=n)

# Affichage des résultats
print("\n=== Résultats de la simulation (méthode géométrique) ===")
print("Nombre de simulations:", n)
print("Probabilité de succès p:", p)
print("Moyenne observée:", simlist.mean())
print("Moyenne théorique:", 1/p)
print("Variance observée:", simlist.var(ddof=1))
print("Variance théorique:", (1 - p) / p**2)
print("Écart-type observé:", simlist.std(ddof=1))
print("Médiane:", np.median(simlist))


plt.figure(figsize=(12, 5))

plt.hist(simlist, bins=50, color="lightgreen", edgecolor="white", density=True)
plt.title("Distribution du nombre de lancers (méthode géométrique)")
plt.xlabel("Nombre de lancers")
plt.ylabel("Densité")

# Moyennes
plt.axvline(simlist.mean(), color="blue", linewidth=2, label="Moyenne observée")
plt.axvline(1/p, color="red", linewidth=2, linestyle="--", label="Moyenne théorique")

plt.legend()
plt.show()

```