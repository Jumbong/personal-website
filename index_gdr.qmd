---
format: 
  html: 
    fontsize: 1.3em
---

<div class="box">
Vous trouverez ici certains exercices de la fiche de TD, qui vous permettront de mieux comprendre les concepts abordés par le professeur pendant les cours. N'hésitez pas à me contacter si vous avez des questions ou des suggestions. Je mettrai l'accent sur la rédaction des corrections.
</div>

# 1. Espaces probabilisés

## Exercice 2

Supposons que $\mathbb{P}(A \cup B) = 0.6$ et $\mathbb{P}(A \cup B^c) = 0.8$.  
Déterminer $\mathbb{P}(A)$.

---

## Correction Exercice 2

On se place sur un espace probabilisé $(\Omega, \mathcal{F}, \mathbb{P})$.  
Soient $A, B \in \mathcal{F}$ deux événements.

On utilise d’abord les propriétés des complémentaires :
$$
(A \cup B)^c = A^c \cap B^c,
\qquad
(A \cup B^c)^c = A^c \cap B.
$$

Les deux événements $(A \cup B)^c$ et $(A \cup B^c)^c$ sont disjoints, car :
$$
(A \cup B)^c \cup (A \cup B^c)^c
= (A^c \cap B^c) \cup (A^c \cap B)
= A^c.
$$

De plus, pour deux événements disjoints $E$ et $F$,
$$
\mathbb{P}(E \cup F) = \mathbb{P}(E) + \mathbb{P}(F).
$$

Par conséquent,
$$
\mathbb{P}(A^c)
= \mathbb{P}\big( (A \cup B)^c \big)
+ \mathbb{P}\big( (A \cup B^c)^c \big).
$$

Or
$$
\mathbb{P}\big( (A \cup B)^c \big)
= 1 - \mathbb{P}(A \cup B)
= 1 - 0.6
= 0.4,
$$
$$
\mathbb{P}\big( (A \cup B^c)^c \big)
= 1 - \mathbb{P}(A \cup B^c)
= 1 - 0.8
= 0.2.
$$

Ainsi,
$$
\mathbb{P}(A^c) = 0.4 + 0.2 = 0.6,
$$
et donc
$$
\mathbb{P}(A) = 1 - \mathbb{P}(A^c) = 1 - 0.6 = 0.4.
$$

---
$$
\boxed{\mathbb{P}(A) = 0.4}
$$


## Exercice 3

Dans une ville, 75% de la population a les cheveux bruns, 50% a les yeux marron, et 35% possède à la fois des cheveux bruns et des yeux marron.  
On sélectionne une personne au hasard dans la ville. Quelle est la probabilité :

- qu’elle ait les yeux marron **ou** des cheveux bruns ?
- qu’elle n’ait ni les yeux marron, ni des cheveux bruns ?

---

## Correction Exercice 3

On se place sur un espace probabilisé $(\Omega, \mathcal{F}, \mathbb{P})$ où $\Omega = \{\text{personnes de la ville}\}$.

On définit les événements :
- $B = \{\text{la personne a les cheveux bruns}\}$,
- $M = \{\text{la personne a les yeux marron}\}$.

D’après l’énoncé :
$$
\mathbb{P}(B) = 0.75, \qquad \mathbb{P}(M) = 0.50, \qquad \mathbb{P}(B \cap M) = 0.35.
$$

---

### 1 Probabilité que la personne ait les yeux marron **ou** des cheveux bruns

On utilise la formule d’addition :
$$
\mathbb{P}(B \cup M) = \mathbb{P}(B) + \mathbb{P}(M) - \mathbb{P}(B \cap M).
$$

Donc
$$
\mathbb{P}(B \cup M) = 0.75 + 0.50 - 0.35 = 0.90.
$$

---

### 2 Probabilité qu’elle n’ait **ni** les yeux marron **ni** des cheveux bruns

On veut $\mathbb{P}(B^c \cap M^c)$.  
Or
$$
B^c \cap M^c = (B \cup M)^c.
$$

Donc
$$
\mathbb{P}(B^c \cap M^c) = 1 - \mathbb{P}(B \cup M) = 1 - 0.90 = 0.10.
$$

---

### Résultats

$$
\boxed{\mathbb{P}(\text{yeux marron ou cheveux bruns}) = 0.90}
$$

$$
\boxed{\mathbb{P}(\text{ni yeux marron ni cheveux bruns}) = 0.10}
$$

## Exercice 4

Nous lançons 3 dés équilibrés et indépendants.  
Calculer la probabilité d’obtenir **au moins un 6** :

- en utilisant la formule d’inclusion–exclusion,
- en calculant la probabilité de **n’obtenir aucun 6**.

---

## Correction Exercice 4

On se place sur un espace probabilisé $(\Omega, \mathcal{F}, \mathbb{P})$ où la probabilité $\mathbb{P}$ est uniforme sur $\Omega$.

L’univers $\Omega$ peut s’écrire de deux façons équivalentes :

- sous forme cartésienne :
$$
\Omega = \{1,2,3,4,5,6\}^3
$$

- sous forme explicite :
$$
\Omega = \{(w_1, w_2, w_3) \mid w_i \in \{1,2,3,4,5,6\},\ i = 1,2,3\}.
$$

On définit les événements suivants :

- $A_1 = \{\text{le premier dé donne un } 6\}$,
- $A_2 = \{\text{le deuxième dé donne un } 6\}$,
- $A_3 = \{\text{le troisième dé donne un } 6\}$.

Nous cherchons
$$
\mathbb{P}(A_1 \cup A_2 \cup A_3),
$$
la probabilité d’obtenir au moins un 6.

---

### 1 Méthode 1 : formule d’inclusion–exclusion

Pour trois événements $A_1, A_2, A_3$, on a
$$
\mathbb{P}(A_1 \cup A_2 \cup A_3)
= \mathbb{P}(A_1) + \mathbb{P}(A_2) + \mathbb{P}(A_3)
- \mathbb{P}(A_1 \cap A_2) - \mathbb{P}(A_1 \cap A_3) - \mathbb{P}(A_2 \cap A_3)
+ \mathbb{P}(A_1 \cap A_2 \cap A_3).
$$

Comme les dés sont équilibrés et indépendants :
$$
\mathbb{P}(A_1) = \mathbb{P}(A_2) = \mathbb{P}(A_3) = \frac{1}{6},
$$
$$
\mathbb{P}(A_i \cap A_j) = \frac{1}{6} \times \frac{1}{6} = \frac{1}{36}, \quad \text{pour } i \ne j,
$$
$$
\mathbb{P}(A_1 \cap A_2 \cap A_3) = \left(\frac{1}{6}\right)^3 = \frac{1}{216}.
$$

Donc :
$$
\mathbb{P}(A_1 \cup A_2 \cup A_3)
= 3 \times \frac{1}{6}
- 3 \times \frac{1}{36}
+ \frac{1}{216}
= \frac{1}{2} - \frac{1}{12} + \frac{1}{216}
= \frac{91}{216}
\simeq 0.421.
$$

---

### 2 Méthode 2 : en passant par l’événement complémentaire

L’événement “obtenir au moins un 6” est le complément de “n’obtenir aucun 6”.

L’événement “aucun 6” s’écrit :
$$
\{A_1 \cup A_2 \cup A_3\}^c
= A_1^c \cap A_2^c \cap A_3^c,
$$
où $A_i^c$ signifie “le $i$-ème dé ne donne pas 6”.

Pour un dé, la probabilité de **ne pas** obtenir 6 est
$$
\mathbb{P}(A_i^c) = \frac{5}{6}.
$$

Par indépendance des dés :
$$
\mathbb{P}(A_1^c \cap A_2^c \cap A_3^c)
= \left(\frac{5}{6}\right)^3 = \frac{125}{216}.
$$

Ainsi,
$$
\mathbb{P}(A_1 \cup A_2 \cup A_3)
= 1 - \mathbb{P}(A_1^c \cap A_2^c \cap A_3^c)
= 1 - \frac{125}{216}
= \frac{91}{216}
\simeq 0.421.
$$

---

### Résultat

Dans les deux cas, on obtient :
$$
\boxed{\mathbb{P}(\text{obtenir au moins un 6 en 3 lancers}) = \frac{91}{216} \approx 0.421.}
$$



Je reviendrai ici sur l'exercice 5 que nous n'avons pas eu la possibilité de mettre oeuvre la simulation monte Carlo.

## Correction de l'exercice 5

Nous sélectionnons un entier au hasard parmi les N = 5,000 entiers qui sont dans l'intervalle [0, 4999]. 
Quelle est la probabilité qu’il soit divisible par 4,7 ou 10 ?

Modifier le code R vu en cours pour estimer cette probabilité par une simulation
de Monte-Carlo pour vérifier ce résultat. Vous utiliserez au moins 50,000
simulations.

Pour résoudre cette exercice, il faut d'abord définir l'espace probabilisé. 
L'espace probabilisé est défini par le triplet (Ω, F, P) où:


- Ω est l'ensemble des résultats possibles. Dans notre cas, Ω = {0, 1, 2, ..., 4999}.

- F est la σ-algèbre des événements. Ici, nous pouvons considérer les événements comme les sous-ensembles de Ω.

- P est la mesure de probabilité. Dans notre cas, chaque entier a une probabilité égale d'être sélectionné. On dit encore que la loi de probabilité est uniforme sur Ω.

Le cardinal de l'ensemble Ω est |Ω| = 5000.
La probabilité d'un élément spécifique dans Ω est donc $P({x}) = \frac{1}{5000}$ pour tout x dans Ω.

Après avoir défini l'espace probabilisé, nous pouvons maintenant définir l'événement A que nous voulons étudier. 
De ce fait, définissons les événements suivants:

- $A_4$ : l'événement que l'entier sélectionné est divisible par 4.
- $A_7$ : l'événement que l'entier sélectionné est divisible par 7.
- $A_{10}$ : l'événement que l'entier sélectionné est divisible par 10.
L'événement A que nous voulons étudier est l'union de ces trois événements:

$$
A = A_4 \cup A_7 \cup A_{10}
$$

Nous cherchons à calculer la probabilité de l'événement A, c'est-à-dire $P(A)$. Pour cela, nous allons utiliser la formule inclusion-exclusion:
$$
\begin{aligned}
\mathbb{P}(A_4 \cup A_7 \cup A_{10})
&= \mathbb{P}(A_4) + \mathbb{P}(A_7) + \mathbb{P}(A_{10}) \\
&\quad - \big\{\, \mathbb{P}(A_4 \cap A_7) + \mathbb{P}(A_4 \cap A_{10}) + \mathbb{P}(A_7 \cap A_{10}) \,\big\} \\
&\quad + \mathbb{P}(A_4 \cap A_7 \cap A_{10}).
\end{aligned}
$$

Calculons maintenant chaque terme de cette formule:
Pour calculer chaque probabilité, nous devons compter le nombre d'entiers dans Ω qui satisfont chaque condition $A_i$.
Et la probabilité de chaque événement est donnée par le rapport du nombre d'entiers satisfaisant la condition sur le cardinal de l'ensemble Ω : 

$$
P(A_i) = \frac{\text{nombre d'entiers satisfaisant } A_i}{5000}.
$$


- Pour $A_4$ : Les entiers divisibles par 4 sont 0, 4, 8, ..., 4996. Ainsi le nombre d'entiers divisibles par 4 est de la forme $4k$ et qui vérifient $0 \leq 4(k+1) \leq 5000 < 4(k+2)$, donc de cette inégalité, on déduit que $k$ vérifie l'inégalité sous dessous :

$$
k \leq \frac{4999}{4} < k+1.
$$

Donc le nombre d'entiers divisibles par 4 correspond à la partie entière de $\frac{4999}{4}$ plus 1 (pour inclure le 0), soit:
$$
\text{nombre d'entiers divisibles par 4} = \left\lfloor\frac{4999}{4}\right\rfloor + 1 = 1249 + 1 = 1250.
$$

Donc, la probabilité que l'entier sélectionné soit divisible par 4 est:
$$
P(A_4) = \frac{1250}{5000} = 0.25.
$$

- Pour $A_7$ : Les entiers divisibles par 7 sont 0, 7, 14, ..., 4996. En suivant le même raisonnement que pour $A_4$, nous trouvons:

$$
\text{nombre d'entiers divisibles par 7} = \left\lfloor\frac{4999}{7}\right\rfloor + 1 = 714 + 1 = 715.
$$

Donc, la probabilité que l'entier sélectionné soit divisible par 7 est:
$$
P(A_7) = \frac{715}{5000} = 0.143.
$$

- Pour $A_{10}$ : Les entiers divisibles par 10 sont 0, 10, 20, ..., 4990. En suivant le même raisonnement que pour $A_4$, nous trouvons:
$$
\text{nombre d'entiers divisibles par 10} = \left\lfloor\frac{4999}{10}\right\rfloor + 1 = 499 + 1 = 500.
$$

Donc, la probabilité que l'entier sélectionné soit divisible par 10 est:
$$
P(A_{10}) = \frac{500}{5000} = 0.1.
$$  

- Pour $A_4 \cap A_7$ : Les entiers qui sont divisibles par 4 et 7 sont ceux qui sont divisibles par 28 qui est le PPCM de 4 et 7. En suivant le même raisonnement que pour $A_4$, nous trouvons:
$$
\text{nombre d'entiers divisibles par 28} = \left\lfloor\frac{4999}{28}\right\rfloor + 1 = 178 + 1 = 179.
$$

Donc, la probabilité que l'entier sélectionné soit divisible par 4 et 7 est:

$$
P(A_4 \cap A_7) = \frac{179}{5000} = 0.0358.
$$

- Pour $A_4 \cap A_{10}$ : Les entiers qui sont divisibles par 4 et 10 sont ceux qui sont divisibles par 20 qui est le PPCM de 4 et 10. En suivant le même raisonnement que pour $A_4, nous trouvons:
$$
\text{nombre d'entiers divisibles par 20} = \left\lfloor\frac{4999}{20}\right\rfloor + 1 = 249 + 1 = 250.
$$

Donc, la probabilité que l'entier sélectionné soit divisible par 4 et 10 est:
$$
P(A_4 \cap A_{10}) = \frac{250}{5000} = 0.05.
$$

- Pour $A_7 \cap A_{10}$ : Les entiers qui sont divisibles par 7 et 10 sont ceux qui sont divisibles par 70 qui est le PPCM de 7 et 10. En suivant le même raisonnement que pour $A_4, nous trouvons:

$$
\text{nombre d'entiers divisibles par 70} = \left\lfloor\frac{4999}{70}\right\rfloor + 1 = 71 + 1 = 72.
$$

Donc, la probabilité que l'entier sélectionné soit divisible par 7 et 10 est:
$$
P(A_7 \cap A_{10}) = \frac{72}{5000} = 0.0144.
$$

- Pour $A_4 \cap A_7 \cap A_{10}$ : Les entiers qui sont divisibles par 4, 7 et 10 sont ceux qui sont divisibles par 140 qui est le PPCM de 4, 7 et 10. En suivant le même raisonnement que pour $A_4$, nous trouvons:

$$
\text{nombre d'entiers divisibles par 140} = \left\lfloor\frac{4999}{140}\right\rfloor + 1 = 35 + 1 = 36.
$$

Donc, la probabilité que l'entier sélectionné soit divisible par 4, 7 et 10 est:
$$
P(A_4 \cap A_7 \cap A_{10}) = \frac{36}{5000} = 0.0072.
$$  

En substituant ces valeurs dans la formule d'inclusion-exclusion, nous obtenons:
$$
\begin{aligned}
P(A)
&= P(A_4) + P(A_7) + P(A_{10}) \\
&\quad - \big\{\, P(A_4 \cap A_7) + P(A_4 \cap A_{10}) + P(A_7 \cap A_{10}) \,\big\} \\
&\quad + P(A_4 \cap A_7 \cap A_{10}) \\
&= 0.25 + 0.143 + 0.1 \\
&\quad - \big\{\, 0.0358 + 0.05 + 0.0144 \,\big\} \\
&\quad + 0.0072 \\
&= 0.493 - 0.1002 + 0.0072 \\
&= 0.4.
\end{aligned}
$$  

Donc, la probabilité qu'un entier sélectionné au hasard parmi les 5000 entiers soit divisible par 4, 7 ou 10 est de 0.4 ou 40%.

Maintenant, nous allons vérifier ce résultat par une simulation de Monte-Carlo en R avec au moins 50,000 simulations.
Une simulation de Monte-Carlo est une méthode statistique qui utilise des échantillons aléatoires pour estimer des propriétés mathématiques ou physiques comme des espérances, des intégrales ou des probabilités. Cette méthode fonctionne en générant un grand nombre de scénarios aléatoires et en observant les résultats pour obtenir une estimation statistique :

- On fixe le nombre de simulations, disons n = 50000.
- On initialise un compteur pour le nombre de succès (entiers divisibles par 4, 7 ou 10).
- Pour chaque simulation, on génère un entier aléatoire entre 0 et 4999.
- On vérifie si cet entier est divisible par 4, 7 ou 10. Si c'est le cas, on incrémente le compteur de succès.
- Après avoir effectué toutes les simulations, on calcule la probabilité estimée comme le ratio du nombre de succès sur le nombre total de simulations.

Voici un exemple de code R pour effectuer cette simulation de Monte-Carlo :

```{python}
# Monte Carlo in Python with running estimate and plot (matplotlib only)

import numpy as np
import matplotlib.pyplot as plt

# --- Parameters (mirror your R snippet) ---
seed = 123
N = 4999               # sample from 0..N (inclusive)
M = 50_000             # number of simulations

rng = np.random.default_rng(seed)
x = rng.integers(low=0, high=N+1, size=M)  # uniform integers in [0, N]

is_div = (x % 4 == 0) | (x % 7 == 0) | (x % 10 == 0)

# Final Monte Carlo estimate (same as R's mean(is_div))
p_hat = is_div.mean()

# Running estimates vs number of simulations
running_est = np.cumsum(is_div) / np.arange(1, M + 1)

# Mean of the running estimates (to draw an horizontal reference line)
mean_running = running_est.mean()

# --- Plot ---
plt.figure(figsize=(7.24, 4.07), dpi=100)  # ~724x407 px

plt.plot(np.arange(1, M + 1), running_est, linewidth=2)
# Add the colored horizontal line at mean_running
plt.axhline(mean_running, linestyle="--", linewidth=2, color="red")
plt.xlabel("Nombre de simulations")
plt.ylabel("Estimation de la probabilité")
plt.title("Convergence de l’estimation Monte-Carlo", fontsize=14, weight="bold")
plt.suptitle(
    "Courbe de l’estimation cumulée; ligne horizontale = moyenne des estimations",
    fontsize=10, color="gray"
)

# Style cues similar to the provided seaborn example
ax = plt.gca()
for spine in ["top", "right"]:
    ax.spines[spine].set_visible(False)
plt.grid(False)

plt.tight_layout()
plt.show()

p_hat


```


# 2. Probabilité conditionnelle et indépendance

## Exercice 2

Une classe contient un total de 108 étudiants. Parmi ceux-ci, 36 indiquent qu’ils pratiquent un sport de haut niveau, et 21 étudiants parmi ces 36 indiquent qu’ils préfèrent s’entraîner le matin plutôt que l’après-midi.  
Parmi les étudiants qui ne pratiquent pas un sport de haut niveau, 24 indiquent qu’ils préfèrent pratiquer le sport le matin plutôt que l’après-midi.

Pour un étudiant pris au hasard dans la classe, calculer les probabilités suivantes :

- probabilité d’être un sportif de haut niveau sachant que l’on préfère s’entraîner le matin ;
- probabilité de préférer s’entraîner le matin sachant qu’on ne pratique pas un sport de haut niveau ;
- probabilité de ne pas pratiquer un sport de haut niveau sachant que l’on préfère s’entraîner l’après-midi.

---

## Correction Exercice 2

On se place sur un espace probabilisé $(\Omega, \mathcal{F}, \mathbb{P})$, où  
$\Omega = \{\text{ensemble des étudiants de la classe}\}$,  
$\mathcal{F} = \mathcal{P}(\Omega)$ (toutes les parties de $\Omega$)  
et $\mathbb{P}$ est la probabilité uniforme sur $\Omega$ (tous les étudiants sont équiprobables).

Nous notons :

- $H = \{\text{l’étudiant pratique un sport de haut niveau}\}$,
- $M = \{\text{l’étudiant préfère s’entraîner le matin}\}$.

D’après l’énoncé, nous avons
$$
\mathbb{P}(H) = \frac{36}{108} = \frac{1}{3},
\qquad
\mathbb{P}(M \mid H) = \frac{21}{36} = \frac{7}{12},
\qquad
\mathbb{P}(M \mid H^c) = \frac{24}{72} = \frac{1}{3}.
$$

---

### 1 Probabilité d’être un sportif de haut niveau sachant que l’on préfère s’entraîner le matin

On cherche $\mathbb{P}(H \mid M)$.

En appliquant la formule de Bayes :
$$
\mathbb{P}(H \mid M) = \frac{\mathbb{P}(M \mid H)\mathbb{P}(H)}{\mathbb{P}(M \mid H)\mathbb{P}(H) + \mathbb{P}(M \mid H^c)\mathbb{P}(H^c)}.
$$

En remplaçant par les valeurs numériques :
$$
\mathbb{P}(H \mid M)
= \frac{\frac{7}{12} \times \frac{1}{3}}
{\frac{7}{12} \times \frac{1}{3} + \frac{1}{3} \times \frac{2}{3}}
= \frac{\frac{7}{36}}{\frac{7}{36} + \frac{2}{9}}
= \frac{7}{15}.
$$

---

### 2 Probabilité de préférer s’entraîner le matin sachant qu’on ne pratique pas un sport de haut niveau

Cette probabilité est directement donnée par l’énoncé :
$$
\mathbb{P}(M \mid H^c) = \frac{1}{3}.
$$

---

### 3 Probabilité de ne pas pratiquer un sport de haut niveau sachant que l’on préfère s’entraîner l’après-midi

On cherche $\mathbb{P}(H^c \mid M^c)$.

On applique encore une fois la formule de Bayes :
$$
\mathbb{P}(H^c \mid M^c)
= \frac{\mathbb{P}(M^c \mid H^c)\mathbb{P}(H^c)}
{\mathbb{P}(M^c \mid H^c)\mathbb{P}(H^c) + \mathbb{P}(M^c \mid H)\mathbb{P}(H)}.
$$

Sachant que
$$
\mathbb{P}(M^c \mid H^c) = \frac{2}{3},
\qquad
\mathbb{P}(M^c \mid H) = \frac{5}{12},
\qquad
\mathbb{P}(H^c) = \frac{2}{3},
\qquad
\mathbb{P}(H) = \frac{1}{3},
$$

nous obtenons
$$
\mathbb{P}(H^c \mid M^c)
= \frac{\frac{2}{3} \times \frac{2}{3}}
{\frac{2}{3} \times \frac{2}{3} + \frac{5}{12} \times \frac{1}{3}}
= \frac{\frac{4}{9}}{\frac{4}{9} + \frac{5}{36}}
= \frac{16}{21}.
$$

---

###  Résumé des résultats

$$
\boxed{\mathbb{P}(H \mid M) = \frac{7}{15}}
$$
$$
\boxed{\mathbb{P}(M \mid H^c) = \frac{1}{3}}
$$
$$
\boxed{\mathbb{P}(H^c \mid M^c) = \frac{16}{21}}
$$

## Exercice 3

Pour chacune des deux affirmations suivantes, montrer qu’elle est vraie pour tous les événements $A$ et $B$ pour lesquels les probabilités conditionnelles sont calculables, ou donner un contre-exemple.

$\mathbb{P}(A \mid B) + \mathbb{P}(A \mid B^c) = 1.$

$\mathbb{P}(A \mid B) + \mathbb{P}(A^c \mid B) = 1.$

## Correction Exercice 3

1. Pour la première affirmation, nous avons en utilisant la formule de Bayes et en supposant que $\mathbb{P}(B) > 0$ et $\mathbb{P}(B^c) > 0$ :

$$
\mathbb{P}(A \mid B)  = \frac{\mathbb{P}(A \cap B)}{\mathbb{P}(B)},
\qquad \text{et} \qquad
\mathbb{P}(A \mid B^c)  = \frac{\mathbb{P}(A \cap B^c)}{\mathbb{P}(B^c)}.
$$

De ce fait, si nous additionnons les deux probabilités conditionnelles, nous obtenons :
$$
\mathbb{P}(A \mid B) + \mathbb{P}(A \mid B^c)
= \frac{\mathbb{P}(A \cap B)}{\mathbb{P}(B)} + \frac{\mathbb{P}(A \cap B^c)}{\mathbb{P}(B^c)}.
$$

Si on se met dans une situation où $\mathbb{P}(B) = \mathbb{P}(B^c) = 0.5$, et en utilisant le fait que $\mathbb{P}(A \cap B) + \mathbb{P}(A \cap B^c) = \mathbb{P}(A)$, nous obtenons :
$$
\mathbb{P}(A \mid B) + \mathbb{P}(A \mid B^c) = 2 \mathbb{P}(A).
$$

Donc, cette affirmation est fausse dès que $\mathbb{P}(A) \neq 0.5$. Et il est facile de trouver un contre-exemple, c'est-à-dire une expérience aléatoire où cette condition n'est pas satisfaite.

En effet, si nous considérons une expérience aléatoire consistant à lancer un dé équilibré à six faces, et nous définissons les événements suivants :
- $A$ : l'événement que le résultat est un 6,
- $B$ : l'événement que le résultat est un nombre pair.

Nous avons :
$$
\mathbb{P}(B) = \frac{3}{6} = 0.5 = \mathbb{P}(B^c),
$$
et 
$$
\mathbb{P}(A) = \frac{1}{6} \neq 0.5.
$$

Et dans ce cas l'affirmation ne tient pas.

2. La seconde affirmation est toujours vraie. On peut le démontrer en utilisant la formule de Bayes :
$$
\mathbb{P}(A \mid B) = \frac{\mathbb{P}(A \cap B)}{\mathbb{P}(B)},
\qquad \text{et} \qquad
\mathbb{P}(A^c \mid B) = \frac{\mathbb{P}(A^c \cap B)}{\mathbb{P}(B)}.
$$

En additionnant les deux probabilités conditionnelles, nous obtenons :
$$
\mathbb{P}(A \mid B) + \mathbb{P}(A^c \mid B)
= \frac{\mathbb{P}(A \cap B)}{\mathbb{P}(B)} + \frac{\mathbb{P}(A^c \cap B)}{\mathbb{P}(B)}
= \frac{\mathbb{P}((A \cap B) \cup (A^c \cap B))}{\mathbb{P}(B)}
= \frac{\mathbb{P}(B)}{\mathbb{P}(B)} = 1.
$$

Ou tout simplement en remarquant que $P(.|B)$ définit une probabilité sur l'espace probabilisé restreint à l'événement B, et donc la somme des probabilités d'un événement et de son complément est égale à 1.

## Exercice 4

Un jeu de 32 cartes contient une carte manquante. Nous sélectionnons au hasard une carte parmi les 31 cartes restantes.  
Calculer la probabilité que la carte tirée soit un cœur, en utilisant un conditionnement sur la carte manquante.

---

## Correction Exercice 4

On se place sur un espace probabilisé $(\Omega,\mathcal{F},\mathbb{P})$ défini ainsi :

- $\Omega$ est l’ensemble des couples
  $$
  \Omega = \{(c_{\text{manq}}, c_{\text{tirée}})\mid c_{\text{manq}}, c_{\text{tirée}} \text{ sont des cartes du jeu de 32, } c_{\text{tirée}}\neq c_{\text{manq}}\}.
  $$
  Autrement dit, on choisit d’abord la carte manquante, puis on tire une carte parmi les 31 restantes.
- Chaque couple possible est supposé équiprobable, donc
  $$
  \mathbb{P}(\{(c_{\text{manq}}, c_{\text{tirée}})\}) = \frac{1}{32\times 31}.
  $$

On introduit les événements suivants :

- $M = \{\text{la carte manquante est un cœur}\}$,
- $T = \{\text{la carte tirée est un cœur}\}$.

Dans un jeu de 32 cartes, il y a 8 cœurs au total.

Si vous avez des difficultés pour définir l’espace probabilisé, je vous conseille de définir uniquement les événements $M$ et $T$ et de raisonner en termes de probabilités conditionnelles.
---

### 1 Probabilités conditionnelles

- Si la carte manquante est un cœur (événement $M$), il reste $7$ cœurs parmi les $31$ cartes encore présentes, donc
  $$
  \mathbb{P}(T \mid M) = \frac{7}{31}.
  $$

- Si la carte manquante n’est **pas** un cœur (événement $M^c$), il reste toujours les $8$ cœurs dans les $31$ cartes présentes, donc
  $$
  \mathbb{P}(T \mid M^c) = \frac{8}{31}.
  $$

Par ailleurs, la carte manquante est choisie au hasard parmi les 32 cartes :

- il y a 8 cœurs possibles pour la carte manquante, donc
  $$
  \mathbb{P}(M) = \frac{8}{32} = \frac{1}{4},
  $$
- et donc
  $$
  \mathbb{P}(M^c) = 1 - \mathbb{P}(M) = \frac{3}{4}.
  $$

---

### 2 Application de la formule des probabilités totales

On veut calculer $\mathbb{P}(T)$, la probabilité que la carte tirée soit un cœur.  
On conditionne par l’événement $M$ (la carte manquante est un cœur ou non) :

$$
\mathbb{P}(T)
= \mathbb{P}(T \mid M)\mathbb{P}(M)
+ \mathbb{P}(T \mid M^c)\mathbb{P}(M^c).
$$

En remplaçant par les valeurs trouvées :

$$
\mathbb{P}(T)
= \frac{7}{31} \times \frac{1}{4}
+ \frac{8}{31} \times \frac{3}{4}
= \frac{7}{124} + \frac{24}{124}
= \frac{31}{124}
= \frac{1}{4}.
$$

---

### Résultat

La probabilité que la carte tirée soit un cœur vaut
$$
\boxed{\mathbb{P}(T) = \frac{1}{4}}.
$$

On retrouve d’ailleurs un résultat intuitif : en moyenne, retirer une carte au hasard puis tirer parmi les restantes ne modifie pas la probabilité d’obtenir un cœur, qui reste $8/32 = 1/4$.


::: {.byline}
<span class="date">May 28, 2025</span>
:::

# Rappels de la séance précédente

L'objectif de la prémière séance de TD était de consolider vos connaissances sur les espaces probabilisés et sur les probabilités conditionnelles.

Vous devez actuellement être capable de :

- Définir un espace probabilisé (Ω, F, P).

- Utiliser les propriétés des probabilités pour calculer des probabilités d'événements simples et composés.

- Vous devez maitriser les lois de Morgan :

  - La loi de Morgan pour l'union : $\overline{A \cup B} = \overline{A} \cap \overline{B}$
  - La loi de Morgan pour l'intersection : $\overline{A \cap B} = \overline{A} \cup \overline{B}$

- Appliquer la formule d'inclusion-exclusion pour calculer la probabilité de l'union de deux événements :

  $$
  P(A \cup B) = P(A) + P(B) - P(A \cap B)
  $$

- Appliquer la formule d'inclusion-exclusion pour calculer la probabilité de l'union de plusieurs événements :

  $$
  \begin{aligned}
  P\left(\bigcup_{i=1}^{n} A_i\right) &= \sum_{i=1}^{n} P(A_i) - \sum_{1 \leq i < j \leq n} P(A_i \cap A_j) \\
  &\quad + \sum_{1 \leq i < j < k \leq n} P(A_i \cap A_j \cap A_k) - \ldots + (-1)^{n+1} P(A_1 \cap A_2 \cap \ldots \cap A_n)
  \end{aligned}
  $$
- Calculer des probabilités conditionnelles en utilisant la formule de Bayes :

$$
  P(A|B) = \frac{P(A \cap B)}{P(B)}.
$$

ou bien encore :

$$
  P(A\cap B) = P(A|B) \cdot P(B).
$$

- Appliquer la loi des probabilités totales pour décomposer des probabilités complexes en utilisant des événements disjoints et exhaustifs :

Par exemple, si $C_1, C_2$ sont deux événements disjoints et exhaustifs, c'est-à-dire que $C_1 \cap C_2 = \emptyset$ et $C_1 \cup C_2 = \Omega$, alors pour tout événement A, on a :

$$
P(A) = P(A|C_1) \cdot P(C_1) + P(A|C_2) \cdot P(C_2).
$$

Et finalement, cela nous permet de calculer la probabilité de $C_1$ sachant A en utilisant la formule de Bayes :

$$
P(C_1|A) = \frac{P(A|C_1) \cdot P(C_1)}{P(A|C_1) \cdot P(C_1) + P(A|C_2) \cdot P(C_2)}.
$$


```{mermaid}
flowchart LR
  A[Espaces probabilisés] --> B(Variable aléatoire X)
  B --> C{type}
  C --> D[Discrète]
  C --> E[Continue]
  D --> F[Caractérisation de la loi de X]
  E --> G[Caractérisation de la loi de X]
```

# 3 Variables aléatoires générales


## Exercice 1

Nous considérons l’expérience aléatoire consistant à lancer 3 fois une pièce de monnaie. Soit $X$ la variable aléatoire donnant le nombre de « Pile » obtenu.

1) Donner la loi de la variable aléatoire $X$.
2) Donner la fonction de répartition de $X$, et la représenter graphiquement.
3) Calculer l’espérance, la variance et le coefficient de variation de $X$.

## Correction de l'exercice 1

1) Nous considérons l'espace probabilisé $(\Omega, \mathcal{F}, P)$ associé à l'expérience aléatoire de lancer 3 fois une pièce de monnaie. L'ensemble des résultats possibles $\Omega$ est donné par :

$$
\Omega = \{PPP, PPF, PFP, PFF, FPP, FPF, FFP, FFF\} = \{P, F\}^3
$$

Nous munissons cet espace de la tribu $\mathcal{F}$ des parties de $\Omega$ et de la probabilité uniforme $P$.

La variable aléatoire $X$ est définie comme le nombre de « Pile » obtenus lors des 3 lancers. Ainsi, $X$ peut prendre les valeurs suivantes :
- $X = 0$ : lorsque le résultat est FFF
- $X = 1$ : lorsque le résultat est PFF, FPF, ou FFP
- $X = 2$ : lorsque le résultat est PPF, PFP, ou FPP
- $X = 3$ : lorsque le résultat est PPP

Cette variable est donc définie comme suit :


$$
X : \Omega^3 \longrightarrow \Omega' = \{0,1,2,3\}
$$

$$
(\omega_1, \omega_2, \omega_3) \longmapsto
\begin{cases}
0 & \text{si } (\omega_1, \omega_2, \omega_3) = (FFF),\\[4pt]
1 & \text{si } (\omega_1, \omega_2, \omega_3) \in \{(PFF),(FPF),(FFP)\},\\[4pt]
2 & \text{si } (\omega_1, \omega_2, \omega_3) \in \{(PPF),(PFP),(FPP)\},\\[4pt]
3 & \text{si } (\omega_1, \omega_2, \omega_3) = (PPP).
\end{cases}
$$

Comme la variable aléatoire X est **discrète**, il suffit de déterminer la probabilité $\mathbb{P}_X$ sur les singletons.

$$
\begin{aligned}
\mathbb{P}_X(0) &= \frac{1}{8},\
\mathbb{P}_X(1) &= \frac{3}{8},\
\mathbb{P}_X(2) &= \frac{3}{8},\
\mathbb{P}_X(3) &= \frac{1}{8}.
\end{aligned}
$$

Ainsi, ( X ) suit une **loi binomiale** :
$$
X \sim \mathcal{B}(n=3,, p=\tfrac{1}{2}).
$$



2. Fonction de répartition

À partir de la définition de la fonction de répartition :
$$
F_X(x) = \mathbb{P}(X \le x),
$$
on obtient :

$$
F_X(x) =
\begin{cases}
0 & \text{si } x < 0,\\[4pt]
\frac{1}{8} & \text{si } 0 \le x < 1,\\[4pt]
\frac{4}{8} = \frac{1}{2} & \text{si } 1 \le x < 2,\\[4pt]
\frac{7}{8} & \text{si } 2 \le x < 3,\\[4pt]
1 & \text{si } 3 \le x.
\end{cases}
$$

Réprésentation graphique de la fonction de répartition :

```{python}
import numpy as np
import matplotlib.pyplot as plt

def F(x):
    x = np.asarray(x)
    return np.where(
        x < 0, 0,
        np.where(
            x < 1, 1/8,
            np.where(
                x < 2, 4/8,
                np.where(
                    x < 3, 7/8,
                    1
                )
            )
        )
    )

# Segments horizontaux de la fonction de répartition
segments = [
    (-1, 0, 0),   # de x=-1 à x=0 : F=0
    (0, 1, 1/8),
    (1, 2, 4/8),
    (2, 3, 7/8),
    (3, 4, 1)
]

plt.figure(figsize=(6,4))

# Tracé des segments horizontaux uniquement
for x_start, x_end, y_val in segments:
    plt.hlines(y_val, x_start, x_end, colors="blue")

# Points ouverts (limite à gauche non incluse)
x_open  = [0,   1,   2,   3]
y_open  = [0, 1/8, 4/8, 7/8]
plt.scatter(x_open, y_open, facecolors="none", edgecolors="black", s=60, zorder=3)

# Points fermés (valeur incluse)
x_closed = [0,   1,   2,   3]
y_closed = [1/8, 4/8, 7/8, 1]
plt.scatter(x_closed, y_closed, color="black", s=60, zorder=3)

# Format graphique
plt.xlabel("x")
plt.ylabel("F(x)")
plt.title("Fonction de répartition ")
plt.grid(True, linestyle="--", alpha=0.4)
plt.xlim(-1, 4)
plt.ylim(-0.05, 1.05)

plt.show()



```

3. Espérance, variance et coefficient de variation

Pour calculer l'espérance, la variance et le coefficient de variation de la variable aléatoire $X$, nous allons utiliser la formule de transfert en temps discret :

$$
\mathbb{E}[g(X)] = \sum_{i} g(x_i) \cdot \mathbb{P}_X(x_i),
$$
où $g$ est une fonction mésurable.

Ainsi, pour l'espérance, nous avons :
$$
\mathbb{E}[X] = \sum_{x=0}^{3} x \cdot \mathbb{P}_X(x) = 0 \cdot \frac{1}{8} + 1 \cdot \frac{3}{8} + 2 \cdot \frac{3}{8} + 3 \cdot \frac{1}{8} = \frac{12}{8} = np =1.5.
$$

Pour la variance, nous utilisons la formule :

$$
\text{Var}(X) = \sum_{x=0}^{3} (x - \mathbb{E}[X])^2 \cdot \mathbb{P}_X(x) = (0 - 1.5)^2 \cdot \frac{1}{8} + (1 - 1.5)^2 \cdot \frac{3}{8} + (2 - 1.5)^2 \cdot \frac{3}{8} + (3 - 1.5)^2 \cdot \frac{1}{8} = \frac{6}{8} = np(1-p) = 0.75.
$$

Enfin, le coefficient de variation est donné par la formule :

$$
CV(X) = \frac{\sigma_X}{\mathbb{E}[X]} = \frac{\sqrt{\text{Var}(X)}}{\mathbb{E}[X]} = \frac{\sqrt{0.75}}{1.5} = \frac{\sqrt{3}}{3} \approx 0.577.
$$

## Exercice 2

Nous considérons l’expérience aléatoire consistant à jeter un dé jusqu’à obtenir un **« 6 »**. Soit $X$ la variable aléatoire donnant le nombre de jets.

1. Donner la loi de la variable aléatoire $X$.
2. Donner la fonction de répartition de $X$, et la représenter graphiquement.
3. Montrer que la fonction génératrice des moments de $X$ vaut :

   $$
   M_X(t) = \frac{pe^t}{1 - e^t(1 - p)},
   $$

   avec $p = \frac{1}{6}$.
    
4. En déduire l’espérance et la variance de $X$, puis son coefficient de variation.

## Correction de l'exercice 2

1) Loi de la variable aléatoire X

Nous considérons l'espace probabilisé $(\Omega^{\mathbb{N}}, \mathcal{P}(\Omega^{\mathbb{N}}), \mathbb{P})$ avec $\Omega = \{1,2,3,4,5,6\}$ muni de la tribu produit et de la probabilité uniforme $\mathbb{P}$.

L'univers $\Omega^{\mathbb{N}}$ correspond à l'ensemble des suites infinies de résultats de lancers de dé. Autrement dit on peut l'écrire comme l'ensemble des suites infinies :
$$
\Omega^{\mathbb{N}} = \{(\omega_1, \omega_2, \ldots) \mid \omega_i \in \Omega = \{1,2,3,4,5,6\} \text{ pour tout } i \in \mathbb{N}\}.
$$

 La variable aléatoire étudiée est

$$
X : \Omega^{\mathbb{N}} \longrightarrow \Omega' = \mathbb{N}^*
$$

$$
(\omega_1, \omega_2, \ldots) \longmapsto
\begin{cases}
1 & \text{si } \omega_1 = \{6\},\\[4pt]
2 & \text{si } \omega_1 \neq \{6\} \text{ et } \omega_2 = \{6\},\\[4pt]
& \cdots\\[4pt]
k & \text{si } \omega_1, \ldots, \omega_{k-1} \neq \{6\} \text{ et } \omega_k = \{6\},\\[4pt]
& \cdots
\end{cases}
$$

Pour tout $k \in \mathbb{N}^*$, on a :

$$
\mathbb{P}(X = k) = \mathbb{P}(\omega_1, \ldots, \omega_{k-1} \neq \{6\} \text{ et } \omega_k = \{6\})
$$

$$
= \prod_{i=1}^{k-1} \mathbb{P}(\omega_i \neq \{6\}) \times \mathbb{P}(\omega_k = \{6\})
$$

$$
= (1 - p)^{k-1}p,
$$

avec $p = \frac{1}{6}$ la probabilité d'obtenir un "6" lors d'un lancer. Il s'agit d'une distribution géométrique de paramètre $p$. 

2) Fonction de répartition

Soit $k \geq 1$. Nous avons :

$$
\mathbb{P}(X \leq k) = \sum_{j=1}^{k} (1-p)^{j-1}p
$$

$$
= p\sum_{j=0}^{k-1} (1-p)^{j} = p\frac{1-(1-p)^k}{1-(1-p)} = 1-(1-p)^k.
$$

A partir de la définition de la fonction de répartition, nous obtenons

$$
F_X(x) = 
\begin{cases}
0 & \text{si } x < 1,\\[4pt]
p & \text{si } 1 \leq x < 2,\\[4pt]
& \cdots,\\[4pt]
1-(1-p)^k & \text{si } k \leq x < k+1,\\[4pt]
& \cdots.
\end{cases}
$$

Représentation graphique de la fonction de répartition :

```{python}
import numpy as np
import matplotlib.pyplot as plt

def plot_cdf_geometrique(p=0.3, k_max=8):
    """
    Trace la fonction de répartition
        F_X(x) = 0                si x < 1
               = 1 - (1-p)^k      si k <= x < k+1   pour k = 1,2,...
    avec une troncature à k_max.
    """
    if not (0 < p < 1):
        raise ValueError("p doit être dans l'intervalle (0, 1).")

    plt.figure(figsize=(6, 4))

    # Segment avant 1 : F(x) = 0 pour 0 <= x < 1
    plt.hlines(0, 0, 1, linewidth=2)

    # Segments horizontaux pour k = 1,...,k_max
    for k in range(1, k_max + 1):
        level = 1 - (1 - p)**k
        plt.hlines(level, k, k + 1, linewidth=2)

    # Points ouverts (limite à gauche, non incluse)
    x_open, y_open = [], []
    # Points fermés (valeur de la FDR)
    x_closed, y_closed = [], []

    for k in range(1, k_max + 1):
        # Limite à gauche en k :
        if k == 1:
            left_limit = 0
        else:
            left_limit = 1 - (1 - p)**(k - 1)

        value_at_k = 1 - (1 - p)**k

        x_open.append(k)
        y_open.append(left_limit)

        x_closed.append(k)
        y_closed.append(value_at_k)

    # Cercles ouverts (non remplis) : limite à gauche
    plt.scatter(
        x_open, y_open,
        facecolors="none",
        edgecolors="black",
        s=60,
        zorder=3
    )

    # Cercles fermés (remplis) : valeur de F_X en k
    plt.scatter(
        x_closed, y_closed,
        color="black",
        s=60,
        zorder=3
    )

    # Mise en forme
    plt.xlabel("x")
    plt.ylabel("F(x)")
    plt.title(f"Fonction de répartition géométrique (p = {p})")
    plt.xlim(0, k_max + 1.5)
    plt.ylim(-0.05, 1.05)
    plt.grid(True, linestyle="--", alpha=0.4)

    plt.show()

# Exemple d'appel
plot_cdf_geometrique(p=0.25, k_max=8)
```

3) Fonction génératrice des moments

En utilisant la formule de transfert en temps discret, nous avons :
$$
M_X(t) = \int e^{tX} d\mathbb{P} = \sum_{x=1}^{\infty} e^{tx}\mathbb{P}_X(x)
$$

$$
= p\sum_{j=1}^{\infty} e^{tj}(1-p)^{j-1}
$$

$$
= pe^t\sum_{j=0}^{\infty} e^{tj}(1-p)^j
$$

$$
= pe^t\sum_{j=0}^{\infty} q^j \text{ avec } q = e^t(1-p)
$$

$$
= \frac{pe^t}{1-q} = \frac{pe^t}{1-e^t(1-p)}.
$$

La série converge si et seulement si $|q| < 1$, c'est-à-dire si $|e^t(1-p)| < 1$, soit $e^t < \frac{1}{1-p}$, ce qui donne $t < \ln\left(\frac{1}{1-p}\right) = -\ln(1-p)$.

4) Espérance, variance et coefficient de variation

Nous savons que la fonction $M_X$ est indéfiniment dérivable dans un voisinage de 0, et que $M'_X(0) = E(X)$ et $M''_X(0) = E(X^2)$. Nous avons après calcul

$$
M'_x(t) = \frac{pe^t}{\{1-e^t(1-p)\}^2},
$$

$$
M''_x(t) = \frac{pe^t\{1-(1-p)^2e^{2t}\}}{\{1-e^t(1-p)\}^4},
$$

dont nous déduisons que

$$
M'_x(0) = \frac{1}{p} \equiv E(X),
$$

$$
M''_x(0) = \frac{p\{1-(1-p)^2\}}{\{1-(1-p)\}^4} = \frac{2-p}{p^2} \equiv E(X^2),
$$

puis

$$
V(X) = \frac{1-p}{p^2},
$$

$$
CV(X) = \sqrt{1-p}.
$$

Numériquement, nous obtenons

$$
E(X) = 6, \quad V(X) = 30, \quad CV(X) = 0.91.
$$


## Exercice 3

Soit $X$ une variable aléatoire admettant un moment d'ordre 2. Nous notons $m = E(X)$ et $\sigma^2 = V(X)$. Soient $X_1, \ldots, X_n$ des variables aléatoires indépendantes et de même loi que $X$. Nous notons

$$
\overline{X}_n = \frac{1}{n}\sum_{i=1}^{n} X_i
$$

la moyenne empirique des $X_i$, $i = 1, \ldots, n$, et

$$
s_X^2 = \frac{1}{n-1}\sum_{i=1}^{n} (X_i - \overline{X}_n)^2
$$

la dispersion des $X_i$, $i = 1, \ldots, n$.

1) Montrer que $E(\overline{X}_n) = m$.

2) Montrer que $E(s_X^2) = \sigma^2$. Vous pourrez utiliser l'identité

$$
s_X^2 = \frac{1}{2n(n-1)} \sum_{i=1}^{n} \sum_{\substack{j=1\\j \neq i}}^{n} (X_i - X_j)^2.
$$

3) Utiliser ces résultats pour obtenir une approximation de Monte-Carlo de l'espérance, la variance et le coefficient de variation de la variable aléatoire $X$ de l'exercice 2.

- Démonstration de l'équivalence des formules de variance

Nous voulons démontrer que :

$$\frac{1}{n-1}\sum_{i=1}^{n}(X_i - \bar{X}_n)^2 = \frac{1}{2n(n-1)}\sum_{i=1}^{n}\sum_{\substack{j=1\\j\neq i}}^{n}(X_i - X_j)^2$$

- Étape 1 : Développement de la somme double

Commençons par développer le membre de droite :

$$\sum_{i=1}^{n}\sum_{\substack{j=1\\j\neq i}}^{n}(X_i - X_j)^2$$

Développons $(X_i - X_j)^2$ :

$$= \sum_{i=1}^{n}\sum_{\substack{j=1\\j\neq i}}^{n}(X_i^2 - 2X_iX_j + X_j^2)$$

$$= \sum_{i=1}^{n}\sum_{\substack{j=1\\j\neq i}}^{n}X_i^2 - 2\sum_{i=1}^{n}\sum_{\substack{j=1\\j\neq i}}^{n}X_iX_j + \sum_{i=1}^{n}\sum_{\substack{j=1\\j\neq i}}^{n}X_j^2$$

- Étape 2 : Simplification de chaque terme

Premier terme :
$$\sum_{i=1}^{n}\sum_{\substack{j=1\\j\neq i}}^{n}X_i^2 = \sum_{i=1}^{n}X_i^2 \cdot (n-1) = (n-1)\sum_{i=1}^{n}X_i^2$$

Car pour chaque $i$ fixé, $X_i^2$ est sommé $(n-1)$ fois (pour tous les $j \neq i$).

Troisième terme :
Par symétrie, le troisième terme donne le même résultat :
$$\sum_{i=1}^{n}\sum_{\substack{j=1\\j\neq i}}^{n}X_j^2 = (n-1)\sum_{j=1}^{n}X_j^2 = (n-1)\sum_{i=1}^{n}X_i^2$$

Deuxième terme :
$$\sum_{i=1}^{n}\sum_{\substack{j=1\\j\neq i}}^{n}X_iX_j$$

Pour chaque paire $(i,j)$ avec $i \neq j$, le produit $X_iX_j$ apparaît dans cette double somme. On peut réécrire :

$$= \sum_{i=1}^{n}X_i\sum_{\substack{j=1\\j\neq i}}^{n}X_j = \sum_{i=1}^{n}X_i\left(\sum_{j=1}^{n}X_j - X_i\right)$$

$$= \sum_{i=1}^{n}X_i \cdot n\bar{X}_n - \sum_{i=1}^{n}X_i^2$$

$$= n\bar{X}_n \cdot n\bar{X}_n - \sum_{i=1}^{n}X_i^2 = n^2\bar{X}_n^2 - \sum_{i=1}^{n}X_i^2$$

- Étape 3 : Assemblage

En rassemblant les trois termes :

$$\sum_{i=1}^{n}\sum_{\substack{j=1\\j\neq i}}^{n}(X_i - X_j)^2 = (n-1)\sum_{i=1}^{n}X_i^2 - 2\left(n^2\bar{X}_n^2 - \sum_{i=1}^{n}X_i^2\right) + (n-1)\sum_{i=1}^{n}X_i^2$$

$$= (n-1)\sum_{i=1}^{n}X_i^2 - 2n^2\bar{X}_n^2 + 2\sum_{i=1}^{n}X_i^2 + (n-1)\sum_{i=1}^{n}X_i^2$$

$$= 2(n-1)\sum_{i=1}^{n}X_i^2 + 2\sum_{i=1}^{n}X_i^2 - 2n^2\bar{X}_n^2$$

$$= 2n\sum_{i=1}^{n}X_i^2 - 2n^2\bar{X}_n^2$$

$$= 2n\left(\sum_{i=1}^{n}X_i^2 - n\bar{X}_n^2\right)$$

- Étape 4 : Lien avec la variance classique

Rappelons la formule classique de la variance :

$$\sum_{i=1}^{n}(X_i - \bar{X}_n)^2 = \sum_{i=1}^{n}(X_i^2 - 2X_i\bar{X}_n + \bar{X}_n^2)$$

$$= \sum_{i=1}^{n}X_i^2 - 2\bar{X}_n\sum_{i=1}^{n}X_i + n\bar{X}_n^2$$

$$= \sum_{i=1}^{n}X_i^2 - 2\bar{X}_n \cdot n\bar{X}_n + n\bar{X}_n^2$$

$$= \sum_{i=1}^{n}X_i^2 - n\bar{X}_n^2$$

- Étape 5 : Conclusion

D'après l'étape 3 :
$$
\sum_{i=1}^{n}\sum_{\substack{j=1\\j\neq i}}^{n}(X_i - X_j)^2 = 2n\left(\sum_{i=1}^{n}X_i^2 - n\bar{X}_n^2\right)
$$


D'après l'étape 4 :
$$
\sum_{i=1}^{n}(X_i - \bar{X}_n)^2 = \sum_{i=1}^{n}X_i^2 - n\bar{X}_n^2
$$

Donc :
$$
\sum_{i=1}^{n}\sum_{\substack{j=1\\j\neq i}}^{n}(X_i - X_j)^2 = 2n\sum_{i=1}^{n}(X_i - \bar{X}_n)^2
$$

En divisant par $2n(n-1)$ :

$$
\frac{1}{2n(n-1)}\sum_{i=1}^{n}\sum_{\substack{j=1\\j\neq i}}^{n}(X_i - X_j)^2 = \frac{1}{n-1}\sum_{i=1}^{n}(X_i - \bar{X}_n)^2
$$

## Correction de l'exercice 3

1) Calcul de $E(\overline{X}_n)$

Par linéarité de l'espérance, nous avons :

$$
E(\overline{X}_n) = E\left(\frac{1}{n}\sum_{i=1}^{n} X_i\right) = \frac{1}{n}\sum_{i=1}^{n} E(X_i) = \frac{1}{n} \cdot n \cdot m = m.
$$



2) Calcul de $E(s_X^2)$

En utilisant l'identité donnée, nous avons en utilisant la linéarité de l'espérance :

$$
E(s_X^2) = E\left(\frac{1}{2n(n-1)} \sum_{i=1}^{n} \sum_{\substack{j=1\\j \neq i}}^{n} (X_i - X_j)^2\right)
$$

$$
= \frac{1}{2n(n-1)} \sum_{i=1}^{n} \sum_{\substack{j=1\\j \neq i}}^{n} E\left((X_i - X_j)^2\right).
$$

Ici on peut utiliser deux méthodes de calcul qui sont équivalentes.

La première consiste à remarquer que pour $i \neq j$ :

$$
E\left((X_i - X_j)^2\right) = E\left(X_i^2\right) - 2E(X_i X_j) + E\left(X_j^2\right)
$$

Comme les $X_i$ sont indépendantes et de même loi, nous avons :

$$
E\left(X_i^2\right) = E\left(X_1^2\right) \quad \text{et} \quad E(X_i X_j) = E(X_1)E(X_2) \quad \text{pour } i \neq j.
$$

De ce fait, nous obtenons :

$$
E\left((X_i - X_j)^2\right) = E\left(X_1^2\right) - 2E(X_1)E(X_2) + E\left(X_2^2\right)
$$

$$
= E\left(X_1^2\right) - 2E(X_1X_2) + E\left(X_2^2\right)
$$

Car $X_1$ et $X_2$ sont indépendantes et de même loi.

Donc finalement :

$$
E\left((X_i - X_j)^2\right) = E\left((X_1 - X_2)^2\right).
$$

Une autre méthode consiste à utiliser la formule de Koenig-Huygens pour la variance. Nous avons :

$$
E\left((X_i - X_j)^2\right) = E\left(X_1^2\right) - 2E(X_1)E(X_2) + E\left(X_2^2\right)
$$

$$
= 2\left(E\left(X_1^2\right) - E(X_1)^2\right) = 2V(X) = 2\sigma^2.
$$

Lorsque nous utilisons la première méthode, nous devons calculer $E\left((X_1 - X_2)^2\right)$ en développant :

$$
E\left((X_1 - X_2)^2\right) = E[{(X_1 - m) - (X_2 - m)}^2]
$$

$$
= E\left((X_1 - m)^2\right) + E\left((X_2 - m)^2\right) - 2E\left((X_1 - m)(X_2 - m)\right)
$$

$$
= E\left((X_1 - m)^2\right) + E\left((X_2 - m)^2\right) - 2E(X_1 - m)\times E(X_2 - m)
$$
Car $X_1$ et $X_2$ sont indépendantes.

$$
= \sigma^2 + \sigma^2 - 2 \cdot 0 \cdot 0 = 2\sigma^2.
$$

Donc, nous obtenons finalement :
$$
E(s_X^2) = \frac{1}{2} \cdot 2\sigma^2 = \sigma^2.
$$

3) Approximation de Monte-Carlo

Ce que nous avons montré aux points précédents, est que nous pouvons estimer l'espérance et la variance de la variable aléatoire $X$ en utilisant les statistiques d'échantillon $\overline{X}_n$ et $s_X^2$. C'est-à-dire en générant un grand nombre de réalisations indépendantes de $X$ et en calculant la moyenne empirique et la dispersion de ces réalisations.

Voici un exemple de code R pour effectuer cette approximation de Monte-Carlo pour la variable aléatoire $X$ de l'exercice 2 :


```default
# Monte Carlo approximation in R

# Répétition de l'expérience aléatoire
# Simulation du nombre de lancers nécessaires pour obtenir un 6

set.seed(360)
n <- 50000
ctr <- 0
simlist <- numeric(n)

while (ctr < n) {
  ctr <- ctr + 1
  
  # Valeur du lancer (initialisation)
  trial <- 0
  
  # Nombre de tentatives (initialisation)
  nb_tent <- 0
  
  while (trial != 6) {
    nb_tent <- nb_tent + 1
    trial <- sample(1:6, 1)
  }
  
  simlist[ctr] <- nb_tent
}

# Affichage des résultats
mean(simlist)
var(simlist)

# Statistiques supplémentaires
cat("\n=== Résultats de la simulation ===\n")
cat("Nombre de simulations:", n, "\n")
cat("Moyenne du nombre de lancers:", mean(simlist), "\n")
cat("Variance:", var(simlist), "\n")
cat("Écart-type:", sd(simlist), "\n")
cat("Minimum:", min(simlist), "\n")
cat("Maximum:", max(simlist), "\n")
cat("Médiane:", median(simlist), "\n")

# Histogramme
hist(simlist, 
     breaks = 30, 
     col = "lightblue", 
     border = "white",
     main = "Distribution du nombre de lancers pour obtenir un 6",
     xlab = "Nombre de lancers",
     ylab = "Fréquence",
     xlim = c(0, max(simlist)))

# Ajout de la moyenne théorique
abline(v = 6, col = "red", lwd = 2, lty = 2)
legend("topright", 
       legend = c("Moyenne observée", "Moyenne théorique = 6"),
       col = c("blue", "red"), 
       lty = c(1, 2), 
       lwd = 2)
```

```{python}

import random
import numpy as np
import matplotlib.pyplot as plt

# Répétition de l'expérience
random.seed(360)
n = 50000
simlist = []

for _ in range(n):
    trial = 0        # valeur du lancer
    nb_tent = 0      # nombre de tentatives
    
    while trial != 6:
        nb_tent += 1
        trial = random.randint(1, 6)  # équivalent de sample(1:6,1)
    
    simlist.append(nb_tent)

simlist = np.array(simlist)

# Résultats
print("\n=== Résultats de la simulation ===")
print("Nombre de simulations:", n)
print("Moyenne du nombre de lancers:", simlist.mean())
print("Variance:", simlist.var(ddof=1))
print("Écart-type:", simlist.std(ddof=1))
print("Minimum:", simlist.min())
print("Maximum:", simlist.max())
print("Médiane:", np.median(simlist))

# Calcul des moyennes cumulées
mean_values = np.cumsum(simlist) / np.arange(1, n + 1)

# Tracé
plt.figure(figsize=(7, 4))
plt.plot(mean_values, label="Moyenne empirique")
plt.axhline(6, color="red", linestyle="--", label="Espérance théorique = 6")

plt.xlabel("n (nombre d'expériences)")
plt.ylabel("Moyenne empirique")
plt.title("Convergence de la moyenne empirique vers l'espérance théorique")
plt.legend()
plt.grid(alpha=0.4)
plt.show() 

```



```default
# Utilisation de la distribution géométrique
# Alternative plus efficace à la simulation par boucle

set.seed(360)
n <- 50000
p <- 1/6
ctr <- 0
simlist <- numeric(n)

while (ctr < n) {
  ctr <- ctr + 1
  
  # Génération d'une réalisation d'une loi géométrique
  # Attention, rgeom(n,p) donne le nombre d'échecs avant
  # le premier succès
  simlist[ctr] <- rgeom(1, p) + 1
}

# Affichage des résultats
mean(simlist)
var(simlist)

# Statistiques détaillées
cat("\n=== Résultats de la simulation (méthode géométrique) ===\n")
cat("Nombre de simulations:", n, "\n")
cat("Probabilité de succès (p):", p, "\n")
cat("Moyenne observée:", mean(simlist), "\n")
cat("Moyenne théorique:", 1/p, "\n")
cat("Variance observée:", var(simlist), "\n")
cat("Variance théorique:", (1-p)/p^2, "\n")
cat("Écart-type observé:", sd(simlist), "\n")
cat("Médiane:", median(simlist), "\n")

# Comparaison graphique
par(mfrow = c(1, 2))

# Histogramme
hist(simlist, 
     breaks = 50, 
     col = "lightgreen", 
     border = "white",
     main = "Distribution du nombre de lancers\n(méthode rgeom)",
     xlab = "Nombre de lancers",
     ylab = "Fréquence",
     probability = TRUE)

# Ajout de la moyenne
abline(v = mean(simlist), col = "blue", lwd = 2)
abline(v = 1/p, col = "red", lwd = 2, lty = 2)
legend("topright", 
       legend = c("Moyenne obs.", "Moyenne théo."),
       col = c("blue", "red"), 
       lty = c(1, 2), 
       lwd = 2,
       cex = 0.8)

# QQ-plot pour vérifier la distribution
qqplot(qgeom(ppoints(n), p) + 1, simlist,
       main = "QQ-Plot : Théorique vs Observé",
       xlab = "Quantiles théoriques (Géométrique)",
       ylab = "Quantiles observés",
       col = "darkgreen",
       pch = 20,
       cex = 0.5)
abline(0, 1, col = "red", lwd = 2)

par(mfrow = c(1, 1))

# Méthode encore plus efficace (vectorisée)
cat("\n=== Méthode vectorisée (plus rapide) ===\n")
set.seed(360)
simlist_vec <- rgeom(n, p) + 1
cat("Moyenne:", mean(simlist_vec), "\n")
cat("Variance:", var(simlist_vec), "\n")
```



```{python}
import numpy as np
import matplotlib.pyplot as plt
import scipy.stats as st

# Paramètres
np.random.seed(360)
n = 50000
p = 1/6

# La loi géométrique de R donne "nombre d'échecs avant succès"
# => équivalent python : st.geom(p).rvs() renvoie déjà nb_tentatives
# MAIS Python compte à partir de 1, donc c'est ce qu’on veut !
simlist = st.geom(p).rvs(size=n)

# Affichage des résultats
print("\n=== Résultats de la simulation (méthode géométrique) ===")
print("Nombre de simulations:", n)
print("Probabilité de succès p:", p)
print("Moyenne observée:", simlist.mean())
print("Moyenne théorique:", 1/p)
print("Variance observée:", simlist.var(ddof=1))
print("Variance théorique:", (1 - p) / p**2)
print("Écart-type observé:", simlist.std(ddof=1))
print("Médiane:", np.median(simlist))


# Moyennes cumulées
mean_values = np.cumsum(simlist) / np.arange(1, n + 1)

# Tracé
plt.figure(figsize=(7, 4))
plt.plot(mean_values, label="Moyenne empirique")
plt.axhline(1/p, color="red", linestyle="--", label=f"Espérance théorique = {1/p:.0f}")

plt.xlabel("n (nombre d'expériences)")
plt.ylabel("Moyenne empirique")
plt.title("Convergence de la moyenne empirique de la loi géométrique")
plt.legend()
plt.grid(alpha=0.4)
plt.show()

```


## Exercice 4

Soient X et Y deux variables aléatoires de carré $\mathbb{P}$-intégrable.

1. Soit $a \in \mathbb{R}$.  
   Écrire  
   $$
   E\big((|X| - a|Y|)^2\big)
   $$
   sous la forme d’un polynôme en a, et calculer son discriminant $\Delta$.

2. Expliquer pourquoi $\Delta \le 0$, et en déduire l’inégalité de Hölder :
   $$
   E(|XY|) \le \sqrt{E(X^2)} \, \sqrt{E(Y^2)}.
   $$

3. En appliquant l’inégalité précédente à des variables bien choisies, en déduire que :
   $$
   |\mathrm{Cov}(X,Y)| \le \sqrt{V(X)} \, \sqrt{V(Y)}.
   $$

L’inégalité de Hölder se généralise sous la forme suivante (admise dans la suite).  
Soient deux nombres réels $p, q \ge 1$ conjugués, c’est-à-dire tels que :
$$
\frac{1}{p} + \frac{1}{q} = 1.
$$

Soient X et Y deux variables aléatoires telles que  
$\int |X|^p \, d\mathbb{P} < \infty$ et $\int |Y|^q \, d\mathbb{P} < \infty$.  
Alors :
$$
E(|XY|) \le \big(E(|X|^p)\big)^{1/p} \, \big(E(|Y|^q)\big)^{1/q}.
$$

Soient maintenant deux réels $r$ et $s$ tels que $1 < r < s < \infty$.  
Soit $Z$ une variable aléatoire telle que $\int |Z|^s \, d\mathbb{P} < \infty$.

4. Vérifier que les réels $\frac{s}{r}$ et $\frac{s}{s-r}$ sont conjugués.  
   En appliquant l’inégalité de Hölder avec des variables aléatoires $X$ et $Y$ bien choisies, montrer que :
   $$
   E(|Z|^r) \le \big(E(|Z|^s)\big)^{r/s}.
   $$

5. En déduire que si une variable aléatoire $Z$ admet un moment d’ordre $s > 1$, alors elle admet un moment d’ordre $r$ pour tous les réels $1 < r < s$.

## Correction de l'exercice 4

1) Calcul de $E\big((|X| - a|Y|)^2\big)$

Nous avons en utilisant la linéarité de l'espérance :

$$
E\big((|X| - a|Y|)^2\big) = E(|X|^2) - 2aE(|X||Y|) + a^2E(|Y|^2).
$$

est sa forme de polynôme en $a$.

Son discriminant est donc donné par :

$$
\Delta = (-2E(|X||Y|))^2 - 4E(|X|^2)E(|Y|^2) = 4\big(E(|X||Y|)^2 - E(|X|^2)E(|Y|^2)\big).
$$

Comme $E\big((|X| - a|Y|)^2\big) \geq 0$ pour tout $a \in \mathbb{R}$, le polynôme ne peut pas avoir deux racines réelles distinctes, donc $\Delta \leq 0$.

3) En appliquant l’inégalité précédente en remplaçant $X$ par $X - E(X)$, et
$Y$ par $Y - E(Y)$, nous obtenons :

$$
E \big( \{ X - E(X) \} \{ Y - E(Y) \} \big)
\le \sqrt{E \big( \{ X - E(X) \}^2 \big)} \, \sqrt{E \big( \{ X - E(X) \}^2 \big)}.
$$

Nous avons ensuite

$$
|\mathrm{Cov}(X,Y)|
  = \big| E \big( \{ X - E(X) \} \{ Y - E(Y) \} \big) \big|
  \le E \big( \{ X - E(X) \} \{ Y - E(Y) \} \big),
$$

ce qui donne le résultat voulu.

---

4) Nous avons bien

$$
\frac{r}{s} + \frac{s - r}{s} = 1.
$$

En appliquant l’inégalité de Hölder générale aux variables aléatoires $X = Z^r$
et $Y = 1$, et avec les nombres conjugués $\frac{s}{r}$ et $\frac{s}{s-r}$,
nous obtenons

$$
E \big( |Z^r \times 1| \big)
\le \Big( E \big( |Z^r|^{\frac{s}{r}} \big) \Big)^{\frac{r}{s}}
     \Big( E \big( |1|^{\frac{s}{s-r}} \big) \Big)^{\frac{s-r}{s}}
     = \Big( E(|Z|^s) \Big)^{\frac{r}{s}}.
$$

$$
\Rightarrow \quad
E(|Z|^r) \le \big( E(|Z|^s) \big)^{\frac{r}{s}}.
$$



5) Cela découle de l’inégalité précédente.

# 4. Variables aléatoires discrètes

## Exercice 1

Dans chacune des situations, identifier si $X$ suit une loi binomiale. Si oui, donner les paramètres $n$ et $p$ correspondants. Sinon, expliquer pourquoi et proposer une modélisation alternative :

- Chaque jour, Dean va déjeuner et il y a 25% de chances qu’il prenne une pizza. Soit $X$ le nombre de fois où il a pris une pizza la semaine dernière.

- Jessica joue au basketball, et elle a 60% de chances de réussir un lancer franc. Soit $X$ le nombre de lancers francs réussis pendant le dernier match.

- Une boîte contient 300 bonbons, dont 100 carambars et 200 chocolats. Sam prend un échantillon de 10 bonbons dans la boîte. Soit $X$ le nombre de carambars dans son échantillon.

- Marie lit un livre de 600 pages. Sur les pages paires, il y a 1% de chances d’avoir une faute d’orthographe. Sur les pages impaires, il y a 2% de chances d’avoir une faute d’orthographe. Soit $X$ le nombre total de fautes d’orthographe dans le livre.

- John lit un livre de 600 pages. Sur chaque page, le nombre de fautes d’orthographe est distribué selon une loi de Bernouilli de paramètre $0.01$. Soit $X$ le nombre total de fautes d’orthographe dans le livre.


## Correction Exercice 1

Dans chaque situation, on indique si $X$ suit une loi binomiale. Si oui, on précise les paramètres $n$ et $p$. Sinon, on propose une modélisation alternative.

---

1. **Dean et les pizzas**

- Chaque jour, Dean prend une pizza avec probabilité $p = 0{,}25$, pendant une semaine ($n = 7$ jours). 
- Ensuite, il faut se poser la question : la décision de prendre une pizza un jour est-elle indépendante des autres jours ? Si oui, alors :
- Le nombre de jours $X$ où il prend une pizza suit donc une loi binomiale :

$$X \sim \mathcal{B}(n = 7,\; p = 0{,}25).$$

---

2. **Jessica et les lancers francs**

- Jessica réussit un lancer franc avec probabilité $p = 0{,}6$, mais le nombre de lancers tentés pendant le match **n’est pas fixé à l’avance**.
- Il ne s’agit donc pas d’une loi binomiale (condition nécessaire : nombre d’essais fixé). Dans une modélisation binomiale, le nombre d’essais $n$ doit être fixé et connu à l’avance. 

Une modélisation possible est :

- On note $N$ le nombre de lancers francs tentés pendant le dernier match. Il y a plusieurs possibilités : Soit $N$ est une constante connue, soit $N$ est une variable aléatoire (par exemple, on peut modéliser $N$ par une loi de Poisson.). Ensuite, on considère le nombre de lancers réussis $X$ conditionnellement à $N$.

- Conditionnellement à $N$, le nombre de lancers réussis suit une loi binomiale :

$$X \mid N \sim \mathcal{B}(N,\; 0{,}6).$$

si les lancers sont indépendants.

---

3. **Boîte de bonbons (carambars/chocolats)**

- La boîte contient $300$ bonbons dont $100$ carambars et $200$ chocolats.
- Sam tire un échantillon de $n = 10$ bonbons **sans remise**.
- Les tirages ne sont pas indépendants (sans remise), donc $X$ **ne suit pas** une loi binomiale.

La loi adaptée est l’hypergéométrique :

- Taille de la population : $N = 300$.
- Nombre de “succès” (carambars) : $K = 100$.
- Taille de l’échantillon : $n = 10$.

On a alors :

$$X \sim \mathcal{H}(N = 300,\; K = 100,\; n = 10).$$

---

4. **Marie et les fautes d’orthographe (1 % / 2 %)**

- Le livre a $600$ pages.
- Sur les pages paires (300 pages), probabilité de faute $p_1 = 0{,}01$.
- Sur les pages impaires (300 pages), probabilité de faute $p_2 = 0{,}02$.
- La probabilité de “succès” (faute) **n’est pas la même** sur tous les essais (pages), donc $X$ **ne suit pas** une loi binomiale.

Une modélisation naturelle est de décomposer :

- On suppose l'indépendance entre les pages.
- $X_{\text{paires}}$ : nombre de fautes sur les pages paires,
- $X_{\text{impaires}}$ : nombre de fautes sur les pages impaires.

On a :

$$
X_{\text{paires}} \sim \mathcal{B}(300,\; 0{,}01), \qquad
X_{\text{impaires}} \sim \mathcal{B}(300,\; 0{,}02),
$$

et

$$
X = X_{\text{paires}} + X_{\text{impaires}}.
$$

---

5. **John et les fautes d’orthographe (Bernoulli 0.01)**

- Le livre a $600$ pages.
- Sur chaque page, le nombre de fautes suit une loi de Bernoulli de paramètre $p = 0{,}01$ (on suppose indépendance entre pages).
- Cette fois, la probabilité de faute est la même pour toutes les pages, et le nombre total de pages $n = 600$ est fixé.

Ainsi, $X$ suit une loi binomiale :

$$X \sim \mathcal{B}(n = 600,\; p = 0{,}01).$$


## Exercice 2

Soit $X_1, \dots, X_n$ une suite de variables aléatoires indépendantes telles que, pour tout $k = 1, \dots, n$,

$$\mathbb{P}(X = \pm 1) = \frac{1}{2}.$$

Soit
$$S = \sum_{i=1}^n X_i.$$
On parle de **marche aléatoire symétrique**, où le point de départ est $0$ avec un déplacement aléatoire à gauche ou à droite à chaque temps.

1. Calculer $E(S)$ et $V(S)$.

Nous supposons maintenant que

$$\mathbb{P}(X = 1) = p \quad \text{et} \quad \mathbb{P}(X = -1) = 1 - p.$$

Si $p > \dfrac{1}{2}$, on parle de **marche aléatoire à dérive positive**.

2. Montrer qu’on peut réécrire $X = aY + b$, avec $Y \sim B(p)$, et $a$ et $b$ deux constantes à déterminer.  
3. En déduire $E(S)$ et $V(S)$.


## Correction — Exercice 2

On considère une suite de variables aléatoires indépendantes $X_1, \dots, X_n$ telles que
$$\mathbb{P}(X = \pm 1) = \frac{1}{2}.$$

On pose :
$$S = \sum_{i=1}^n X_i.$$

---

#### 1) Calculer $E(S)$ et $V(S)$ dans le cas symétrique

Pour chaque $X_i$ :

- $\mathbb{E}(X_i) = 1 \cdot \frac{1}{2} + (-1) \cdot \frac{1}{2} = 0$,

- $\mathbb{E}(X_i^2) = 1^2 = 1$ d’où $\operatorname{Var}(X_i) = \mathbb{E}(X_i^2) - \mathbb{E}(X_i)^2 = 1$.


Par linéarité de l’espérance :
$$\mathbb{E}(S) = \sum_{i=1}^n \mathbb{E}(X_i) = 0,$$

Comme les $X_i$ sont indépendantes :
$$\operatorname{Var}(S) = \sum_{i=1}^n \operatorname{Var}(X_i) = n.$$

---

Nous supposons maintenant que
$$\mathbb{P}(X = 1) = p \quad \text{et} \quad \mathbb{P}(X = -1) = 1 - p,$$
avec $p > \frac{1}{2}$ (marche aléatoire à dérive positive).

---

#### 2) Montrer que l’on peut réécrire $X = aY + b$, avec $Y \sim B(p)$

Il faut jouer ici avec le support des variables aléatoires. L'idée est de partir d'une variable de Bernoulli $Y$ qui prend les valeurs $0$ et $1$, et de la transformer linéairement pour obtenir une variable $X$ qui prend les valeurs $-1$ et $1$. Comme $P(X = 1) = p$ et $P(X = -1) = 1 - p$, on peut définir $Y$ comme une variable de Bernoulli telle que $P(Y = 1) = p$ et $P(Y = 0) = 1 - p$.

On cherche $a$ et $b$ tels que :
- si $Y = 1$, alors $X = 1$,
- si $Y = 0$, alors $X = -1$.

On résout :
$$
\begin{cases}
a \cdot 1 + b = 1,\\
a \cdot 0 + b = -1.
\end{cases}
$$

D'où $b = -1$ et $a = 2$.

Ainsi :
$$X = 2Y - 1 \quad \text{avec} \quad Y \sim \text{Bernoulli}(p).$$

---

####  3) En déduire $E(S)$ et $V(S)$ dans le cas général

Comme $S = \sum_{i=1}^n X_i$ et $X_i = 2Y_i - 1$ avec $Y_i \sim B(p)$ indépendantes :

Par linéarité de l’espérance :

$$
\mathbb{E}(X_i) = \mathbb{E}{(2Y_i - 1)} =
2\,\mathbb{E}(Y_i) - 1 = 2p - 1.
$$

Calculons la variance :

On sait que pour toute variable aléatoire $X = aY + b$, on a $\operatorname{Var}(X) = a^2\,\operatorname{Var}(Y)$. Donc :
$$
\operatorname{Var}(X_i) = \operatorname{Var}(2Y_i - 1) = 2^2\,\operatorname{Var}(Y_i) =
   4\,\operatorname{Var}(Y_i) = 4\,p(1 - p).
$$
Donc :

$$
\mathbb{E}(S) = \sum_{i=1}^n \mathbb{E}(X_i) = n(2p - 1),
$$

$$
\operatorname{Var}(S) = \sum_{i=1}^n \operatorname{Var}(X_i) = 4n\,p(1 - p).
$$

---

####  Résumé final

| Type de marche | $\mathbb{E}(S)$ | $\operatorname{Var}(S)$ |
|---------------|----------------|-------------------------|
| Symétrique ($p = \tfrac12$) | $0$ | $n$ |
| Dérive positive ($p > \tfrac12$) | $n(2p - 1)$ | $4n\,p(1 - p)$ |

## Exercice 3

Identifier les lois des variables aléatoires suivantes, en se basant sur leur fonction génératrice des moments :

1. $M_X(t) = 0.8 e^t + 0.2$

2. $M_Y(t) = \dfrac{0.1 e^t}{1 - 0.9 e^t}$

3. $M_Z(t) = (0.3 e^t + 0.7)^{14}$

## Correction — Exercice 3
### Rappel : fonctions génératrices des moments

1. **Loi de Bernoulli de paramètre $p$**

On a
$$
\mathbb{P}(X = 1) = p, \qquad \mathbb{P}(X = 0) = 1 - p.
$$

Alors la fonction génératrice des moments est :
$$
M_X(t) = \mathbb{E}(e^{tX})
= (1-p) e^{t \cdot 0} + p e^{t \cdot 1}
= (1 - p) + p e^t.
$$

2. **Loi géométrique de paramètre $p$ sur $\{1,2,\dots\}$**

On prend la convention
$$
\mathbb{P}(Y = k) = (1 - p)^{k-1} p, \quad k \ge 1.
$$

Alors
$$
M_Y(t) = \mathbb{E}(e^{tY})
= \sum_{k=1}^{\infty} e^{tk} (1-p)^{k-1} p
= p e^t \sum_{k=0}^{\infty} \big((1-p)e^t\big)^k.
$$

Pour $| (1-p)e^t | < 1$, c’est une série géométrique :
$$
\sum_{k=0}^{\infty} r^k = \frac{1}{1-r}.
$$

Donc
$$
M_Y(t) = \frac{p e^t}{1 - (1-p) e^t}.
$$

3. **Loi binomiale $\mathcal{B}(n,p)$**

Soit $Z \sim \mathcal{B}(n,p)$. On peut écrire
$$
Z = X_1 + \cdots + X_n,
$$
où les $X_i$ sont indépendantes et suivent toutes une Bernoulli$(p)$.

On sait que, pour une Bernoulli$(p)$,
$$
M_{X_i}(t) = (1-p) + p e^t.
$$

Par indépendance,
$$
M_Z(t) = \mathbb{E}(e^{tZ})
= \mathbb{E}\big(e^{t(X_1 + \cdots + X_n)}\big)
= \prod_{i=1}^n \mathbb{E}(e^{tX_i})
= \big((1-p) + p e^t\big)^n.
$$

---

### Identification des lois

1. On a
$$
M_X(t) = 0.8 e^t + 0.2 = 0.2 + 0.8 e^t.
$$

En comparant avec $(1-p) + p e^t$, on obtient $p = 0.8$.

> Donc $X$ suit une loi de Bernoulli de paramètre $0.8$ :
> $$
> X \sim \text{Bernoulli}(0.8).
> $$

2. On a
$$
M_Y(t) = \dfrac{0.1 e^t}{1 - 0.9 e^t}.
$$

En comparant avec
$$
M_Y(t) = \frac{p e^t}{1 - (1-p) e^t},
$$
on lit $p = 0.1$ et $1-p = 0.9$.

> Donc $Y$ suit une loi géométrique de paramètre $p = 0.1$ (sur $\{1,2,\dots\}$) :
> $$
> Y \sim \text{Géométrique}(p = 0.1).
> $$

3. On a
$$
M_Z(t) = (0.3 e^t + 0.7)^{14}
= \big( (1 - 0.3) + 0.3 e^t \big)^{14}.
$$

En comparant avec
$$
M_Z(t) = \big( (1-p) + p e^t \big)^n,
$$
on obtient $p = 0.3$ et $n = 14$.

> Donc $Z$ suit une loi binomiale $\mathcal{B}(14, 0.3)$ :
> $$
> Z \sim \mathcal{B}(n = 14, p = 0.3).
> $$

## Exercice 4

Soit $X$ une variable aléatoire telle que
$$\mathbb{P}(X = k) = \frac{k}{10} \quad \text{pour } k = 1, 2, 3, 4.$$

Soit $Y$ une variable aléatoire indépendante de $X$, et suivant la même distribution.

Calculer la loi de probabilité de $X + Y$.

## Correction — Exercice 4


On a
$$
\mathbb{P}(X = k) = \frac{k}{10}, \quad k = 1,2,3,4,
$$
et $Y$ est indépendante de $X$ et de même loi.


|       | Y = 1 | Y = 2 | Y = 3 | Y = 4 |
| ----- | ----- | ----- | ----- | ----- |
| X = 1 | 2     | 3     | 4     | 5     |
| X = 2 | 3     | 4     | 5     | 6     |
| X = 3 | 4     | 5     | 6     | 7     |
| X = 4 | 5     | 6     | 7     | 8     |


La variable $S = X + Y$ prend des valeurs entières de $2$ à $8$.


La variable aléatoire $X + Y$ prend ses valeurs dans $\{2, \dots, 8\}$.  

Pour $k \in \{2, \dots, 8\}$, nous avons en utilisant la formule des probabilités totales :

$$
\mathbb{P}(X + Y = k)
= \sum_{l=1}^{4} \mathbb{P}(X + Y = k \mid Y = l)\,\mathbb{P}(Y = l)
$$

$$
= \sum_{l=1}^{4} \mathbb{P}(Y = k - l \mid Y = l)\,\mathbb{P}(Y = l)
$$

$$
= \sum_{l=1}^{4} \mathbb{P}(X = k - l)\,\mathbb{P}(Y = l) \quad \text{car $X$ et $Y$ sont indépendantes.}
$$

où la somme porte sur les $k$ tels que $1 \le k \le 4$ et $1 \le k- l \le 4$.

On note
$$
p_1 = 0.1,\quad p_2 = 0.2,\quad p_3 = 0.3,\quad p_4 = 0.4.
$$

On calcule alors, cas par cas :

- Pour $k = 2$  
  $$(X,Y) = (1,1) \quad\Rightarrow\quad \mathbb{P}(S=2) = p_1 p_1 = 0.1\times 0.1 = 0.01.$$

- Pour $k = 3$  
  $$(X,Y) = (1,2),(2,1)$$
  $$
  \mathbb{P}(S=3) = p_1 p_2 + p_2 p_1 = 0.1\times 0.2 + 0.2\times 0.1 = 0.04.
  $$

- Pour $s = 4$  
  $$(X,Y) = (1,3),(2,2),(3,1)$$
  $$
  \mathbb{P}(S=4) = p_1 p_3 + p_2 p_2 + p_3 p_1
  = 0.1\times 0.3 + 0.2\times 0.2 + 0.3\times 0.1
  = 0.10.
  $$

- Pour $k = 5$  
  $$(X,Y) = (1,4),(2,3),(3,2),(4,1)$$
  $$
  \mathbb{P}(S=5) = p_1 p_4 + p_2 p_3 + p_3 p_2 + p_4 p_1
  = 0.04 + 0.06 + 0.06 + 0.04 = 0.20.
  $$

- Pour $k = 6$  
  $$(X,Y) = (2,4),(3,3),(4,2)$$
  $$
  \mathbb{P}(S=6) = p_2 p_4 + p_3 p_3 + p_4 p_2
  = 0.2\times 0.4 + 0.3\times 0.3 + 0.4\times 0.2
  = 0.08 + 0.09 + 0.08 = 0.25.
  $$

- Pour $k = 7$  
  $$(X,Y) = (3,4),(4,3)$$
  $$
  \mathbb{P}(S=7) = p_3 p_4 + p_4 p_3
  = 0.3\times 0.4 + 0.4\times 0.3
  = 0.24.
  $$

- Pour $k = 8$  
  $$(X,Y) = (4,4) \quad\Rightarrow\quad \mathbb{P}(S=8) = p_4 p_4 = 0.4\times 0.4 = 0.16.$$

On obtient donc la loi de $S = X+Y$ :

$$
\begin{array}{c|ccccccc}
s      & 2    & 3    & 4    & 5    & 6    & 7    & 8    \\ \hline
\mathbb{P}(S=s) & 0.01 & 0.04 & 0.10 & 0.20 & 0.25 & 0.24 & 0.16
\end{array}
$$

(On vérifie que la somme vaut bien $1$: $0.01 + 0.04 + 0.10 + 0.20 + 0.25 + 0.24 + 0.16 = 1$.)

## Exercice 5

Un statisticien a modélisé le nombre de mots d’une recherche sur internet en utilisant une loi de Poisson. Supposons que la longueur moyenne est de 3 mots, et soit $X$ le nombre de mots d’une recherche. Comme une recherche ne peut pas être vide, nous utilisons une modélisation par une loi de probabilité restreinte définie par

$$\mathbb{P}(X = k) = \mathbb{P}(Y = k \mid Y \neq 0) \quad \text{où } Y \sim \text{Pois}(\lambda).$$

1) Trouver la loi de $X$.  
2) Donner la valeur de $\lambda$ correspondant à une longueur moyenne de 3 mots.  
3) Quelle est la probabilité d’avoir une recherche de 6 mots ou plus ?

## Correction — Exercice 5

1) Trouver la loi de $X$

Pour déterminer la loi d'une variable aléatoire $X$ , il faut d'abord définir son support, c'est-à-dire l'ensemble des valeurs que $X$ peut prendre avec une probabilité non nulle. Dans ce cas, le support de $X$ est l'ensemble des entiers naturels positifs, car une recherche ne peut pas être vide.

Nous notons le support de $X$ par $\Omega_X = \{1, 2, 3, \ldots\}$ = $\mathbb{N}^*$.

Une fois le support identifié, nous pouvons calculer la probabilité associée à chaque valeur du support de $X$. 

Rappelons nous que l'ensemble ${X \in A} = {w \in \Omega : X(w) \in A} = X^-1(A)$.
Pour tout $k \in \Omega_X$, nous avons :

${X = k} = {w \in \Omega : X(w) = k} = X^{-1}(\{k\})$. où $\Omega$ est l'ensemble des issues possibles de l'expérience aléatoire.

Donc, pour tout $k \in \Omega_X$, nous avons :

On part de la définition de $X$ comme loi de Poisson **conditionnée à être non nulle** :

1. **Définition de $X$ comme loi restreinte :**
   $$
   \mathbb{P}(X = k)
   = \mathbb{P}(Y = k \mid Y \neq 0).
   $$
   C’est donné dans l’énoncé : $X$ est la loi de $Y$ sachant que $Y$ ne vaut pas 0.

2. **Formule de probabilité conditionnelle :**
   $$
   \mathbb{P}(Y = k \mid Y \neq 0)
   = \frac{\mathbb{P}(Y = k \cap Y \neq 0)}{\mathbb{P}(Y \neq 0)}.
   $$
   Par définition :  
   $\displaystyle \mathbb{P}(A \mid B) = \frac{\mathbb{P}(A \cap B)}{\mathbb{P}(B)}$.

3. **Simplification de l'intersection :**
   $$
   \mathbb{P}(Y = k \cap Y \neq 0) = \mathbb{P}(Y = k)
   $$
   car si $k \ge 1$, alors l'événement « $Y = k$ » implique automatiquement « $Y \neq 0$ ».  
   Donc l’intersection ne change rien.

4. **Remplacement du dénominateur :**
   $$
   \mathbb{P}(Y \neq 0) = 1 - \mathbb{P}(Y = 0).
   $$
   C’est la propriété générale :  
   $\displaystyle \mathbb{P}(B^c) = 1 - \mathbb{P}(B)$.

5. **Utilisation de la formule de la loi de Poisson :**
   $$
   \mathbb{P}(Y = k) = e^{-\lambda} \frac{\lambda^k}{k!},
   \qquad
   \mathbb{P}(Y = 0) = e^{-\lambda}.
   $$

6. **Substitution dans la formule :**
   $$
   \mathbb{P}(X = k)
   = \frac{e^{-\lambda} \frac{\lambda^k}{k!}}{1 - e^{-\lambda}}.
   $$





###  Résultat final :
$$
\boxed{\mathbb{P}(X = k) = \frac{e^{-\lambda} }{1 - e^{-\lambda}} \cdot \frac{\lambda^k}{k!}, \quad k = 1, 2, 3, \ldots}
$$


### 2) Calcul de l’espérance de $X$

Nous avons la loi :
$$
\mathbb{P}(X = k)
= \frac{e^{-\lambda} \lambda^k}{k!\,(1 - e^{-\lambda})}, \qquad k \ge 1.
$$

L’espérance vaut en utilisant le théorème de transfert :
$$
\mathbb{E}(X)
= \sum_{k=1}^{+\infty} k \, \mathbb{P}(X = k)
= \sum_{k=1}^{\infty} k \, \frac{e^{-\lambda} \lambda^k}{k!\,(1 - e^{-\lambda})}.
$$

**On factorise les constantes** :
$$
\mathbb{E}(X)
= \frac{e^{-\lambda}}{1 - e^{-\lambda}}
   \sum_{k=1}^{\infty} k \frac{\lambda^{k}}{k!}.
$$

---

Justification du passage suivant

On réécrit :
$$
k\frac{\lambda^{k}}{k!}
= \lambda \frac{\lambda^{k-1}}{(k-1)!}.
$$

En effet :
$$
\frac{k}{k!} = \frac{1}{(k-1)!}.
$$

Donc :
$$
\sum_{k=1}^{\infty} k \frac{\lambda^k}{k!}
= \lambda \sum_{k=1}^{\infty} \frac{\lambda^{k-1}}{(k-1)!}.
$$

On effectue le **changement d’indice**  $j = k - 1$ :

- quand $k = 1$, $j = 0$,
- quand $k \to \infty$, $j \to \infty$.

D’où :
$$
\sum_{k=1}^{\infty} \frac{\lambda^{k-1}}{(k-1)!}
= \sum_{j=0}^{\infty} \frac{\lambda^{j}}{j!}.
$$

Or cette somme est la **série de Taylor de l’exponentielle** :
$$
\sum_{j=0}^{\infty} \frac{\lambda^{j}}{j!} = e^{\lambda}.
$$



On revient à l’expression de l’espérance

On obtient donc :
$$
\mathbb{E}(X)
= \frac{e^{-\lambda}}{1 - e^{-\lambda}} \cdot \lambda e^{\lambda}.
$$
Puis en simplifiant \( e^{-\lambda} e^{\lambda} = 1 \) :
$$    
\boxed{
\mathbb{E}(X) = \frac{\lambda}{1 - e^{-\lambda}}.
}
$$    

---

### Détermination de $\lambda$ pour une espérance de 3

En résolvant :
$$
\frac{\lambda}{1 - e^{-\lambda}} = 3,
$$

```{python}

#Utilisons Newton-Raphson pour résoudre l'équation

from scipy.optimize import newton
import numpy as np

# Définir la fonction
def equation(lmbda):
    return lmbda / (1 - np.exp(-lmbda)) - 3

# Résoudre avec Newton-Raphson
lambda_solution = newton(equation, x0=1.0)
print(lambda_solution)

```

on obtient numériquement :
$$
\boxed{\lambda \approx 2.82}.
$$

3) Probabilité d’avoir une recherche de 6 mots ou plus

On cherche :
$$
\mathbb{P}(X \ge 6).
$$

Or :
$$
\mathbb{P}(X \ge 6) = 1 - \mathbb{P}(X \le 5).
$$

Et comme \(X\) est la loi de Poisson tronquée :
$$
\mathbb{P}(X \le 5)
= \sum_{k=1}^{5} \mathbb{P}(X = k)
= \sum_{k=1}^{5}
\frac{e^{-\lambda} \lambda^k}{k!\,(1 - e^{-\lambda})}.
$$

On factorise pour simplifier :
$$  
\mathbb{P}(X \le 5)
= \frac{e^{-\lambda}}{1 - e^{-\lambda}}
\sum_{k=1}^{5} \frac{\lambda^k}{k!}.
$$

Donc :

$$
\boxed{
\mathbb{P}(X \ge 6)
= 1 -
\frac{e^{-\lambda}}{1 - e^{-\lambda}}
\sum_{k=1}^{5} \frac{\lambda^k}{k!}
}.
$$

---

 Avec $\lambda \approx 2.82$, on calcule :

Le calcul numérique donne :
$$
\boxed{\mathbb{P}(X \ge 6) \approx 0.07}
$$

soit **environ 7%**.

## Exercice 6

La loi de probabilité jointe de deux variables $X$ et $Y$ est définie par

$$
\mathbb{P}(X = x, Y = y)
= \frac{1}{e^{2} \, y! \, (x - y)!}
\qquad \text{pour } x \in \mathbb{N} \text{ et } y = 0, \ldots, x.
$$

1) Trouver la loi de probabilité de $X$.  
2) En déduire la loi de probabilité de $Y$ sachant que $X = x$.

## Correction — Exercice 6

Avant de résoudre l’exercice, rappelons les formules de pascal :

$$
\binom{x}{y} = \frac{x!}{y! (x-y)!},
$$
et
$$
(a + b)^x = \sum_{y=0}^{x} \binom{x}{y} a^y b^{x-y}.
$$

Pour a = 1 et b = 1, on obtient :
$$
2^x = \sum_{y=0}^{x} \binom{x}{y}.
$$

1) Trouver la loi de probabilité de $X$

Le support de X est $\mathbb{N}$.

Pour tout $x \in \mathbb{N}$, on utilise la formule des probabilités totales :
$$
\mathbb{P}(X = x)
= \mathbb{P}(X = x, \Omega)
= \mathbb{P}(X = x, \bigcup_{y=0}^{x} \{Y = y\})
= \sum_{y=0}^{x} \mathbb{P}(X = x, Y = y).
$$

- $\Omega$ est l’événement certain.
- Les événements {Y=0}, \ldots, {Y=x} sont disjoints et forment une partition des valeurs possibles de Y quand X=x.
- La formule générale de probabilité totale dit :
  $$
  \mathbb{P}(A) = \sum_i \mathbb{P}(A \cap B_i)
  \quad \text{si les } B_i \text{ forment une partition}.
  $$


En théorie de probabilité, on écrit souvent $P(A, B)$ pour $P(A \cap B)$.

On remplace par la loi jointe donnée :
$$
\mathbb{P}(X = x)
= \sum_{y=0}^{x} \frac{1}{e^{2} \, y! \, (x - y)!}.
$$

On factorise :
$$
\mathbb{P}(X = x)
= \frac{1}{e^{2}} \sum_{y=0}^{x} \frac{1}{y! \, (x - y)!}.
$$

On utilise la formule de pascal pour réécrire la somme :
On utilise l’identité :
$$
\binom{x}{y} = \frac{x!}{y! (x-y)!}.
$$

D’où :
$$
\frac{1}{y!(x-y)!} = \frac{1}{x!}\binom{x}{y}.
$$

Ainsi :
$$
\sum_{y=0}^{x} \frac{1}{y!(x-y)!}
= \frac{1}{x!} \sum_{y=0}^{x} \binom{x}{y}.
$$

Or :
$$
\sum_{y=0}^{x} \binom{x}{y} = 2^x,
$$
car c’est le développement de \((1+1)^x\).

On obtient donc :
On obtient :
$$
\mathbb{P}(X = x)
= \frac{1}{e^{2}} \cdot \frac{2^x}{x!}
= e^{-2} \frac{2^x}{x!}.
$$


Nous reconnaissons la loi de Poisson de paramètre $\lambda = 2$.


$$
\boxed{X \sim \text{Poisson}(2)}.
$$

2) Soient $x, y \in \mathbb{N}$. Nous avons

$$
\mathbb{P}(Y = y \mid X = x)
= \frac{\mathbb{P}(X = x, Y = y)}{\mathbb{P}(X = x)},
$$

et cette probabilité est nulle si $y > x$.  
Si $y \le x$, nous avons :

$$
\mathbb{P}(Y = y \mid X = x)
= \frac{1}{e^{2} y! (x - y)!} \times \frac{e^{2} x!}{2^{x}}
$$

$$
= \binom{x}{y} \left(\frac{1}{2}\right)^{y} \left(\frac{1}{2}\right)^{x - y}.
$$

Nous reconnaissons la densité d’une loi binomiale de paramètres $m = x$ et  
$p = \frac{1}{2}$.

Donc :
$$
\boxed{Y \mid X = x \sim \mathcal{B}\left(x, \frac{1}{2}\right)}.
$$

## Exercice 7

Soient $X_1, \ldots, X_n$ une suite de variables aléatoires i.i.d. suivant une loi $\mathcal{L}$.
Nous supposons que les paramètres de la loi $\mathcal{L}$ sont entièrement caractérisés
par les moments $\mu_1 \equiv \mathbb{E}(X), \ldots, \mu_k \equiv \mathbb{E}(X^k)$.
La *méthode des moments* consiste à estimer les paramètres de la loi $\mathcal{L}$
en remplaçant les moments inconnus par leur estimateur empirique

$$
\hat{\mu}_k = \frac{1}{n} \sum_{i=1}^n (X_i)^k.
$$

Nous supposons que les variables aléatoires $X_1, \ldots, X_n$ suivent une loi de Bernoulli
de paramètre $p$.

1) Exprimer $p$ en fonction du premier moment de $X$.  
2) En déduire une estimation de $p$ basée sur l’échantillon.

Nous supposons que les variables aléatoires $X_1, \ldots, X_n$ suivent une loi binomiale
de paramètres $m$ et $p$.

3) Exprimer $m$ et $p$ en fonction des deux premiers moments de $X$.  
4) En déduire une estimation de $m$ et de $p$ basée sur l’échantillon.

### Correction Exercice 7

1) Nous avons  
$$
p = \mathbb{E}(X) = \mu_1.
$$

2) Le paramètre \(p\) est donc estimé par
$$
\hat{p} = \hat{\mu}_1 = \frac{1}{n} \sum_{i=1}^n X_i.
$$

Notons que dans ce cas précis, il s’agit d’un estimateur sans biais.

---

3) Nous avons, pour une loi binomiale \(\mathcal{B}(m,p)\) :
$$
\mathbb{E}(X) = mp
\qquad\text{et}\qquad
\operatorname{Var}(X) = \mathbb{E}(X^2) - \{\mathbb{E}(X)\}^2 = mp(1 - p).
$$

Nous en déduisons que
$$
1 - p = \frac{\mu_2}{\mu_1} - \mu_1,
$$

puis
$$
p = 1 - \frac{\mu_2}{\mu_1} + \mu_1,
$$

et
$$
m = \frac{\mu_1}{1 - \frac{\mu_2}{\mu_1} + \mu_1}.
$$

---

4) Nous obtenons les estimateurs par la méthode des moments :

$$
\hat{p} = 1 - \frac{\hat{\mu}_2}{\hat{\mu}_1} + \hat{\mu}_1,
$$

$$
\hat{m} = 
\frac{\hat{\mu}_1}{1 - \frac{\hat{\mu}_2}{\hat{\mu}_1} + \hat{\mu}_1}.
$$

À noter que ces estimateurs n’ont pas de raison particulière d’être sans biais.

## Exercice 8

Soit $X \sim \text{Pois}(\lambda)$.

1) Donner la fonction génératrice des moments de $X$ et sa fonction caractéristique.  

2) Calculer $\mathbb{E}(X^3)$.  
3) Calculer la probabilité que $X$ soit impair.  
   Utiliser un développement en série entière de $e^\lambda$ et $e^{-\lambda}$.  

4) Calculer $\mathbb{E}(X!)$.

## Correction — Exercice 8

Avant de commencer, rappelons que :

{X impair} qui s'écrit en latex $X \text{ impair}$ est l'ensemble des issues où la variable aléatoire $X$ prend des valeurs impaires. Cet ensemble peut être représenté comme l'union des événements disjoints {X = 1}, {X = 3}, {X = 5}, etc.

On peut donc écrire :

L’événement $X$ est impair s’écrit :

$$
\{X \text{ impair}\}
= \{\omega \in \Omega : X(\omega) \text{ est impair}\}
$$

Par définition d’un entier impair :

$$  
\{X \text{ impair}\}
= \{\omega : X(\omega) = 1\}
  \cup \{\omega : X(\omega) = 3\}
  \cup \{\omega : X(\omega) = 5\}
  \cup \cdots
$$

C’est-à-dire :

$$
\{X \text{ impair}\}
= \{\omega \in \Omega : \exists\, k \in \mathbb{N}, \; X(\omega) = 2k + 1\}.
$$

En notation abrégée :

$$
\{X \text{ impair}\}
= \{X = 1\} \cup \{X = 3\} \cup \{X = 5\} \cup \cdots
$$

et de manière compacte :

$$
\boxed{
\{X \text{ impair}\}
= \bigcup_{k=0}^{\infty} \{X = 2k + 1\}.
}
$$


1) Fonction génératrice des moments et fonction caractéristique

$\textbf{1)}$ Soit $t > 0$. Nous avons

$$
M_X(t)
= \mathbb{E}\{\exp(tX)\}
= e^{-\lambda} \sum_{k=0}^{+\infty} \exp(tk)\, \frac{\lambda^k}{k!}
$$

$$ 
= e^{-\lambda} \sum_{k=0}^{+\infty} \frac{\{\lambda \exp(t)\}^k}{k!}
= e^{-\lambda} \times \exp\{\lambda \exp(t)\}
$$

$$
= \exp\{\lambda(\exp(t) - 1)\}.
$$

On vérifie au passage que la fonction génératrice des moments est ici bien
définie pour tout $t \in \mathbb{R}$.

La fonction caractéristique se calcule de façon analogue :

$$
\varphi_X(t)
= \mathbb{E}\{\exp(itX)\}
= e^{-\lambda} \sum_{k=0}^{+\infty} \exp(itk)\, \frac{\lambda^k}{k!}
$$

$$
= e^{-\lambda} \sum_{k=0}^{+\infty} \frac{\{\lambda \exp(it)\}^k}{k!}
= e^{-\lambda} \times \exp\{\lambda \exp(it)\}
$$

$$
= \exp\{\lambda(\exp(it) - 1)\}.
$$

2) Calcul de $\mathbb{E}(X^3)$

Nous utilisons la fonction génératrice des moments, qui est ici indéfiniment dérivable.

Rappel :
$$
M_X(t) = \exp\{\lambda(e^t - 1)\}.
$$

Nous calculons ses dérivées successives.

#### Première dérivée

$$
M_X'(t)
= \lambda e^t \exp\{\lambda(e^t - 1)\}.
$$
#### Deuxième dérivée

$$
M_X''(t)
= \left(\lambda e^t + \lambda^2 e^{2t}\right)
  \exp\{\lambda(e^t - 1)\}.
$$
#### Troisième dérivée

$$
M_X^{(3)}(t)
= \left(\lambda e^t + 3\lambda^2 e^{2t} + \lambda^3 e^{3t}\right)
  \exp\{\lambda(e^t - 1)\}.
$$
Nous pouvons maintenant obtenir le troisième moment.

---

### Calcul de  $\mathbb{E}(X^3)$

Par définition :
$$
\mathbb{E}(X^3) = M_X^{(3)}(0).
$$

Comme $e^{0}=1$, nous obtenons :
$$
M_X^{(3)}(0)
= \left(\lambda + 3\lambda^2 + \lambda^3\right)\exp\{0\}
= \lambda^3 + 3\lambda^2 + \lambda.
$$

$$
\boxed{\mathbb{E}(X^3)=\lambda^3 + 3\lambda^2 + \lambda.}
$$

3) Calcul de la probabilité que $X$ soit impair

Nous avons

$$
\mathbb{P}(X \text{ impair})
= \mathbb{P}\left(\bigcup_{k=0}^{\infty} \{X = 2k + 1\}\right)
= \sum_{k=0}^{\infty} \mathbb{P}(X = 2k + 1)
$$

$$
= \sum_{k=0}^{\infty} e^{-\lambda} \frac{\lambda^{2k + 1}}{(2k + 1)!}
= e^{-\lambda} \sum_{k=0}^{\infty} \frac{\lambda^{2k + 1}}{(2k + 1)!}.
$$ 


D’autre part,
$$
e^\lambda = \sum_{k=0}^{+\infty} \frac{\lambda^k}{k!}
= 1 + \lambda + \frac{\lambda^2}{2} + \frac{\lambda^3}{3!} + \frac{\lambda^4}{4!} + \cdots
$$  

et
$$
e^{-\lambda} = \sum_{k=0}^{+\infty} \frac{(-\lambda)^k}{k!}
= 1 - \lambda + \frac{\lambda^2}{2} - \frac{\lambda^3}{3!} + \frac{\lambda^4}{4!} + \cdots
$$

Donc,
$$
e^\lambda - e^{-\lambda}
= \sum_{k=0}^{+\infty} \frac{\lambda^k}{k!}
+ \sum_{k=0}^{+\infty} \frac{(-1)^{k+1}\lambda^k}{k!}
= 2 \sum_{k=0}^{+\infty} \frac{\lambda^{2k+1}}{(2k+1)!}.
$$

Et finalement,
$$
\mathbb{P}(X \text{ impair})
= \frac{e^{-\lambda}(e^\lambda - e^{-\lambda})}{2}
= \frac{1 - e^{-2\lambda}}{2}.
$$

4) Nous avons

$$
E(X!) = \sum_{k=0}^{+\infty} e^{-\lambda} \frac{\lambda^k}{k!} \times k!
$$

$$
= e^{-\lambda} \sum_{k=0}^{+\infty} \lambda^k
= 
\begin{cases}
\dfrac{e^{-\lambda}}{1 - \lambda}, & \text{si } |\lambda| < 1, \\[6pt]
+\infty, & \text{si } |\lambda| \ge 1.
\end{cases}
$$

## Exercice 9

Soient \(X_1, \ldots, X_n\) une suite de variables aléatoires i.i.d. suivant une loi de Bernoulli de paramètre \(p\).  
Trouver la loi de \(X_1\) sachant que \(X_1 + \cdots + X_n = k\).

---

## Correction Exercice 9

Soit \(x \in \{0,1\}\). Nous avons, en utilisant la formule de Bayes :

$$
\mathbb{P}(X_1 = x \mid X_1 + \cdots + X_n = k)
= \frac{\mathbb{P}(X_1 + \cdots + X_n = k \mid X_1 = x)\mathbb{P}(X_1 = x)}
{\mathbb{P}(X_1 + \cdots + X_n = k)}.
$$

Or :

$$
\mathbb{P}(X_1 + \cdots + X_n = k) = \binom{n}{k} p^k (1-p)^{\,n-k},
$$

puisque \(X_1 + \cdots + X_n \sim \mathcal{B}(n,p)\).

---

Ensuite :

$$
X_2 + \cdots + X_n \sim \mathcal{B}(n-1,p),
$$

donc

$$
\mathbb{P}(X_2 + \cdots + X_n = k - x)
= \binom{n-1}{k-x} p^{\,k-x} (1-p)^{\,n-1-k+x}.
$$

De plus :

$$
\mathbb{P}(X_1 = x) = p^x (1-p)^{\,1-x}.
$$

---

Nous obtenons finalement :

$$
\mathbb{P}(X_1 = x \mid X_1 + \cdots + X_n = k)
= \frac{\binom{n-1}{k-x} p^{\,k-x}(1-p)^{\,n-1-k+x} \, p^x (1-p)^{\,1-x}}
{\binom{n}{k} p^k (1-p)^{\,n-k}}.
$$

En simplifiant, on trouve :

$$
\mathbb{P}(X_1 = x \mid X_1 + \cdots + X_n = k)
= \frac{\binom{n-1}{k-x}}{\binom{n}{k}}.
$$

On calcule les deux cas :
- Si $x = 0$ :
  $$
  \mathbb{P}(X_1 = 0 \mid X_1 + \cdots + X_n = k)
  = \frac{\binom{n-1}{k}}{\binom{n}{k}}
  = \frac{(n-1)! \, k! \, (n-k)!}{(n-k)! \, (n)! \, k!}
  = \frac{n-k}{n}.
  $$
- Si $x = 1$ :
  $$
  \mathbb{P}(X_1 = 1 \mid X_1 + \cdots + X_n = k)
  = \frac{\binom{n-1}{k-1}}{\binom{n}{k}}
  = \frac{(n-1)! \, (k-1)! \, (n-k)!}{(n-k)! \, (n)! \, k!}
  = \frac{k}{n}.
  $$

Donc 

$$
\boxed{X_1 \mid (X_1 + \cdots + X_n = k) \sim \mathcal{B}\left(1, \frac{k}{n}\right)}.
$$

# 5 Variables aléatoires continues

## Exercice 1

Une variable aléatoire $X$ possède la fonction de densité

$$
f(x) = c x \quad \text{pour } 0 < x < 1.
$$

1. Trouver la valeur de $c$.  
2. Calculer $\mathbb{P}(X < 0.5)$.  
3. Calculer $\mathbb{E}(X)$.  
4. Calculer la fonction génératrice des moments de $X$.

## Correction — Exercice 1

On considère la densité

$$
f(x) = c x \quad \text{pour } 0 < x < 1.
$$

---

1. Détermination de $c$

$f$ est une densité si et seulement si elle vérifie les deux conditions suivantes :

1. $f(x) \ge 0$ pour tout $x \in \mathbb{R}$.  
   C’est le cas ici si $c \ge 0$.
2. L’intégrale de $f$ sur $\mathbb{R}$ vaut 1.  
   Ici, cela revient à :


$$
\int_{-\infty}^{+\infty} f(x) \, dx = 1.
$$

Calcul :

$$
\int_{-\infty}^{+\infty} f(x) \, dx
= \int_0^1 c x \, dx
= c \left[ \frac{x^2}{2} \right]_0^1
= \frac{c}{2}.
$$

Donc :

$$
\frac{c}{2} = 1 \quad \Longrightarrow \quad c = 2.
$$

---

2. Calcul de $\mathbb{P}(X < 0.5)$

$$
\mathbb{P}(X < 0.5) = \int_{-\infty}^{0.5} f(x) \, dx
= \int_0^{0.5} 2x \, dx.
$$

$$
= 2 \left[ \frac{x^2}{2} \right]_0^{0.5}
= (0.5)^2 = 0.25.
$$

---

3. Calcul de $\mathbb{E}(X)$

$$
\mathbb{E}(X) = 
= \int_{-\infty}^{+\infty} x \, f(x) \, dx
= \int_0^1 x \cdot 2x \, dx
= \int_0^1 2x^2 \, dx.
$$

$$
\int_0^1 2x^2 \, dx = 2 \left[ \frac{x^3}{3} \right]_0^1 = \frac{2}{3}.
$$

Donc :

$$
\mathbb{E}(X) = \frac{2}{3}.
$$

---

4. Fonction génératrice des moments (MGF)

La fonction génératrice des moments est :

soit $t \in \mathbb{R}$,
$$
M_X(t) = \mathbb{E}(e^{tX})
= \int_{-\infty}^{+\infty} e^{tx} \, f(x) \, dx
= \int_0^1 2x e^{tx} \, dx.
$$

On calcule l’intégrale par parties.

Posons :

- $u = x$ donc $du = dx$
- $dv = e^{tx} dx$ donc $v = \frac{e^{tx}}{t}$

Alors :

Si $t \neq 0$,
$$
\int_0^1 x e^{tx} dx
= \left[ \frac{x e^{tx}}{t} \right]_0^1
- \int_0^1 \frac{e^{tx}}{t} dx.
$$

On obtient :

$$
\int_0^1 x e^{tx} dx
= \frac{e^{t}}{t}
- \frac{1}{t} \int_0^1 e^{tx} dx.
$$

Or :

$$
\int_0^1 e^{tx} dx
= \left[ \frac{e^{tx}}{t} \right]_0^1
= \frac{e^{t} - 1}{t}.
$$

Donc :

$$
\int_0^1 x e^{tx} dx
= \frac{e^{t}}{t}
- \frac{e^{t} - 1}{t^2}.
$$

Donc si $t \neq 0$,

$$
M_X(t) = 2 \left( 
\frac{e^{t}}{t}
- \frac{e^{t} - 1}{t^2}
\right).
$$

Et si $t = 0$, on a :

$$
M_X(0) = \mathbb{E}(e^{0}) = \mathbb{E}(1) = 1.
$$

---

Donc la fonction génératrice des moments est donnée par :
$$
\boxed{
M_X(t) =
\begin{cases}
2 \left( 
\frac{e^{t}}{t}
- \frac{e^{t} - 1}{t^2}
\right), & \text{si } t \neq 0, \\[6pt]
1, & \text{si } t = 0.
\end{cases}
}
$$

## Exercice 2

Soit la fonction définie par  
$$f(x) = c\,x(1 - x)$$  
pour $x \in [0,1]$, et $0$ sinon.

1. Pour quelle valeur de $c$ est-ce une densité de probabilité ?

2. Déterminer la fonction de répartition de cette loi et sa médiane.

## Exercice 2 — Correction

On considère la fonction :

$$
f(x) = c\,x(1-x), \quad x \in [0,1],
$$

et $f(x)=0$ sinon.

---

1) Pour quelle valeur de $c$ est-ce une densité ?

Pour que $f$ soit une densité, il faut :

$$
\int_0^1 c\,x(1-x)\,dx = 1.
$$

Calculons l’intégrale :

$$
\int_0^1 x(1-x)\,dx
= \int_0^1 (x - x^2)\,dx
= \left[\frac{x^2}{2} - \frac{x^3}{3}\right]_0^1
= \frac12 - \frac13
= \frac16.
$$

Donc :

$$
c \cdot \frac{1}{6} = 1
\quad \Longrightarrow \quad c = 6.
$$

---

2) Fonction de répartition $F(x)$

Pour $x < 0$ :
$$
F(x) = 0.
$$

Pour $x \in [0,1]$ :
$$
F(x) = \int_0^x 6\,t(1-t)\,dt.
$$

Calculons :

$$
\int_0^x 6(t - t^2)\,dt
= 6\left[ \frac{t^2}{2} - \frac{t^3}{3} \right]_0^x
= 6\left( \frac{x^2}{2} - \frac{x^3}{3} \right).
$$

Donc :

$$
F(x) = 3x^2 - 2x^3.
$$

 Pour $x > 1$ :
$$
F(x) = 1.
$$

---

Médiane

La médiane $m$ vérifie :

$$
F(m) = 0.5.
$$

Donc :

$$
3m^2 - 2m^3 = \frac12.
$$

Soit :

$$
2m^3 - 3m^2 + \frac12 = 0.
$$

Ceci est équivalent à :

$$
4m^3 - 6m^2 + 1 = 0.
$$

C'est une équation polynomiale du troisième degré.
Une racine m de cette équation se trouve dans l'intervalle [0,1].

Etant donné une telle équation, les racines s'écrivent comme le rapport des diviseurs du terme constant sur les diviseurs du coefficient dominant, qui est 4 dans ce cas.
Les diviseurs de 1 sont $\pm 1$, et les diviseurs de 4 sont $\pm 1, \pm 2, \pm 4$.
Donc les racines rationnelles possibles sont $\pm 1, \pm \frac{1}{2}, \pm \frac{1}{4}$.

On doit choisir parmi ces valeurs celles qui sont dans l'intervalle [0,1], c'est-à-dire $1, \frac{1}{2}, \frac{1}{4}$.

En testant ces valeurs dans l'équation, on trouve que la racine dans l'intervalle [0,1] est :
$$
m = \frac{1}{2}
$$

Donc la médiane est :
$$
\boxed{m = \frac{1}{2}}.
$$

Une autre méthode pour déterminer la médiane consiste à remarquer que la fonction de densité admet la droite d'équation $x = \frac{1}{2}$ comme axe de symétrie.

Pour ce faire, on peut soit tracer la fonction de densité : 

```{python}

import numpy as np
import matplotlib.pyplot as plt

# Define density on R, zero outside [0,1]
def f(x):
    return np.where((x>=0)&(x<=1), 6*x*(1-x), 0)

# Real line range
x = np.linspace(-5, 5, 800)
y = f(x)

plt.figure(figsize=(7,4))
plt.plot(x, y, linewidth=2)
plt.axvline(0.5, linewidth=2, color='red', linestyle='--', label='x=0.5 (médiane)')
plt.legend()
plt.xlabel("x")
plt.ylabel("f(x)")
plt.title("Density f(x)=6x(1-x) on the whole real line (0 outside [0,1])")
plt.grid(True)
plt.show()

```

Lorsque que la droite d'équation $x = a$, est un axe de symétrie pour une fonction $f$, dérivable, alors $f'(a) = 0$.

Et dans notre cas, on trouve bien $f'(\frac{1}{2}) = 0$.
Et $a = \frac{1}{2}$ est la médiane recherchée.

## Exercice 3

Supposons que la fonction de répartition d’une variable aléatoire $X$, correspondant au temps en mois avant décès pour une personne atteinte de cancer, est donnée par

$$
F(x) =
\begin{cases}
0, & x \le 0, \\
1 - e^{-0.03 x^{1.2}}, & x > 0.
\end{cases}
$$

1) Vérifier que $F$ est bien une fonction de répartition.  

2) Calculer la probabilité de survivre au moins 12 mois.  

3) Donner une densité de $X$.

## Correction Exercice 3

1) La fonction $F$ est croissante, continue sur $\mathbb{R}$, avec  
$\displaystyle \lim_{x \to -\infty} F(x) = 0$  
et  
$\displaystyle \lim_{x \to +\infty} F(x) = 1$.  

C’est donc bien une fonction de répartition.

---

2) Nous avons

$$
\mathbb{P}(X \ge 12)
= 1 - F(12)
= 55.3\%.
$$

---

3) Une densité, obtenue par dérivation, vaut

$$
f(x) =
\begin{cases}
0, & x \le 0, \\[6pt]
0.03\, x^{0.2} e^{-0.03 x^{1.2}}, & x > 0.
\end{cases}
$$

## Exercice 4

Soit la fonction $F$ définie par $F(x) = 1 - \exp(-x/2)$ pour $x > 0$, et $0$ sinon.

1) Justifier que $F$ est une fonction de répartition.  
2) Déterminer les quantiles d’ordres 0.25 et 0.75.  
3) Soit $X$ une variable aléatoire suivant cette loi, calculer $\mathbb{P}(1 < X \le 2)$.

---

## Correction Exercice 4

1)  
Une fonction de répartition doit être croissante, cadlag (continue à droite, limites à gauche), avec

$$
\lim_{x \to -\infty} F(x) = 0
\quad\text{et}\quad
\lim_{x \to +\infty} F(x) = 1.
$$

La fonction $F(\cdot)$ est nulle sur $]-\infty, 0]$.  
Pour tout $x \in ]0, +\infty[$, $F(\cdot)$ est dérivable en $x$ avec

$$
F'(x) = \frac{1}{2} \exp\left(-\frac{x}{2}\right) > 0,
$$

donc la fonction $F(\cdot)$ est bien croissante.  
Elle est continue sur $\mathbb{R}$, donc en particulier continue à droite et avec des limites à gauche.  

Nous obtenons également facilement que

$$
\lim_{x \to -\infty} F(x) = 0
\quad\text{et}\quad
\lim_{x \to +\infty} F(x) = 1.
$$

---

2) Quantiles

On cherche $x = F^{-1}(0.25)$ :

$$
F(x) = 1 - \exp\left(-\frac{x}{2}\right) = \frac14
\;\Longleftrightarrow\;
\exp\left(-\frac{x}{2}\right) = \frac34
\;\Longleftrightarrow\;
x = -2 \ln\left(\frac34\right).
$$

Ensuite, pour $x = F^{-1}(0.75)$ :

$$
F(x) = 1 - \exp\left(-\frac{x}{2}\right) = \frac34
\;\Longleftrightarrow\;
\exp\left(-\frac{x}{2}\right) = \frac14
\;\Longleftrightarrow\;
x = -2 \ln\left(\frac14\right).
$$

---

3) Probabilité

Par définition de la fonction de répartition,

$$
\mathbb{P}(1 < X \le 2)
= F(2) - F(1)
= \exp\left(-\frac{1}{2}\right) - \exp(-1).
= 0.238.
$$














