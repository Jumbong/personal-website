[
  {
    "objectID": "Meduim/RegressionLogistique/bias_sample.html",
    "href": "Meduim/RegressionLogistique/bias_sample.html",
    "title": "Sampling Bias and Class Imbalance in Maximum-likelihood Logistic Regression",
    "section": "",
    "text": "In this article, the impact of sampling bias (sample dataset distribution different from the population distribution) and class imbalance on logistic regression models is explored. We hypothesize that the predictive performance of a logistic regression model is related to the sampling bias associated with the data and it has a performance advantage when the data is balanced. The hypothesis is testing with two simulated datasets : a balanced dataset (50:50) and an imbalanced dataset (80:20). Each dataset will be sampled to produce samples with the following distribution : 50:50, 60:40, 70:30, 80:20, 90:10, 95:5, 99:1.\nThe performance of the logistic regression model will be evaluated using the Area Under the Curve (AUC), Area Under the Precision-Recall Curve (AU-PCR), Precision, Recall, and F1-score.\nMonte-Carlo simulations will be carried out to evaluate the distribution of the performance metrics for each of the samples and insure the robustness of the results.\nThis study gives three main results :\nA. The predicted probability using a maximum logistic regression (MLLR) model is closest to the true probability when the sample has the same class distribution as the original population. Therefore, in probabilistic modeling with MLLR, it is important to create a sample that matches the class distribution of the original population rather than ensuring equal class sampling, see Figure 3 and Figure 6.\nB. AUC measures how well probabilistic classifiers predict. It ranges from 0.5 (random) to 1 (perfect). AUC shows class separability regardless of class imbalance or sampling bias, see Figure 5 and Figure 8.\nC. We recommend AUC to evaluate class separability in probabilistic models. To analyse sampling biais as well as the difference in the true and predicted probabilities, AUC-PR, Recall, precision and f1-score can be used as indicator.\nThe protocol of this paper is as follows. First, we describe how to simulate data. Next, we present the methodology. Finally, we present the results."
  },
  {
    "objectID": "Meduim/RegressionLogistique/bias_sample.html#numerical-approach-to-determine-parameters-alpha-and-beta-knowing-the-proportion-of-y1.",
    "href": "Meduim/RegressionLogistique/bias_sample.html#numerical-approach-to-determine-parameters-alpha-and-beta-knowing-the-proportion-of-y1.",
    "title": "Sampling Bias and Class Imbalance in Maximum-likelihood Logistic Regression",
    "section": "1.1 Numerical Approach to determine parameters \\(\\alpha\\) and \\(\\beta\\) knowing the proportion of y=1.",
    "text": "1.1 Numerical Approach to determine parameters \\(\\alpha\\) and \\(\\beta\\) knowing the proportion of y=1.\nThe numerical approach consists to determine, for a given value of \\(\\alpha =-10\\), the value of \\(\\beta\\) that will allow to have a proportion of y=1 equal to 0.5 in the case of a balanced dataset and 0.2 in the case of an imbalanced dataset.\nThe optimization problem can be formulated as follows :\n\\[\n\\min_{\\beta} \\left( \\text{prop} - \\frac{1}{n}\\sum_{i=1}^{n} \\frac{\\exp(\\alpha + \\beta x_i)}{1 + \\exp(\\alpha + \\beta x_i)} \\right)^2\n\\]\nwhere \\(\\text{prop}\\) is the proportion of y=1 in the dataset, \\(x_i\\) is the predictor variable, and \\(n\\) is the number of observations.\nThe optimization problem can be solved using the scipy.optimize.minimize function with the Nelder-Mead method.\n\nimport numpy as np\nfrom scipy.optimize import minimize\n\n\n# Define the logistic function\ndef logistic_function(x, alpha, beta):\n    return 1 / (1 + np.exp(-(alpha + beta * x)))\n\n# Objective function: minimize the squared difference between mean(pi) and 0.2\n\ndef objective(alpha, prop, beta, n):\n    x = np.random.uniform(0, 10, n)  # Simulate x values\n    pi = logistic_function(x, alpha, beta)\n    return (np.mean(pi) - 0.2)**2  # Target mean(pi) = 0.2\n\n# Initial guesses for alpha and beta\n\ninitial_params = [0]\n\n# Optimize alpha and beta\nresult = minimize(lambda params: objective(-10, 0.2, params, 50000), initial_params, method='Nelder-Mead')\n\n# Get optimized alpha and beta\n\nbeta_opt = result.x\nprint(f\"Optimized alpha: {-10}, beta: {beta_opt}\")\n\n# Generate x and simulate y\nx = np.random.uniform(0, 10, 1000)\npi = logistic_function(x, -10, beta_opt)\ny = (np.random.uniform(0, 1, 1000) &lt; pi).astype(int)\n\n# Verify proportions\ny_mean = np.mean(y)\nprint(f\"Proportion of y=1: {y_mean:.2f}, y=0: {1-y_mean:.2f}\")\n\nOptimized alpha: -10, beta: [1.23775]\nProportion of y=1: 0.20, y=0: 0.80"
  },
  {
    "objectID": "Meduim/RegressionLogistique/bias_sample.html#simulated-data-generation-with-alpha--10",
    "href": "Meduim/RegressionLogistique/bias_sample.html#simulated-data-generation-with-alpha--10",
    "title": "Sampling Bias and Class Imbalance in Maximum-likelihood Logistic Regression",
    "section": "Simulated Data Generation with \\(\\alpha = -10\\)",
    "text": "Simulated Data Generation with \\(\\alpha = -10\\)\nLet’s consider two cases :\n\nCase A : A balanced dataset with 50:50 distribution of y=0 and y=1.\nCase B : An imbalanced dataset with 80:20 distribution of y=0 and y=1.\n\nThe code below generates the data for the two cases and plots the proportion of y=1 as a function of beta.\nThe graph Figure 1, both the datasets have a total of 50,000 events, with the Case A dataset having a class distribution of about 50:50 and Case B dataset having a class distribution of about 80:20.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Inputs : n_events, alpha, beta, random_state\n# Outputs : x, y, prop_y0, prop_y1\n# Objective : Simulate data from a logistic model with given alpha, beta.\n\ndef simulate_case_data(n_events, alpha, beta, random_state=42):\n    \"\"\"\n    Simulate data from a logistic model with given alpha, beta.\n    \n    x ~ Uniform(0, 10), y ~ Bernoulli(pi(x)), \n    where pi(x) = exp(alpha + beta*x) / (1 + exp(alpha + beta*x)).\n\n    Parameters\n    ----------\n    n_events : int\n        Number of observations (events) to generate.\n    alpha : float\n        Intercept (alpha) for the logistic function.\n    beta : float\n        Slope (beta) for the logistic function.\n    random_state : int\n        Seed for reproducibility.\n\n    Returns\n    -------\n    x : np.ndarray of shape (n_events,)\n        Predictor values sampled from Uniform(0,10).\n    y : np.ndarray of shape (n_events,)\n        Binary outcomes (0 or 1) from Bernoulli trials.\n    prop_y0 : float\n        Proportion of y==0 in the dataset.\n    prop_y1 : float\n        Proportion of y==1 in the dataset.\n    \"\"\"\n    np.random.seed(random_state)\n    \n    # 1) Draw x from Uniform(0,10)\n    x = np.random.uniform(0, 10, size=n_events)\n    \n    # 2) Compute pi(x, alpha, beta)\n    logit = alpha + beta*x\n    pi = np.exp(logit) / (1.0 + np.exp(logit))\n    \n    # 3) Generate y via Bernoulli(pi)\n    dummy = np.random.rand(n_events)\n    y = (dummy &lt; pi).astype(int)\n    \n    # 4) Calculate proportions of 0 and 1\n    prop_y0 = np.mean(y == 0)\n    prop_y1 = np.mean(y == 1)\n    \n    return x, y, prop_y0, prop_y1\n\n# ---------------- Example usage ----------------\n\n\n# Case A: alpha=-10, beta=2 --&gt; expected ~50:50 distribution\nxA, yA, p0_A, p1_A = simulate_case_data(\n    n_events=50000,\n    alpha=-10,\n    beta=2,\n    random_state=42\n)\n\n# Case B: alpha=-10, beta=3.85 --&gt; expected ~80:20 distribution\nxB, yB, p0_B, p1_B = simulate_case_data(\n    n_events=50000,\n    alpha=-10,\n    beta=beta_opt,\n    random_state=42\n)\n\n# Verify proportions\n\n# Suppose p0_A, p1_A, p0_B, p1_B are already defined\n# e.g., p0_A = 0.50; p1_A = 0.50; p0_B = 0.80; p1_B = 0.20\n\nfig, axes = plt.subplots(1, 2, figsize=(5, 3))  # 1 ligne, 2 colonnes\n\ncolors = ['royalblue', 'darkorange']  # Couleurs distinctes pour y=0 et y=1\n\n# -------- LEFT SUBPLOT: Case A -----------\nax1 = axes[0]\nbar_container_A = ax1.bar(['y=0', 'y=1'], [p0_A, p1_A], color=colors)\nax1.set_title('Case A')\nax1.set_xlabel('Classe')\nax1.set_ylabel('Proportion')\nax1.set_ylim([0, 1])  # Echelle de 0 à 1\nax1.bar_label(bar_container_A, fmt='%.2f')\n\n# -------- RIGHT SUBPLOT: Case B -----------\nax2 = axes[1]\nbar_container_B = ax2.bar(['y=0', 'y=1'], [p0_B, p1_B], color=colors)\nax2.set_title('Case B')\nax2.set_xlabel('Classe')\nax2.set_ylabel('Proportion')\nax2.set_ylim([0, 1])\nax2.bar_label(bar_container_B, fmt='%.2f')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\nFigure 1: Simulated data from logistic model with alpha=-10, beta=2 and alpha=-10, beta=beta_opt\n\n\n\n\n\nThe theoretical properties of the simulated datasets are presented in Figure 2. The left subplot shows the probability of y=1 as a function of x for Case A and Case B. The right subplot shows the logit function as a function of x for Case A and Case B. The logit function is given by \\(\\alpha + \\beta x\\).\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef logistic(x, alpha, beta):\n    return np.exp(alpha + beta*x) / (1.0 + np.exp(alpha + beta*x))\n\nx_values = np.linspace(0, 10, 50000)\n\n# Case A\nalpha_A, beta_A = -10, 2\npi_A = logistic(x_values, alpha_A, beta_A)\nlogit_A = alpha_A + beta_A * x_values\n\n# Case B\nalpha_B, beta_B = -10, beta_opt\npi_B = logistic(x_values, alpha_B, beta_B)\nlogit_B = alpha_B + beta_B * x_values\n\nfig, axes = plt.subplots(2, 2, figsize=(5, 5))  # Taille un peu plus grande\n\n# (a) Probability vs. x for Case A\naxes[0, 0].plot(x_values, pi_A, color='b', label='Probability')\naxes[0, 0].set_title('Case A: Probability vs. x')\naxes[0, 0].set_xlabel('x')\naxes[0, 0].set_ylabel(r'$\\pi(x,\\alpha,\\beta)$')\naxes[0, 0].axhline(y=0.5, color='k', linestyle='--', label='y=0.5')\naxes[0, 0].axvline(x=5, color='gray', linestyle='--', label='x=5')\naxes[0, 0].set_ylim([0,1])  # Probabilité entre 0 et 1\naxes[0, 0].legend(loc='best')\n\n# (b) Logit vs. x for Case A\naxes[0, 1].plot(x_values, logit_A, color='b', label='Logit')\naxes[0, 1].set_title('Case A: Logit vs. x')\naxes[0, 1].set_xlabel('x')\naxes[0, 1].set_ylabel(r'$\\alpha + \\beta x$')\naxes[0, 1].legend(loc='best')\n\n# (c) Probability vs. x for Case B\naxes[1, 0].plot(x_values, pi_B, color='r', label='Probability')\naxes[1, 0].set_title('Case B: Probability vs. x')\naxes[1, 0].set_xlabel('x')\naxes[1, 0].set_ylabel(r'$\\pi(x,\\alpha,\\beta)$')\naxes[1, 0].axhline(y=0.2, color='k', linestyle='--', label='y=0.2')\naxes[1, 0].axvline(x=5, color='gray', linestyle='--', label='x=5')\naxes[1, 0].set_ylim([0,1])\naxes[1, 0].legend(loc='best')\n\n# (d) Logit vs. x for Case B\naxes[1, 1].plot(x_values, logit_B, color='r', label='Logit')\naxes[1, 1].set_title('Case B: Logit vs. x')\naxes[1, 1].set_xlabel('x')\naxes[1, 1].set_ylabel(r'$\\alpha + \\beta x$')\naxes[1, 1].legend(loc='best')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\nFigure 2: Logit vs. x for Case A and Case B"
  },
  {
    "objectID": "Meduim/RegressionLogistique/bias_sample.html#case-a-balanced-dataset",
    "href": "Meduim/RegressionLogistique/bias_sample.html#case-a-balanced-dataset",
    "title": "Sampling Bias and Class Imbalance in Maximum-likelihood Logistic Regression",
    "section": "Case A : Balanced Dataset",
    "text": "Case A : Balanced Dataset\nThe Case A dataset has a balanced class distribution of 50:50. This will be the true distribution of the dataset. Eight random samples will be extracted from the Case A population with varying class distributions of 50:50, 60:40, 70:30, 80:20, 90:10, 95:5, and 99:1. A sample with a class distributiion of 60:40 from the Case A is referred as \\(A_{60:40}\\). The sample size for each of the eight samples is determined by fixing the lenght of the majority class (class 0) at 5000. In order word, for the \\(A_{60:40}\\) sample, the number of observations in class 0 is 5000 and the number of observations in class 1 is :\n\\[\n\\text{Number of observations in class 1} = \\frac{40}{60} \\times 5000 = 3333\n\\]\n\nEight sub-samples generated from Case A.\nThe code below generates the eight sub-samples from the Case A dataset with varying class distributions.\n\ndef create_subsample_fixed_majority(\n    X, y, \n    fraction_class0=0.6,  # e.g., 0.6 =&gt; 60:40\n    majority_class0_size=5000,\n    random_state=42\n):\n    \"\"\"\n    Extract a subsample where the number of class-0 = majority_class0_size,\n    and overall fraction of class-0 is fraction_class0.\n    \n    Returns X_sub, y_sub.\n    \"\"\"\n    np.random.seed(random_state)\n    \n    # Indices of class 0 and 1 in the population\n    idx_0 = np.where(y == 0)[0]\n    idx_1 = np.where(y == 1)[0]\n    \n    # We fix #class0 = 5000\n    n0 = majority_class0_size\n    \n    # fraction_class0 = n0 / (n0 + n1) =&gt; n1 = n0 * (1 - p)/p\n    p = fraction_class0\n    n1 = int(round(n0 * (1 - p) / p))\n    \n    chosen_0 = np.random.choice(idx_0, size=n0, replace=False)\n    chosen_1 = np.random.choice(idx_1, size=n1, replace=False)\n    \n    chosen_indices = np.concatenate([chosen_0, chosen_1])\n    np.random.shuffle(chosen_indices)\n    \n    return X[chosen_indices], y[chosen_indices]\n\nThe code below gives examples of generating the eight sub-samples from the Case A dataset with varying class distributions. Figure 3 shows the distribution of the dependent variable y for each of the eight sub-samples.\n\n# Module : Generation\n# Inputs : fraction_class0 = [0.5, 0.6, 0.7, 0.8, 0.9, 0.95, 0.99], create_subsample_fixed_majority.\n# Ourputs : A dictionnarie with keys in fraction_class0 and values a tuple (X_sub, y_sub).\n\nfractions_class0 = [0.50, 0.60, 0.70, 0.80, 0.90, 0.95, 0.98, 0.99]\n\nsamples_A = {}\nfor frac0 in fractions_class0:\n    X_sub, y_sub = create_subsample_fixed_majority(\n        xA, yA, \n        fraction_class0=frac0, \n        majority_class0_size=5000,\n        random_state=42\n    )\n    # Store or process each sample\n    samples_A[frac0] = (X_sub, y_sub)\n\n\nffig, axes = plt.subplots(4, 2, figsize=(6, 8))\naxes = axes.ravel()  # on a maintenant 8 sous-graphiques\n\n# 1) Déterminer la fréquence max pour fixer une échelle cohérente\nall_counts = [np.bincount(y_sub) for _, (_, y_sub) in samples_A.items()]\nglobal_max_count = max(counts.max() for counts in all_counts)\n\nfor i, (label, (X_sub, y_sub)) in enumerate(samples_A.items()):\n    ax = axes[i]\n    \n    # 2) Histogramme sur 2 bins =&gt; classes 0 et 1\n    ax.hist(y_sub, bins=[-0.5, 0.5, 1.5],  # histogramme \"catégoriel\"\n            color='steelblue', edgecolor='black', alpha=0.7)\n    \n    # 3) Titre + proportion de y=1\n    mean_y = np.mean(y_sub)\n    ax.set_title(f\"{label}:{mean_y:.2f}\", fontsize=9)\n    \n    # 4) Ajuster l’axe X pour forcer l’affichage (0,1)\n    ax.set_xticks([0, 1])\n    ax.set_xticklabels(['0', '1'], fontsize=8)\n    \n    # 5) Limiter l’axe Y pour comparer visuellement entre les sous-figures\n    ax.set_ylim(0, global_max_count)\n    \n    # 6) N’afficher “Frequency” que sur la première colonne\n    #    pour éviter la répétition\n    if i % 2 == 0:\n        ax.set_ylabel('Frequency', fontsize=9)\n    else:\n        ax.set_ylabel('')\n    \n    # 7) Ajouter un label X plus discret\n    ax.set_xlabel('y', fontsize=9)\n    \n    # 8) Afficher le count exact sur chacune des barres\n    counts = np.bincount(y_sub)\n    for j, c in enumerate(counts):\n        ax.text(j, c + 0.5, str(c), ha='center', va='bottom', fontsize=8)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\nFigure 3: Distribution of y for Case A sub-samples\n\n\n\n\n\nA maximum-likelihood logistic regression model will be fitted to each of the eight sub-samples. A plot of the true \\(p(x)\\) versus the estimated \\(p(x)\\) is presented in Figure 4.\n\nimport numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm\n\n# Inputs : samples_A, fractions_class0\n# Outputs : fig\n\n\n\n\n# For illustration, let's create a 2 x 4 grid to show all eight ratio-samples\nfig, axes = plt.subplots(nrows=4, ncols=2, figsize=(6,8))\naxes = axes.ravel()  # flatten into 1D array [ax0, ax1, ..., ax7]\n\n# Hardcode alpha=-10, beta=2 for \"true\" logistic in Case A\nALPHA_TRUE = -10\nBETA_TRUE  = 2\n\nfor i, (label, (X_sub, y_sub)) in enumerate(samples_A.items()):\n    ax = axes[i]\n    \n    # 1) Put the data into a DataFrame for convenience\n    df_sub = pd.DataFrame({\n        'X': X_sub,       # predictor\n        'y': y_sub        # binary outcome\n    })\n    \n    # 2) Add a constant column for the intercept in statsmodels\n    df_sub = sm.add_constant(df_sub, has_constant='add')  \n    # Now df_sub has columns ['const', 'X', 'y']\n    \n    # 3) Fit the logistic model\n    model = sm.Logit(df_sub['y'], df_sub[['const', 'X']])\n    results = model.fit(disp=False)  # disp=False to suppress output\n    \n    # 4) Predict the fitted probability\n    df_sub['pi_pred'] = results.predict(df_sub[['const', 'X']])\n    \n    # 5) Compute the \"true\" pi for comparison\n    df_sub['pi_true'] = logistic(df_sub['X'].values, alpha=ALPHA_TRUE, beta=BETA_TRUE)\n    \n    # 6) Plot pi_true vs. pi_pred\n    ax.scatter(df_sub['pi_true'], df_sub['pi_pred'], \n               alpha=0.3, s=10, color='blue', edgecolors='none')\n    # Add a diagonal line for reference\n    ax.plot([0, 1], [0, 1], color='black', linestyle='--')\n    # 7) Decorate the subplot\n    ax.set_xlabel(\"True pi(x)\")\n    ax.set_ylabel(\"Predicted pi_hat(x)\")\n    ax.set_title(f\"{label}:{1-label:.2f}\")  # e.g., \"Case A60:40\"\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\nFigure 4: True vs. estimated p(x) for Case A sub-samples\n\n\n\n\n\nFigure 4 shows that the sample (Case \\(A_{50:50}\\)) with no class imbalance and sampling bias has the best fit between the true and estimated probabilities. As the class imbalance increases from 60:40 to 99:1, the fit between the true and estimated probabilities deteriorates. To have more confidence in the results, the distribution of the performance metrics for each of the eight sub-samples using monte-carlo simulations will be carried out.\n\n\nDistribution of the performance metrics for the eight sub-samples from Case A with monte-carlo simulations.\nWe perform 1000 monte-carlo simulations for each of the eight sub-samples from Case A. For each simulation, we fit a maximum-likelihood logistic regression model and compute the performance metrics : AUC, AU-PCR, Precision, Recall, F1-score. In order words, in the end of this exercise, we will have 1000 values for each of the performance metrics for each of the eight sub-samples.\nThe code below gives the distribution of the performance metrics for each of the eight sub-samples from Case A with monte-carlo simulations.\n\nfrom sklearn.metrics import (\n    roc_auc_score,\n    precision_score,\n    recall_score,\n    f1_score,\n    average_precision_score\n)\n\ndef evaluate_model_performance(y_true, y_proba, threshold=0.5):\n    \"\"\"\n    Given true labels and predicted probabilities, compute AUC, AU-PRC,\n    Precision, Recall, and F1 at a chosen threshold.\n    \"\"\"\n    # 1) AUC (ROC)\n    auc = roc_auc_score(y_true, y_proba)\n    \n    # 2) AU-PRC (average precision)\n    auprc = average_precision_score(y_true, y_proba)\n    \n    # 3) Convert probas -&gt; hard predictions\n    y_pred = (y_proba &gt;= threshold).astype(int)\n    \n    # 4) Precision, Recall, F1\n    prec = precision_score(y_true, y_pred, zero_division=0)\n    rec  = recall_score(y_true, y_pred, zero_division=0)\n    f1   = f1_score(y_true, y_pred, zero_division=0)\n    \n    return auc, auprc, prec, rec, f1\n\nfrom sklearn.model_selection import train_test_split\n\nMC_RUNS = 1000\nSAMPLE_SIZE = 5000  # e.g., majority class size if using a fixed majority approach\nratios = [0.50, 0.60, 0.70, 0.80, 0.90, 0.95, 0.98, 0.99]\nresults_list = []\n\n#for r, (X_sub, y_sub) in samples_A.items():\nfor r in ratios:\n    for mc_i in range(MC_RUNS):\n        \n        \n        # 2) Split the subsample into train/test\n        #    stratify ensures class distribution is preserved\n        X_sub, y_sub = create_subsample_fixed_majority(\n          xA, yA,\n          fraction_class0=r,\n          majority_class0_size=SAMPLE_SIZE,\n          random_state=None\n      )\n\n        X_train, X_test, y_train, y_test = train_test_split(\n            X_sub,\n            y_sub,\n            test_size=0.3,\n            random_state=42,\n            stratify=y_sub\n        )\n        \n        # Prepare DataFrame for the training set\n        df_train = pd.DataFrame({'X': X_train, 'y': y_train})\n        df_train = sm.add_constant(df_train, prepend=True, has_constant='add')  \n        # =&gt; columns: ['const', 'X', 'y']\n        \n        # 3) Fit logistic regression on the TRAIN portion\n        logit_model = sm.Logit(df_train['y'], df_train[['const', 'X']])\n        result = logit_model.fit(disp=False)\n        \n        # 4) Predict probabilities on the TEST portion\n        df_test = pd.DataFrame({'X': X_test})\n        df_test = sm.add_constant(df_test, prepend=True, has_constant='add')\n        \n        y_proba_test = result.predict(df_test[['const', 'X']])\n        \n        # 5) Evaluate performance metrics on the TEST set\n        auc, auprc, prec, rec, f1 = evaluate_model_performance(y_test, y_proba_test, threshold=0.5)\n        \n        # 6) Store results\n        results_list.append({\n            'ratio_0': r,\n            'auc': auc,\n            'auprc': auprc,\n            'precision': prec,\n            'recall': rec,\n            'f1': f1\n        })\n\n# Convert collected results to a DataFrame\ndf_results_A = pd.DataFrame(results_list)\n# Compute the mean performance metrics for each ratio\ndf_results_A.groupby('ratio_0').mean()\n\n\n\n\n\n\n\n\nauc\nauprc\nprecision\nrecall\nf1\n\n\nratio_0\n\n\n\n\n\n\n\n\n\n0.50\n0.984082\n0.984669\n0.930641\n0.930979\n0.930783\n\n\n0.60\n0.984122\n0.978174\n0.924246\n0.908146\n0.916079\n\n\n0.70\n0.983972\n0.968164\n0.914901\n0.882000\n0.898063\n\n\n0.80\n0.984141\n0.952157\n0.903439\n0.842323\n0.871622\n\n\n0.90\n0.984098\n0.916591\n0.897012\n0.777216\n0.832299\n\n\n0.95\n0.984102\n0.871136\n0.885726\n0.710380\n0.787117\n\n\n0.98\n0.984290\n0.799221\n0.866131\n0.618839\n0.717647\n\n\n0.99\n0.983755\n0.737946\n0.863657\n0.552467\n0.663418\n\n\n\n\n\n\n\nThe mean of the performance metrics for each of the eight sub-samples from Case A is presented in Figure 5.\n\n# Module : Plotting\n# Inputs : df_results_A\n# Outputs : fig showing the performance metrics vs the ratios\n# Objective : Group data by ratio_0 and plot the performance metrics.\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\ndf_grouped_A = df_results_A.groupby('ratio_0').mean().reset_index()\nmetrics = ['auc', 'auprc', 'precision', 'recall', 'f1']\ncolours = ['blue', 'orange', 'green', 'red', 'purple']\n\nplt.figure(figsize=(6, 5))\n\nfor metric, colour in zip(metrics, colours):\n    plt.plot(df_grouped_A['ratio_0'], df_grouped_A[metric], label=metric, color=colour, marker='o')\n\n# Improve readability with grid and styling\nplt.xlabel(\"Rate of Y = 0\", fontsize=12, weight='bold')\nplt.ylabel(\"Mean of Performance Metrics\", fontsize=12, weight='bold')\nplt.title(\"Mean of Performance Metrics vs Ratios\", fontsize=14, weight='bold')\nplt.xticks(fontsize=10)\nplt.yticks(fontsize=10)\nplt.grid(False)\nplt.legend(title=\"Metrics\", fontsize=10, title_fontsize=12, loc=\"best\")\nplt.tight_layout()\n\nplt.show()\n\n\n\n\n\n\n\nFigure 5: Performance metrics Vs. Ratios for Case A\n\n\n\n\n\nAs the class imbalance and sampling bias increase, the performance metrics deteriorate except for the AUC metric. It seems that the AUC metric is not sensitive to class imbalance and sampling bias."
  },
  {
    "objectID": "Meduim/RegressionLogistique/bias_sample.html#case-b-imbalanced-dataset",
    "href": "Meduim/RegressionLogistique/bias_sample.html#case-b-imbalanced-dataset",
    "title": "Sampling Bias and Class Imbalance in Maximum-likelihood Logistic Regression",
    "section": "Case B : Imbalanced Dataset",
    "text": "Case B : Imbalanced Dataset\nThe Case B dataset has an imbalanced class distribution of 80:20. This will be the true distribution of the dataset. Eight random samples will be extracted from the Case B population with varying class distributions of 50:50, 60:40, 70:30, 80:20, 90:10, 95:5, and 99:1.\n\nEight sub-samples generated from Case B.\nThe code below generates the eight sub-samples from the Case B dataset with varying class distributions. Figure 6 shows the distribution of the dependent variable y for each of the eight sub-samples.\n\n# Module : Generation\n# Inputs : fraction_class0 = [0.5, 0.6, 0.7, 0.8, 0.9, 0.95, 0.99], create_subsample_fixed_majority.\n# Ourputs : A dictionnarie with keys in fraction_class0 and values a tuple (X_sub, y_sub).\n\nfractions_class0 = [0.50, 0.60, 0.70, 0.80, 0.90, 0.95, 0.98, 0.99]\n\nsamples_B = {}\nfor frac0 in fractions_class0:\n    X_sub, y_sub = create_subsample_fixed_majority(\n        xB, yB, \n        fraction_class0=frac0, \n        majority_class0_size=5000,\n        random_state=42\n    )\n    # Store or process each sample\n    samples_B[frac0] = (X_sub, y_sub)\n\n\nfig, axes = plt.subplots(4, 2, figsize=(6, 8))\naxes = axes.ravel()  # on a maintenant 8 sous-graphiques\n\n# 1) Déterminer la fréquence max pour fixer une échelle cohérente\nall_counts = [np.bincount(y_sub) for _, (_, y_sub) in samples_B.items()]\nglobal_max_count = max(counts.max() for counts in all_counts)\n\nfor i, (label, (X_sub, y_sub)) in enumerate(samples_B.items()):\n    ax = axes[i]\n    \n    # 2) Histogramme sur 2 bins =&gt; classes 0 et 1\n    ax.hist(y_sub, bins=[-0.5, 0.5, 1.5],  # histogramme \"catégoriel\"\n            color='steelblue', edgecolor='black', alpha=0.7)\n    \n    # 3) Titre + proportion de y=1\n    mean_y = np.mean(y_sub)\n    ax.set_title(f\"{label}:{mean_y:.2f}\", fontsize=9)\n    \n    # 4) Ajuster l’axe X pour forcer l’affichage (0,1)\n    ax.set_xticks([0, 1])\n    ax.set_xticklabels(['0', '1'], fontsize=8)\n    \n    # 5) Limiter l’axe Y pour comparer visuellement entre les sous-figures\n    ax.set_ylim(0, global_max_count)\n    \n    # 6) N’afficher “Frequency” que sur la première colonne\n    #    pour éviter la répétition\n    if i % 2 == 0:\n        ax.set_ylabel('Frequency', fontsize=9)\n    else:\n        ax.set_ylabel('')\n    \n    # 7) Ajouter un label X plus discret\n    ax.set_xlabel('y', fontsize=9)\n    \n    # 8) Afficher le count exact sur chacune des barres\n    counts = np.bincount(y_sub)\n    for j, c in enumerate(counts):\n        ax.text(j, c + 0.5, str(c), ha='center', va='bottom', fontsize=8)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\nFigure 6: Distribution of y for Case B sub-samples\n\n\n\n\n\nA maximum-likelihood logistic regression model will be fitted to each of the eight sub-samples. A plot of the true \\(p(x)\\) versus the estimated \\(p(x)\\) is presented in Figure 7.\n\nimport numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm\n\n# Inputs : samples_B, fractions_class0\n# Outputs : fig\n\n\n\n\n# For illustration, let's create a 2 x 4 grid to show all eight ratio-samples\nfig, axes = plt.subplots(nrows=4, ncols=2, figsize=(6,8))\naxes = axes.ravel()  # flatten into 1D array [ax0, ax1, ..., ax7]\n\n# Hardcode alpha=-10, beta=2 for \"true\" logistic in Case A\nALPHA_TRUE = -10\nBETA_TRUE  = beta_opt\nprint(BETA_TRUE)\n\nfor i, (label, (X_sub, y_sub)) in enumerate(samples_B.items()):\n    ax = axes[i]\n    \n    # 1) Put the data into a DataFrame for convenience\n    df_sub = pd.DataFrame({\n        'X': X_sub,       # predictor\n        'y': y_sub        # binary outcome\n    })\n    \n    # 2) Add a constant column for the intercept in statsmodels\n    df_sub = sm.add_constant(df_sub, has_constant='add')  \n    # Now df_sub has columns ['const', 'X', 'y']\n    \n    # 3) Fit the logistic model\n    model = sm.Logit(df_sub['y'], df_sub[['const', 'X']])\n    results = model.fit(disp=False)  # disp=False to suppress output\n    \n    # 4) Predict the fitted probability\n    df_sub['pi_pred'] = results.predict(df_sub[['const', 'X']])\n    \n    # 5) Compute the \"true\" pi for comparison\n    df_sub['pi_true'] = logistic(df_sub['X'].values, alpha=ALPHA_TRUE, beta=BETA_TRUE)\n    \n    \n    # 6) Plot pi_true vs. pi_pred\n    ax.scatter(df_sub['pi_true'], df_sub['pi_pred'], \n               alpha=0.3, s=10, color='blue', edgecolors='none')\n    # Add a diagonal line for reference\n    ax.plot([0, 1], [0, 1], color='black', linestyle='--')\n    # 7) Decorate the subplot\n    ax.set_xlabel(\"True pi(x)\")\n    ax.set_ylabel(\"Predicted pi_hat(x)\")\n    ax.set_title(f\"{label}:{1-label:.2f}\")  # e.g., \"Case A60:40\"\n\nplt.tight_layout()\nplt.show()\n\n[1.23775]\n\n\n\n\n\n\n\n\nFigure 7: True vs. estimated p(x) for Case B sub-samples\n\n\n\n\n\nHere, it is evident from Figure 7 that the sample (Case \\(B_{50:50}\\)) with balance class no longer has the best fit between the true and estimated probabilities.\nThe sample (Case \\(B_{80:20}\\)) that performs the best does not have the sampling bias because it that case, the class distribution of the sample (80:20) is equal to the class distribution of the population (80:20). Furthermore, as the sample bias increases, the maximum-likelihood logistic regression model’s highly under- or overpredicts the probability.\nWhen the distribution of the minority in the sample is less than the distribution of the minority in the population, the model underpredicts the probability. Conversely, when the distribution of the minority in the sample is greater than the distribution of the minority in the population, the model overpredicts the probability.\n\n\nDistribution of the performance metrics for the eight sub-samples from Case B with monte-carlo simulations.\nWe perform 1000 monte-carlo simulations for each of the eight sub-samples from Case B. For each simulation, we fit a maximum-likelihood logistic regression model and compute the performance metrics : AUC, AU-PCR, Precision, Recall, F1-score. In order words, in the end of this exercise, we will have 1000 values for each of the performance metrics for each of the eight sub-samples.\nThe code below gives the distribution of the performance metrics for each of the eight sub-samples from Case B with monte-carlo simulations.\n\nC_RUNS = 1000\nSAMPLE_SIZE = 5000  # e.g., majority class size if using a fixed majority approach\nratios = [0.50, 0.60, 0.70, 0.80, 0.90, 0.95, 0.98, 0.99]\nresults_list = []\n\n#for r, (X_sub, y_sub) in samples_B.items():\nfor r in ratios:\n    # 1) Create a random subsample\n    #    Use None or vary random_state so each iteration is unique\n    for mc_i in range(MC_RUNS):\n      X_sub, y_sub = create_subsample_fixed_majority(\n          xB, yB,\n          fraction_class0=r,\n          majority_class0_size=SAMPLE_SIZE,\n          random_state=None\n      )\n      \n      # 2) Split the subsample into train/test\n      #    stratify ensures class distribution is preserved\n      X_train, X_test, y_train, y_test = train_test_split(\n          X_sub,\n          y_sub,\n          test_size=0.3,\n          random_state=42,\n          stratify=y_sub\n      )\n      \n      # Prepare DataFrame for the training set\n      df_train = pd.DataFrame({'X': X_train, 'y': y_train})\n      df_train = sm.add_constant(df_train, prepend=True, has_constant='add')  \n      # =&gt; columns: ['const', 'X', 'y']\n      \n      # 3) Fit logistic regression on the TRAIN portion\n      logit_model = sm.Logit(df_train['y'], df_train[['const', 'X']])\n      result = logit_model.fit(disp=False)\n      \n      # 4) Predict probabilities on the TEST portion\n      df_test = pd.DataFrame({'X': X_test})\n      df_test = sm.add_constant(df_test, prepend=True, has_constant='add')\n      \n      y_proba_test = result.predict(df_test[['const', 'X']])\n      \n      # 5) Evaluate performance metrics on the TEST set\n      auc, auprc, prec, rec, f1 = evaluate_model_performance(y_test, y_proba_test, threshold=0.5)\n      \n      # 6) Store results\n      results_list.append({\n          'ratio_0': r,\n          'auc': auc,\n          'auprc': auprc,\n          'precision': prec,\n          'recall': rec,\n          'f1': f1\n      })\n\n# Convert collected results to a DataFrame\ndf_results_B = pd.DataFrame(results_list)\n# Compute the mean performance metrics for each ratio\ndf_results_B.groupby('ratio_0').mean()\n\n\n\n\n\n\n\n\nauc\nauprc\nprecision\nrecall\nf1\n\n\nratio_0\n\n\n\n\n\n\n\n\n\n0.50\n0.946381\n0.933513\n0.857843\n0.911224\n0.883693\n\n\n0.60\n0.945966\n0.904535\n0.826579\n0.870215\n0.847776\n\n\n0.70\n0.946137\n0.863102\n0.791813\n0.812588\n0.801942\n\n\n0.80\n0.946245\n0.794082\n0.751109\n0.724216\n0.737141\n\n\n0.90\n0.945992\n0.652381\n0.672349\n0.527701\n0.590304\n\n\n0.95\n0.946195\n0.499444\n0.613992\n0.301671\n0.400875\n\n\n0.98\n0.946513\n0.319746\n0.114888\n0.014935\n0.025306\n\n\n0.99\n0.946483\n0.218801\n0.000000\n0.000000\n0.000000\n\n\n\n\n\n\n\nThe mean of the performance metrics for each of the eight sub-samples from Case B is presented in Figure 8.\n\n# Module : Plotting\n# Inputs : df_results_B\n# Objective : Group data by ratio_0 and plot the performance metrics.\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndf_grouped_B = df_results_B.groupby('ratio_0').mean().reset_index()\n\nplt.figure(figsize=(6, 6))\n\nfor metric, colour in zip(metrics, colours):\n    plt.plot(df_grouped_B['ratio_0'], df_grouped_B[metric], label=metric, color=colour, marker='o')\n\n# Improve readability with grid and styling\n\n# Add vertical lines at 0.8.\nplt.axvline(x=0.8, color='black', linestyle='--', linewidth=1.0)\nplt.xlabel(\"Ratio of Y=0\", fontsize=12, weight='bold')\nplt.ylabel(\"Mean of Performance Metrics\", fontsize=12, weight='bold')\nplt.title(\"Mean of Performance Metrics vs Ratios\", fontsize=14, weight='bold')\nplt.xticks(fontsize=10)\nplt.yticks(fontsize=10)\n#plt.grid(False, which='both', linestyle='--', linewidth=0.5, alpha=0.7)\nplt.legend(title=\"Metrics\", fontsize=10, title_fontsize=12, loc=\"best\")\nplt.tight_layout()\n\nplt.show()\n\n\n\n\n\n\n\nFigure 8: Performance metrics Vs. Ratios for Case B\n\n\n\n\n\nWhen the distribution of the minority in the sample is less than the distribution of the minority in the population, the performance metrics deteriorate considerably; when the distribution of the minority in the sample is greater than the distribution of the minority in the population, the performance metrics improve. Similar to Case A, there is not a significant change in the AUC metric values due to class imbalance and sampling bias.\nNext, it can be interesting to compare the performance measures from the samples that have the best performance in Case A (Case \\(A_{50:50}\\)) and Case B (Case \\(B_{80:20}\\)). In case, the sample \\(A_{50:50}\\) that performs the best has no sampling bias and class imbalance, while the sample \\(B_{80:20}\\) that performs the best has no sampling bias but has class imbalance. From these comparisons, it can be concluded that the performance of maximum-likelihood logistic regression is more sensitive to sampling bias than class imbalance."
  },
  {
    "objectID": "00_tds/2_model_selection.html",
    "href": "00_tds/2_model_selection.html",
    "title": "Model Selection",
    "section": "",
    "text": "Introduction\nReducing the number of variables in a regression model is not merely a technical exercise; it is a strategic choice that must be guided by the objectives of the analysis. In a previous work, we demonstrated how simple tools, such as correlation analysis or the Variance Inflation Factor (VIF), can already shrink a dataset with hundreds of predictors into a far more compact model. Yet, even after this initial reduction, models often still contain too many variables to work effectively. A smaller model with fewer predictors offers several advantages: it may yield better predictions than a larger model, it is more parsimonious [hence easier to interpret] and it often generalizes better. As more variables are added, the model’s bias decreases but its variance increases. This is the essence of the bias–variance trade-off: too few variables lead to high bias (underfitting), whereas too many lead to high variance (overfitting). Good predictive performance requires a balance between the two.\nThis is where variable selection and dimension reduction methods come into play. There are multiple approaches to address this challenge. Some methods inherently handle multicollinearity, such as Principal Component Analysis (PCA) combined with linear regression, or sparse regression techniques like the Lasso. Others, such as traditional model selection methods, require explicit treatment of multicollinearity, for example by monitoring the VIF. In model selection, two core problems arise: (i) assigning a score to each model that reflects, in some sense, how “good” the model is, and (ii) searching through the set of candidate models to identify the one with the best score.\nImportantly, the choice of scoring criterion depends on the fundamental objective of the regression. In linear regression, three main objectives can be distinguished:\nA model chosen to minimize the mean squared error of the coefficients (parameter estimation) is not necessarily the best for identifying the most relevant variables (variable selection) or for maximizing predictive accuracy (prediction). In this article, we focus on model selection methods. We first review the different scoring criteria used for model selection, and then present the strategies that allow us to navigate the space of possible models and select the most relevant variables according to the goal of the regression. The first section introduces the framework of model selection. The second section discusses the different scoring criteria used to evaluate models."
  },
  {
    "objectID": "00_tds/2_model_selection.html#mallows-c_p-statistic",
    "href": "00_tds/2_model_selection.html#mallows-c_p-statistic",
    "title": "Model Selection",
    "section": "Mallow’s \\(C_p\\) statistic",
    "text": "Mallow’s \\(C_p\\) statistic\nThe Mallow’s \\(C_p\\) statistic is a popular method for model selection. It is defined as: \\[\n\\hat{R}(S) = \\hat{R_{tr}}(S) + 2|S| \\hat{\\sigma}^2\n\\]\nwhere \\(|S|\\) is the number of terms in \\(S\\) and \\(\\hat{\\sigma}^2\\) is the estimated of \\(\\sigma^2\\), the variance of the error term obtained from the full model with all variables(\\(k\\)). It is a measure of the training error plus a biais correction. The first term measures the fit of the model to the data, while the second term measures the complexity of the model. More the model is complex, more the second term will be large and the Mallow’s \\(C_p\\) statistic will be large. The goal is to find the model that minimizes the Mallow’s \\(C_p\\) statistic.\nThe Mallow’s \\(C_p\\) statistic is seen as a trade-off between the fit of the model and its complexity. Thus find a good model involves trading off fit and complexity."
  },
  {
    "objectID": "00_tds/2_model_selection.html#likelihood-and-penalization",
    "href": "00_tds/2_model_selection.html#likelihood-and-penalization",
    "title": "Model Selection",
    "section": "Likelihood and penalization",
    "text": "Likelihood and penalization\nThe approach below to estimate the prediction risk is based on the maximum likelihood estimation of the parameters. In the hypothesis that the error term is normally distributed, the likelihood function is given by: \\[\n\\begin{align*}\n\\mathcal{l}(Y, \\beta, \\sigma^2)\n&= \\log l(Y, \\beta, \\sigma^2) \\\\\n&= -\\frac{n}{2} \\log(2\\pi) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} (Y_i - X_i\\beta)^2 - \\frac{n}{2} \\log(\\sigma^2).\n\\end{align*}\n\\]\nIf you compute the maximum likelihood estimate of the parameters \\(\\beta\\) and \\(\\sigma^2\\), for the model \\(S\\), that have \\(|S|\\) variables, you will get respectively : \\(\\hat{\\beta}(S)_{MV} = \\hat{\\beta}(S)_{MCO}\\) and \\(\\hat{\\sigma}(S)^2_{MV} = \\frac{1}{n}\\sum_{i=1}^{n} (Y_i - \\hat{Y}_i(S))^2\\).\nThe log-likelihood of the model for the model \\(S\\) which has \\(|S|\\) variables is then given by: \\[\n\\mathcal{l(S)} = -\\frac{n}{2}(1 + \\log(2\\pi)) - \\frac{n}{2} \\log \\frac{\\sum_{i=1}^{n} (Y_i - \\hat{Y}_i(S))^2}{n}\n\\]\nChoosing the model that maximizes the log-likelihood is equivalent to choosing the model that have the smallest residual sum of squares (RSS), that is: \\[\n\\hat{R}(S) = \\frac{1}{n}\\sum_{i=1}^{n} (Y_i - \\hat{Y}_i(S))^2\n\\]\nIn order to minimize a criterion, we work with the opposite of the log-likelihood and the criteria will be generally defined as: \\[\n- 2\\mathcal{l}(S) + 2|S| f(n)  \n\\]\nwhere \\(f(n)\\) is a function of penalization that depends on the sample size \\(n\\). This relation allows us to define the AIC and BIC criteria, which are defined below.\n\nAkaike Information Criterion (AIC)\nAnother method for model selection is the Akaike Information Criterion (AIC). The idea behind the AIC is to find a model that minimizes the information loss. The idea is to choose S to minimize the AIC criterion, which is defined as: \\[\n\\text{AIC}(S) =  - 2\\mathbfcal{l}_S + 2|S|\n\\] where \\(\\mathbfcal{l}_S\\) is the log-likelihood of the model \\(S\\) evaluated at the maximum likelihood estimates of the parameters. Here \\(f(n) = 2\\).\nThis can be thought of goodness of fit plus a complexity. When we want to choose two models, we will prefer the one with the lower AIC.\n\n\nBayesian Information Criterion (BIC)\nThe Bayesian Information Criterion (BIC) is another method for model selection. It is similar to the AIC, and BIC is defined as: \\[\n\\text{BIC}(S) = -2\\mathbfcal{l}_S + 2|S| \\frac{1}{2} \\log(n)\n\\]\nwhere \\(\\mathbfcal{l}_S\\) is the log-likelihood of the model \\(S\\) evaluated at the maximum likelihood estimates of the parameters. We call it Bayesian Information Criterion because it can be derived from a Bayesian perspective. In fact let \\(S = \\{S_1, \\ldots, S_m\\}\\) denoted a set of models. When we assigns a prior probability to each model \\(S_i\\) as \\(\\pi(S_i) = \\frac{1}{m}\\), the posterior probability of the model \\(S_i\\) given the data is proportional to the likelihood of the model \\(S_i\\) given the data, that is: \\[\n\\pi(S_i | \\text{data}) \\propto \\frac{e^{-\\frac{1}{2} \\text{BIC}(S_i)}}{\\sum_{j=1}^{m} e^{-\\frac{1}{2} \\text{BIC}(S_j)}}\n\\]\nHence, choosing the model that minimizes the BIC is equivalent to choosing the model with the highest posterior probability given the data. It also has a interpretation in terms of description length. It puts a more severe penalty for complexity than the AIC, which is why it is often preferred when the sample size is large. In fact, by definition, \\(f(n) = \\frac{1}{2} \\log(n)\\). Ainsi, more the sample size \\(n\\) is large, more the penalty is lower when we compare with the penalty of the AIC. However, this penalty is generally greater than 2 (When \\(n &gt; 7\\)), donc the BIC tends to select smaller models than the AIC. The use of this criterion is similar to the use of the AIC, so when we want to choose two models, we will prefer the one with the lower BIC."
  },
  {
    "objectID": "00_tds/2_model_selection.html#leave-one-out-cross-validation-loocv-and-k-fold-cross-validation",
    "href": "00_tds/2_model_selection.html#leave-one-out-cross-validation-loocv-and-k-fold-cross-validation",
    "title": "Model Selection",
    "section": "Leave-One-Out Cross-Validation (LOOCV) and k-Fold Cross-Validation",
    "text": "Leave-One-Out Cross-Validation (LOOCV) and k-Fold Cross-Validation\nYet another method for model selection is leave-one-out cross-validation (LOOCV). In this case, the risk estimator is given by:\n\\[\n\\hat{R}_{LOOCV}(S) = \\sum_{i=1}^{n} \\left(\\hat{Y}_{-i}(S) - Y_{(i)}\\right)^2\n\\]\nWhere _{-i}(S) is the prediction for \\(Y_i\\) using the model \\(S\\) fitted on all data except the \\(i\\)-th observation, and \\(Y_{(i)}\\) is the \\(i\\)-th observation of the response variable. It can be shown that \\[\n\\hat{R}_{LOOCV}(S) = \\sum_{i=1}^{n} (\\frac{\\hat{Y}_{i}(S) - Y_{i}}{1 - h_{ii}(S)})^2\n\\] where \\(h_{ii}(S)\\) is the \\(i\\)-th diagonal element of the hat matrix \\(H_S = X_S(X_S^TX_S)^{-1}X_S^T\\) for the model \\(S\\).\nThus, one need not actually drop each observation and refit the model. A generalization of LOOCV is k-fold cross-validation."
  },
  {
    "objectID": "00_tds/2_model_selection.html#k-fold-cross-validation",
    "href": "00_tds/2_model_selection.html#k-fold-cross-validation",
    "title": "Model Selection",
    "section": "K-Fold Cross-Validation",
    "text": "K-Fold Cross-Validation\nIn this approach, the data is divided into \\(k\\) groups, or folds (commonly \\(k = 5\\) or \\(k = 10\\)). One fold is omitted, and the model is fitted using the remaining \\(k-1\\) folds. The fitted model is then used to predict the responses for the omitted fold. The risk is estimated as\n\\[\n\\sum_i \\left( Y_i - \\hat{Y}_i \\right)^2,\n\\]\nwhere the sum is taken over the data points in the omitted fold. This process is repeated for each of the \\(k\\) folds, and the average of the \\(k\\) risk estimates is taken as the overall risk estimate.\nThis method is particularly appropriate when the primary objective of the regression is prediction. In this context, alternative performance measures such as the Mean Absolute Error (MAE) or the Root Mean Squared Error (RMSE) can also be used to evaluate the model’s predictive performance."
  },
  {
    "objectID": "00_tds/2_model_selection.html#other-criteria",
    "href": "00_tds/2_model_selection.html#other-criteria",
    "title": "Model Selection",
    "section": "Other criteria",
    "text": "Other criteria\nIn the literature, in addition to the criteria presented above, there are other measures that can be used for model selection. For example, the adjusted \\(R_a^2\\) is a widely used criterion, defined as\n\\[\n\\text{Adjusted } R_a^2(S) = 1 - \\frac{n - 1}{n - |S| - 1} \\left( 1 - R_a^2(S) \\right).\n\\]\nAnother option is to use nested model tests, such as the F-test. The F-test compares two nested models—specifically, a model \\(S_1\\) whose covariates form a subset of those in a larger model \\(S_2\\). The null hypothesis states that the additional variables in \\(S_2\\) do not improve the fit of the model compared to \\(S_1\\).\nThe methods presented above primarily address two key objectives of linear regression: parameter estimation and variable selection."
  },
  {
    "objectID": "00_tds/2_model_selection.html#exhaustive-search",
    "href": "00_tds/2_model_selection.html#exhaustive-search",
    "title": "Model Selection",
    "section": "Exhaustive Search",
    "text": "Exhaustive Search\nThis approach evaluates every possible model and selects the one with the best score. Feasible only for small \\(k\\), as computation becomes prohibitive for large numbers of covariates."
  },
  {
    "objectID": "00_tds/2_model_selection.html#stepwise-search",
    "href": "00_tds/2_model_selection.html#stepwise-search",
    "title": "Model Selection",
    "section": "Stepwise Search",
    "text": "Stepwise Search\nThe methods presented here aim to identify a local optimum—that is, a model that performs better than its neighboring models. Such methods are generally recommended only when exhaustive search is not feasible (for example, when both \\(n\\) and \\(p\\) are large).\n\nForward Stepwise Selection\nIn this approach, we first select a scoring criterion (AIC, BIC, Mallows’ \\(C_p\\), etc.). We start with an empty model and, at each step, add the variable that provides the greatest improvement in the criterion. This process continues until no variable improves the score or all variables are included in the model.\n\n\nBackward Stepwise Selection\nHere, we also begin by selecting a scoring criterion (AIC, BIC, Mallows’ \\(C_p\\), etc.). We start with the full model containing all variables and, at each step, remove the variable whose removal most improves the criterion. We continue removing variables one at a time until no further improvement is possible or only the essential variables remain.\n\n\nStepwise Selection (Mixed Method)\nIn the mixed approach, we again begin by selecting a scoring criterion (AIC, BIC, Mallows’ \\(C_p\\), etc.). We start with an empty model and add variables one at a time, as in forward selection, until no additional variable improves the score. Then, we proceed as in backward selection, removing variables one at a time if doing so improves the criterion. This process stops when no further improvement can be made or all variables are included in the model."
  },
  {
    "objectID": "00_tds/2_model_selection.html#presentation-of-the-dataset",
    "href": "00_tds/2_model_selection.html#presentation-of-the-dataset",
    "title": "Model Selection",
    "section": "Presentation of the dataset",
    "text": "Presentation of the dataset\nWe use the Communities dataset from the UCI Machine Learning Repository, which contains socio-economic and demographic information about communities in the United States. It includes over 100 variables, with the response variable being the number of violent crimes per population (violentCrimesPerPop). We apply the model selection techniques discussed above to identify the variables most strongly associated with the response."
  },
  {
    "objectID": "00_tds/2_model_selection.html#handling-missing-values",
    "href": "00_tds/2_model_selection.html#handling-missing-values",
    "title": "Model Selection",
    "section": "Handling Missing Values",
    "text": "Handling Missing Values\nIn this analysis, we remove all rows containing missing values. An alternative approach is to drop variables with a high proportion of missing values (e.g., more than 10%) and then determine whether the remaining missing values are Missing At Random (MAR), applying an appropriate imputation method if needed. Here, however, we remove all incomplete rows. After this step, the dataset contains no missing values and includes 103 variables, comprising the response variable violentCrimesPerPop and the covariates."
  },
  {
    "objectID": "00_tds/2_model_selection.html#selection-of-relevant-variables-using-expert-judgment",
    "href": "00_tds/2_model_selection.html#selection-of-relevant-variables-using-expert-judgment",
    "title": "Model Selection",
    "section": "Selection of Relevant Variables Using Expert Judgment",
    "text": "Selection of Relevant Variables Using Expert Judgment\nWe apply expert knowledge to assess the relevance of each variable and determine whether its correlation with the response variable is meaningful. This involves consultation between the statistician and domain experts to understand the context and importance of each variable. For this dataset, we remove communityname for simplicity, as it is a categorical variable with many levels, and fold, which is used only for cross-validation. This leaves 101 variables, including the response variable violentCrimesPerPop and the covariates."
  },
  {
    "objectID": "00_tds/2_model_selection.html#reducing-covariates-using-a-correlation-threshold-of-0.6",
    "href": "00_tds/2_model_selection.html#reducing-covariates-using-a-correlation-threshold-of-0.6",
    "title": "Model Selection",
    "section": "Reducing Covariates Using a Correlation Threshold of 0.6",
    "text": "Reducing Covariates Using a Correlation Threshold of 0.6\nTo further reduce the number of covariates, we compute the correlation matrix for all covariates and the response variable. When multiple covariates are highly correlated with each other (correlation greater than 0.6), we retain only the one with the highest correlation to the response. This approach reduces the number of covariates while mitigating multicollinearity.\nAfter applying this and the previous steps, the dataset is reduced to 19 covariates with the VIF below 5. These steps are documented in detail in my article Feature Selection.\n\nVariance Inflation Factors (compact)\n\n\nVariable\nVIF\n\n\n\n\npctWFarmSelf\n1.329818\n\n\nindianPerCap\n1.067939\n\n\nAsianPerCap\n1.221415\n\n\nPctEmplManu\n1.700146\n\n\nPctEmplProfServ\n1.734482\n\n\nPctKids2Par\n2.442528\n\n\nPctWorkMom\n1.232448\n\n\nPctImmigRec10\n1.668404\n\n\nPctVacantBoarded\n1.828584\n\n\nPctVacMore6Mos\n1.816510\n\n\nMedYrHousBuilt\n2.587958\n\n\nPctWOFullPlumb\n1.475350\n\n\nMedRentPctHousInc\n1.462474\n\n\nMedOwnCostPctIncNoMtg\n1.452386\n\n\nPctSameHouse85\n2.382161\n\n\nLandArea\n1.289480\n\n\nPopDens\n2.360121\n\n\nPctUsePubTrans\n2.127730\n\n\nLemasPctOfficDrugUn\n1.308731"
  },
  {
    "objectID": "00_tds/2_model_selection.html#model-selection-with-backward-stepwise-selection",
    "href": "00_tds/2_model_selection.html#model-selection-with-backward-stepwise-selection",
    "title": "Model Selection",
    "section": "Model Selection with Backward Stepwise Selection",
    "text": "Model Selection with Backward Stepwise Selection\nWith 19 variables, the total number of possible models is \\(524,288\\), which may be computationally infeasible for some systems. To reduce the number of models to evaluate, we use a stepwise selection procedure. We implement a function, stepwise_selection, that selects the most relevant variables based on a chosen selection criterion and method (forward, backward, or mixed). In this example, we use Mallows’ \\(C_p\\) as the selection criterion and apply both forward and backward stepwise selection methods.\n\nResult of the backward stepwise selection using Mallow’s \\(C_p\\) criterion\nUsing Mallows’ \\(C_p\\) criterion within the backward stepwise selection procedure, we first remove the variable pctWFarmSelf because its exclusion reduces the criterion to \\(C_p = 41.74\\), which is lower than that of the full model. Next, we remove PctWOFullPlumb, whose removal decreases \\(C_p\\) to \\(41.69895\\). Finally, we remove indianPerCap, resulting in a further reduction to \\(C_p = 41.66073\\). In total, three variables are eliminated from the model.\n\n\nResult of the forward stepwise selection using Mallow’s \\(C_p\\) criterion\nThis method is generally recommended for model selection when the number of variables is large. Because forward stepwise selection starts with an empty model and adds variables one at a time, it is less computationally intensive than backward stepwise selection. Using this approach, we select exactly the same variables as in the backward stepwise selection presented above. The graphic below shows the variables that are successively added to the model, along with the corresponding Mallow’s \\(C_p\\) values. The first variable added is PctKids2Par, followed by PctWorkMom, LandArea, and so on, until the final model is reached with a \\(C_p\\) value of \\(41.66073\\).\n\n# Lire les noms de colonnes à partir du fichier .names\nwith open(\"data/communities.names\", \"r\") as f:\n    lines = f.readlines()\n\n# Extraire les noms à partir des lignes définissant les attributs\ncolumns = []\nstart_extracting = False\nfor line in lines:\n    if line.startswith(\"@attribute\"):\n        parts = line.split()\n        if len(parts) &gt; 1:\n            columns.append(parts[1])\n    elif \"attribute information:\" in line.lower():\n        start_extracting = True\n    elif start_extracting and line.strip() and not line.startswith('@'):\n        # handle lines like: \"1. state: continuous\"\n        if ':' in line:\n            name = line.split(':')[0].strip().split()[-1]\n            columns.append(name)\n\n# Supprimer les doublons tout en conservant l'ordre\nfrom collections import OrderedDict\ncolumns = list(OrderedDict.fromkeys(columns))\n\n# Vérifie que ça correspond au nombre de colonnes du fichier data\nprint(f\"{len(columns)} colonnes uniques\")\n\n135 colonnes uniques\n\n\n\nimport pandas as pd\n\n# Charger le fichier communities.data\ndf = pd.read_csv(\"data/communities.data\", header=None, names=columns, na_values='?')\n\n# Vérifier la forme\nprint(df.shape)\ndf.head()\n\n(1994, 135)\n\n\n\n\n\n\n\n\n\nstate\ncounty\ncommunity\ncommunityname\nfold\npopulation\nhouseholdsize\nracepctblack\nracePctWhite\nracePctAsian\n...\nLemasPctOfficDrugUn\nPolicBudgPerPop\nViolentCrimesPerPop\nStatistics\nPopulation)\nPapers\nRequest\npaper\nStates\nBaveja\n\n\n\n\n0\n8\nNaN\nNaN\nLakewoodcity\n1\n0.19\n0.33\n0.02\n0.90\n0.12\n...\n0.32\n0.14\n0.20\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n1\n53\nNaN\nNaN\nTukwilacity\n1\n0.00\n0.16\n0.12\n0.74\n0.45\n...\n0.00\nNaN\n0.67\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2\n24\nNaN\nNaN\nAberdeentown\n1\n0.00\n0.42\n0.49\n0.56\n0.17\n...\n0.00\nNaN\n0.43\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n3\n34\n5.0\n81440.0\nWillingborotownship\n1\n0.04\n0.77\n1.00\n0.08\n0.12\n...\n0.00\nNaN\n0.12\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n4\n42\n95.0\n6096.0\nBethlehemtownship\n1\n0.01\n0.55\n0.02\n0.95\n0.09\n...\n0.00\nNaN\n0.03\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n5 rows × 135 columns\n\n\n\n\ndf['fold'].value_counts()  # Voir combien de points dans chaque fold\n\nfold\n1     200\n2     200\n3     200\n4     200\n5     199\n6     199\n7     199\n8     199\n9     199\n10    199\nName: count, dtype: int64\n\n\n\n# Liste des colonnes sans aucune valeur manquante\ncols_without_na = df.columns[df.notna().all()].tolist()\n\nprint(f\"{len(cols_without_na)} colonnes sans NA :\")\n\nprint(cols_without_na)\ndf_clean = df.dropna(axis=1)\nprint(len(df_clean.columns))\ndf_clean.to_csv(\"data/communities_clean.csv\", index=False)\n\n103 colonnes sans NA :\n['state', 'communityname', 'fold', 'population', 'householdsize', 'racepctblack', 'racePctWhite', 'racePctAsian', 'racePctHisp', 'agePct12t21', 'agePct12t29', 'agePct16t24', 'agePct65up', 'numbUrban', 'pctUrban', 'medIncome', 'pctWWage', 'pctWFarmSelf', 'pctWInvInc', 'pctWSocSec', 'pctWPubAsst', 'pctWRetire', 'medFamInc', 'perCapInc', 'whitePerCap', 'blackPerCap', 'indianPerCap', 'AsianPerCap', 'HispPerCap', 'NumUnderPov', 'PctPopUnderPov', 'PctLess9thGrade', 'PctNotHSGrad', 'PctBSorMore', 'PctUnemployed', 'PctEmploy', 'PctEmplManu', 'PctEmplProfServ', 'PctOccupManu', 'PctOccupMgmtProf', 'MalePctDivorce', 'MalePctNevMarr', 'FemalePctDiv', 'TotalPctDiv', 'PersPerFam', 'PctFam2Par', 'PctKids2Par', 'PctYoungKids2Par', 'PctTeen2Par', 'PctWorkMomYoungKids', 'PctWorkMom', 'NumIlleg', 'PctIlleg', 'NumImmig', 'PctImmigRecent', 'PctImmigRec5', 'PctImmigRec8', 'PctImmigRec10', 'PctRecentImmig', 'PctRecImmig5', 'PctRecImmig8', 'PctRecImmig10', 'PctSpeakEnglOnly', 'PctNotSpeakEnglWell', 'PctLargHouseFam', 'PctLargHouseOccup', 'PersPerOccupHous', 'PersPerOwnOccHous', 'PersPerRentOccHous', 'PctPersOwnOccup', 'PctPersDenseHous', 'PctHousLess3BR', 'MedNumBR', 'HousVacant', 'PctHousOccup', 'PctHousOwnOcc', 'PctVacantBoarded', 'PctVacMore6Mos', 'MedYrHousBuilt', 'PctHousNoPhone', 'PctWOFullPlumb', 'OwnOccLowQuart', 'OwnOccMedVal', 'OwnOccHiQuart', 'RentLowQ', 'RentMedian', 'RentHighQ', 'MedRent', 'MedRentPctHousInc', 'MedOwnCostPctInc', 'MedOwnCostPctIncNoMtg', 'NumInShelters', 'NumStreet', 'PctForeignBorn', 'PctBornSameState', 'PctSameHouse85', 'PctSameCity85', 'PctSameState85', 'LandArea', 'PopDens', 'PctUsePubTrans', 'LemasPctOfficDrugUn', 'ViolentCrimesPerPop']\n103\n\n\n\n# Extract response variable and covariates\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\n\ndf = pd.read_csv(\"data/communities_clean.csv\")\nresponse = 'ViolentCrimesPerPop'\ncovariates = [col for col in df.columns if col not in ['fold', response, 'state', 'county', 'community', 'communityname', 'NumStreet']]\n\n\nspearman_corr = df[covariates + [response]].corr(method='spearman')\nplt.figure(figsize=(12, 10))\nsns.heatmap(spearman_corr, annot=True, cmap=\"coolwarm\", fmt=\".2f\", linewidths=0.5)\nplt.title(\"Correlation Matrix Heatmap\")\nplt.show()\n\n\n\n\n\n\n\n\n\n# Step 2: Correlation of each variable with response R\nspearman_corr_with_R = spearman_corr[response].drop(response)  # exclude R-R\n\n# Step 3: Identify pairs of covariates with strong inter-correlation (e.g., &gt; 0.9)\nstrong_pairs = []\nthreshold = 0.6\ncovariates = spearman_corr_with_R.index\n\nfor i, var1 in enumerate(covariates):\n    for var2 in covariates[i+1:]:\n        if abs(spearman_corr.loc[var1, var2]) &gt; threshold:\n            strong_pairs.append((var1, var2))\n\n# Step 4: From each correlated pair, keep only the variable most correlated with R\nto_keep = set()\nto_discard = set()\n\nfor var1, var2 in strong_pairs:\n    if abs(spearman_corr_with_R[var1]) &gt;= abs(spearman_corr_with_R[var2]):\n        to_keep.add(var1)\n        to_discard.add(var2)\n    else:\n        to_keep.add(var2)\n        to_discard.add(var1)\n\n# Final selection: all covariates excluding the ones to discard due to redundancy\nfinal_selected_variables = [var for var in covariates if var not in to_discard]\n\nfinal_selected_variables\nprint(f\"Number of selected variables: {len(final_selected_variables)}\")\n\nNumber of selected variables: 19\n\n\n\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom statsmodels.tools.tools import add_constant\nfrom sklearn.preprocessing import StandardScaler\nimport numpy as np\n\nX = df[final_selected_variables]  \n\nX_with_const = add_constant(X)  \n\nvif_data = pd.DataFrame()\nvif_data[\"variable\"] = X_with_const.columns\nvif_data[\"VIF\"] = [variance_inflation_factor(X_with_const.values, i)\n                   for i in range(X_with_const.shape[1])]\n\nvif_data = vif_data[vif_data[\"variable\"] != \"const\"]\n\nprint(vif_data)\n\n                 variable       VIF\n1            pctWFarmSelf  1.329818\n2            indianPerCap  1.067939\n3             AsianPerCap  1.221415\n4             PctEmplManu  1.700146\n5         PctEmplProfServ  1.734482\n6             PctKids2Par  2.442528\n7              PctWorkMom  1.232448\n8           PctImmigRec10  1.668404\n9        PctVacantBoarded  1.828584\n10         PctVacMore6Mos  1.816510\n11         MedYrHousBuilt  2.587958\n12         PctWOFullPlumb  1.475350\n13      MedRentPctHousInc  1.462474\n14  MedOwnCostPctIncNoMtg  1.452386\n15         PctSameHouse85  2.382161\n16               LandArea  1.289480\n17                PopDens  2.360121\n18         PctUsePubTrans  2.127730\n19    LemasPctOfficDrugUn  1.308731\n\n\n\nimport pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\nfrom collections import OrderedDict\n\ndef stepwise_selection(df, target, strategy='forward', metric='AIC', verbose=True):\n    \"\"\"\n    Variable selection using stepwise regression (forward or backward) and AIC/BIC criterion.\n    \n    Parameters\n    ----------\n    df : pandas.DataFrame\n        DataFrame containing the predictors and the target column.\n    target : str\n        Name of the target column.\n    strategy : str\n        'forward' or 'backward'.\n    metric : str\n        'AIC' or 'BIC'.\n    verbose : bool\n        Whether to print the steps.\n    \n    Returns\n    -------\n    selected_vars : list\n        List of variables selected in the final model.\n    best_model : statsmodels.regression.linear_model.RegressionResultsWrapper\n        Fitted OLS model with selected variables.\n    history : list of dict\n        Steps taken (variables, metric).\n    \"\"\"\n    # Vérification des NaN\n    if df.isnull().values.any():\n        raise ValueError(\"Des valeurs manquantes sont présentes dans le DataFrame. Veuillez les gérer avant d'appliquer la sélection.\")\n    \n    X = df.drop(columns=[target])\n    y = df[target]\n    variables = list(X.columns)\n    history = []\n    \n    # Initialisation\n    if strategy == 'forward':\n        selected = []\n        remaining = variables.copy()\n    elif strategy == 'backward':\n        selected = variables.copy()\n        remaining = []\n    else:\n        raise ValueError(\"strategy must be 'forward' or 'backward'\")\n    \n    current_score = np.inf\n    best_new_score = np.inf\n    \n    step = 0\n    while True:\n        step += 1\n        scores_with_candidates = []\n        \n        if strategy == 'forward':\n            # Tester chaque variable qui n'est pas dans 'selected'\n            candidates = [var for var in remaining if var not in selected]\n            for candidate in candidates:\n                vars_to_test = selected + [candidate]\n                X_train = sm.add_constant(X[vars_to_test])\n                model = sm.OLS(y, X_train).fit()\n                if metric == 'AIC':\n                    score = model.aic\n                elif metric == 'BIC':\n                    score = model.bic\n                scores_with_candidates.append((score, candidate, vars_to_test))\n            if not scores_with_candidates:\n                break\n            # Chercher la variable qui améliore le plus la métrique\n            scores_with_candidates.sort()\n            best_new_score, best_candidate, vars_to_test = scores_with_candidates[0]\n            \n            if verbose:\n                print(f\"\\nÉtape {step}:\")\n                print(\"Candidats testés:\", [(v, round(s, 2)) for s, v, _ in scores_with_candidates])\n                print(f\"Meilleure variable à ajouter : {best_candidate} (score={round(best_new_score,2)})\")\n            \n            # Critère d'arrêt\n            if len(selected) == 0 or best_new_score &lt; current_score - 1e-6:\n                selected.append(best_candidate)\n                remaining.remove(best_candidate)\n                current_score = best_new_score\n                history.append({\n                    'step': step,\n                    'selected': selected.copy(),\n                    'score': current_score,\n                    'modified': best_candidate\n                })\n            else:\n                if verbose:\n                    print(\"Plus aucune variable n'améliore le score.\")\n                break\n        \n        elif strategy == 'backward':\n            # Tester la suppression de chaque variable restante\n            if len(selected) == 0:\n                break\n            scores_with_candidates = []\n            for candidate in selected:\n                vars_to_test = [var for var in selected if var != candidate]\n                if len(vars_to_test) == 0:\n                    continue\n                X_train = sm.add_constant(X[vars_to_test])\n                model = sm.OLS(y, X_train).fit()\n                if metric == 'AIC':\n                    score = model.aic\n                elif metric == 'BIC':\n                    score = model.bic\n                scores_with_candidates.append((score, candidate, vars_to_test))\n            if not scores_with_candidates:\n                break\n            # Chercher la variable dont la suppression améliore le plus la métrique\n            scores_with_candidates.sort()\n            best_new_score, worst_candidate, vars_to_test = scores_with_candidates[0]\n            \n            if verbose:\n                print(f\"\\nÉtape {step}:\")\n                print(\"Suppressions testées:\", [(v, round(s, 2)) for s, v, _ in scores_with_candidates])\n                print(f\"Meilleure variable à retirer : {worst_candidate} (score={round(best_new_score,2)})\")\n            \n            # Critère d'arrêt\n            if best_new_score &lt; current_score - 1e-6:\n                selected.remove(worst_candidate)\n                current_score = best_new_score\n                history.append({\n                    'step': step,\n                    'selected': selected.copy(),\n                    'score': current_score,\n                    'modified': worst_candidate\n                })\n            else:\n                if verbose:\n                    print(\"Aucune suppression n'améliore le score.\")\n                break\n    \n    # Fit final model\n    X_final = sm.add_constant(X[selected])\n    best_model = sm.OLS(y, X_final).fit()\n    if verbose:\n        print(\"\\nVariables sélectionnées :\", selected)\n        print(f\"Score final ({metric}): {round(best_model.aic if metric=='AIC' else best_model.bic,2)}\")\n    \n    \n    return selected, best_model, history\n\n# Graphique de l'historique des scores\n\n\nimport matplotlib.pyplot as plt\n\nimport matplotlib.pyplot as plt\n\ndef plot_stepwise_crosses(history, all_vars, metric=\"AIC\", title=None):\n    \"\"\"\n    Affiche le graphique stepwise type heatmap à croix :\n    - X : variables explicatives modifiées à au moins une étape (ordre d'apparition)\n    - Y : score (AIC/BIC) à chaque étape (de l'historique)\n    - Croix noire : variable modifiée à chaque étape\n    - Vide ailleurs\n    - Courbe du score\n    \"\"\"\n    n_steps = len(history)\n    scores = [h['score'] for h in history]\n    \n    # Extraire la liste ordonnée des variables effectivement modifiées\n    modified_vars = []\n    for h in history:\n        var = h['modified']\n        if var not in modified_vars and var is not None:\n            modified_vars.append(var)\n    \n    n_mod_vars = len(modified_vars)\n    \n    # Construction des positions X pour les croix (selon modified_vars)\n    mod_pos = [modified_vars.index(h['modified']) if h['modified'] in modified_vars else None for h in history]\n\n    fig, ax = plt.subplots(figsize=(min(1.3 * n_mod_vars, 8), 6))\n    # Placer la croix noire à chaque étape\n    for i, x in enumerate(mod_pos):\n        if x is not None:\n            ax.scatter(x, scores[i], color='black', marker='x', s=100, zorder=3)\n    # Tracer la courbe du score\n    ax.plot(range(n_steps), scores, color='gray', alpha=0.7, linewidth=2, zorder=1)\n    # Axe X : labels verticaux, police réduite (uniquement variables modifiées)\n    ax.set_xticks(range(n_mod_vars))\n    ax.set_xticklabels(modified_vars, rotation=90, fontsize=10)\n    ax.set_xlabel(\"Variables modifiées\")\n    ax.set_ylabel(metric)\n    ax.set_title(title or f\"Stepwise ({metric}) – Variables modifiées à chaque étape\")\n    ax.grid(True, axis='y', alpha=0.2)\n    plt.tight_layout()\n    plt.show()\n\n\ndf_for_stepwise = df[final_selected_variables + [response]].dropna()\n\n\nselected_vars, best_model, history = stepwise_selection(\n    df=df_for_stepwise,\n    target='ViolentCrimesPerPop',\n    strategy='backward',    # Ou 'backward'\n    metric='AIC',          # Ou 'BIC'\n    verbose=True\n)\nprint(best_model.summary())\n\n\nÉtape 1:\nSuppressions testées: [('pctWFarmSelf', np.float64(-2050.99)), ('PctWOFullPlumb', np.float64(-2050.95)), ('indianPerCap', np.float64(-2050.84)), ('MedRentPctHousInc', np.float64(-2048.38)), ('PctVacMore6Mos', np.float64(-2047.19)), ('PctEmplManu', np.float64(-2045.67)), ('PctImmigRec10', np.float64(-2045.63)), ('PctUsePubTrans', np.float64(-2042.0)), ('PctEmplProfServ', np.float64(-2040.42)), ('MedOwnCostPctIncNoMtg', np.float64(-2039.47)), ('AsianPerCap', np.float64(-2039.39)), ('PctSameHouse85', np.float64(-2037.29)), ('LemasPctOfficDrugUn', np.float64(-2035.93)), ('PopDens', np.float64(-2034.44)), ('PctWorkMom', np.float64(-2031.19)), ('PctVacantBoarded', np.float64(-2021.95)), ('LandArea', np.float64(-2020.45)), ('MedYrHousBuilt', np.float64(-2016.35)), ('PctKids2Par', np.float64(-1246.96))]\nMeilleure variable à retirer : pctWFarmSelf (score=-2050.99)\n\nÉtape 2:\nSuppressions testées: [('PctWOFullPlumb', np.float64(-2052.95)), ('indianPerCap', np.float64(-2052.83)), ('MedRentPctHousInc', np.float64(-2050.35)), ('PctVacMore6Mos', np.float64(-2049.07)), ('PctEmplManu', np.float64(-2047.65)), ('PctImmigRec10', np.float64(-2047.54)), ('PctUsePubTrans', np.float64(-2043.85)), ('PctEmplProfServ', np.float64(-2042.15)), ('MedOwnCostPctIncNoMtg', np.float64(-2041.43)), ('AsianPerCap', np.float64(-2041.35)), ('PctSameHouse85', np.float64(-2039.27)), ('LemasPctOfficDrugUn', np.float64(-2037.86)), ('PopDens', np.float64(-2036.4)), ('PctWorkMom', np.float64(-2033.17)), ('PctVacantBoarded', np.float64(-2023.94)), ('LandArea', np.float64(-2022.31)), ('MedYrHousBuilt', np.float64(-2017.9)), ('PctKids2Par', np.float64(-1227.34))]\nMeilleure variable à retirer : PctWOFullPlumb (score=-2052.95)\n\nÉtape 3:\nSuppressions testées: [('indianPerCap', np.float64(-2054.79)), ('MedRentPctHousInc', np.float64(-2052.31)), ('PctVacMore6Mos', np.float64(-2051.07)), ('PctEmplManu', np.float64(-2049.64)), ('PctImmigRec10', np.float64(-2049.39)), ('PctUsePubTrans', np.float64(-2045.84)), ('PctEmplProfServ', np.float64(-2044.03)), ('MedOwnCostPctIncNoMtg', np.float64(-2043.43)), ('AsianPerCap', np.float64(-2043.35)), ('PctSameHouse85', np.float64(-2041.14)), ('LemasPctOfficDrugUn', np.float64(-2039.86)), ('PopDens', np.float64(-2038.15)), ('PctWorkMom', np.float64(-2034.37)), ('PctVacantBoarded', np.float64(-2025.71)), ('LandArea', np.float64(-2024.0)), ('MedYrHousBuilt', np.float64(-2019.8)), ('PctKids2Par', np.float64(-1169.01))]\nMeilleure variable à retirer : indianPerCap (score=-2054.79)\n\nÉtape 4:\nSuppressions testées: [('MedRentPctHousInc', np.float64(-2054.16)), ('PctVacMore6Mos', np.float64(-2053.0)), ('PctEmplManu', np.float64(-2051.47)), ('PctImmigRec10', np.float64(-2051.2)), ('PctUsePubTrans', np.float64(-2047.79)), ('PctEmplProfServ', np.float64(-2045.85)), ('AsianPerCap', np.float64(-2045.34)), ('MedOwnCostPctIncNoMtg', np.float64(-2045.21)), ('PctSameHouse85', np.float64(-2043.08)), ('LemasPctOfficDrugUn', np.float64(-2041.75)), ('PopDens', np.float64(-2040.09)), ('PctWorkMom', np.float64(-2036.16)), ('PctVacantBoarded', np.float64(-2027.57)), ('LandArea', np.float64(-2025.81)), ('MedYrHousBuilt', np.float64(-2021.8)), ('PctKids2Par', np.float64(-1165.99))]\nMeilleure variable à retirer : MedRentPctHousInc (score=-2054.16)\nAucune suppression n'améliore le score.\n\nVariables sélectionnées : ['AsianPerCap', 'PctEmplManu', 'PctEmplProfServ', 'PctKids2Par', 'PctWorkMom', 'PctImmigRec10', 'PctVacantBoarded', 'PctVacMore6Mos', 'MedYrHousBuilt', 'MedRentPctHousInc', 'MedOwnCostPctIncNoMtg', 'PctSameHouse85', 'LandArea', 'PopDens', 'PctUsePubTrans', 'LemasPctOfficDrugUn']\nScore final (AIC): -2054.79\n                             OLS Regression Results                            \n===============================================================================\nDep. Variable:     ViolentCrimesPerPop   R-squared:                       0.621\nModel:                             OLS   Adj. R-squared:                  0.618\nMethod:                  Least Squares   F-statistic:                     202.8\nDate:                 Sun, 05 Oct 2025   Prob (F-statistic):               0.00\nTime:                         21:51:57   Log-Likelihood:                 1044.4\nNo. Observations:                 1994   AIC:                            -2055.\nDf Residuals:                     1977   BIC:                            -1960.\nDf Model:                           16                                         \nCovariance Type:             nonrobust                                         \n=========================================================================================\n                            coef    std err          t      P&gt;|t|      [0.025      0.975]\n-----------------------------------------------------------------------------------------\nconst                     0.6121      0.041     14.766      0.000       0.531       0.693\nAsianPerCap               0.0611      0.018      3.374      0.001       0.026       0.097\nPctEmplManu              -0.0475      0.021     -2.298      0.022      -0.088      -0.007\nPctEmplProfServ          -0.0789      0.024     -3.297      0.001      -0.126      -0.032\nPctKids2Par              -0.7673      0.023    -33.369      0.000      -0.812      -0.722\nPctWorkMom               -0.0915      0.020     -4.534      0.000      -0.131      -0.052\nPctImmigRec10             0.0496      0.021      2.356      0.019       0.008       0.091\nPctVacantBoarded          0.1080      0.020      5.402      0.000       0.069       0.147\nPctVacMore6Mos           -0.0437      0.023     -1.941      0.052      -0.088       0.000\nMedYrHousBuilt            0.1306      0.022      5.916      0.000       0.087       0.174\nMedRentPctHousInc         0.0370      0.023      1.615      0.107      -0.008       0.082\nMedOwnCostPctIncNoMtg    -0.0681      0.020     -3.393      0.001      -0.108      -0.029\nPctSameHouse85            0.1011      0.027      3.693      0.000       0.047       0.155\nLandArea                  0.1854      0.033      5.564      0.000       0.120       0.251\nPopDens                   0.0991      0.024      4.078      0.000       0.051       0.147\nPctUsePubTrans            0.0609      0.020      2.990      0.003       0.021       0.101\nLemasPctOfficDrugUn       0.0591      0.015      3.868      0.000       0.029       0.089\n==============================================================================\nOmnibus:                      389.964   Durbin-Watson:                   2.035\nProb(Omnibus):                  0.000   Jarque-Bera (JB):             1040.967\nSkew:                           1.034   Prob(JB):                    9.06e-227\nKurtosis:                       5.873   Cond. No.                         29.4\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\nall_vars = [col for col in df.columns if col != 'target']\nplot_stepwise_crosses(history, all_vars, metric=\"AIC\", title=\"Sélection de variables – Stepwise\")\n\n\n\n\n\n\n\n\n\nimport numpy as np\nimport statsmodels.api as sm\n\ndef compute_score(y, X, vars_to_test, metric, full_model_mse=None):\n    X_train = sm.add_constant(X[vars_to_test])\n    model = sm.OLS(y, X_train).fit()\n    n = len(y)\n    p = len(vars_to_test) + 1  # +1 pour la constante\n\n    if metric == 'AIC':\n        return model.aic\n\n    elif metric == 'BIC':\n        return model.bic\n\n    elif metric == 'Cp':\n        if full_model_mse is None:\n            raise ValueError(\"full_model_mse doit être fourni pour calculer Cp Mallows.\")\n        rss = sum(model.resid ** 2)\n        return rss + 2 * p * full_model_mse\n\n    elif metric == 'R2_adj':\n        return -model.rsquared_adj  # négatif pour maximiser\n\n    else:\n        raise ValueError(\"Métrique inconnue. Utilisez 'AIC', 'BIC', 'Cp' ou 'R2_adj'.\")\n\ndef get_best_candidate(y, X, selected, candidates, metric, strategy, full_model_mse=None):\n    scores_with_candidates = []\n    for candidate in candidates:\n        vars_to_test = selected + [candidate] if strategy == 'forward' else [var for var in selected if var != candidate]\n        score = compute_score(y, X, vars_to_test, metric, full_model_mse)\n        scores_with_candidates.append((score, candidate, vars_to_test))\n\n    scores_with_candidates.sort()\n    print(\"Suppressions testées:\", [(v, round(s, 2)) for s, v, _ in scores_with_candidates])\n    return scores_with_candidates[0] if scores_with_candidates else (None, None, None)\n\ndef stepwise_selection(df, target, strategy='forward', metric='AIC', verbose=True):\n    if df.isnull().values.any():\n        raise ValueError(\"Des valeurs manquantes sont présentes dans le DataFrame.\")\n\n    X = df.drop(columns=[target])\n    y = df[target]\n    variables = list(X.columns)\n\n    selected = [] if strategy == 'forward' else variables.copy()\n    remaining = variables.copy() if strategy == 'forward' else []\n\n    # Calcul préalable du MSE du modèle complet pour Cp Mallows\n    if metric == 'Cp':\n        X_full = sm.add_constant(X)\n        full_model = sm.OLS(y, X_full).fit()\n        full_model_mse = sum(full_model.resid ** 2) / (len(y) - len(variables) - 1)\n    else:\n        full_model_mse = None\n\n    current_score = np.inf\n    history = []\n    step = 0\n\n    while True:\n        step += 1\n        candidates = remaining if strategy == 'forward' else selected\n        best_score, best_candidate, vars_to_test = get_best_candidate(y, X, selected, candidates, metric, strategy, full_model_mse)\n\n        if best_candidate is None:\n            if verbose:\n                print(\"Aucun candidat disponible.\")\n            break\n\n        if verbose:\n            action = \"ajouter\" if strategy == 'forward' else \"retirer\"\n            print(f\"\\nÉtape {step}: Meilleure variable à {action} : {best_candidate} (score={round(best_score,5)})\")\n\n\n        improvement = best_score &lt; current_score - 1e-6\n\n        if improvement:\n            if strategy == 'forward':\n                selected.append(best_candidate)\n                remaining.remove(best_candidate)\n            else:\n                selected.remove(best_candidate)\n\n            current_score = best_score\n            history.append({\n                'step': step,\n                'selected': selected.copy(),\n                'score': current_score,\n                'modified': best_candidate\n            })\n        else:\n            if verbose:\n                print(\"Aucune amélioration supplémentaire du score.\")\n            break\n\n    X_final = sm.add_constant(X[selected])\n    best_model = sm.OLS(y, X_final).fit()\n\n    if verbose:\n        print(\"\\nVariables sélectionnées :\", selected)\n        final_score = best_model.aic if metric == 'AIC' else best_model.bic\n        if metric == 'Cp':\n            final_score = compute_score(y, X, selected, metric, full_model_mse)\n        elif metric == 'R2_adj':\n            final_score = -compute_score(y, X, selected, metric)\n        print(f\"Score final ({metric}): {round(final_score,5)}\")\n\n    return selected, best_model, history\n\n\nselected_vars, best_model, history = stepwise_selection(\n    df=df_for_stepwise,\n    target='ViolentCrimesPerPop',\n    strategy='backward',    # Ou 'backward'\n    metric='Cp',           # Ou 'AIC', 'BIC', 'R2_adj'\n    verbose=True\n)\nprint(best_model.summary())\nplot_stepwise_crosses(history, df_for_stepwise.columns.tolist(), metric=\"Cp\", title=\"Sélection de variables – Stepwise avec Cp Mallows\")\n\nSuppressions testées: [('pctWFarmSelf', 41.74), ('PctWOFullPlumb', 41.74), ('indianPerCap', 41.74), ('MedRentPctHousInc', 41.79), ('PctVacMore6Mos', 41.82), ('PctEmplManu', 41.85), ('PctImmigRec10', 41.85), ('PctUsePubTrans', 41.92), ('PctEmplProfServ', 41.96), ('MedOwnCostPctIncNoMtg', 41.98), ('AsianPerCap', 41.98), ('PctSameHouse85', 42.02), ('LemasPctOfficDrugUn', 42.05), ('PopDens', 42.08), ('PctWorkMom', 42.15), ('PctVacantBoarded', 42.34), ('LandArea', 42.37), ('MedYrHousBuilt', 42.46), ('PctKids2Par', 62.08)]\n\nÉtape 1: Meilleure variable à retirer : pctWFarmSelf (score=41.73966)\nSuppressions testées: [('PctWOFullPlumb', 41.7), ('indianPerCap', 41.7), ('MedRentPctHousInc', 41.75), ('PctVacMore6Mos', 41.78), ('PctEmplManu', 41.81), ('PctImmigRec10', 41.81), ('PctUsePubTrans', 41.89), ('PctEmplProfServ', 41.92), ('MedOwnCostPctIncNoMtg', 41.94), ('AsianPerCap', 41.94), ('PctSameHouse85', 41.98), ('LemasPctOfficDrugUn', 42.01), ('PopDens', 42.04), ('PctWorkMom', 42.11), ('PctVacantBoarded', 42.3), ('LandArea', 42.33), ('MedYrHousBuilt', 42.43), ('PctKids2Par', 62.7)]\n\nÉtape 2: Meilleure variable à retirer : PctWOFullPlumb (score=41.69895)\nSuppressions testées: [('indianPerCap', 41.66), ('MedRentPctHousInc', 41.71), ('PctVacMore6Mos', 41.74), ('PctEmplManu', 41.77), ('PctImmigRec10', 41.77), ('PctUsePubTrans', 41.84), ('PctEmplProfServ', 41.88), ('MedOwnCostPctIncNoMtg', 41.89), ('AsianPerCap', 41.9), ('PctSameHouse85', 41.94), ('LemasPctOfficDrugUn', 41.97), ('PopDens', 42.0), ('PctWorkMom', 42.08), ('PctVacantBoarded', 42.26), ('LandArea', 42.3), ('MedYrHousBuilt', 42.39), ('PctKids2Par', 64.57)]\n\nÉtape 3: Meilleure variable à retirer : indianPerCap (score=41.66073)\nSuppressions testées: [('MedRentPctHousInc', 41.67), ('PctVacMore6Mos', 41.7), ('PctEmplManu', 41.73), ('PctImmigRec10', 41.73), ('PctUsePubTrans', 41.8), ('PctEmplProfServ', 41.84), ('AsianPerCap', 41.86), ('MedOwnCostPctIncNoMtg', 41.86), ('PctSameHouse85', 41.9), ('LemasPctOfficDrugUn', 41.93), ('PopDens', 41.96), ('PctWorkMom', 42.05), ('PctVacantBoarded', 42.22), ('LandArea', 42.26), ('MedYrHousBuilt', 42.34), ('PctKids2Par', 64.69)]\n\nÉtape 4: Meilleure variable à retirer : MedRentPctHousInc (score=41.67325)\nAucune amélioration supplémentaire du score.\n\nVariables sélectionnées : ['AsianPerCap', 'PctEmplManu', 'PctEmplProfServ', 'PctKids2Par', 'PctWorkMom', 'PctImmigRec10', 'PctVacantBoarded', 'PctVacMore6Mos', 'MedYrHousBuilt', 'MedRentPctHousInc', 'MedOwnCostPctIncNoMtg', 'PctSameHouse85', 'LandArea', 'PopDens', 'PctUsePubTrans', 'LemasPctOfficDrugUn']\nScore final (Cp): 41.66073\n                             OLS Regression Results                            \n===============================================================================\nDep. Variable:     ViolentCrimesPerPop   R-squared:                       0.621\nModel:                             OLS   Adj. R-squared:                  0.618\nMethod:                  Least Squares   F-statistic:                     202.8\nDate:                 Sun, 05 Oct 2025   Prob (F-statistic):               0.00\nTime:                         21:51:57   Log-Likelihood:                 1044.4\nNo. Observations:                 1994   AIC:                            -2055.\nDf Residuals:                     1977   BIC:                            -1960.\nDf Model:                           16                                         \nCovariance Type:             nonrobust                                         \n=========================================================================================\n                            coef    std err          t      P&gt;|t|      [0.025      0.975]\n-----------------------------------------------------------------------------------------\nconst                     0.6121      0.041     14.766      0.000       0.531       0.693\nAsianPerCap               0.0611      0.018      3.374      0.001       0.026       0.097\nPctEmplManu              -0.0475      0.021     -2.298      0.022      -0.088      -0.007\nPctEmplProfServ          -0.0789      0.024     -3.297      0.001      -0.126      -0.032\nPctKids2Par              -0.7673      0.023    -33.369      0.000      -0.812      -0.722\nPctWorkMom               -0.0915      0.020     -4.534      0.000      -0.131      -0.052\nPctImmigRec10             0.0496      0.021      2.356      0.019       0.008       0.091\nPctVacantBoarded          0.1080      0.020      5.402      0.000       0.069       0.147\nPctVacMore6Mos           -0.0437      0.023     -1.941      0.052      -0.088       0.000\nMedYrHousBuilt            0.1306      0.022      5.916      0.000       0.087       0.174\nMedRentPctHousInc         0.0370      0.023      1.615      0.107      -0.008       0.082\nMedOwnCostPctIncNoMtg    -0.0681      0.020     -3.393      0.001      -0.108      -0.029\nPctSameHouse85            0.1011      0.027      3.693      0.000       0.047       0.155\nLandArea                  0.1854      0.033      5.564      0.000       0.120       0.251\nPopDens                   0.0991      0.024      4.078      0.000       0.051       0.147\nPctUsePubTrans            0.0609      0.020      2.990      0.003       0.021       0.101\nLemasPctOfficDrugUn       0.0591      0.015      3.868      0.000       0.029       0.089\n==============================================================================\nOmnibus:                      389.964   Durbin-Watson:                   2.035\nProb(Omnibus):                  0.000   Jarque-Bera (JB):             1040.967\nSkew:                           1.034   Prob(JB):                    9.06e-227\nKurtosis:                       5.873   Cond. No.                         29.4\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\n\n\n\n\n\n\n# R2_adj\nselected_vars, best_model, history = stepwise_selection(\n    df=df_for_stepwise,\n    target='ViolentCrimesPerPop',\n    strategy='forward',    # Ou 'backward'\n    metric='Cp',      # Ou 'AIC', 'BIC', 'Cp'\n    verbose=True\n)\nprint(best_model.summary())\nplot_stepwise_crosses(history, df_for_stepwise.columns.tolist(), metric=\"Cp\", title=\"Variable selections – Stepwise with Cp Mallows\")\n\ndf_clean[selected_vars + ['ViolentCrimesPerPop','fold']].to_csv(\"data/communities_data.csv\", index=False)\n\nSuppressions testées: [('PctKids2Par', 49.28), ('PctVacantBoarded', 83.05), ('PctWOFullPlumb', 93.9), ('LemasPctOfficDrugUn', 95.12), ('MedRentPctHousInc', 96.84), ('PctImmigRec10', 99.07), ('PopDens', 99.7), ('LandArea', 104.08), ('AsianPerCap', 105.65), ('PctSameHouse85', 105.65), ('PctUsePubTrans', 105.71), ('pctWFarmSelf', 105.73), ('PctWorkMom', 105.81), ('MedYrHousBuilt', 106.96), ('indianPerCap', 107.37), ('PctEmplProfServ', 107.71), ('MedOwnCostPctIncNoMtg', 107.95), ('PctEmplManu', 108.05), ('PctVacMore6Mos', 108.22)]\n\nÉtape 1: Meilleure variable à ajouter : PctKids2Par (score=49.2775)\nSuppressions testées: [('PctWorkMom', 47.64), ('LandArea', 47.64), ('LemasPctOfficDrugUn', 47.69), ('PctUsePubTrans', 48.11), ('PctVacantBoarded', 48.21), ('PopDens', 48.26), ('MedYrHousBuilt', 48.68), ('PctVacMore6Mos', 48.75), ('PctEmplManu', 48.82), ('PctImmigRec10', 48.82), ('AsianPerCap', 48.92), ('PctEmplProfServ', 49.01), ('MedRentPctHousInc', 49.02), ('MedOwnCostPctIncNoMtg', 49.05), ('indianPerCap', 49.25), ('pctWFarmSelf', 49.25), ('PctWOFullPlumb', 49.26), ('PctSameHouse85', 49.28)]\n\nÉtape 2: Meilleure variable à ajouter : PctWorkMom (score=47.64234)\nSuppressions testées: [('LandArea', 46.02), ('LemasPctOfficDrugUn', 46.23), ('MedYrHousBuilt', 46.83), ('PctUsePubTrans', 46.91), ('PctVacMore6Mos', 46.92), ('PopDens', 47.04), ('PctVacantBoarded', 47.05), ('PctImmigRec10', 47.25), ('PctEmplManu', 47.33), ('MedOwnCostPctIncNoMtg', 47.4), ('PctEmplProfServ', 47.41), ('AsianPerCap', 47.45), ('PctSameHouse85', 47.6), ('MedRentPctHousInc', 47.61), ('indianPerCap', 47.62), ('pctWFarmSelf', 47.64), ('PctWOFullPlumb', 47.68)]\n\nÉtape 3: Meilleure variable à ajouter : LandArea (score=46.01599)\nSuppressions testées: [('PopDens', 44.86), ('PctVacMore6Mos', 45.26), ('PctUsePubTrans', 45.28), ('LemasPctOfficDrugUn', 45.3), ('MedYrHousBuilt', 45.55), ('PctImmigRec10', 45.7), ('PctVacantBoarded', 45.76), ('PctEmplProfServ', 45.78), ('AsianPerCap', 45.81), ('MedOwnCostPctIncNoMtg', 45.83), ('PctEmplManu', 45.86), ('MedRentPctHousInc', 45.97), ('indianPerCap', 45.99), ('pctWFarmSelf', 46.01), ('PctSameHouse85', 46.03), ('PctWOFullPlumb', 46.05)]\n\nÉtape 4: Meilleure variable à ajouter : PopDens (score=44.86027)\nSuppressions testées: [('MedYrHousBuilt', 43.57), ('LemasPctOfficDrugUn', 44.49), ('PctVacMore6Mos', 44.51), ('MedOwnCostPctIncNoMtg', 44.57), ('PctVacantBoarded', 44.59), ('PctEmplProfServ', 44.66), ('AsianPerCap', 44.68), ('PctEmplManu', 44.71), ('PctImmigRec10', 44.74), ('PctUsePubTrans', 44.82), ('PctSameHouse85', 44.86), ('MedRentPctHousInc', 44.87), ('indianPerCap', 44.88), ('PctWOFullPlumb', 44.89), ('pctWFarmSelf', 44.9)]\n\nÉtape 5: Meilleure variable à ajouter : MedYrHousBuilt (score=43.57156)\nSuppressions testées: [('PctVacantBoarded', 42.94), ('LemasPctOfficDrugUn', 43.2), ('PctUsePubTrans', 43.26), ('PctSameHouse85', 43.29), ('AsianPerCap', 43.32), ('MedOwnCostPctIncNoMtg', 43.57), ('PctEmplManu', 43.57), ('PctEmplProfServ', 43.57), ('pctWFarmSelf', 43.59), ('PctVacMore6Mos', 43.6), ('indianPerCap', 43.61), ('PctImmigRec10', 43.61), ('MedRentPctHousInc', 43.61), ('PctWOFullPlumb', 43.61)]\n\nÉtape 6: Meilleure variable à ajouter : PctVacantBoarded (score=42.93717)\nSuppressions testées: [('LemasPctOfficDrugUn', 42.59), ('AsianPerCap', 42.67), ('PctUsePubTrans', 42.69), ('PctSameHouse85', 42.8), ('PctVacMore6Mos', 42.82), ('MedOwnCostPctIncNoMtg', 42.89), ('PctEmplProfServ', 42.93), ('pctWFarmSelf', 42.94), ('PctEmplManu', 42.95), ('PctImmigRec10', 42.97), ('PctWOFullPlumb', 42.97), ('indianPerCap', 42.97), ('MedRentPctHousInc', 42.98)]\n\nÉtape 7: Meilleure variable à ajouter : LemasPctOfficDrugUn (score=42.59134)\nSuppressions testées: [('AsianPerCap', 42.34), ('PctUsePubTrans', 42.4), ('PctSameHouse85', 42.43), ('PctVacMore6Mos', 42.51), ('MedOwnCostPctIncNoMtg', 42.55), ('PctEmplProfServ', 42.58), ('pctWFarmSelf', 42.61), ('PctEmplManu', 42.61), ('PctImmigRec10', 42.63), ('indianPerCap', 42.63), ('MedRentPctHousInc', 42.63), ('PctWOFullPlumb', 42.63)]\n\nÉtape 8: Meilleure variable à ajouter : AsianPerCap (score=42.34164)\nSuppressions testées: [('PctUsePubTrans', 42.21), ('PctVacMore6Mos', 42.25), ('PctSameHouse85', 42.26), ('MedOwnCostPctIncNoMtg', 42.28), ('PctEmplProfServ', 42.3), ('PctImmigRec10', 42.36), ('PctEmplManu', 42.36), ('pctWFarmSelf', 42.37), ('MedRentPctHousInc', 42.38), ('PctWOFullPlumb', 42.38), ('indianPerCap', 42.38)]\n\nÉtape 9: Meilleure variable à ajouter : PctUsePubTrans (score=42.2062)\nSuppressions testées: [('MedOwnCostPctIncNoMtg', 42.05), ('PctEmplProfServ', 42.12), ('PctVacMore6Mos', 42.12), ('PctSameHouse85', 42.14), ('PctImmigRec10', 42.23), ('MedRentPctHousInc', 42.24), ('pctWFarmSelf', 42.24), ('PctEmplManu', 42.25), ('indianPerCap', 42.25), ('PctWOFullPlumb', 42.25)]\n\nÉtape 10: Meilleure variable à ajouter : MedOwnCostPctIncNoMtg (score=42.0496)\nSuppressions testées: [('PctSameHouse85', 41.9), ('PctEmplProfServ', 41.93), ('PctVacMore6Mos', 42.03), ('MedRentPctHousInc', 42.08), ('pctWFarmSelf', 42.08), ('PctImmigRec10', 42.08), ('PctWOFullPlumb', 42.09), ('indianPerCap', 42.09), ('PctEmplManu', 42.09)]\n\nÉtape 11: Meilleure variable à ajouter : PctSameHouse85 (score=41.90327)\nSuppressions testées: [('PctVacMore6Mos', 41.84), ('PctEmplProfServ', 41.86), ('PctImmigRec10', 41.87), ('MedRentPctHousInc', 41.9), ('PctEmplManu', 41.93), ('pctWFarmSelf', 41.94), ('indianPerCap', 41.94), ('PctWOFullPlumb', 41.94)]\n\nÉtape 12: Meilleure variable à ajouter : PctVacMore6Mos (score=41.8416)\nSuppressions testées: [('PctEmplProfServ', 41.81), ('PctImmigRec10', 41.82), ('MedRentPctHousInc', 41.84), ('PctEmplManu', 41.86), ('indianPerCap', 41.88), ('PctWOFullPlumb', 41.88), ('pctWFarmSelf', 41.88)]\n\nÉtape 13: Meilleure variable à ajouter : PctEmplProfServ (score=41.81169)\nSuppressions testées: [('PctEmplManu', 41.75), ('PctImmigRec10', 41.77), ('MedRentPctHousInc', 41.78), ('indianPerCap', 41.85), ('pctWFarmSelf', 41.85), ('PctWOFullPlumb', 41.85)]\n\nÉtape 14: Meilleure variable à ajouter : PctEmplManu (score=41.7455)\nSuppressions testées: [('PctImmigRec10', 41.67), ('MedRentPctHousInc', 41.73), ('PctWOFullPlumb', 41.78), ('indianPerCap', 41.78), ('pctWFarmSelf', 41.79)]\n\nÉtape 15: Meilleure variable à ajouter : PctImmigRec10 (score=41.67325)\nSuppressions testées: [('MedRentPctHousInc', 41.66), ('indianPerCap', 41.71), ('PctWOFullPlumb', 41.71), ('pctWFarmSelf', 41.71)]\n\nÉtape 16: Meilleure variable à ajouter : MedRentPctHousInc (score=41.66073)\nSuppressions testées: [('indianPerCap', 41.7), ('PctWOFullPlumb', 41.7), ('pctWFarmSelf', 41.7)]\n\nÉtape 17: Meilleure variable à ajouter : indianPerCap (score=41.69895)\nAucune amélioration supplémentaire du score.\n\nVariables sélectionnées : ['PctKids2Par', 'PctWorkMom', 'LandArea', 'PopDens', 'MedYrHousBuilt', 'PctVacantBoarded', 'LemasPctOfficDrugUn', 'AsianPerCap', 'PctUsePubTrans', 'MedOwnCostPctIncNoMtg', 'PctSameHouse85', 'PctVacMore6Mos', 'PctEmplProfServ', 'PctEmplManu', 'PctImmigRec10', 'MedRentPctHousInc']\nScore final (Cp): 41.66073\n                             OLS Regression Results                            \n===============================================================================\nDep. Variable:     ViolentCrimesPerPop   R-squared:                       0.621\nModel:                             OLS   Adj. R-squared:                  0.618\nMethod:                  Least Squares   F-statistic:                     202.8\nDate:                 Sun, 05 Oct 2025   Prob (F-statistic):               0.00\nTime:                         21:51:58   Log-Likelihood:                 1044.4\nNo. Observations:                 1994   AIC:                            -2055.\nDf Residuals:                     1977   BIC:                            -1960.\nDf Model:                           16                                         \nCovariance Type:             nonrobust                                         \n=========================================================================================\n                            coef    std err          t      P&gt;|t|      [0.025      0.975]\n-----------------------------------------------------------------------------------------\nconst                     0.6121      0.041     14.766      0.000       0.531       0.693\nPctKids2Par              -0.7673      0.023    -33.369      0.000      -0.812      -0.722\nPctWorkMom               -0.0915      0.020     -4.534      0.000      -0.131      -0.052\nLandArea                  0.1854      0.033      5.564      0.000       0.120       0.251\nPopDens                   0.0991      0.024      4.078      0.000       0.051       0.147\nMedYrHousBuilt            0.1306      0.022      5.916      0.000       0.087       0.174\nPctVacantBoarded          0.1080      0.020      5.402      0.000       0.069       0.147\nLemasPctOfficDrugUn       0.0591      0.015      3.868      0.000       0.029       0.089\nAsianPerCap               0.0611      0.018      3.374      0.001       0.026       0.097\nPctUsePubTrans            0.0609      0.020      2.990      0.003       0.021       0.101\nMedOwnCostPctIncNoMtg    -0.0681      0.020     -3.393      0.001      -0.108      -0.029\nPctSameHouse85            0.1011      0.027      3.693      0.000       0.047       0.155\nPctVacMore6Mos           -0.0437      0.023     -1.941      0.052      -0.088       0.000\nPctEmplProfServ          -0.0789      0.024     -3.297      0.001      -0.126      -0.032\nPctEmplManu              -0.0475      0.021     -2.298      0.022      -0.088      -0.007\nPctImmigRec10             0.0496      0.021      2.356      0.019       0.008       0.091\nMedRentPctHousInc         0.0370      0.023      1.615      0.107      -0.008       0.082\n==============================================================================\nOmnibus:                      389.964   Durbin-Watson:                   2.035\nProb(Omnibus):                  0.000   Jarque-Bera (JB):             1040.967\nSkew:                           1.034   Prob(JB):                    9.06e-227\nKurtosis:                       5.873   Cond. No.                         29.4\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
  },
  {
    "objectID": "00_tds/1_fourrier_transformation.html",
    "href": "00_tds/1_fourrier_transformation.html",
    "title": "Fourrier Transformation",
    "section": "",
    "text": "When we were in our third year of the Risk Management program at an engineering school, we took a course on the calibration of stochastic processes. To assess our understanding, the instructor gave us a selection of academic papers and asked us to choose one, study it thoroughly, and reproduce its results.\nWe chose the paper by El Kolei (2013), which proposes a method for estimating the parameters of stochastic volatility models based on contrast minimization and deconvolution.\nTo do so, we had to implement an optimization algorithm that relies on the Fourier transform to evaluate the following function:\n\\[\n\\hat{f}(\\nu) = \\int_{-\\infty}^{+\\infty} f_{\\theta}(t) e^{-2i\\pi\\nu t} dt, \\quad\\text{for all } \\nu \\in \\mathbb{R}\n\\] Where \\(f_{\\theta}(y) = \\frac{1}{2\\sqrt{\\pi}} \\left(\\frac{-i \\phi y \\gamma^2 \\exp\\left(-\\frac{y^2}{2} \\gamma^2\\right)}{\\exp\\left(-i \\mathcal{E} y\\right) 2^{iy} \\Gamma\\left(\\frac{1}{2} + iy\\right)}\\right)\\) and \\(\\theta = (\\phi, \\gamma, \\mathcal{E})\\) is the vector of parameters to be estimated.\nTo compute the Fourier transform of \\(f\\), El Kolei (2013) uses the left Riemann sum (rectangle quadrature) method implemented in MATLAB and suggests that using the Fast Fourier Transform (FFT) algorithm could accelerate the computation of the Fourier transform.\nWe decided to reproduce the paper using Python. Implementing the left rectangle quadrature method was straightforward, and we initially believed that the scipy.fft and numpy.fft libraries would allow us to compute the Fourier transform of \\(f\\) using the FFT algorithm.\nHowever, we were surprised to discover that these functions do not compute the Fourier transform of a continuous function but rather the discrete Fourier transform (DFT) of a finite sequence. The plot below illustrates this observation.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.fft import fft, fftfreq, fftshift\n\n# Define the function f(t) = exp(-pi * t^2)\ndef f(t):\n    return np.exp(-np.pi * t**2)\n\n# Parameters\nN = 1024\nT = 1.0 / 64\nt = np.linspace(-N/2*T, N/2*T, N, endpoint=False)\ny = f(t)\n\n# FFT with scipy\nyf_scipy = fftshift(fft(y)) * T\nxf = fftshift(fftfreq(N, T))\nFT_exact = np.exp(-np.pi * xf**2)\n\n# FFT with numpy\nyf_numpy = np.fft.fftshift(np.fft.fft(y)) * T\nxf_numpy = np.fft.fftshift(np.fft.fftfreq(N, T))\n\n# Plot with subplot_mosaic\nfig, axs = plt.subplot_mosaic([[\"scipy\", \"numpy\"]], figsize=(7, 5), layout=\"constrained\", sharey=True)\n\n# Scipy FFT\naxs[\"scipy\"].plot(xf, FT_exact, 'k-', linewidth=1.5, label='Exact FT')\naxs[\"scipy\"].plot(xf, np.real(yf_scipy), 'r--', linewidth=1, label='FFT (scipy)')\naxs[\"scipy\"].set_xlim(-6, 6)\naxs[\"scipy\"].set_ylim(-1, 1)\naxs[\"scipy\"].set_title(\"Scipy FFT\")\naxs[\"scipy\"].set_xlabel(\"Frequency\")\naxs[\"scipy\"].set_ylabel(\"Amplitude\")\naxs[\"scipy\"].legend()\naxs[\"scipy\"].grid(False)\n\n# NumPy FFT\naxs[\"numpy\"].plot(xf_numpy, FT_exact, 'k-', linewidth=1.5, label='Exact FT')\naxs[\"numpy\"].plot(xf_numpy, np.real(yf_numpy), 'b--', linewidth=1, label='FFT (numpy)')\naxs[\"numpy\"].set_xlim(-6, 6)\naxs[\"numpy\"].set_title(\"NumPy FFT\")\naxs[\"numpy\"].set_xlabel(\"Frequency\")\naxs[\"numpy\"].legend()\naxs[\"numpy\"].grid(False)\n\nplt.suptitle(\"Comparison of FFT Implementations vs. Exact Fourier Transform\", fontsize=14)\nplt.show()\n\n\n\n\n\n\n\n\nThis motivation led us to write this article to explain how to compute the Fourier transform of a function using both the left Riemann sum (rectangle quadrature) method and the Fast Fourier Transform (FFT) algorithm.\nThe paper by El Kolei (2013) illustrates an application of the Fourier transform in finance and ecology. Beyond these domains, the Fourier transform plays a key role in spectral analysis, solving partial differential equations, evaluating integrals, and computing series sums. It is widely used in physics, engineering, and signal processing.\nIn the literature, several articles discuss approximating the Fourier transform and implementing it using numerical methods. However, we have not found a source as clear and comprehensive as the one by Balac (2011), which proposes a quadrature-based approach to compute the Fourier transform and explains how to use the FFT algorithm to compute the discrete Fourier transform.\nWe use Balac (2011) as our primary reference to show how to approximate the Fourier transform in Python using both methods: the left rectangle quadrature method and the Fast Fourier Transform (FFT) algorithm.\n\n\nWe adopt the framework presented by Balac (2011) to define the Fourier transform of a function \\(f\\) and to introduce its properties.\nThe functions considered belong to the space of integrable functions, denoted \\(L^1(\\mathbb{R,K})\\), consisting of all functions \\(f:\\mathbb{R} \\to \\mathbb{K}\\) [where \\(\\mathbb{K}\\) is either \\(\\mathbb{R}\\) or \\(\\mathbb{C}\\)]. These functions are integrable in the sense of Lebesgue, meaning that the integral of their absolute value is finite: \\[\n\\int_{-\\infty}^{+\\infty} |f(t)| dt &lt; +\\infty.\n\\]\nSo, for a function \\(f\\) to belong to \\(L^1(\\mathbb{R}, \\mathbb{K})\\), the product \\(f(t) \\cdot e^{-2i\\pi\\nu t}\\) must also be integrable for all \\(\\nu \\in \\mathbb{R}\\). In that case, the Fourier transform of \\(f\\), denoted \\(\\hat{f}\\) (or sometimes \\(\\mathcal{F}(f)\\)), is defined for all \\(\\nu \\in \\mathbb{R}\\) by:\n\\[\n\\hat{f}(\\nu) = \\int_{-\\infty}^{+\\infty}\nf(t) \\, e^{-2i\\pi\\nu t} \\, dt.\n\\]\nThe key takeaway is that the Fourier transform \\(\\hat{f}\\) of a function \\(f\\) is a complex-valued, linear function that depends on the frequency \\(\\nu\\). If \\(f \\in L^1(\\mathbb{R})\\), is real-valued and even, then \\(\\hat{f}\\) is also real-valued and even. Conversely, if \\(f\\) is real-valued and odd, then \\(\\hat{f}\\) is purely imaginary and odd as well.\nFor some functions, the Fourier transform can be computed analytically. For example, for the function \\(f : t \\in \\mathbb{R} \\mapsto \\mathbb{1}_{[-\\frac{a}{2}, \\frac{a}{2}]}(t)\\), the Fourier transform is given by: \\[\n\\hat{f} : \\nu \\in \\mathbb{R} \\mapsto a sinc(a \\pi \\nu)\n\\] where \\(sinc(t) = \\frac{\\sin(t)}{t}\\) is the sinc function.\nHowever, for many functions, the Fourier transform cannot be computed analytically. In such cases, we can use numerical methods to approximate it. We will explore these numerical approaches in the following sections of this article.\n\n\n\nThe Fourier transform of a function \\(f\\) is defined as an integral over the entire real line. However, for the functions that are integral in the sense of lebesgue and that have a practical applications tend to 0 as \\(|t| \\to +\\infty\\). And we can approximate the Fourier transform by integrating over a finite interval \\([-T, T]\\). If the lenght of the interval is large enough, or if the function decays quickly when t tends to infinity, this approximation will be accurate.\n\\[\n\\hat{f}(\\nu) \\approx \\int_{-T}^{T} f(t) e^{-2i\\pi\\nu t} dt\n\\]\nIn his article, Balac (2011) goes further by showing that approximating the Fourier transform involves three key mathematical tools:\n\nFourier series, in the context of a periodic signal,\nThe Fourier transform, for non-periodic signals,\nThe discrete Fourier transform, for discrete signals.\n\nFor each of these tools, computing the Fourier transform essentially comes down to evaluating the following integral: \\[\n\\hat{f}(\\nu) \\approx \\int_{-\\frac{T}{2}}^{\\frac{T}{2}} f(t) e^{-2i\\pi\\nu t} dt\n\\] I recommend reading his article for more details on these three tools. Next, we will focus on the numerical computation of the Fourier transform using quadrature methods, a technique for performing numerical integration.\n\n\n\nWe show that computing the Fourier transform of a function \\(f\\) consists to approximating it by the integral the following integral over the interval \\([-\\frac{T}{2}, \\frac{T}{2}]\\): \\[\n\\boxed{\n\\underbrace{\n  \\int_{-\\infty}^{+\\infty} f(t)\\, e^{-2\\pi i \\nu t} \\, dt\n}_{=\\,\\hat{f}(\\nu)}\n\\approx\n\\underbrace{\n  \\int_{-\\frac{T}{2}}^{\\frac{T}{2}} f(t)\\, e^{-2\\pi i \\nu t} \\, dt\n}_{=\\,\\tilde{S}(\\nu)}\n}\n\\]\nwhere T is a large enough number such that the integral converges. Une valeur approchée of the integral \\(\\tilde{S}(\\nu) = \\int_{-\\frac{T}{2}}^{\\frac{T}{2}} f(t)\\, e^{-2\\pi i \\nu t} \\, dt\\) can be computed using quadrature methods. In the next section, we will approximate the integral using the quadrature method of rectangles à gauche.\n\n\n\nTo compute the integral \\(\\tilde{S}(\\nu) = \\int_{-\\frac{T}{2}}^{\\frac{T}{2}} f(t)\\, e^{-2\\pi i \\nu t} \\, dt\\), using the quadrature method of left rectangles, we follow these steps:\n\nDiscretization of the Interval:: We divide the interval \\([-\\frac{T}{2}, \\frac{T}{2}]\\) into \\(N\\) uniform subintervals of length \\(h_t = \\frac{T}{N}\\). The discretization points[eft endpoints of the rectangles] in the interval are given by:\n\n\\[\nt_k = -\\frac{T}{2} + k \\cdot h_t, \\quad k = 0, 1, \\ldots, N-1.\n\\]\n\nApproximation of the Integral: Using the Chasles relation, we can approximate the integral \\(\\tilde{S}(\\nu)\\) as follows:\n\n\\[\n\\tilde{S}(\\nu) = \\int_{-\\frac{T}{2}}^{\\frac{T}{2}} f(t)\\, e^{-2\\pi i \\nu t} \\, dt = \\sum_{k=0}^{N-1}  \\int_{t_k}^{t_{k+1}} f(t)\\, e^{-2\\pi i \\nu t} \\, dt.\n\\]\nBy taking into account that we have \\(t_{k+1} - t_k = h_t\\), and \\(t_k = -\\frac{T}{2} + k \\cdot h_t = T(\\frac{k}{N} - \\frac{1}{2})\\), we can rewrite the integral as: \\[\n\\boxed{\n\\tilde{S}(\\nu) = \\sum_{k=0}^{N-1} f(t_k) e^{-2\\pi i \\nu t_k} h_t.\n}\n\\]\nWe call it the quadrature method of left rectangles because it uses the left endpoint \\(t_k\\) of each subinterval to approximate the value of the function \\(f(t)\\) at that point.\n\nFinal Formula: The final formula for the approximation of the Fourier transform is given by:\n\n\\[\n\\boxed{\n\\forall \\nu \\in \\mathbb{R} \\quad\n\\underbrace{\n\\int_{-\\frac{T}{2}}^{\\frac{T}{2}} f(t)\\, e^{-2\\pi i \\nu t} \\, dt\n}_{=\\,\\tilde{S}(\\nu)}\n\\approx\n\\underbrace{\n\\frac{T}{n} e^{i \\pi \\nu T} \\sum_{k=0}^{n-1} f_k\\, e^{-2 i \\pi \\nu T k / n}\n}_{=\\,\\tilde{S}_n(\\nu)}\n\\quad \\text{où } f_k = f\\left( \\frac{2k - n}{2n} T \\right).\n}\n\\]\n\n\nThe function tfquad below implements the left rectangle quadrature method to compute the Fourier transform of a function f at a given frequency nu.\n\nimport numpy as np\n\ndef tfquad(f, nu, n, T):\n    \"\"\"\n    Computes the Fourier transform of a function f at frequency nu\n    using left Riemann sum quadrature over the interval [-T/2, T/2].\n\n    Parameters:\n    ----------\n    f : callable\n        The function to transform. Must accept a NumPy array as input.\n    nu : float\n        The frequency at which to evaluate the Fourier transform.\n    n : int\n        Number of quadrature points.\n    T : float\n        Width of the time window [-T/2, T/2].\n\n    Returns:\n    -------\n    tfnu : complex\n        Approximated value of the Fourier transform at frequency nu.\n    \"\"\"\n    k = np.arange(n)\n    t_k = (k / n - 0.5) * T\n    weights = np.exp(-2j * np.pi * nu * T * k / n)\n    prefactor = (T / n) * np.exp(1j * np.pi * nu * T)\n\n    return prefactor * np.sum(f(t_k) * weights)\n\nWe can also use SciPy’s quad function to define the Fourier transform of a function f at a given frequency nu. The function tf_integral below implements this approach. It uses numerical integration to compute the Fourier transform of f over the interval \\([-T/2, T/2]\\).\n\nfrom scipy.integrate import quad\n\ndef tf_integral(f, nu, T):\n    \"\"\"Compute FT of f at frequency nu over [-T/2, T/2] using scipy quad.\"\"\"\n    real_part = quad(lambda t: np.real(f(t) * np.exp(-2j * np.pi * nu * t)), -T/2, T/2)[0]\n    imag_part = quad(lambda t: np.imag(f(t) * np.exp(-2j * np.pi * nu * t)), -T/2, T/2)[0]\n    return real_part + 1j * imag_part\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# ----- Function Definitions -----\n\ndef f(t):\n    \"\"\"Indicator function on [-1, 1].\"\"\"\n    return np.where(np.abs(t) &lt;= 1, 1.0, 0.0)\n\ndef exact_fourier_transform(nu):\n    \"\"\"Analytical FT of the indicator function over [-1, 1].\"\"\"\n    # f̂(ν) = ∫_{-1}^{1} e^{-2πiνt} dt = 2 * sinc(2ν)\n    return 2 * np.sinc(2 * nu)\n\n\n# ----- Computation -----\n\nT = 2.0\nn = 32\nnu_vals = np.linspace(-6, 6, 500)\nexact_vals = exact_fourier_transform(nu_vals)\ntfquad_vals = np.array([tfquad(f, nu, n, T) for nu in nu_vals])\n\n# Compute the approximation using scipy integral\ntf_integral_vals = np.array([tf_integral(f, nu, T) for nu in nu_vals])\n\n# ----- Plotting -----\nfig, axs = plt.subplot_mosaic([[\"tfquad\", \"quad\"]], figsize=(7.24, 4.07), dpi=100, layout=\"constrained\")\n\n# Plot using tfquad implementation\naxs[\"tfquad\"].plot(nu_vals, np.real(exact_vals), 'b-', linewidth=2, label=r'$\\hat{f}$ (exact)')\naxs[\"tfquad\"].plot(nu_vals, np.real(tfquad_vals), 'r--', linewidth=1.5, label=r'approximation $\\hat{S}_n$')\naxs[\"tfquad\"].set_title(\"TF avec tfquad (rectangles)\")\naxs[\"tfquad\"].set_xlabel(r'$\\nu$')\naxs[\"tfquad\"].grid(False)\naxs[\"tfquad\"].set_ylim(-0.5, 2.1)\n\n# Plot using scipy.integrate.quad\naxs[\"quad\"].plot(nu_vals, np.real(exact_vals), 'b-', linewidth=2, label=r'$\\hat{f}$ (quad)')\naxs[\"quad\"].plot(nu_vals, np.real(tf_integral_vals), 'r--', linewidth=1.5, label=r'approximation $\\hat{S}_n$')\naxs[\"quad\"].set_title(\"TF avec scipy.integrate.quad\")\naxs[\"quad\"].set_xlabel(r'$\\nu$')\naxs[\"quad\"].set_ylabel('Amplitude')\naxs[\"quad\"].grid(False)\naxs[\"quad\"].set_ylim(-0.5, 2.1)\n\n\n\n# --- Global legend below the plots ---\n# Take handles from one subplot only (assumes labels are consistent)\nhandles, labels = axs[\"quad\"].get_legend_handles_labels()\nfig.legend(handles, labels,\n           loc='lower center', bbox_to_anchor=(0.5, -0.05),\n           ncol=3, frameon=False)\n\nplt.suptitle(\"Comparison of FFT Implementations vs. Exact Fourier Transform\", fontsize=14)\n\nplt.show()\n\n\n\n\n\n\n\n\nWe are now able to compute the Fourier transform of a function using the left rectangle quadrature method. Let’s take a closer look at the characteristics and limitations of this approximation.\n\n\n\n\n\n\nBalac (2011) highlights that the Fourier transform \\(\\hat{f}\\) of the function \\(f\\) is inherently oscillatory. This behavior arises from the complex exponential term \\(e^{-2\\pi i \\nu t}\\) in the integral definition of the transform.\nTo illustrate this, the figure below shows the function\n\\[\nf : t \\in \\mathbb{R} \\mapsto e^{-t^2} \\in \\mathbb{R}\n\\]\nalong with the real and imaginary parts of its Fourier transform\n\\[\n\\hat{f} : \\nu \\in \\mathbb{R} \\mapsto \\hat{f}(\\nu) \\in \\mathbb{C},\n\\]\nevaluated at \\(\\nu = \\frac{5}{2}\\).\nAlthough \\(f\\) is smooth, we clearly observe strong oscillations in \\(\\hat{f}\\), highlighting the influence of the exponential kernel.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nnu = 5 / 2\nt1 = np.linspace(-8, 8, 1000)\nt2 = np.linspace(-4, 4, 1000)\n\nf = lambda t: np.exp(-t**2)\nphi = lambda t: f(t) * np.exp(-2j * np.pi * nu * t)\n\nf_vals = f(t1)\nphi_vals = phi(t2)\n\n# Plot\nfig, axs = plt.subplots(1, 2, figsize=(7.24, 4.07), dpi=100)\n\naxs[0].plot(t1, f_vals, 'k', linewidth=2)\naxs[0].set_xlim(-8, 8)\naxs[0].set_ylim(0, 1)\naxs[0].set_title(r\"$f(t) = e^{-t^2}$\")\naxs[0].grid(True)\n\naxs[1].plot(t2, np.real(phi_vals), 'b', label=r\"$\\Re(\\phi)$\", linewidth=2)\naxs[1].plot(t2, np.imag(phi_vals), 'r', label=r\"$\\Im(\\phi)$\", linewidth=2)\naxs[1].set_xlim(-4, 4)\naxs[1].set_ylim(-1, 1)\naxs[1].set_title(r\"$\\phi(t) = f(t)e^{-2i\\pi\\nu t}$, $\\nu=5/2$\")\naxs[1].legend()\naxs[1].grid(True)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nThese rapid variations can create difficulties in the numerical approximation of the Fourier transform using quadrature methods, even when a large number of points \\(n\\) is used. One way to overcome this issue is by using the Fast Fourier Transform (FFT) algorithm.\n\n\n\n\nWe observe that even if the function \\(f\\) is not periodic, its Fourier transform approximation \\(\\hat{f}\\) appears periodic. In fact, the function \\(\\hat{S}_n\\), resulting from the quadrature method, is periodic with period\n\\[\n\\Delta \\nu = \\frac{n}{T}.\n\\] \\[\n\\forall \\nu \\in \\mathbb{R} \\quad\n\\widehat{S}_n\\left(\\nu + \\frac{n}{T} \\right)\n= \\frac{T}{n} e^{i\\pi \\nu T} \\sum_{k=0}^{n-1} f_k \\, e^{-2i\\pi\\left(\\nu + \\frac{n}{T} \\right)T \\frac{k}{n}}\n= \\frac{T}{n} e^{i\\pi \\nu T} \\sum_{k=0}^{n-1} f_k \\, e^{-2i\\pi \\nu T \\frac{k}{n}} \\underbrace{e^{-2i\\pi k}}_{=1}\n\\]\n\\[\n\\phantom{\\forall \\nu \\in \\mathbb{R} \\quad\n\\widehat{S}_n\\left(\\nu + \\frac{n}{T} \\right)}\n= \\frac{T}{n} e^{i\\pi \\nu T} \\sum_{k=0}^{n-1} f_k \\, e^{-2i\\pi \\nu T \\frac{k}{n}} = \\widehat{S}_n(\\nu).\n\\]\nThis periodicity of \\(\\hat{S}_n\\) implies that it is not possible to compute the Fourier transform for all frequencies \\(\\nu \\in \\mathbb{R}\\) using the quadrature method when the parameters \\(T\\) and \\(n\\) are fixed. In fact, it becomes impossible to compute \\(\\hat{f}(\\nu)\\) accurately when\n\\[\n|\\nu| \\geq \\nu_{\\text{max}},\n\\]\nwhere \\(\\nu_{\\text{max}} = \\frac{n}{T}\\) is the maximum frequency that can be resolved due to the periodic nature of \\(\\hat{S}_n\\).\nAs a result, in practice, to compute the Fourier transform for large frequencies, one must increase either the time window \\(T\\) or the number of points \\(n\\).\nFurthermore, by evaluating the error in approximating \\(\\hat{f}(\\nu)\\) using the left rectangle quadrature method, we find that the approximation is reliable at frequency \\(\\nu\\) when the following condition holds:\n\\[\n|\\nu| \\ll \\frac{n}{T}\n\\]\n\\[\n\\frac{\\nu T}{n} \\ll 1.\n\\]\nEpstein (2005) shows that when using the Fast Fourier Transform (FFT) algorithm, it is possible to accurately compute the Fourier transform of a function \\(f\\) for all frequencies \\(\\nu \\in \\mathbb{R}\\), even when \\(\\frac{\\nu T}{n}\\) is close to 1 — provided that \\(f\\) is piecewise continuous and has compact support.\n\n\n\nIn this section, we denote by \\(\\hat{S}_n(\\nu)\\) the approximation of the Fourier transform \\(\\hat{f}(\\nu)\\) of the function \\(f\\) at a point \\(\\nu \\in \\left[-\\frac{\\nu_{\\text{max}}}{2}, \\frac{\\nu_{\\text{max}}}{2}\\right]\\), where \\(\\nu_{\\text{max}} = \\frac{n}{T}\\), i.e., \\[\n\\boxed{\n\\hat{f}(\\nu) \\approx \\hat{S}_n(\\nu) = \\frac{T}{n} e^{i\\pi \\nu T} \\sum_{k=0}^{n-1} f_k\\, e^{-2 i \\pi \\nu T k / n}.\n}\n\\]\nWe now present the Fourier transform algorithm used to approximate \\(\\hat{f}(\\nu)\\). We will not go into the details of the Fast Fourier Transform (FFT) algorithm in this article. Balac (2011) provides a simplified explanation of the FFT, and for more in-depth technical details, we recommend the original article by Cooley and Tukey (1965).\nWhat is important to understand is that the use of the FFT algorithm to approximate the Fourier transform of a function \\(f\\) is based on the result shown by Epstein (2005), which states that when \\(\\hat{S}_n(\\nu)\\) is evaluated at the frequencies \\(\\nu_j = \\frac{j}{T}\\) for \\(j = 0, 1, \\dots, n - 1\\), it gives a good approximation of the continuous Fourier transform \\(\\hat{f}(\\nu)\\).\nMoreover, \\(\\hat{S}_n\\) is known to be periodic. This periodicity gives a symmetric role to the indices \\(j \\in \\{0, 1, \\dots, n - 1\\}\\) and \\(k \\in \\{-\\frac{n}{2}, -\\frac{n}{2} + 1, \\dots, -1\\}\\). In fact, the values of the Fourier transform of \\(f\\) over the interval \\(\\left[ -\\frac{\\nu_{\\text{max}}}{2}, \\frac{\\nu_{\\text{max}}}{2} \\right]\\) can be derived from the values of \\(\\hat{S}_n\\) at the points \\(\\nu_j = \\frac{j}{T}\\), for \\(j = 0, 1, \\dots, n - 1\\), as follows:\n\\[\n\\widehat{S}_n(\\nu'_j) = \\frac{T}{n} (-1)^j \\sum_{k=0}^{n-1} f_k\\, e^{-2i\\pi j \\frac{k}{n}}\n=\n\\begin{cases}\n\\widehat{S}_n(\\nu_j) & \\text{si } j \\in \\left\\{0, \\dots, \\frac{n}{2} - 1 \\right\\} \\\\\n\\widehat{S}_n(\\nu_{j-n}) & \\text{si } j \\in \\left\\{ \\frac{n}{2}, \\dots, n-1 \\right\\}\n\\end{cases}\n\\]\n\\[\n\\text{where we used the relation} \\quad\ne^{-2i\\pi j \\frac{k}{n}} = e^{-2i\\pi (j-n) \\frac{k}{n}} \\times \\underbrace{e^{-2i\\pi k}}_{=1}\n= e^{-2i\\pi (j-n) \\frac{k}{n}}\n\\quad \\text{for } j \\in \\left\\{ \\frac{n}{2}, \\dots, n-1 \\right\\}.\n\\]\nThis relationship shows that we can compute the Fourier transform \\(\\hat{S}_n\\left( \\frac{j}{T} \\right)\\) for \\(j = -\\frac{n}{2}, \\ldots, \\frac{n}{2} - 1\\). Moreover, when \\(n\\) is a power of 2, it can be shown that the computation becomes faster (see Balac 2011). This process is known as the Fast Fourier Transform (FFT).\nTo summarize, we have shown that the Fourier transform of the function \\(f\\) can be approximated over the interval \\(\\left[-\\frac{T}{2}, \\frac{T}{2}\\right]\\) at the frequencies \\(\\nu_j = \\frac{j}{T}\\) for \\(j = -\\frac{n}{2}, \\ldots, \\frac{n}{2} - 1\\), where \\(n = 2^m\\) for some integer \\(m \\geq 0\\), by applying the FFT algorithm as follows:\n\nConstruct the finite sequence \\(F\\) of values \\(f\\left( \\frac{2k - n}{2n} T \\right)\\) for \\(k = 0, 1, \\ldots, n - 1\\).\nCompute the discrete Fourier transform \\(\\hat{F}\\) of the sequence \\(F\\) using the Fast Fourier Transform (FFT) algorithm, which is given by\n\\[\n\\hat{F}_j = \\sum_{k=0}^{n-1} f_k \\, e^{-2i\\pi \\frac{jk}{n}}, \\quad \\text{for } j = 0, 1, \\ldots, n-1.\n\\]\nReindex and symmetrize the values to span \\(j = -\\frac{n}{2}, \\ldots, -1\\).\nMultiply each value in the array by \\(\\frac{T}{n} (-1)^{j-1}\\), where \\(j \\in \\{1, \\ldots, n\\}\\).\n\nThis yields an array corresponding to the values of the Fourier transform \\(\\hat{f}(\\nu_j)\\), with \\(\\nu_j = \\frac{j}{T}\\) for \\(j = -\\frac{n}{2}, \\ldots, \\frac{n}{2} - 1\\).\nThe following Python function tffft implements these steps to compute the Fourier transform of a given function.\n\nimport numpy as np\nfrom scipy.fft import fft, fftshift\n\ndef tffft(f, T, n):\n    \"\"\"\n    Calcule la transformée de Fourier approchée d'une fonction f à support dans [-T/2, T/2],\n    en utilisant l’algorithme FFT.\n\n    Paramètres\n    ----------\n    f : callable\n        Fonction à transformer (doit être vectorisable avec numpy).\n    T : float\n        Largeur de la fenêtre temporelle (intervalle [-T/2, T/2]).\n    n : int\n        Nombre de points de discrétisation (doit être une puissance de 2 pour FFT efficace).\n\n    Retours\n    -------\n    tf : np.ndarray\n        Valeurs approximées de la transformée de Fourier aux fréquences discrètes.\n    freq_nu : np.ndarray\n        Fréquences discrètes correspondantes (de -n/(2T) à (n/2 - 1)/T).\n    \"\"\"\n    h = T / n\n    t = -0.5 * T + np.arange(n) * h  # noeuds temporels\n    F = f(t)                         # échantillonnage de f\n    tf = h * (-1) ** np.arange(n) * fftshift(fft(F))  # TF approximée\n    freq_nu = -n / (2 * T) + np.arange(n) / T              # fréquences ν_j = j/T\n\n    return tf, freq_nu, t\n\nThe following program illustrates how to compute the Fourier transform of the Gaussian function \\(f(t) = e^{-10t^2}\\) over the interval \\([-10, 10]\\), using the Fast Fourier Transform (FFT) algorithm.\n\n# Parameters\na = 10\nf = lambda t: np.exp(-a * t**2)\nT = 10\nn = 2**8  # 256\n\n# Compute the Fourier transform using FFT\ntf, nu, t = tffft(f, T, n)\n\n# Plotting\nfig, axs = plt.subplots(1, 2, figsize=(7.24, 4.07), dpi=100)\n\naxs[0].plot(t, f(t), '-g', linewidth=3)\naxs[0].set_xlabel(\"time\")\naxs[0].set_title(\"Considered Function\")\naxs[0].set_xlim(-6, 6)\naxs[0].set_ylim(-0.5, 1.1)\naxs[0].grid(True)\n\naxs[1].plot(nu, np.abs(tf), '-b', linewidth=3)\naxs[1].set_xlabel(\"frequency\")\naxs[1].set_title(\"Fourier Transform using FFT\")\naxs[1].set_xlim(-15, 15)\naxs[1].set_ylim(-0.5, 1)\naxs[1].grid(True)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nThe method we have just presented allows us to compute and visualize the Fourier transform of a function \\(f\\) at discrete points \\(\\nu_j = \\frac{j}{T}\\), for \\(j = -\\frac{n}{2}, \\ldots, \\frac{n}{2} - 1\\), where \\(n\\) is a power of 2. These points lie within the interval \\(\\left[-\\frac{n}{2T}, \\frac{n}{2T}\\right]\\). However, this approach does not allow us to evaluate the Fourier transform at arbitrary points \\(\\nu\\) in the same interval when \\(\\nu\\) is not of the form \\(\\nu_j = \\frac{j}{T}\\).\nTo compute the Fourier transform of a function \\(f\\) at a point \\(\\nu\\) that does not coincide with one of the sampling frequencies \\(\\nu_j\\), interpolation methods can be used (e.g., linear, polynomial, spline interpolation, etc.). In this article, we adopt the approach proposed by Balac (2011), which relies on Shannon’s interpolation theorem to compute the Fourier transform of a function \\(f\\) at any point \\(\\nu\\).\n\n\n\nQue nous dit le théorème de Shannon ? Il nous dit que pour une fonction \\(g\\) à bande limitée c’est-à-dire dont la transformée de Fourier \\(\\hat{g}\\) est nulle en dehors d’un intervalle \\([-\\frac{B}{2}, \\frac{B}{2}]\\), on peut reconstruire la fonction \\(g\\) à partir de ses échantillons \\(g_k = g\\left(\\frac{k}{B}\\right)\\) pour \\(k \\in \\mathbb{Z}\\). Si on note \\(\\nu_c\\) le plus petit réel positif tel que \\(\\hat{g}\\) est nulle en déhors de l’intervalle \\([-2 \\pi \\nu_c, 2 \\pi \\nu_c]\\), alors on a la formule d’interpolation de Shannon : Pour tout \\(t \\in \\mathbb{R}\\), et \\(\\alpha\\) un réel positif, vérifiant \\(\\alpha \\geq \\frac{1}{2 \\nu_c}\\), on a :\n\\[\ng(t) = \\sum_{k \\in \\mathbb{Z}} g(k\\alpha)\\, \\text{sinc}\\left(\\frac{\\pi}{\\alpha}(t - k\\alpha)\\right)\n\\]\noù \\(\\text{sinc}(x) = \\frac{\\sin(x)}{x}\\) est la fonction sinus cardinal sinc.\nBalac (2011) montre que lorsque la fonction \\(f\\) est à support borné dans un intervalle \\([-T/2, T/2]\\), on peut utiliser le théorème d’interpolation de Shannon pour calculer la transformée de Fourier \\(\\hat{f}(\\nu)\\) pour tout \\(\\nu \\in \\mathbb{R}\\) en utilisant les valeurs de la transformée de Fourier discrète \\(\\hat{S}_n(\\nu_j)\\) pour \\(j = -\\frac{n}{2}, \\ldots, \\frac{n}{2} - 1\\). Pour ce faire, il considère \\(\\alpha = \\frac{1}{T}\\) et obtient pour tout \\(\\nu \\in \\mathbb{R}\\) la formule d’interpolation de Shannon suivante : \\[\n\\hat{f}(\\nu) = \\sum_{j=-\\frac{n}{2}}^{\\frac{n}{2}-1} \\hat{S}_n\\left(\\frac{j}{T}\\right)\\, \\text{sinc}\\left(\\pi T\\left(\\nu - \\frac{j}{T}\\right)\\right)\n\\]\nLe programme ci-dessous illustre l’utilisation du théorème d’interpolation de Shannon pour calculer la transformée de Fourier d’une fonction \\(f\\) en un point \\(\\nu\\).\n\nimport numpy as np\n\ndef shannon(tf, nu, T):\n    \"\"\"\n    Approximates the value of the Fourier transform of function f at frequency 'nu'\n    using its discrete values computed from the FFT.\n\n    Parameters:\n    - tf : numpy array, discrete Fourier transform values (centered with fftshift) at frequencies j/T for j = -n/2, ..., n/2 - 1\n    - nu : float, frequency at which to approximate the Fourier transform\n    - T  : float, time window width used for the FFT\n\n    Returns:\n    - tfnu : approximation of the Fourier transform at frequency 'nu'\n    \"\"\"\n    n = len(tf)\n    tfnu = 0.0\n    for j in range(n):\n        k = j - n // 2  # correspond à l'indice j dans {-n/2, ..., n/2 - 1}\n        tfnu += tf[j] * np.sinc(T * nu - k)  # np.sinc(x) = sin(pi x)/(pi x) en numpy\n\n    return tfnu\n\nEnfin, nous pouvons definir la\n\ndef fourier_at_nu(f, T, n, nu):\n    \"\"\"\n    Computes the Fourier transform of f at frequency nu\n    using tffft + Shannon interpolation.\n    \"\"\"\n    tf, _, _ = tffft(f, T, n)\n    return shannon(tf, nu, T)\n\n\na = 0.5\nf = lambda t: np.exp(-a * np.abs(t))                          # Fonction à transformer\nfhat_exact = lambda nu: (2 * a) / (a**2 + 4 * np.pi**2 * nu**2)  # TF exacte\n\nT = 40     # largeur de la fenêtre\nn = 2**10  # nombre de points de discrétisation\n\n# Calcul pour nu = 3/T\nnu = 3 / T\n# Valeur exacte de la TF\nexact_value = fhat_exact(nu)\n# Approximation de la TF en nu \napprox_value = fourier_at_nu(f, T, n, nu)\nprint(f\"Exact value at nu={nu}: {exact_value}\")\nprint(f\"Approximation at nu={nu}: {np.real(approx_value)}\")\n\n# Calcul pour nu = pi/T\nnu = np.pi / T\n# Valeur exacte de la TF\nexact_value = fhat_exact(nu)\n# Approximation de la TF en nu\napprox_value = fourier_at_nu(f, T, n, nu)\nprint(f\"Exact value at nu={nu}: {exact_value}\")\nprint(f\"Approximation at nu={nu}: {np.real(approx_value)}\")\n\nExact value at nu=0.075: 2.118347413776218\nApproximation at nu=0.075: 2.1185707502943534\nExact value at nu=0.07853981633974483: 2.0262491352594427\nApproximation at nu=0.07853981633974483: 2.0264201680784835\n\n\nUne alternative à l’interpolation de Shannon est d’utiliser d’autres méthodes d’interpolation comme l’interpolation de Lagrange ou l’interpolation de Newton.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.fft import fft, fftfreq, fftshift, ifftshift\nfrom scipy.integrate import quad\n\n# ------------------------------\n# 1. Define the function and its exact Fourier Transform\n# ------------------------------\ndef f(t):\n    \"\"\"Function f(t) = exp(-πt²)\"\"\"\n    return np.exp(-np.pi * t**2)\n\ndef exact_fourier_transform(nu):\n    \"\"\"Exact FT of f(t) = exp(-πt²) is also exp(-πν²)\"\"\"\n    return np.exp(-np.pi * nu**2)\n\n# ------------------------------\n# 2. Define parameters\n# ------------------------------\nN = 1024\nT = 1.0 / 64\nt = np.linspace(-N/2*T, N/2*T, N, endpoint=False)\ny = f(t)\n\n# ------------------------------\n# 3. FFT with SciPy\n# ------------------------------\nyf_scipy = fftshift(fft(ifftshift(y))) * T\nxf_scipy = fftshift(fftfreq(N, T))\n\n# ------------------------------\n# 4. FFT with NumPy\n# ------------------------------\nyf_numpy = np.fft.fftshift(np.fft.fft(np.fft.ifftshift(y))) * T\nxf_numpy = np.fft.fftshift(np.fft.fftfreq(N, T))\n\n# ------------------------------\n# 5. Numerical integration method (quadrature)\n# ------------------------------\ndef tf_integral(f, nu, T):\n    \"\"\"Compute FT of f at frequency nu over [-T/2, T/2] using scipy quad.\"\"\"\n    real_part = quad(lambda t: np.real(f(t) * np.exp(-2j * np.pi * nu * t)), -T/2, T/2)[0]\n    imag_part = quad(lambda t: np.imag(f(t) * np.exp(-2j * np.pi * nu * t)), -T/2, T/2)[0]\n    return real_part + 1j * imag_part\n\n# Compute values for comparison\nnu_vals = np.linspace(-6, 6, 400)\nFT_exact = exact_fourier_transform(nu_vals)\nFT_integral = np.array([tf_integral(f, nu, 8) for nu in nu_vals])\n\n# Interpolate FFT results for better comparison\n# (FFT frequencies may not align exactly with nu_vals)\nfrom scipy.interpolate import interp1d\ninterp_scipy = interp1d(xf_scipy, np.real(yf_scipy), kind='cubic', fill_value=\"extrapolate\")\ninterp_numpy = interp1d(xf_numpy, np.real(yf_numpy), kind='cubic', fill_value=\"extrapolate\")\n\nFT_scipy = interp_scipy(nu_vals)\nFT_numpy = interp_numpy(nu_vals)\n\n# ------------------------------\n# 6. Plot comparison\n# ------------------------------\nplt.figure(figsize=(8, 5))\nplt.plot(nu_vals, np.real(FT_exact), 'k-', linewidth=2, label='Exact FT (Analytical)')\nplt.plot(nu_vals, np.real(FT_integral), 'r--', linewidth=1.5, label='Numerical Integral (quad)')\nplt.plot(nu_vals, FT_scipy, 'b-.', linewidth=1.2, label='FFT (SciPy)')\nplt.plot(nu_vals, FT_numpy, 'g:', linewidth=1.2, label='FFT (NumPy)')\n\nplt.title(\"Comparison of Fourier Transform Methods\", fontsize=13)\nplt.xlabel(\"Frequency (ν)\")\nplt.ylabel(\"Amplitude (Real part)\")\nplt.xlim(-6, 6)\nplt.ylim(-0.2, 1.1)\nplt.grid(True, linestyle=':', alpha=0.6)\nplt.legend(frameon=False, loc='upper right')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.fft import fft, fftfreq, fftshift, ifftshift\n\n\n# Define the function f(t) = exp(-pi * t^2)\ndef f(t):\n    return np.exp(-np.pi * t**2)\n\n# Parameters\nN = 1024\nT = 1.0 / 64\nt = np.linspace(-N/2*T, N/2*T, N, endpoint=False)\ny = f(t)\n\n# FFT with scipy\nyf_scipy = fftshift(fft(ifftshift(y))) * T\nxf = fftshift(fftfreq(N, T))\nFT_exact = np.exp(-np.pi * xf**2)\n\n# FFT with numpy\nyf_numpy = np.fft.fftshift(np.fft.fft(np.fft.ifftshift(y))) * T\nxf_numpy = np.fft.fftshift(np.fft.fftfreq(N, T))\n\n# Plot with subplot_mosaic\nfig, axs = plt.subplot_mosaic([[\"scipy\", \"numpy\"]], figsize=(7, 5), layout=\"constrained\", sharey=True)\n\n# Scipy FFT\naxs[\"scipy\"].plot(xf, FT_exact, 'k-', linewidth=1.5, label='Exact FT')\naxs[\"scipy\"].plot(xf, np.real(yf_scipy), 'r--', linewidth=1, label='FFT (scipy)')\naxs[\"scipy\"].set_xlim(-6, 6)\naxs[\"scipy\"].set_ylim(-1, 1)\naxs[\"scipy\"].set_title(\"Scipy FFT\")\naxs[\"scipy\"].set_xlabel(\"Frequency\")\naxs[\"scipy\"].set_ylabel(\"Amplitude\")\naxs[\"scipy\"].legend()\naxs[\"scipy\"].grid(False)\n\n# NumPy FFT\naxs[\"numpy\"].plot(xf_numpy, FT_exact, 'k-', linewidth=1.5, label='Exact FT')\naxs[\"numpy\"].plot(xf_numpy, np.real(yf_numpy), 'b--', linewidth=1, label='FFT (numpy)')\naxs[\"numpy\"].set_xlim(-6, 6)\naxs[\"numpy\"].set_title(\"NumPy FFT\")\naxs[\"numpy\"].set_xlabel(\"Frequency\")\naxs[\"numpy\"].legend()\naxs[\"numpy\"].grid(False)\n\nplt.suptitle(\"Comparison of FFT Implementations vs. Exact Fourier Transform\", fontsize=14)\nplt.show()"
  },
  {
    "objectID": "00_tds/1_fourrier_transformation.html#definition-and-properties-of-the-fourier-transform",
    "href": "00_tds/1_fourrier_transformation.html#definition-and-properties-of-the-fourier-transform",
    "title": "Fourrier Transformation",
    "section": "",
    "text": "We adopt the framework presented by Balac (2011) to define the Fourier transform of a function \\(f\\) and to introduce its properties.\nThe functions considered belong to the space of integrable functions, denoted \\(L^1(\\mathbb{R,K})\\), consisting of all functions \\(f:\\mathbb{R} \\to \\mathbb{K}\\) [where \\(\\mathbb{K}\\) is either \\(\\mathbb{R}\\) or \\(\\mathbb{C}\\)]. These functions are integrable in the sense of Lebesgue, meaning that the integral of their absolute value is finite: \\[\n\\int_{-\\infty}^{+\\infty} |f(t)| dt &lt; +\\infty.\n\\]\nSo, for a function \\(f\\) to belong to \\(L^1(\\mathbb{R}, \\mathbb{K})\\), the product \\(f(t) \\cdot e^{-2i\\pi\\nu t}\\) must also be integrable for all \\(\\nu \\in \\mathbb{R}\\). In that case, the Fourier transform of \\(f\\), denoted \\(\\hat{f}\\) (or sometimes \\(\\mathcal{F}(f)\\)), is defined for all \\(\\nu \\in \\mathbb{R}\\) by:\n\\[\n\\hat{f}(\\nu) = \\int_{-\\infty}^{+\\infty}\nf(t) \\, e^{-2i\\pi\\nu t} \\, dt.\n\\]\nThe key takeaway is that the Fourier transform \\(\\hat{f}\\) of a function \\(f\\) is a complex-valued, linear function that depends on the frequency \\(\\nu\\). If \\(f \\in L^1(\\mathbb{R})\\), is real-valued and even, then \\(\\hat{f}\\) is also real-valued and even. Conversely, if \\(f\\) is real-valued and odd, then \\(\\hat{f}\\) is purely imaginary and odd as well.\nFor some functions, the Fourier transform can be computed analytically. For example, for the function \\(f : t \\in \\mathbb{R} \\mapsto \\mathbb{1}_{[-\\frac{a}{2}, \\frac{a}{2}]}(t)\\), the Fourier transform is given by: \\[\n\\hat{f} : \\nu \\in \\mathbb{R} \\mapsto a sinc(a \\pi \\nu)\n\\] where \\(sinc(t) = \\frac{\\sin(t)}{t}\\) is the sinc function.\nHowever, for many functions, the Fourier transform cannot be computed analytically. In such cases, we can use numerical methods to approximate it. We will explore these numerical approaches in the following sections of this article."
  },
  {
    "objectID": "00_tds/1_fourrier_transformation.html#how-to-approximate-the-fourier-transform",
    "href": "00_tds/1_fourrier_transformation.html#how-to-approximate-the-fourier-transform",
    "title": "Fourrier Transformation",
    "section": "",
    "text": "The Fourier transform of a function \\(f\\) is defined as an integral over the entire real line. However, for the functions that are integral in the sense of lebesgue and that have a practical applications tend to 0 as \\(|t| \\to +\\infty\\). And we can approximate the Fourier transform by integrating over a finite interval \\([-T, T]\\). If the lenght of the interval is large enough, or if the function decays quickly when t tends to infinity, this approximation will be accurate.\n\\[\n\\hat{f}(\\nu) \\approx \\int_{-T}^{T} f(t) e^{-2i\\pi\\nu t} dt\n\\]\nIn his article, Balac (2011) goes further by showing that approximating the Fourier transform involves three key mathematical tools:\n\nFourier series, in the context of a periodic signal,\nThe Fourier transform, for non-periodic signals,\nThe discrete Fourier transform, for discrete signals.\n\nFor each of these tools, computing the Fourier transform essentially comes down to evaluating the following integral: \\[\n\\hat{f}(\\nu) \\approx \\int_{-\\frac{T}{2}}^{\\frac{T}{2}} f(t) e^{-2i\\pi\\nu t} dt\n\\] I recommend reading his article for more details on these three tools. Next, we will focus on the numerical computation of the Fourier transform using quadrature methods, a technique for performing numerical integration."
  },
  {
    "objectID": "00_tds/1_fourrier_transformation.html#numerical-computation-of-the-fourier-transform",
    "href": "00_tds/1_fourrier_transformation.html#numerical-computation-of-the-fourier-transform",
    "title": "Fourrier Transformation",
    "section": "",
    "text": "We show that computing the Fourier transform of a function \\(f\\) consists to approximating it by the integral the following integral over the interval \\([-\\frac{T}{2}, \\frac{T}{2}]\\): \\[\n\\boxed{\n\\underbrace{\n  \\int_{-\\infty}^{+\\infty} f(t)\\, e^{-2\\pi i \\nu t} \\, dt\n}_{=\\,\\hat{f}(\\nu)}\n\\approx\n\\underbrace{\n  \\int_{-\\frac{T}{2}}^{\\frac{T}{2}} f(t)\\, e^{-2\\pi i \\nu t} \\, dt\n}_{=\\,\\tilde{S}(\\nu)}\n}\n\\]\nwhere T is a large enough number such that the integral converges. Une valeur approchée of the integral \\(\\tilde{S}(\\nu) = \\int_{-\\frac{T}{2}}^{\\frac{T}{2}} f(t)\\, e^{-2\\pi i \\nu t} \\, dt\\) can be computed using quadrature methods. In the next section, we will approximate the integral using the quadrature method of rectangles à gauche."
  },
  {
    "objectID": "00_tds/1_fourrier_transformation.html#quadrature-method-of-left-rectangles",
    "href": "00_tds/1_fourrier_transformation.html#quadrature-method-of-left-rectangles",
    "title": "Fourrier Transformation",
    "section": "",
    "text": "To compute the integral \\(\\tilde{S}(\\nu) = \\int_{-\\frac{T}{2}}^{\\frac{T}{2}} f(t)\\, e^{-2\\pi i \\nu t} \\, dt\\), using the quadrature method of left rectangles, we follow these steps:\n\nDiscretization of the Interval:: We divide the interval \\([-\\frac{T}{2}, \\frac{T}{2}]\\) into \\(N\\) uniform subintervals of length \\(h_t = \\frac{T}{N}\\). The discretization points[eft endpoints of the rectangles] in the interval are given by:\n\n\\[\nt_k = -\\frac{T}{2} + k \\cdot h_t, \\quad k = 0, 1, \\ldots, N-1.\n\\]\n\nApproximation of the Integral: Using the Chasles relation, we can approximate the integral \\(\\tilde{S}(\\nu)\\) as follows:\n\n\\[\n\\tilde{S}(\\nu) = \\int_{-\\frac{T}{2}}^{\\frac{T}{2}} f(t)\\, e^{-2\\pi i \\nu t} \\, dt = \\sum_{k=0}^{N-1}  \\int_{t_k}^{t_{k+1}} f(t)\\, e^{-2\\pi i \\nu t} \\, dt.\n\\]\nBy taking into account that we have \\(t_{k+1} - t_k = h_t\\), and \\(t_k = -\\frac{T}{2} + k \\cdot h_t = T(\\frac{k}{N} - \\frac{1}{2})\\), we can rewrite the integral as: \\[\n\\boxed{\n\\tilde{S}(\\nu) = \\sum_{k=0}^{N-1} f(t_k) e^{-2\\pi i \\nu t_k} h_t.\n}\n\\]\nWe call it the quadrature method of left rectangles because it uses the left endpoint \\(t_k\\) of each subinterval to approximate the value of the function \\(f(t)\\) at that point.\n\nFinal Formula: The final formula for the approximation of the Fourier transform is given by:\n\n\\[\n\\boxed{\n\\forall \\nu \\in \\mathbb{R} \\quad\n\\underbrace{\n\\int_{-\\frac{T}{2}}^{\\frac{T}{2}} f(t)\\, e^{-2\\pi i \\nu t} \\, dt\n}_{=\\,\\tilde{S}(\\nu)}\n\\approx\n\\underbrace{\n\\frac{T}{n} e^{i \\pi \\nu T} \\sum_{k=0}^{n-1} f_k\\, e^{-2 i \\pi \\nu T k / n}\n}_{=\\,\\tilde{S}_n(\\nu)}\n\\quad \\text{où } f_k = f\\left( \\frac{2k - n}{2n} T \\right).\n}\n\\]\n\n\nThe function tfquad below implements the left rectangle quadrature method to compute the Fourier transform of a function f at a given frequency nu.\n\nimport numpy as np\n\ndef tfquad(f, nu, n, T):\n    \"\"\"\n    Computes the Fourier transform of a function f at frequency nu\n    using left Riemann sum quadrature over the interval [-T/2, T/2].\n\n    Parameters:\n    ----------\n    f : callable\n        The function to transform. Must accept a NumPy array as input.\n    nu : float\n        The frequency at which to evaluate the Fourier transform.\n    n : int\n        Number of quadrature points.\n    T : float\n        Width of the time window [-T/2, T/2].\n\n    Returns:\n    -------\n    tfnu : complex\n        Approximated value of the Fourier transform at frequency nu.\n    \"\"\"\n    k = np.arange(n)\n    t_k = (k / n - 0.5) * T\n    weights = np.exp(-2j * np.pi * nu * T * k / n)\n    prefactor = (T / n) * np.exp(1j * np.pi * nu * T)\n\n    return prefactor * np.sum(f(t_k) * weights)\n\nWe can also use SciPy’s quad function to define the Fourier transform of a function f at a given frequency nu. The function tf_integral below implements this approach. It uses numerical integration to compute the Fourier transform of f over the interval \\([-T/2, T/2]\\).\n\nfrom scipy.integrate import quad\n\ndef tf_integral(f, nu, T):\n    \"\"\"Compute FT of f at frequency nu over [-T/2, T/2] using scipy quad.\"\"\"\n    real_part = quad(lambda t: np.real(f(t) * np.exp(-2j * np.pi * nu * t)), -T/2, T/2)[0]\n    imag_part = quad(lambda t: np.imag(f(t) * np.exp(-2j * np.pi * nu * t)), -T/2, T/2)[0]\n    return real_part + 1j * imag_part\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# ----- Function Definitions -----\n\ndef f(t):\n    \"\"\"Indicator function on [-1, 1].\"\"\"\n    return np.where(np.abs(t) &lt;= 1, 1.0, 0.0)\n\ndef exact_fourier_transform(nu):\n    \"\"\"Analytical FT of the indicator function over [-1, 1].\"\"\"\n    # f̂(ν) = ∫_{-1}^{1} e^{-2πiνt} dt = 2 * sinc(2ν)\n    return 2 * np.sinc(2 * nu)\n\n\n# ----- Computation -----\n\nT = 2.0\nn = 32\nnu_vals = np.linspace(-6, 6, 500)\nexact_vals = exact_fourier_transform(nu_vals)\ntfquad_vals = np.array([tfquad(f, nu, n, T) for nu in nu_vals])\n\n# Compute the approximation using scipy integral\ntf_integral_vals = np.array([tf_integral(f, nu, T) for nu in nu_vals])\n\n# ----- Plotting -----\nfig, axs = plt.subplot_mosaic([[\"tfquad\", \"quad\"]], figsize=(7.24, 4.07), dpi=100, layout=\"constrained\")\n\n# Plot using tfquad implementation\naxs[\"tfquad\"].plot(nu_vals, np.real(exact_vals), 'b-', linewidth=2, label=r'$\\hat{f}$ (exact)')\naxs[\"tfquad\"].plot(nu_vals, np.real(tfquad_vals), 'r--', linewidth=1.5, label=r'approximation $\\hat{S}_n$')\naxs[\"tfquad\"].set_title(\"TF avec tfquad (rectangles)\")\naxs[\"tfquad\"].set_xlabel(r'$\\nu$')\naxs[\"tfquad\"].grid(False)\naxs[\"tfquad\"].set_ylim(-0.5, 2.1)\n\n# Plot using scipy.integrate.quad\naxs[\"quad\"].plot(nu_vals, np.real(exact_vals), 'b-', linewidth=2, label=r'$\\hat{f}$ (quad)')\naxs[\"quad\"].plot(nu_vals, np.real(tf_integral_vals), 'r--', linewidth=1.5, label=r'approximation $\\hat{S}_n$')\naxs[\"quad\"].set_title(\"TF avec scipy.integrate.quad\")\naxs[\"quad\"].set_xlabel(r'$\\nu$')\naxs[\"quad\"].set_ylabel('Amplitude')\naxs[\"quad\"].grid(False)\naxs[\"quad\"].set_ylim(-0.5, 2.1)\n\n\n\n# --- Global legend below the plots ---\n# Take handles from one subplot only (assumes labels are consistent)\nhandles, labels = axs[\"quad\"].get_legend_handles_labels()\nfig.legend(handles, labels,\n           loc='lower center', bbox_to_anchor=(0.5, -0.05),\n           ncol=3, frameon=False)\n\nplt.suptitle(\"Comparison of FFT Implementations vs. Exact Fourier Transform\", fontsize=14)\n\nplt.show()\n\n\n\n\n\n\n\n\nWe are now able to compute the Fourier transform of a function using the left rectangle quadrature method. Let’s take a closer look at the characteristics and limitations of this approximation."
  },
  {
    "objectID": "00_tds/1_fourrier_transformation.html#characterizing-the-approximation-using-the-left-rectangle-quadrature-method",
    "href": "00_tds/1_fourrier_transformation.html#characterizing-the-approximation-using-the-left-rectangle-quadrature-method",
    "title": "Fourrier Transformation",
    "section": "",
    "text": "Balac (2011) highlights that the Fourier transform \\(\\hat{f}\\) of the function \\(f\\) is inherently oscillatory. This behavior arises from the complex exponential term \\(e^{-2\\pi i \\nu t}\\) in the integral definition of the transform.\nTo illustrate this, the figure below shows the function\n\\[\nf : t \\in \\mathbb{R} \\mapsto e^{-t^2} \\in \\mathbb{R}\n\\]\nalong with the real and imaginary parts of its Fourier transform\n\\[\n\\hat{f} : \\nu \\in \\mathbb{R} \\mapsto \\hat{f}(\\nu) \\in \\mathbb{C},\n\\]\nevaluated at \\(\\nu = \\frac{5}{2}\\).\nAlthough \\(f\\) is smooth, we clearly observe strong oscillations in \\(\\hat{f}\\), highlighting the influence of the exponential kernel.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nnu = 5 / 2\nt1 = np.linspace(-8, 8, 1000)\nt2 = np.linspace(-4, 4, 1000)\n\nf = lambda t: np.exp(-t**2)\nphi = lambda t: f(t) * np.exp(-2j * np.pi * nu * t)\n\nf_vals = f(t1)\nphi_vals = phi(t2)\n\n# Plot\nfig, axs = plt.subplots(1, 2, figsize=(7.24, 4.07), dpi=100)\n\naxs[0].plot(t1, f_vals, 'k', linewidth=2)\naxs[0].set_xlim(-8, 8)\naxs[0].set_ylim(0, 1)\naxs[0].set_title(r\"$f(t) = e^{-t^2}$\")\naxs[0].grid(True)\n\naxs[1].plot(t2, np.real(phi_vals), 'b', label=r\"$\\Re(\\phi)$\", linewidth=2)\naxs[1].plot(t2, np.imag(phi_vals), 'r', label=r\"$\\Im(\\phi)$\", linewidth=2)\naxs[1].set_xlim(-4, 4)\naxs[1].set_ylim(-1, 1)\naxs[1].set_title(r\"$\\phi(t) = f(t)e^{-2i\\pi\\nu t}$, $\\nu=5/2$\")\naxs[1].legend()\naxs[1].grid(True)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nThese rapid variations can create difficulties in the numerical approximation of the Fourier transform using quadrature methods, even when a large number of points \\(n\\) is used. One way to overcome this issue is by using the Fast Fourier Transform (FFT) algorithm."
  },
  {
    "objectID": "00_tds/1_fourrier_transformation.html#the-approximation-obtained-using-the-left-rectangle-quadrature-method-is-periodic-in-nature.",
    "href": "00_tds/1_fourrier_transformation.html#the-approximation-obtained-using-the-left-rectangle-quadrature-method-is-periodic-in-nature.",
    "title": "Fourrier Transformation",
    "section": "",
    "text": "We observe that even if the function \\(f\\) is not periodic, its Fourier transform approximation \\(\\hat{f}\\) appears periodic. In fact, the function \\(\\hat{S}_n\\), resulting from the quadrature method, is periodic with period\n\\[\n\\Delta \\nu = \\frac{n}{T}.\n\\] \\[\n\\forall \\nu \\in \\mathbb{R} \\quad\n\\widehat{S}_n\\left(\\nu + \\frac{n}{T} \\right)\n= \\frac{T}{n} e^{i\\pi \\nu T} \\sum_{k=0}^{n-1} f_k \\, e^{-2i\\pi\\left(\\nu + \\frac{n}{T} \\right)T \\frac{k}{n}}\n= \\frac{T}{n} e^{i\\pi \\nu T} \\sum_{k=0}^{n-1} f_k \\, e^{-2i\\pi \\nu T \\frac{k}{n}} \\underbrace{e^{-2i\\pi k}}_{=1}\n\\]\n\\[\n\\phantom{\\forall \\nu \\in \\mathbb{R} \\quad\n\\widehat{S}_n\\left(\\nu + \\frac{n}{T} \\right)}\n= \\frac{T}{n} e^{i\\pi \\nu T} \\sum_{k=0}^{n-1} f_k \\, e^{-2i\\pi \\nu T \\frac{k}{n}} = \\widehat{S}_n(\\nu).\n\\]\nThis periodicity of \\(\\hat{S}_n\\) implies that it is not possible to compute the Fourier transform for all frequencies \\(\\nu \\in \\mathbb{R}\\) using the quadrature method when the parameters \\(T\\) and \\(n\\) are fixed. In fact, it becomes impossible to compute \\(\\hat{f}(\\nu)\\) accurately when\n\\[\n|\\nu| \\geq \\nu_{\\text{max}},\n\\]\nwhere \\(\\nu_{\\text{max}} = \\frac{n}{T}\\) is the maximum frequency that can be resolved due to the periodic nature of \\(\\hat{S}_n\\).\nAs a result, in practice, to compute the Fourier transform for large frequencies, one must increase either the time window \\(T\\) or the number of points \\(n\\).\nFurthermore, by evaluating the error in approximating \\(\\hat{f}(\\nu)\\) using the left rectangle quadrature method, we find that the approximation is reliable at frequency \\(\\nu\\) when the following condition holds:\n\\[\n|\\nu| \\ll \\frac{n}{T}\n\\]\n\\[\n\\frac{\\nu T}{n} \\ll 1.\n\\]\nEpstein (2005) shows that when using the Fast Fourier Transform (FFT) algorithm, it is possible to accurately compute the Fourier transform of a function \\(f\\) for all frequencies \\(\\nu \\in \\mathbb{R}\\), even when \\(\\frac{\\nu T}{n}\\) is close to 1 — provided that \\(f\\) is piecewise continuous and has compact support."
  },
  {
    "objectID": "00_tds/1_fourrier_transformation.html#computing-the-fourier-transform-at-frequency-nu-using-the-fft-algorithm.",
    "href": "00_tds/1_fourrier_transformation.html#computing-the-fourier-transform-at-frequency-nu-using-the-fft-algorithm.",
    "title": "Fourrier Transformation",
    "section": "",
    "text": "In this section, we denote by \\(\\hat{S}_n(\\nu)\\) the approximation of the Fourier transform \\(\\hat{f}(\\nu)\\) of the function \\(f\\) at a point \\(\\nu \\in \\left[-\\frac{\\nu_{\\text{max}}}{2}, \\frac{\\nu_{\\text{max}}}{2}\\right]\\), where \\(\\nu_{\\text{max}} = \\frac{n}{T}\\), i.e., \\[\n\\boxed{\n\\hat{f}(\\nu) \\approx \\hat{S}_n(\\nu) = \\frac{T}{n} e^{i\\pi \\nu T} \\sum_{k=0}^{n-1} f_k\\, e^{-2 i \\pi \\nu T k / n}.\n}\n\\]\nWe now present the Fourier transform algorithm used to approximate \\(\\hat{f}(\\nu)\\). We will not go into the details of the Fast Fourier Transform (FFT) algorithm in this article. Balac (2011) provides a simplified explanation of the FFT, and for more in-depth technical details, we recommend the original article by Cooley and Tukey (1965).\nWhat is important to understand is that the use of the FFT algorithm to approximate the Fourier transform of a function \\(f\\) is based on the result shown by Epstein (2005), which states that when \\(\\hat{S}_n(\\nu)\\) is evaluated at the frequencies \\(\\nu_j = \\frac{j}{T}\\) for \\(j = 0, 1, \\dots, n - 1\\), it gives a good approximation of the continuous Fourier transform \\(\\hat{f}(\\nu)\\).\nMoreover, \\(\\hat{S}_n\\) is known to be periodic. This periodicity gives a symmetric role to the indices \\(j \\in \\{0, 1, \\dots, n - 1\\}\\) and \\(k \\in \\{-\\frac{n}{2}, -\\frac{n}{2} + 1, \\dots, -1\\}\\). In fact, the values of the Fourier transform of \\(f\\) over the interval \\(\\left[ -\\frac{\\nu_{\\text{max}}}{2}, \\frac{\\nu_{\\text{max}}}{2} \\right]\\) can be derived from the values of \\(\\hat{S}_n\\) at the points \\(\\nu_j = \\frac{j}{T}\\), for \\(j = 0, 1, \\dots, n - 1\\), as follows:\n\\[\n\\widehat{S}_n(\\nu'_j) = \\frac{T}{n} (-1)^j \\sum_{k=0}^{n-1} f_k\\, e^{-2i\\pi j \\frac{k}{n}}\n=\n\\begin{cases}\n\\widehat{S}_n(\\nu_j) & \\text{si } j \\in \\left\\{0, \\dots, \\frac{n}{2} - 1 \\right\\} \\\\\n\\widehat{S}_n(\\nu_{j-n}) & \\text{si } j \\in \\left\\{ \\frac{n}{2}, \\dots, n-1 \\right\\}\n\\end{cases}\n\\]\n\\[\n\\text{where we used the relation} \\quad\ne^{-2i\\pi j \\frac{k}{n}} = e^{-2i\\pi (j-n) \\frac{k}{n}} \\times \\underbrace{e^{-2i\\pi k}}_{=1}\n= e^{-2i\\pi (j-n) \\frac{k}{n}}\n\\quad \\text{for } j \\in \\left\\{ \\frac{n}{2}, \\dots, n-1 \\right\\}.\n\\]\nThis relationship shows that we can compute the Fourier transform \\(\\hat{S}_n\\left( \\frac{j}{T} \\right)\\) for \\(j = -\\frac{n}{2}, \\ldots, \\frac{n}{2} - 1\\). Moreover, when \\(n\\) is a power of 2, it can be shown that the computation becomes faster (see Balac 2011). This process is known as the Fast Fourier Transform (FFT).\nTo summarize, we have shown that the Fourier transform of the function \\(f\\) can be approximated over the interval \\(\\left[-\\frac{T}{2}, \\frac{T}{2}\\right]\\) at the frequencies \\(\\nu_j = \\frac{j}{T}\\) for \\(j = -\\frac{n}{2}, \\ldots, \\frac{n}{2} - 1\\), where \\(n = 2^m\\) for some integer \\(m \\geq 0\\), by applying the FFT algorithm as follows:\n\nConstruct the finite sequence \\(F\\) of values \\(f\\left( \\frac{2k - n}{2n} T \\right)\\) for \\(k = 0, 1, \\ldots, n - 1\\).\nCompute the discrete Fourier transform \\(\\hat{F}\\) of the sequence \\(F\\) using the Fast Fourier Transform (FFT) algorithm, which is given by\n\\[\n\\hat{F}_j = \\sum_{k=0}^{n-1} f_k \\, e^{-2i\\pi \\frac{jk}{n}}, \\quad \\text{for } j = 0, 1, \\ldots, n-1.\n\\]\nReindex and symmetrize the values to span \\(j = -\\frac{n}{2}, \\ldots, -1\\).\nMultiply each value in the array by \\(\\frac{T}{n} (-1)^{j-1}\\), where \\(j \\in \\{1, \\ldots, n\\}\\).\n\nThis yields an array corresponding to the values of the Fourier transform \\(\\hat{f}(\\nu_j)\\), with \\(\\nu_j = \\frac{j}{T}\\) for \\(j = -\\frac{n}{2}, \\ldots, \\frac{n}{2} - 1\\).\nThe following Python function tffft implements these steps to compute the Fourier transform of a given function.\n\nimport numpy as np\nfrom scipy.fft import fft, fftshift\n\ndef tffft(f, T, n):\n    \"\"\"\n    Calcule la transformée de Fourier approchée d'une fonction f à support dans [-T/2, T/2],\n    en utilisant l’algorithme FFT.\n\n    Paramètres\n    ----------\n    f : callable\n        Fonction à transformer (doit être vectorisable avec numpy).\n    T : float\n        Largeur de la fenêtre temporelle (intervalle [-T/2, T/2]).\n    n : int\n        Nombre de points de discrétisation (doit être une puissance de 2 pour FFT efficace).\n\n    Retours\n    -------\n    tf : np.ndarray\n        Valeurs approximées de la transformée de Fourier aux fréquences discrètes.\n    freq_nu : np.ndarray\n        Fréquences discrètes correspondantes (de -n/(2T) à (n/2 - 1)/T).\n    \"\"\"\n    h = T / n\n    t = -0.5 * T + np.arange(n) * h  # noeuds temporels\n    F = f(t)                         # échantillonnage de f\n    tf = h * (-1) ** np.arange(n) * fftshift(fft(F))  # TF approximée\n    freq_nu = -n / (2 * T) + np.arange(n) / T              # fréquences ν_j = j/T\n\n    return tf, freq_nu, t\n\nThe following program illustrates how to compute the Fourier transform of the Gaussian function \\(f(t) = e^{-10t^2}\\) over the interval \\([-10, 10]\\), using the Fast Fourier Transform (FFT) algorithm.\n\n# Parameters\na = 10\nf = lambda t: np.exp(-a * t**2)\nT = 10\nn = 2**8  # 256\n\n# Compute the Fourier transform using FFT\ntf, nu, t = tffft(f, T, n)\n\n# Plotting\nfig, axs = plt.subplots(1, 2, figsize=(7.24, 4.07), dpi=100)\n\naxs[0].plot(t, f(t), '-g', linewidth=3)\naxs[0].set_xlabel(\"time\")\naxs[0].set_title(\"Considered Function\")\naxs[0].set_xlim(-6, 6)\naxs[0].set_ylim(-0.5, 1.1)\naxs[0].grid(True)\n\naxs[1].plot(nu, np.abs(tf), '-b', linewidth=3)\naxs[1].set_xlabel(\"frequency\")\naxs[1].set_title(\"Fourier Transform using FFT\")\naxs[1].set_xlim(-15, 15)\naxs[1].set_ylim(-0.5, 1)\naxs[1].grid(True)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nThe method we have just presented allows us to compute and visualize the Fourier transform of a function \\(f\\) at discrete points \\(\\nu_j = \\frac{j}{T}\\), for \\(j = -\\frac{n}{2}, \\ldots, \\frac{n}{2} - 1\\), where \\(n\\) is a power of 2. These points lie within the interval \\(\\left[-\\frac{n}{2T}, \\frac{n}{2T}\\right]\\). However, this approach does not allow us to evaluate the Fourier transform at arbitrary points \\(\\nu\\) in the same interval when \\(\\nu\\) is not of the form \\(\\nu_j = \\frac{j}{T}\\).\nTo compute the Fourier transform of a function \\(f\\) at a point \\(\\nu\\) that does not coincide with one of the sampling frequencies \\(\\nu_j\\), interpolation methods can be used (e.g., linear, polynomial, spline interpolation, etc.). In this article, we adopt the approach proposed by Balac (2011), which relies on Shannon’s interpolation theorem to compute the Fourier transform of a function \\(f\\) at any point \\(\\nu\\)."
  },
  {
    "objectID": "00_tds/1_fourrier_transformation.html#using-shannons-interpolation-theorem-to-compute-the-fourier-transform-of-a-function-f-at-a-point-nu",
    "href": "00_tds/1_fourrier_transformation.html#using-shannons-interpolation-theorem-to-compute-the-fourier-transform-of-a-function-f-at-a-point-nu",
    "title": "Fourrier Transformation",
    "section": "",
    "text": "Que nous dit le théorème de Shannon ? Il nous dit que pour une fonction \\(g\\) à bande limitée c’est-à-dire dont la transformée de Fourier \\(\\hat{g}\\) est nulle en dehors d’un intervalle \\([-\\frac{B}{2}, \\frac{B}{2}]\\), on peut reconstruire la fonction \\(g\\) à partir de ses échantillons \\(g_k = g\\left(\\frac{k}{B}\\right)\\) pour \\(k \\in \\mathbb{Z}\\). Si on note \\(\\nu_c\\) le plus petit réel positif tel que \\(\\hat{g}\\) est nulle en déhors de l’intervalle \\([-2 \\pi \\nu_c, 2 \\pi \\nu_c]\\), alors on a la formule d’interpolation de Shannon : Pour tout \\(t \\in \\mathbb{R}\\), et \\(\\alpha\\) un réel positif, vérifiant \\(\\alpha \\geq \\frac{1}{2 \\nu_c}\\), on a :\n\\[\ng(t) = \\sum_{k \\in \\mathbb{Z}} g(k\\alpha)\\, \\text{sinc}\\left(\\frac{\\pi}{\\alpha}(t - k\\alpha)\\right)\n\\]\noù \\(\\text{sinc}(x) = \\frac{\\sin(x)}{x}\\) est la fonction sinus cardinal sinc.\nBalac (2011) montre que lorsque la fonction \\(f\\) est à support borné dans un intervalle \\([-T/2, T/2]\\), on peut utiliser le théorème d’interpolation de Shannon pour calculer la transformée de Fourier \\(\\hat{f}(\\nu)\\) pour tout \\(\\nu \\in \\mathbb{R}\\) en utilisant les valeurs de la transformée de Fourier discrète \\(\\hat{S}_n(\\nu_j)\\) pour \\(j = -\\frac{n}{2}, \\ldots, \\frac{n}{2} - 1\\). Pour ce faire, il considère \\(\\alpha = \\frac{1}{T}\\) et obtient pour tout \\(\\nu \\in \\mathbb{R}\\) la formule d’interpolation de Shannon suivante : \\[\n\\hat{f}(\\nu) = \\sum_{j=-\\frac{n}{2}}^{\\frac{n}{2}-1} \\hat{S}_n\\left(\\frac{j}{T}\\right)\\, \\text{sinc}\\left(\\pi T\\left(\\nu - \\frac{j}{T}\\right)\\right)\n\\]\nLe programme ci-dessous illustre l’utilisation du théorème d’interpolation de Shannon pour calculer la transformée de Fourier d’une fonction \\(f\\) en un point \\(\\nu\\).\n\nimport numpy as np\n\ndef shannon(tf, nu, T):\n    \"\"\"\n    Approximates the value of the Fourier transform of function f at frequency 'nu'\n    using its discrete values computed from the FFT.\n\n    Parameters:\n    - tf : numpy array, discrete Fourier transform values (centered with fftshift) at frequencies j/T for j = -n/2, ..., n/2 - 1\n    - nu : float, frequency at which to approximate the Fourier transform\n    - T  : float, time window width used for the FFT\n\n    Returns:\n    - tfnu : approximation of the Fourier transform at frequency 'nu'\n    \"\"\"\n    n = len(tf)\n    tfnu = 0.0\n    for j in range(n):\n        k = j - n // 2  # correspond à l'indice j dans {-n/2, ..., n/2 - 1}\n        tfnu += tf[j] * np.sinc(T * nu - k)  # np.sinc(x) = sin(pi x)/(pi x) en numpy\n\n    return tfnu\n\nEnfin, nous pouvons definir la\n\ndef fourier_at_nu(f, T, n, nu):\n    \"\"\"\n    Computes the Fourier transform of f at frequency nu\n    using tffft + Shannon interpolation.\n    \"\"\"\n    tf, _, _ = tffft(f, T, n)\n    return shannon(tf, nu, T)\n\n\na = 0.5\nf = lambda t: np.exp(-a * np.abs(t))                          # Fonction à transformer\nfhat_exact = lambda nu: (2 * a) / (a**2 + 4 * np.pi**2 * nu**2)  # TF exacte\n\nT = 40     # largeur de la fenêtre\nn = 2**10  # nombre de points de discrétisation\n\n# Calcul pour nu = 3/T\nnu = 3 / T\n# Valeur exacte de la TF\nexact_value = fhat_exact(nu)\n# Approximation de la TF en nu \napprox_value = fourier_at_nu(f, T, n, nu)\nprint(f\"Exact value at nu={nu}: {exact_value}\")\nprint(f\"Approximation at nu={nu}: {np.real(approx_value)}\")\n\n# Calcul pour nu = pi/T\nnu = np.pi / T\n# Valeur exacte de la TF\nexact_value = fhat_exact(nu)\n# Approximation de la TF en nu\napprox_value = fourier_at_nu(f, T, n, nu)\nprint(f\"Exact value at nu={nu}: {exact_value}\")\nprint(f\"Approximation at nu={nu}: {np.real(approx_value)}\")\n\nExact value at nu=0.075: 2.118347413776218\nApproximation at nu=0.075: 2.1185707502943534\nExact value at nu=0.07853981633974483: 2.0262491352594427\nApproximation at nu=0.07853981633974483: 2.0264201680784835\n\n\nUne alternative à l’interpolation de Shannon est d’utiliser d’autres méthodes d’interpolation comme l’interpolation de Lagrange ou l’interpolation de Newton.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.fft import fft, fftfreq, fftshift, ifftshift\nfrom scipy.integrate import quad\n\n# ------------------------------\n# 1. Define the function and its exact Fourier Transform\n# ------------------------------\ndef f(t):\n    \"\"\"Function f(t) = exp(-πt²)\"\"\"\n    return np.exp(-np.pi * t**2)\n\ndef exact_fourier_transform(nu):\n    \"\"\"Exact FT of f(t) = exp(-πt²) is also exp(-πν²)\"\"\"\n    return np.exp(-np.pi * nu**2)\n\n# ------------------------------\n# 2. Define parameters\n# ------------------------------\nN = 1024\nT = 1.0 / 64\nt = np.linspace(-N/2*T, N/2*T, N, endpoint=False)\ny = f(t)\n\n# ------------------------------\n# 3. FFT with SciPy\n# ------------------------------\nyf_scipy = fftshift(fft(ifftshift(y))) * T\nxf_scipy = fftshift(fftfreq(N, T))\n\n# ------------------------------\n# 4. FFT with NumPy\n# ------------------------------\nyf_numpy = np.fft.fftshift(np.fft.fft(np.fft.ifftshift(y))) * T\nxf_numpy = np.fft.fftshift(np.fft.fftfreq(N, T))\n\n# ------------------------------\n# 5. Numerical integration method (quadrature)\n# ------------------------------\ndef tf_integral(f, nu, T):\n    \"\"\"Compute FT of f at frequency nu over [-T/2, T/2] using scipy quad.\"\"\"\n    real_part = quad(lambda t: np.real(f(t) * np.exp(-2j * np.pi * nu * t)), -T/2, T/2)[0]\n    imag_part = quad(lambda t: np.imag(f(t) * np.exp(-2j * np.pi * nu * t)), -T/2, T/2)[0]\n    return real_part + 1j * imag_part\n\n# Compute values for comparison\nnu_vals = np.linspace(-6, 6, 400)\nFT_exact = exact_fourier_transform(nu_vals)\nFT_integral = np.array([tf_integral(f, nu, 8) for nu in nu_vals])\n\n# Interpolate FFT results for better comparison\n# (FFT frequencies may not align exactly with nu_vals)\nfrom scipy.interpolate import interp1d\ninterp_scipy = interp1d(xf_scipy, np.real(yf_scipy), kind='cubic', fill_value=\"extrapolate\")\ninterp_numpy = interp1d(xf_numpy, np.real(yf_numpy), kind='cubic', fill_value=\"extrapolate\")\n\nFT_scipy = interp_scipy(nu_vals)\nFT_numpy = interp_numpy(nu_vals)\n\n# ------------------------------\n# 6. Plot comparison\n# ------------------------------\nplt.figure(figsize=(8, 5))\nplt.plot(nu_vals, np.real(FT_exact), 'k-', linewidth=2, label='Exact FT (Analytical)')\nplt.plot(nu_vals, np.real(FT_integral), 'r--', linewidth=1.5, label='Numerical Integral (quad)')\nplt.plot(nu_vals, FT_scipy, 'b-.', linewidth=1.2, label='FFT (SciPy)')\nplt.plot(nu_vals, FT_numpy, 'g:', linewidth=1.2, label='FFT (NumPy)')\n\nplt.title(\"Comparison of Fourier Transform Methods\", fontsize=13)\nplt.xlabel(\"Frequency (ν)\")\nplt.ylabel(\"Amplitude (Real part)\")\nplt.xlim(-6, 6)\nplt.ylim(-0.2, 1.1)\nplt.grid(True, linestyle=':', alpha=0.6)\nplt.legend(frameon=False, loc='upper right')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.fft import fft, fftfreq, fftshift, ifftshift\n\n\n# Define the function f(t) = exp(-pi * t^2)\ndef f(t):\n    return np.exp(-np.pi * t**2)\n\n# Parameters\nN = 1024\nT = 1.0 / 64\nt = np.linspace(-N/2*T, N/2*T, N, endpoint=False)\ny = f(t)\n\n# FFT with scipy\nyf_scipy = fftshift(fft(ifftshift(y))) * T\nxf = fftshift(fftfreq(N, T))\nFT_exact = np.exp(-np.pi * xf**2)\n\n# FFT with numpy\nyf_numpy = np.fft.fftshift(np.fft.fft(np.fft.ifftshift(y))) * T\nxf_numpy = np.fft.fftshift(np.fft.fftfreq(N, T))\n\n# Plot with subplot_mosaic\nfig, axs = plt.subplot_mosaic([[\"scipy\", \"numpy\"]], figsize=(7, 5), layout=\"constrained\", sharey=True)\n\n# Scipy FFT\naxs[\"scipy\"].plot(xf, FT_exact, 'k-', linewidth=1.5, label='Exact FT')\naxs[\"scipy\"].plot(xf, np.real(yf_scipy), 'r--', linewidth=1, label='FFT (scipy)')\naxs[\"scipy\"].set_xlim(-6, 6)\naxs[\"scipy\"].set_ylim(-1, 1)\naxs[\"scipy\"].set_title(\"Scipy FFT\")\naxs[\"scipy\"].set_xlabel(\"Frequency\")\naxs[\"scipy\"].set_ylabel(\"Amplitude\")\naxs[\"scipy\"].legend()\naxs[\"scipy\"].grid(False)\n\n# NumPy FFT\naxs[\"numpy\"].plot(xf_numpy, FT_exact, 'k-', linewidth=1.5, label='Exact FT')\naxs[\"numpy\"].plot(xf_numpy, np.real(yf_numpy), 'b--', linewidth=1, label='FFT (numpy)')\naxs[\"numpy\"].set_xlim(-6, 6)\naxs[\"numpy\"].set_title(\"NumPy FFT\")\naxs[\"numpy\"].set_xlabel(\"Frequency\")\naxs[\"numpy\"].legend()\naxs[\"numpy\"].grid(False)\n\nplt.suptitle(\"Comparison of FFT Implementations vs. Exact Fourier Transform\", fontsize=14)\nplt.show()"
  },
  {
    "objectID": "Meduim/RegressionLogistique/1_variable_engineering.html",
    "href": "Meduim/RegressionLogistique/1_variable_engineering.html",
    "title": "Missing Data Assessment",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\n#random seed\nnp.random.seed(991)\n#simulate the variable x\nx = np.random.normal(loc= 50,\nscale= 3,\nsize= 1000)\n#equidistant binning\nx_edb= pd.cut(x= x,\nbins= 4)\n#check bins\nx_edb.value_counts().sort_index()\n\n(41.515, 46.471]    124\n(46.471, 51.407]    559\n(51.407, 56.343]    297\n(56.343, 61.278]     20\nName: count, dtype: int64\n\n\n\n#quantile binning\nx_qub= pd.qcut(x= x,\nq= 4)\n#check bins\nx_qub.value_counts().sort_index()\n\n(41.534, 47.897]    250\n(47.897, 49.991]    250\n(49.991, 52.048]    250\n(52.048, 61.278]    250\nName: count, dtype: int64\n\n\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Random seed for reproducibility\nnp.random.seed(991)\n\n# Simulate the variable x\nx = np.random.normal(loc=50, scale=3, size=1000)\n\n# Equidistant binning\nx_edb = pd.cut(x, bins=4)\n\n# Quantile binning\nx_qb = pd.qcut(x, q=4)\n\n# Plot\nfig, axes = plt.subplots(2, 2, figsize=(12, 5),\n                         gridspec_kw={'height_ratios':[1,1], 'width_ratios':[1,1]})\nplt.subplots_adjust(wspace=0.3, hspace=0.5)\n\n# Histogram of original values\naxes[0,0].hist(x, bins=30, color='lightgray', edgecolor='none')\naxes[0,0].set_title(\"Original values\", fontsize=13, weight='bold')\naxes[0,0].set_xlabel(\"x\")\naxes[0,0].set_ylabel(\"count\")\n\n# Remove empty cell (bottom-left)\nfig.delaxes(axes[1,0])\n\n# Equidistant binning counts\nx_edb.value_counts().sort_index().plot(kind='bar', color='lightcoral', ax=axes[0,1])\naxes[0,1].set_title(\"Equidistant binning\", fontsize=12, weight='bold')\naxes[0,1].set_xlabel(\"\")\naxes[0,1].set_ylabel(\"\")\n\n# Quantile binning counts\nx_qb.value_counts().sort_index().plot(kind='bar', color='lightcoral', ax=axes[1,1])\naxes[1,1].set_title(\"Quantile binning\", fontsize=12, weight='bold')\naxes[1,1].set_xlabel(\"\")\naxes[1,1].set_ylabel(\"\")\n\n# Clean style\nfor ax in axes.flat:\n    if ax:\n        ax.spines['top'].set_visible(False)\n        ax.spines['right'].set_visible(False)\n\nplt.show()\n\n\n\n\n\n\n\n\n\\[\nx_{\\text{normalized}} = \\frac{x - x_{\\min}}{x_{\\max} - x_{\\min}} \\, ( \\text{new}_{\\max} - \\text{new}_{\\min} ) + \\text{new}_{\\min}\n\\]\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# -------------------------------\n# Data simulation\n# -------------------------------\nnp.random.seed(123)\nx = np.random.normal(loc=50, scale=3, size=1000)\n\n# Normalization function (min-max scaling)\ndef normalize(x, new_min, new_max):\n    x_r = np.ptp(x)\n    x_t = (x - np.min(x)) / x_r * (new_max - new_min) + new_min\n    return x_t\n\nx_n = normalize(x=x, new_min=0, new_max=10)\n\n# -------------------------------\n# Visualization setup\n# -------------------------------\nsns.set_theme(style=\"whitegrid\", font_scale=1.1)\n\nfig, axes = plt.subplots(1, 2, figsize=(10, 5))\nplt.subplots_adjust(wspace=0.3)\n\n# Histogram of original values\naxes[0].hist(x, bins=30, color='lightgray', edgecolor='black', alpha=0.7)\naxes[0].set_title(\"Original values\", fontsize=13, weight='bold')\naxes[0].set_xlabel(\"x\")\naxes[0].set_ylabel(\"count\")\n\n# Histogram of transformed values\naxes[1].hist(x_n, bins=30, color='lightcoral', edgecolor='black', alpha=0.8)\naxes[1].set_title(\"Transformed values\", fontsize=13, weight='bold')\naxes[1].set_xlabel(\"x normalized\")\naxes[1].set_ylabel(\"count\")\n\n# -------------------------------\n# Global title & annotation\n# -------------------------------\nfig.suptitle(\"Comparison Before and After Normalization\", fontsize=14, weight='bold', y=1.03)\nfig.text(0.5, 0.01,\n         \"Normalization rescales the data to a defined range (here 0–10) without changing its distribution shape.\",\n         ha='center', fontsize=10, color='gray')\n\n# -------------------------------\n# Style adjustments\n# -------------------------------\nfor ax in axes:\n    ax.spines['top'].set_visible(False)\n    ax.spines['right'].set_visible(False)\n    ax.grid(False)\n\nplt.tight_layout()\n\nplt.show()\n\n\n\n\n\n\n\n\n\\[\nx_{\\text{standardized}} = \\frac{x - \\mu}{\\sigma}\n\\]\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# -------------------------------\n# Random seed\n# -------------------------------\nnp.random.seed(123)\n\n# Simulate the variable x\nx = np.random.normal(loc=70, scale=5, size=1000)\n\n# Check mean and standard deviation\nprint([np.mean(x), np.std(x)])\n\n# -------------------------------\n# Standardization function\n# -------------------------------\ndef standardize(x):\n    \"\"\"Standardize the variable x.\"\"\"\n    x_s = (x - np.mean(x)) / np.std(x)\n    return x_s\n\n# Run the function\nx_s = standardize(x)\n\n# Check the standardized mean and std\nprint([np.mean(x_s), np.std(x_s)])\n\n# -------------------------------\n# Visualization setup\n# -------------------------------\nsns.set_theme(style=\"whitegrid\", font_scale=1.1)\n\nfig, axes = plt.subplots(1, 2, figsize=(10, 5))\nplt.subplots_adjust(wspace=0.3)\n\n# Histogram: Original values\naxes[0].hist(x, bins=30, color='lightgray', edgecolor='black', alpha=0.7)\naxes[0].set_title(\"Original values\", fontsize=13, weight='bold')\naxes[0].set_xlabel(\"x\")\naxes[0].set_ylabel(\"count\")\n\n# Histogram: Standardized values\naxes[1].hist(x_s, bins=30, color='lightcoral', edgecolor='black', alpha=0.8)\naxes[1].set_title(\"Transformed values\", fontsize=13, weight='bold')\naxes[1].set_xlabel(\"x standardized\")\naxes[1].set_ylabel(\"count\")\n\n# -------------------------------\n# Global title and note\n# -------------------------------\nfig.suptitle(\"Comparison Before and After Standardization\", fontsize=14, weight='bold', y=1.03)\nfig.text(0.5, 0.01,\n         \"Standardization centers the data (μ = 0) and rescales it so that σ = 1, without altering its shape.\",\n         ha='center', fontsize=10, color='gray')\n\n# -------------------------------\n# Clean style adjustments\n# -------------------------------\nfor ax in axes:\n    ax.grid(False)  # no gridlines\n    ax.spines['top'].set_visible(False)\n    ax.spines['right'].set_visible(False)\n\nplt.tight_layout()\nplt.show()\n\n[np.float64(69.80217931959604), np.float64(5.003937687581167)]\n[np.float64(1.0222933610748442e-15), np.float64(1.0)]\n\n\n\n\n\n\n\n\n\n\\[\nx_{\\text{transformed}} = \\Phi^{-1} \\left( \\frac{r - k}{n - 2k + 1} \\right)\n\\]\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import rankdata, norm\n\n# -------------------------------\n# Random seed\n# -------------------------------\nnp.random.seed(984)\n\n# Simulate a bimodal variable\nx1 = np.random.normal(loc=10, scale=2, size=500)\nx2 = np.random.normal(loc=20, scale=3, size=500)\nx = np.concatenate((x1, x2))\n\n# -------------------------------\n# Rank-based inverse normal transformation\n# -------------------------------\ndef inverse_normal_transform(x, k=3/8, method=\"average\"):\n    \"\"\"\n    Apply rank-based inverse normal transformation (Blom method by default).\n    x : array-like\n        Input variable\n    k : float, optional\n        Constant (default=3/8)\n    method : str, optional\n        Method for dealing with ties ('average' recommended)\n    \"\"\"\n    n = len(x)\n    r = rankdata(a=x, method=method)\n    xt = norm.ppf((r - k) / (n - 2 * k + 1))\n    return xt\n\n# Apply the transformation\nx_trans = inverse_normal_transform(x)\n\n# -------------------------------\n# Visualization setup\n# -------------------------------\nsns.set_theme(style=\"white\", font_scale=1.1)\nfig, axes = plt.subplots(1, 2, figsize=(10, 5))\nplt.subplots_adjust(wspace=0.3)\n\n# -------------------------------\n# Original variable histogram\n# -------------------------------\naxes[0].hist(x, bins=30, color='lightgray', edgecolor='black', alpha=0.7)\naxes[0].set_title(\"Original values\", fontsize=13, weight='bold')\naxes[0].set_xlabel(\"x\")\naxes[0].set_ylabel(\"count\")\n\n# -------------------------------\n# Transformed variable histogram\n# -------------------------------\naxes[1].hist(x_trans, bins=30, color='lightcoral', edgecolor='black', alpha=0.8)\naxes[1].set_title(\"Rank–based inverse normal transformation\", fontsize=13, weight='bold')\naxes[1].set_xlabel(\"x transformed\")\naxes[1].set_ylabel(\"count\")\n\n# -------------------------------\n# Global title and caption\n# -------------------------------\nfig.suptitle(\"Comparison Before and After Rank–Based Inverse Normal Transformation\",\n             fontsize=14, weight='bold', y=1.03)\nfig.text(0.5, 0.01,\n         \"The transformation converts a non-normal distribution into a shape approximating a standard normal distribution.\",\n         ha='center', fontsize=10, color='gray')\n\n# -------------------------------\n# Style adjustments\n# -------------------------------\nfor ax in axes:\n    ax.grid(False)  # remove gridlines\n    ax.spines['top'].set_visible(False)\n    ax.spines['right'].set_visible(False)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\\[\nWoE = \\ln \\left( \\frac{dist.ne}{dist.e} \\right)\n\\]\n\nimport pandas as pd\n\n# -------------------------------\n# Simulate data\n# -------------------------------\ndata = pd.DataFrame({\n    \"category\": [\"A\", \"B\", \"A\", \"C\", \"B\"],\n    \"value\": [1, 2, 3, 4, 5]\n})\n\n# -------------------------------\n# Dummy encoding (equivalent to model.matrix(~ 0 + category, data))\n# -------------------------------\ndummy = pd.get_dummies(data[\"category\"], prefix=\"category\", drop_first=True).astype(int)\n\n# Display the dummy-encoded DataFrame\nprint(dummy)\n\n   category_B  category_C\n0           0           0\n1           1           0\n2           0           0\n3           0           1\n4           1           0\n\n\n\nimport pandas as pd\nimport statsmodels.api as sm\ny = data[\"value\"]\nX_with_const = sm.add_constant(dummy)   # like adding intercept in R\nmodel = sm.OLS(y, X_with_const).fit()\n\n# -------------------------------\n# Summary of the regression\n# -------------------------------\nprint(model.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                  value   R-squared:                       0.350\nModel:                            OLS   Adj. R-squared:                 -0.300\nMethod:                 Least Squares   F-statistic:                    0.5385\nDate:                Fri, 17 Oct 2025   Prob (F-statistic):              0.650\nTime:                        21:33:08   Log-Likelihood:                -7.7506\nNo. Observations:                   5   AIC:                             21.50\nDf Residuals:                       2   BIC:                             20.33\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          2.0000      1.275      1.569      0.257      -3.485       7.485\ncategory_B     1.5000      1.803      0.832      0.493      -6.257       9.257\ncategory_C     2.0000      2.208      0.906      0.461      -7.500      11.500\n==============================================================================\nOmnibus:                          nan   Durbin-Watson:                   1.500\nProb(Omnibus):                    nan   Jarque-Bera (JB):                0.510\nSkew:                           0.000   Prob(JB):                        0.775\nKurtosis:                       1.435   Cond. No.                         3.60\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n/Users/juniorjumbong/Desktop/personal-website/.venv/lib/python3.13/site-packages/statsmodels/stats/stattools.py:74: ValueWarning:\n\nomni_normtest is not valid with less than 8 observations; 5 samples were given.\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "Others/spurious_reg.html",
    "href": "Others/spurious_reg.html",
    "title": "Linear Regression in Time Series: Sources of Spurious Regression",
    "section": "",
    "text": "It is common when analyzing the relationship between a dependent time series and several independent time series, to use the regression model. In their well know paper, Granger and Newbold (1974) found several articles in the literature, presenting regression models with apparently high goodness of fit, measured by the coefficient of determination, \\(R^2\\), but with very low durbin-watson statistics.\nWhat is particularly surprising is that almost all econometric textbook warns about the danger of autocorrelated errors, yet this issue persist in many published papers. Granger and Newbold (1974) identified several examples. For instance, they found published equations with \\(R^2 = 0.997\\) and the the Durbin-Watson statistic (d) equal to 0.53. The most extreme, the found is an equation with \\(R^2 = 0.999\\) and \\(d = 0.093\\).\nThese clear examples of what is called spurious regression, where the results look statistically impressive but are in fact misleading, falsely suggesting a strong relationship between the variables when no such relationship exists.\nThis honestly made me laugh because, during my internships, I saw many colleagues using regression models for time series data, evaluating performance purely based on the \\(R^2\\), especially when it was high (close to 1), along with metrics like the Mean Squared Error (MSE) or the Mean Absolute Error (MAE), without taking into account the autocorrelation of the residuals.\nWhen I came across this article, I realized how common this issue is. That is why I wanted to show you how crucial to always check the autocorrelation of the residuals when performing a regression analysis with time series data.\n\n\n\nOur post provides :\n\nA detailed explanation of the results from Granger and Newbold (1974).\nA Python simulation replicating the key results presented in their article.\n\n\n\n\nThe classic regression model tests assume independent data. However, in the case of time series, observations are not independent — they are autocorrelated, and sometimes we even observe what’s called serial correlation, which is the correlation between successive values of the series. For example, today’s GDP is strongly correlated with the GDP of the previous quarter. This autocorrelation of the data can lead to correlation in the errors, which influence the results of the regression analysis.\nThere are three main consequences of autocorrelated errors in regression analysis:\n\nEstimation of the coefficients of the model is inefficient.\nForecasts based on the regression equation are sub-optimal.\nThe hypothesis tests of the coefficients are invalid.\n\nThe first two points are well documented in the literature. However, Granger and Newbold (1974) focused specifically on the third point, showing that it’s possible to obtain very high \\(R^2\\) values, even though the models have no real economic meaning — what we call spurious regressions.\nTo support their analysis, the authors first present some key results from time series analysis. Then, they explain how nonsense regressions can occur, and finally, they run simulations to demonstrate that if the variables follow a random walk (or close to it), and if you include variables in the regression that shouldn’t be there, then it becomes the rule rather than the exception to obtain misleading results\nTo walk you through this paper, the next section will introduce the random walk and the ARIMA(0,1,1) process. In section 3, we will explain how Granger and Newbold (1974) describe the emergence of nonsense regressions, with examples illustrated in section 4. Finally, we’ll show how to avoid spurious regressions when working with time series data."
  },
  {
    "objectID": "Others/spurious_reg.html#motivation",
    "href": "Others/spurious_reg.html#motivation",
    "title": "Linear Regression in Time Series: Sources of Spurious Regression",
    "section": "",
    "text": "It is common when analyzing the relationship between a dependent time series and several independent time series, to use the regression model. In their well know paper, Granger and Newbold (1974) found several articles in the literature, presenting regression models with apparently high goodness of fit, measured by the coefficient of determination, \\(R^2\\), but with very low durbin-watson statistics.\nWhat is particularly surprising is that almost all econometric textbook warns about the danger of autocorrelated errors, yet this issue persist in many published papers. Granger and Newbold (1974) identified several examples. For instance, they found published equations with \\(R^2 = 0.997\\) and the the Durbin-Watson statistic (d) equal to 0.53. The most extreme, the found is an equation with \\(R^2 = 0.999\\) and \\(d = 0.093\\).\nThese clear examples of what is called spurious regression, where the results look statistically impressive but are in fact misleading, falsely suggesting a strong relationship between the variables when no such relationship exists.\nThis honestly made me laugh because, during my internships, I saw many colleagues using regression models for time series data, evaluating performance purely based on the \\(R^2\\), especially when it was high (close to 1), along with metrics like the Mean Squared Error (MSE) or the Mean Absolute Error (MAE), without taking into account the autocorrelation of the residuals.\nWhen I came across this article, I realized how common this issue is. That is why I wanted to show you how crucial to always check the autocorrelation of the residuals when performing a regression analysis with time series data."
  },
  {
    "objectID": "Others/spurious_reg.html#contribution",
    "href": "Others/spurious_reg.html#contribution",
    "title": "Linear Regression in Time Series: Sources of Spurious Regression",
    "section": "",
    "text": "Our post provides :\n\nA detailed explanation of the results from Granger and Newbold (1974).\nA Python simulation replicating the key results presented in their article."
  },
  {
    "objectID": "Others/spurious_reg.html#objectives",
    "href": "Others/spurious_reg.html#objectives",
    "title": "Linear Regression in Time Series: Sources of Spurious Regression",
    "section": "",
    "text": "The classic regression model tests assume independent data. However, in the case of time series, observations are not independent — they are autocorrelated, and sometimes we even observe what’s called serial correlation, which is the correlation between successive values of the series. For example, today’s GDP is strongly correlated with the GDP of the previous quarter. This autocorrelation of the data can lead to correlation in the errors, which influence the results of the regression analysis.\nThere are three main consequences of autocorrelated errors in regression analysis:\n\nEstimation of the coefficients of the model is inefficient.\nForecasts based on the regression equation are sub-optimal.\nThe hypothesis tests of the coefficients are invalid.\n\nThe first two points are well documented in the literature. However, Granger and Newbold (1974) focused specifically on the third point, showing that it’s possible to obtain very high \\(R^2\\) values, even though the models have no real economic meaning — what we call spurious regressions.\nTo support their analysis, the authors first present some key results from time series analysis. Then, they explain how nonsense regressions can occur, and finally, they run simulations to demonstrate that if the variables follow a random walk (or close to it), and if you include variables in the regression that shouldn’t be there, then it becomes the rule rather than the exception to obtain misleading results\nTo walk you through this paper, the next section will introduce the random walk and the ARIMA(0,1,1) process. In section 3, we will explain how Granger and Newbold (1974) describe the emergence of nonsense regressions, with examples illustrated in section 4. Finally, we’ll show how to avoid spurious regressions when working with time series data."
  },
  {
    "objectID": "Others/spurious_reg.html#random-walk",
    "href": "Others/spurious_reg.html#random-walk",
    "title": "Linear Regression in Time Series: Sources of Spurious Regression",
    "section": "Random Walk",
    "text": "Random Walk\nLet \\(X_t\\) be a time series. We say that \\(X_t\\) follows a random walk if its representation is given by:\n\\[\nX_t = X_{t-1} + \\epsilon_t\n\\tag{1}\\]\nwhere \\(\\epsilon_t\\) is a white noise. It can be writen as a sum of white noise :\n\\[\nX_t =  X_0 + \\sum_{i=1}^{t} \\epsilon_i\n\\tag{2}\\]\nwhere \\(X_0\\) is the initial value of the series.\nThe random walk is a non-stationary time series. In fact, if we take the Variance of \\(X_t\\), we have:\n\\[\nV(X_t) = t\\sigma^2\n\\tag{3}\\]\nwhich is increasing with time.\nThis model have been found to represent well certain price series, particularly in speculative markets.\nFor many others series,\n\\[\nX_t - X_{t-1} = \\epsilon_t - \\theta \\epsilon_{t-1}\n\\tag{4}\\]\nhas been found to provide a good representation.\nThose non-stationary series are often employed as bench-marks against which the forecasting performance of other models is judged."
  },
  {
    "objectID": "Others/spurious_reg.html#arima011-process",
    "href": "Others/spurious_reg.html#arima011-process",
    "title": "Linear Regression in Time Series: Sources of Spurious Regression",
    "section": "ARIMA(0,1,1) Process",
    "text": "ARIMA(0,1,1) Process\nThe ARIMA(0,1,1) process is given by:\n\\[\nX_t = X_{t-1} + \\epsilon_t - \\theta \\epsilon_{t-1}\n\\tag{5}\\]\nwhere \\(\\epsilon_t\\) is a white noise. The ARIMA(0,1,1) process is non-stationary. It can be written as a sum of independent random walk and white noise :\n\\[\nX_t =  X_0 + random walk + white noise\n\\tag{6}\\]\nIn the next section, we will see how Nonsense Regression can arise when we regress a dependent series on independent series that follow a random walk."
  },
  {
    "objectID": "00_tds/5_missing_data_assessment.html",
    "href": "00_tds/5_missing_data_assessment.html",
    "title": "Missing Data Assessment",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\n\n# --- Reproducibility ---\nnp.random.seed(42)\n\n# --- Parameters ---\nn = 10000\n\n# --- Utility Functions ---\ndef generate_continuous(mean, std, size, missing_rate=0.0):\n    \"\"\"Generate a continuous variable with optional MCAR missingness.\"\"\"\n    values = np.random.normal(loc=mean, scale=std, size=size)\n    if missing_rate &gt; 0:\n        mask = np.random.rand(size) &lt; missing_rate\n        values[mask] = np.nan\n    return values\n\ndef generate_categorical(levels, probs, size, missing_rate=0.0):\n    \"\"\"Generate a categorical variable with optional MCAR missingness.\"\"\"\n    values = np.random.choice(levels, size=size, p=probs).astype(float)\n    if missing_rate &gt; 0:\n        mask = np.random.rand(size) &lt; missing_rate\n        values[mask] = np.nan\n    return values\n\n# --- Variable Generation ---\nvariables = {\n    \"cont_mcar\": generate_continuous(mean=100, std=20, size=n, missing_rate=0.20),\n    \"cat_mcar\": generate_categorical(levels=[0, 1], probs=[0.7, 0.3], size=n, missing_rate=0.10),\n    \"cont_full\": generate_continuous(mean=50, std=10, size=n),\n    \"cat_full\": generate_categorical(levels=[0, 1], probs=[0.6, 0.4], size=n),\n    \"target\": np.random.choice([0, 1], size=n, p=[0.5, 0.5])\n}\n\n# --- Build DataFrame ---\ndf = pd.DataFrame(variables)\n\n# --- Display Summary ---\nprint(df.head())\nprint(\"\\nMissing value counts:\")\nprint(df.isnull().sum())\n\n    cont_mcar  cat_mcar  cont_full  cat_full  target\n0  109.934283       0.0  58.674277       0.0       1\n1         NaN       0.0  41.178927       0.0       1\n2         NaN       1.0  39.448885       1.0       1\n3         NaN       1.0  47.594213       0.0       1\n4   95.316933       0.0  58.287854       1.0       0\n\nMissing value counts:\ncont_mcar    1952\ncat_mcar      993\ncont_full       0\ncat_full        0\ntarget          0\ndtype: int64\n\n\n\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\n\ndef stratified_split(df, strat_vars, test_size=0.3, random_state=None):\n    \"\"\"\n    Split a DataFrame into train and test sets with stratification\n    based on one or multiple variables.\n\n    Parameters\n    ----------\n    df : pandas.DataFrame\n        The input dataset.\n    strat_vars : list or str\n        Column name(s) used for stratification.\n    test_size : float, default=0.3\n        Proportion of the dataset to include in the test split.\n    random_state : int, optional\n        Random seed for reproducibility.\n\n    Returns\n    -------\n    train_df : pandas.DataFrame\n        Training set.\n    test_df : pandas.DataFrame\n        Test set.\n    \"\"\"\n    # Ensure strat_vars is a list\n    if isinstance(strat_vars, str):\n        strat_vars = [strat_vars]\n\n    # Create a combined stratification key\n    strat_key = df[strat_vars].astype(str).fillna(\"MISSING\").agg(\"_\".join, axis=1)\n\n    # Perform stratified split\n    train_df, test_df = train_test_split(\n        df,\n        test_size=test_size,\n        stratify=strat_key,\n        random_state=random_state\n    )\n\n    return train_df, test_df\n\n\n# --- Exemple d'utilisation ---\n# Stratification sur cat_mcar, cat_full et target\ntrain_df, test_df = stratified_split(df, strat_vars=[\"cat_mcar\", \"cat_full\", \"target\"], test_size=0.3, random_state=42)\n\nprint(f\"Train size: {train_df.shape[0]}  ({len(train_df)/len(df):.1%})\")\nprint(f\"Test size:  {test_df.shape[0]}  ({len(test_df)/len(df):.1%})\")\n\nTrain size: 7000  (70.0%)\nTest size:  3000  (30.0%)\n\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# --- Step 1: Train/Test Split with Stratification ---\ntrain_df, test_df = stratified_split(\n    df,\n    strat_vars=[\"cat_mcar\", \"cat_full\", \"target\"],\n    test_size=0.3,\n    random_state=42\n)\n\n# --- Step 2: Create the R indicator on the training set ---\ntrain_df = train_df.copy()\ntrain_df[\"R_cont_mcar\"] = np.where(train_df[\"cont_mcar\"].isnull(), 0, 1)\n\n# --- Step 3: Prepare the data for comparison ---\ndf_obs = pd.DataFrame({\n    \"cont_full\": train_df.loc[train_df[\"R_cont_mcar\"] == 1, \"cont_full\"],\n    \"Group\": \"Observed (R=1)\"\n})\n\ndf_miss = pd.DataFrame({\n    \"cont_full\": train_df.loc[train_df[\"R_cont_mcar\"] == 0, \"cont_full\"],\n    \"Group\": \"Missing (R=0)\"\n})\n\ndf_all = pd.concat([df_obs, df_miss])\n\n# --- Step 4: KS Test before plotting ---\nfrom scipy.stats import ks_2samp\nstat, p_value = ks_2samp(\n    train_df.loc[train_df[\"R_cont_mcar\"] == 1, \"cont_full\"],\n    train_df.loc[train_df[\"R_cont_mcar\"] == 0, \"cont_full\"]\n)\n\n# --- Step 5: Visualization with KS result ---\nplt.figure(figsize=(8, 6))\nsns.boxplot(\n    x=\"Group\", \n    y=\"cont_full\", \n    data=df_all,\n    palette=\"Set2\",\n    width=0.6,\n    fliersize=3\n)\n\n# Add red diamonds for means\nmeans = df_all.groupby(\"Group\")[\"cont_full\"].mean()\nfor i, m in enumerate(means):\n    plt.scatter(i, m, color=\"red\", marker=\"D\", s=50, zorder=3, label=\"Mean\" if i == 0 else \"\")\n\n# Title and KS test result\nplt.title(\"Distribution of cont_full by Missingness of cont_mcar (Train Set)\",\n          fontsize=14, weight=\"bold\")\n\n# Add KS result as text box\ntextstr = f\"KS Statistic = {stat:.3f}\\nP-value = {p_value:.3f}\"\nplt.gca().text(\n    0.05, 0.95, textstr,\n    transform=plt.gca().transAxes,\n    fontsize=10,\n    verticalalignment='top',\n    bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"white\", alpha=0.8)\n)\n\nplt.ylabel(\"cont_full\", fontsize=12)\nplt.xlabel(\"\")\nsns.despine()\nplt.legend()\nplt.show()\n\n/var/folders/v8/l5q0bw4s2ln17s59y7cc86rm0000gn/T/ipykernel_57463/38423988.py:38: FutureWarning:\n\n\n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n\n\n\n\n\n\n\n\n\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy.stats import chi2_contingency\n\n# --- Step 1: Build contingency table on the TRAIN set ---\ncontingency_table = pd.crosstab(train_df[\"R_cont_mcar\"], train_df[\"cat_full\"])\nchi2, p_value, dof, expected = chi2_contingency(contingency_table)\n\n# --- Step 2: Compute proportions for each group ---\n# --- Recompute proportions but flip the axes ---\nprops = contingency_table.div(contingency_table.sum(axis=1), axis=0)\n\n# Transform for plotting: Group (R) on x-axis, Category as hue\ndf_props = props.reset_index().melt(\n    id_vars=\"R_cont_mcar\",\n    var_name=\"Category\",\n    value_name=\"Proportion\"\n)\n\n# Map R values to clear labels\ndf_props[\"Group\"] = df_props[\"R_cont_mcar\"].map({1: \"Observed (R=1)\", 0: \"Missing (R=0)\"})\n\n# --- Plot: Group on x-axis, bars show proportions of each category ---\nsns.set_theme(style=\"whitegrid\")\nplt.figure(figsize=(8,6))\n\nsns.barplot(\n    x=\"Group\", y=\"Proportion\", hue=\"Category\",\n    data=df_props, palette=\"Set2\"\n)\n\n# Title and Chi² result\nplt.title(\"Proportion of cat_full by Observed/Missing Status of cont_mcar (Train Set)\",\n          fontsize=14, weight=\"bold\")\n\n# Add Chi² result as a text box\ntextstr = f\"Chi² = {chi2:.3f}, p = {p_value:.3f}\"\nplt.gca().text(\n    0.05, 0.95, textstr,\n    transform=plt.gca().transAxes,\n    fontsize=10,\n    verticalalignment='top',\n    bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"white\", alpha=0.8)\n)\n\nplt.xlabel(\"Observed / Missing Group (R)\")\nplt.ylabel(\"Proportion\")\nplt.legend(title=\"cat_full Category\")\nsns.despine()\nplt.show()"
  },
  {
    "objectID": "00_tds/5_missing_data_assessment.html#assessing-the-mcar-assumption",
    "href": "00_tds/5_missing_data_assessment.html#assessing-the-mcar-assumption",
    "title": "Missing Data Assessment",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\n\n# --- Reproducibility ---\nnp.random.seed(42)\n\n# --- Parameters ---\nn = 10000\n\n# --- Utility Functions ---\ndef generate_continuous(mean, std, size, missing_rate=0.0):\n    \"\"\"Generate a continuous variable with optional MCAR missingness.\"\"\"\n    values = np.random.normal(loc=mean, scale=std, size=size)\n    if missing_rate &gt; 0:\n        mask = np.random.rand(size) &lt; missing_rate\n        values[mask] = np.nan\n    return values\n\ndef generate_categorical(levels, probs, size, missing_rate=0.0):\n    \"\"\"Generate a categorical variable with optional MCAR missingness.\"\"\"\n    values = np.random.choice(levels, size=size, p=probs).astype(float)\n    if missing_rate &gt; 0:\n        mask = np.random.rand(size) &lt; missing_rate\n        values[mask] = np.nan\n    return values\n\n# --- Variable Generation ---\nvariables = {\n    \"cont_mcar\": generate_continuous(mean=100, std=20, size=n, missing_rate=0.20),\n    \"cat_mcar\": generate_categorical(levels=[0, 1], probs=[0.7, 0.3], size=n, missing_rate=0.10),\n    \"cont_full\": generate_continuous(mean=50, std=10, size=n),\n    \"cat_full\": generate_categorical(levels=[0, 1], probs=[0.6, 0.4], size=n),\n    \"target\": np.random.choice([0, 1], size=n, p=[0.5, 0.5])\n}\n\n# --- Build DataFrame ---\ndf = pd.DataFrame(variables)\n\n# --- Display Summary ---\nprint(df.head())\nprint(\"\\nMissing value counts:\")\nprint(df.isnull().sum())\n\n    cont_mcar  cat_mcar  cont_full  cat_full  target\n0  109.934283       0.0  58.674277       0.0       1\n1         NaN       0.0  41.178927       0.0       1\n2         NaN       1.0  39.448885       1.0       1\n3         NaN       1.0  47.594213       0.0       1\n4   95.316933       0.0  58.287854       1.0       0\n\nMissing value counts:\ncont_mcar    1952\ncat_mcar      993\ncont_full       0\ncat_full        0\ntarget          0\ndtype: int64\n\n\n\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\n\ndef stratified_split(df, strat_vars, test_size=0.3, random_state=None):\n    \"\"\"\n    Split a DataFrame into train and test sets with stratification\n    based on one or multiple variables.\n\n    Parameters\n    ----------\n    df : pandas.DataFrame\n        The input dataset.\n    strat_vars : list or str\n        Column name(s) used for stratification.\n    test_size : float, default=0.3\n        Proportion of the dataset to include in the test split.\n    random_state : int, optional\n        Random seed for reproducibility.\n\n    Returns\n    -------\n    train_df : pandas.DataFrame\n        Training set.\n    test_df : pandas.DataFrame\n        Test set.\n    \"\"\"\n    # Ensure strat_vars is a list\n    if isinstance(strat_vars, str):\n        strat_vars = [strat_vars]\n\n    # Create a combined stratification key\n    strat_key = df[strat_vars].astype(str).fillna(\"MISSING\").agg(\"_\".join, axis=1)\n\n    # Perform stratified split\n    train_df, test_df = train_test_split(\n        df,\n        test_size=test_size,\n        stratify=strat_key,\n        random_state=random_state\n    )\n\n    return train_df, test_df\n\n\n# --- Exemple d'utilisation ---\n# Stratification sur cat_mcar, cat_full et target\ntrain_df, test_df = stratified_split(df, strat_vars=[\"cat_mcar\", \"cat_full\", \"target\"], test_size=0.3, random_state=42)\n\nprint(f\"Train size: {train_df.shape[0]}  ({len(train_df)/len(df):.1%})\")\nprint(f\"Test size:  {test_df.shape[0]}  ({len(test_df)/len(df):.1%})\")\n\nTrain size: 7000  (70.0%)\nTest size:  3000  (30.0%)\n\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# --- Step 1: Train/Test Split with Stratification ---\ntrain_df, test_df = stratified_split(\n    df,\n    strat_vars=[\"cat_mcar\", \"cat_full\", \"target\"],\n    test_size=0.3,\n    random_state=42\n)\n\n# --- Step 2: Create the R indicator on the training set ---\ntrain_df = train_df.copy()\ntrain_df[\"R_cont_mcar\"] = np.where(train_df[\"cont_mcar\"].isnull(), 0, 1)\n\n# --- Step 3: Prepare the data for comparison ---\ndf_obs = pd.DataFrame({\n    \"cont_full\": train_df.loc[train_df[\"R_cont_mcar\"] == 1, \"cont_full\"],\n    \"Group\": \"Observed (R=1)\"\n})\n\ndf_miss = pd.DataFrame({\n    \"cont_full\": train_df.loc[train_df[\"R_cont_mcar\"] == 0, \"cont_full\"],\n    \"Group\": \"Missing (R=0)\"\n})\n\ndf_all = pd.concat([df_obs, df_miss])\n\n# --- Step 4: KS Test before plotting ---\nfrom scipy.stats import ks_2samp\nstat, p_value = ks_2samp(\n    train_df.loc[train_df[\"R_cont_mcar\"] == 1, \"cont_full\"],\n    train_df.loc[train_df[\"R_cont_mcar\"] == 0, \"cont_full\"]\n)\n\n# --- Step 5: Visualization with KS result ---\nplt.figure(figsize=(8, 6))\nsns.boxplot(\n    x=\"Group\", \n    y=\"cont_full\", \n    data=df_all,\n    palette=\"Set2\",\n    width=0.6,\n    fliersize=3\n)\n\n# Add red diamonds for means\nmeans = df_all.groupby(\"Group\")[\"cont_full\"].mean()\nfor i, m in enumerate(means):\n    plt.scatter(i, m, color=\"red\", marker=\"D\", s=50, zorder=3, label=\"Mean\" if i == 0 else \"\")\n\n# Title and KS test result\nplt.title(\"Distribution of cont_full by Missingness of cont_mcar (Train Set)\",\n          fontsize=14, weight=\"bold\")\n\n# Add KS result as text box\ntextstr = f\"KS Statistic = {stat:.3f}\\nP-value = {p_value:.3f}\"\nplt.gca().text(\n    0.05, 0.95, textstr,\n    transform=plt.gca().transAxes,\n    fontsize=10,\n    verticalalignment='top',\n    bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"white\", alpha=0.8)\n)\n\nplt.ylabel(\"cont_full\", fontsize=12)\nplt.xlabel(\"\")\nsns.despine()\nplt.legend()\nplt.show()\n\n/var/folders/v8/l5q0bw4s2ln17s59y7cc86rm0000gn/T/ipykernel_57463/38423988.py:38: FutureWarning:\n\n\n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n\n\n\n\n\n\n\n\n\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy.stats import chi2_contingency\n\n# --- Step 1: Build contingency table on the TRAIN set ---\ncontingency_table = pd.crosstab(train_df[\"R_cont_mcar\"], train_df[\"cat_full\"])\nchi2, p_value, dof, expected = chi2_contingency(contingency_table)\n\n# --- Step 2: Compute proportions for each group ---\n# --- Recompute proportions but flip the axes ---\nprops = contingency_table.div(contingency_table.sum(axis=1), axis=0)\n\n# Transform for plotting: Group (R) on x-axis, Category as hue\ndf_props = props.reset_index().melt(\n    id_vars=\"R_cont_mcar\",\n    var_name=\"Category\",\n    value_name=\"Proportion\"\n)\n\n# Map R values to clear labels\ndf_props[\"Group\"] = df_props[\"R_cont_mcar\"].map({1: \"Observed (R=1)\", 0: \"Missing (R=0)\"})\n\n# --- Plot: Group on x-axis, bars show proportions of each category ---\nsns.set_theme(style=\"whitegrid\")\nplt.figure(figsize=(8,6))\n\nsns.barplot(\n    x=\"Group\", y=\"Proportion\", hue=\"Category\",\n    data=df_props, palette=\"Set2\"\n)\n\n# Title and Chi² result\nplt.title(\"Proportion of cat_full by Observed/Missing Status of cont_mcar (Train Set)\",\n          fontsize=14, weight=\"bold\")\n\n# Add Chi² result as a text box\ntextstr = f\"Chi² = {chi2:.3f}, p = {p_value:.3f}\"\nplt.gca().text(\n    0.05, 0.95, textstr,\n    transform=plt.gca().transAxes,\n    fontsize=10,\n    verticalalignment='top',\n    bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"white\", alpha=0.8)\n)\n\nplt.xlabel(\"Observed / Missing Group (R)\")\nplt.ylabel(\"Proportion\")\nplt.legend(title=\"cat_full Category\")\nsns.despine()\nplt.show()"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Meduim Publications",
    "section": "",
    "text": "This website serves as the treasury of my intellectual voyage, housing the jewels of my published articles on Medium, the budding seeds of works in progress, and the pillars of knowledge acquired during my third year in the Risk Management program at the National School of Statistics and Information Analysis in Rennes.\n\n\nMeduim Publications\nMedium is an online publishing platform for sharing ideas, stories, and knowledge through articles written by individuals and organizations.\n\nCredit Scoring modellling approach.\n\nDefinition of default and construction of the database.\nOutliers Identification and Treatment\nMissing data analysis\nDatabase splitting\nPre-selection of explanatory variables\nMethodological Approach To Merge Modalities of qualitative variables\nMethodological Approach To Discretize Quantitative variables\nMonotony and Stability\nOversampling\nLogistic Regression\n\nDefinition\nFitting The logistic Regression\nEvaluation of the logistic Regression\n\nAnalysis of the statistical significance of the model\nThe overall fit of the model : Likelihhod ratio test\nStatistical significance of individual regression coefficients: Likelihood ratio test and Z statistic\nHosmer-Lemeshow test\n\nContribution of variables\n\nModel performances\n\nAccuracy Ratio or Gini\nstability test\n\n\nTimes Series\nTowards data science (TDS) :\n\nProportional Odds Model for Ordinal Logistic Regression\nSpurious Regression in Time Series\nMultiple Linear Regression Analysis\nFourier Transformation\nStepwise Selection Made Simple: Improve Your Regression Models in Python\nIs Your Training Data Representative?\nPSI and Cramér’s V\nImplementing the Fourier Transform Numerically in Python: A Step-by-Step Guide\n\nOthers :\n\nSampling Bias and Class Imbalance in Maximum-likelihood Logistic Regression\nNumber of optimal defaults in credit rating\n\n\n\n\nLearning SAS\n\nBackground :\n\nPrerequisites\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "LearningSas/prerequisites.html",
    "href": "LearningSas/prerequisites.html",
    "title": "Learning SAS",
    "section": "",
    "text": "This document is a compilation of what is essential to know about SAS programming. The are two notions that are fondamental to understand before diving into the SaS programming language. These are the data step and the proc step.\nThe data step consists to create a dataset and to manipulate it. The proc step is used to analyze the data.\nBefore starting, let’s talk about the libname.\n\n\nThe libname statement is used to assign a library reference name to a physical location. The example below shows how to assign the reference name Inputs and outputs to the physical location /home/u63691422/EPG1V2/data and /home/u63691422/EPG1V2/Jumbong_Training/Outputs respectively.\nTlibname Inputs \"/home/u63691422/EPG1V2/data\" ;\nlibname Outputs \"/home/u63691422/EPG1V2/Jumbong_Training/Outputs\";"
  },
  {
    "objectID": "LearningSas/prerequisites.html#operator-summary-table",
    "href": "LearningSas/prerequisites.html#operator-summary-table",
    "title": "Learning SAS",
    "section": "Operator summary table",
    "text": "Operator summary table\n\nTable of comparison operators\n\n\n\nOperator\nSymbol\nMeaning\n\n\n\n\nLT\n&lt;\nLess than\n\n\nGT\n&gt;\nGreater than\n\n\nLE\n&lt;=\nLess than or equal to\n\n\nGE\n&gt;=\nGreater than or equal to\n\n\nEQ\n=\nEqual to\n\n\nNE\n^=\nNot equal to\n\n\nIN\nIN\nIn a list of values\n\n\n\n\n\nLogical operators table\n\n\n\nOperator\nSymbol\nMeaning\n\n\n\n\nAND\n&\nLogical AND\n\n\nOR\n!\nLogical OR\n\n\nNOT\n~\nLogical NOT\n\n\n\n\n\nArithmetic operators table\n\n\n\nOperator\nSymbol\nMeaning\n\n\n\n\nADD\n+\nAddition\n\n\nSUB\n-\nSubtraction\n\n\nMUL\n*\nMultiplication\n\n\nDIV\n/\nDivision\n\n\nPOW\n**\nExponentiation\n\n\nMOD\n%\nModulus\n\n\nMax\n&lt;&gt;\nMaximum\n\n\nMin\n&gt;&lt;\nMinimum\n\n\n\n\n\nOperator of concatenation\nThe concatenation operator is used to concatenate two or more character strings. The concatenation operator is represented by two vertical bars (||)."
  },
  {
    "objectID": "LearningSas/prerequisites.html#variables-selection",
    "href": "LearningSas/prerequisites.html#variables-selection",
    "title": "Learning SAS",
    "section": "Variables Selection",
    "text": "Variables Selection\nIn order to select only some variables from the data source, the keep statement is used.\nDATA Outputs.mydata;\n    SET Inputs.mydata;\n    KEEP var1 var2 var3;\nRUN;"
  },
  {
    "objectID": "LearningSas/prerequisites.html#observation-filtration",
    "href": "LearningSas/prerequisites.html#observation-filtration",
    "title": "Learning SAS",
    "section": "Observation filtration",
    "text": "Observation filtration\n\nIf it is important to extract only observations m and n from the data source, the obs statement is used.\n\nDATA Outputs.mydata;\n    SET Inputs.mydata (firstobs=m obs=n);\nRUN;\n\nIn order to select observations that meet certain conditions, the WHERE statement is used.\n\nDATA Outputs.mydata;\n    SET Inputs.mydata;\n    WHERE CONDITION;\nRUN;"
  },
  {
    "objectID": "LearningSas/prerequisites.html#cumulative-sum-by-group-of-a-variable.",
    "href": "LearningSas/prerequisites.html#cumulative-sum-by-group-of-a-variable.",
    "title": "Learning SAS",
    "section": "Cumulative sum by group of a variable.",
    "text": "Cumulative sum by group of a variable.\n\nproc sort data=INPUTS.class_update out=class_sorted;\n   by Sex;\nrun;\n\ndata output;\n   set class_sorted(keep=Sex Age);\n   by Sex;\n   retain s_age;\n   if first.Sex then s_age = Age; /* Réinitialiser pour chaque groupe */\n   else s_age + Age; /* Cumul des âges */\n  \n   if last.Sex then output;\nrun;\n\nRUN;\n\nThis code compute the cumulative sum of the variable Age by group of the variable and then take the end last element of each group which corresponds to the cumulative sum of the variable Age by group.\n\n## Conactenation and control concatenation\n\n```default\nDATA Outputs.mydata; \n    SET table1 table2;\nRUN;\nDATA Outputs.mydata; \n    SET table1 table2;\n    BY ID;\nRUN;"
  },
  {
    "objectID": "LearningSas/prerequisites.html#compute-the-frequency-of-a-variable-in-a-dataset-and-the-frequency-of-missing-values-of-the-variable.",
    "href": "LearningSas/prerequisites.html#compute-the-frequency-of-a-variable-in-a-dataset-and-the-frequency-of-missing-values-of-the-variable.",
    "title": "Learning SAS",
    "section": "Compute the frequency of a variable in a dataset and the frequency of missing values of the variable.",
    "text": "Compute the frequency of a variable in a dataset and the frequency of missing values of the variable.\nPROC FREQ data = Outputs.mydata ;\n    table variable/missing;\nRUN;"
  },
  {
    "objectID": "LearningSas/prerequisites.html#frequency-on-each-sheet-of-an-excel-file",
    "href": "LearningSas/prerequisites.html#frequency-on-each-sheet-of-an-excel-file",
    "title": "Learning SAS",
    "section": "Frequency on each sheet of an Excel file",
    "text": "Frequency on each sheet of an Excel file\n\n/* Module : Macro \nInput : \n    - data : dataset\n    - list_var : macro variable containing the list of variables to compute the frequency\n    - path : path to save the excel file\nOutput :\n    - Excel file containing the frequency of the variables\n*/\n\n%macro export_freq(data, list_var, path);\n    %let nbvar = %sysfunc(countw(&list_var));\n\n    %do i = 1 %to &nbvar;\n        %let var = %scan(&list_var, &i);\n        PROC FREQ data = &data ;\n            tables &var /out = freq&var missing;\n        RUN;\n\n        PROC EXPORT data = freq&var\n            outfile = \"&path\"\n            dbms = xlsx replace;\n            sheet = \"&var\";\n        RUN;\n    %end;\n%mend export_freq;\n\n%export_freq(Outputs.mydata, var1 var2 var3, /home/u63691422/EPG1V2/Jumbong_Training/Outputs/freq.xlsx);"
  },
  {
    "objectID": "LearningSas/prerequisites.html#if-and-else-if-statement",
    "href": "LearningSas/prerequisites.html#if-and-else-if-statement",
    "title": "Learning SAS",
    "section": "if and else if statement",
    "text": "if and else if statement\ndata Outputs.mydata;\n    set;\n    format methode_agregee $20.;\n    if methode_eng_agregee_1 in(\"IRB\") or methode_eng_agregee_2 in(\"IRB\") then methode_agregee = \"IRB\";\n    else if methode_eng_agregee_1 in(\"PD\") or methode_eng_agregee_2 in(\"PD\") then methode_agregee = \"PD\";\n    else if methode_eng_agregee_1 in(\"LGD\") or methode_eng_agregee_2 in(\"LGD\") then methode_agregee = \"LGD\";\n    else methode_agregee = \"Non renseigné\";\nrun;"
  },
  {
    "objectID": "LearningSas/prerequisites.html#organizing-your-workspace-effectively",
    "href": "LearningSas/prerequisites.html#organizing-your-workspace-effectively",
    "title": "Learning SAS",
    "section": "“Organizing Your Workspace Effectively”",
    "text": "“Organizing Your Workspace Effectively”\nTo work efficiently, it is essential to structure your workspace properly. For a well-organized project, create a main directory named after the project. This directory should include at least three subfolders .\n\nInput (00_Inputs): Stores all raw data and input files.\n\nOutput (01_Codes): Contains all generated data and results.\n\nCodes (03_Outputs): Holds all scripts and working files, possibly with additional subfolders.\n\nEach file of each subfolder should resolve a specific task. Example : - The initialisation for all libraries should be in the file : 00_Libname_Init.sas - The file which contains all macros should be named : 00_Macros.sas"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "Jumbong Junior\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "Meduim/RegressionLogistique/MissForest.html",
    "href": "Meduim/RegressionLogistique/MissForest.html",
    "title": "Missing data treatment and imputation in python",
    "section": "",
    "text": "Introduction\n# Importation des bibliothèques nécessaires import pandas as pd import numpy as np from sklearn.experimental import enable_iterative_imputer # Activation de l’IterativeImputer from sklearn.impute import IterativeImputer from sklearn.ensemble import RandomForestRegressor\n\n\nDéfinition de la fonction\ndef miss_forest_application_fold_mmsa(data_base_train, data_base_test, list_raw_ratios): # Assurez-vous que les valeurs manquantes sont bien définies comme np.nan # data_base_train = data_base_train.replace({None: np.nan, ‘NA’: np.nan, ‘null’: np.nan}) #data_base_test = data_base_test.replace({None: np.nan, ‘NA’: np.nan, ‘null’: np.nan})\n# Préparation des données d'entraînement\nbase_raw_ratios_train = data_base_train[list_raw_ratios].copy()\n\n# Imputation sur la base d'entraînement\nimputer = IterativeImputer(\n    estimator=RandomForestRegressor(n_estimators=100, random_state=42),\n    max_iter=10,\n    random_state=42\n)\nimp_train = pd.DataFrame(\n    imputer.fit_transform(base_raw_ratios_train),\n    columns=list_raw_ratios,\n    index=base_raw_ratios_train.index\n)\n\n# Remplacement des valeurs imputées dans la base d'entraînement originale\ndata_base_train.update(imp_train)\n\n# Imputation sur la base de test\nbase_raw_ratios_test = data_base_test[list_raw_ratios].copy()\nimp_test = pd.DataFrame(\n    imputer.transform(base_raw_ratios_test),\n    columns=list_raw_ratios,\n    index=base_raw_ratios_test.index\n)\n\n# Remplacement des valeurs imputées dans la base de test originale\ndata_base_test.update(imp_test)\n\n# Calcul des erreurs (facultatif)\nerrors = abs(imp_train - base_raw_ratios_train)\nrmse_train = pd.DataFrame({\n    \"ratios\": list_raw_ratios,\n    \"RMSE\": np.sqrt((errors**2).mean(axis=0))\n})\n\nprint(f\" The error is given by {errors}\")\n\n# Sélection des variables à garder ou à supprimer\nvars_to_keep = rmse_train.loc[rmse_train[\"RMSE\"] &lt; 1000, \"ratios\"]\nvars_to_drop = rmse_train.loc[rmse_train[\"RMSE\"] &gt; 0.5, \"ratios\"]\n\n# Filtrage des variables dans les bases d'entraînement et de test\nfinal_train = data_base_train.loc[:, ~data_base_train.columns.isin(vars_to_drop)]\nfinal_test = data_base_test.loc[:, data_base_test.columns.isin(vars_to_keep)]\n\n# Résultats finaux\nreturn {\n    \"final_train\": final_train,\n    \"final_test\": final_test\n}\n\n\nExemple de DataFrame\ndata_train = pd.DataFrame({ “A”: [1, 2, np.nan, 4], “B”: [np.nan, 2, 3, 4], “C”: [5, 6, 7, np.nan] })\ndata_test = pd.DataFrame({ “A”: [np.nan, 2, 3], “B”: [1, np.nan, 3], “C”: [4, 5, np.nan] })\nlist_vars = [“A”, “B”, “C”] results = miss_forest_application_fold_mmsa(data_train, data_test, list_vars)\nresults[“final_train”], results[“final_test”]\n\n```{python}\n\nimport pandas as pd\nimport numpy as np\nimport sys\nimport sklearn.neighbors._base\nsys.modules['sklearn.neighbors.base'] = sklearn.neighbors._base\nfrom missingpy import MissForest\n\n\n# No module named sklearn.neighbors.base\n\n# Sample DataFrame with missing values\ndata = {\n    'A': [1, 2, np.nan, 4],\n    'B': [np.nan, 2, 3, 4],\n    'C': ['cat', np.nan, 'dog', 'cat'],\n    'D': [1.5, 2.5, np.nan, 4.5]\n}\ndf = pd.DataFrame(data)\n\nprint(\"Original DataFrame:\")\nprint(df)\n\n# Convert categorical column to numerical codes (if needed)\ndf['C'] = df['C'].astype('category').cat.codes.replace(-1, np.nan)\n\n# Instantiate MissForest\nimputer = MissForest(random_state=42)\n\n# Impute missing values\nimputed_array = imputer.fit_transform(df.values)\n\n# Convert back to a DataFrame\ndf_imputed = pd.DataFrame(imputed_array, columns=df.columns)\n\n# Convert categorical columns back to original categories\ndf_imputed['C'] = df_imputed['C'].round().astype(int).map({0: 'cat', 1: 'dog'})\n\nprint(\"\\nDataFrame After Imputation:\")\nprint(df_imputed)\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "Others/number_obs_for_model.html",
    "href": "Others/number_obs_for_model.html",
    "title": "Optimal Number of default for constructing a model",
    "section": "",
    "text": "A common step before building a model is to determine the optimal number of observations required. In credit scoring, it’s not only essential to know the total number of observations, but also the optimal number of defaults needed to develop a meaningful and reliable model.\nIn the case of a low-default portfolio (LDP), defaults are rare, making it difficult to assess whether constructing a model is even possible. Rather than building a model blindly and relying on luck for acceptable performance, it is crucial to first determine the minimum number of defaults required to justify model development.\nThis determination must satisfy both statistical and regulatory constraints. From a statistical perspective, the sample must be representative, with a distribution that reflects the underlying population. On the regulatory side, authorities typically require that a rating model include at least seven distinct grades (e.g., AAA, AA, A, BBB, BB, B, C).\nFailing to meet these conditions can lead to regulatory sanctions, delays due to corrective actions, and financial costs for the institution.\nThis brings us to the question, What is the optimal number of defaults required to build a seven-grade rating model for a low-default portfolio that satisfies both statistical rigor and regulatory expectations, while ensuring heterogeneity between rating grades?"
  },
  {
    "objectID": "Others/number_obs_for_model.html#introduction",
    "href": "Others/number_obs_for_model.html#introduction",
    "title": "Optimal Number of default for constructing a model",
    "section": "",
    "text": "A common step before building a model is to determine the optimal number of observations required. In credit scoring, it’s not only essential to know the total number of observations, but also the optimal number of defaults needed to develop a meaningful and reliable model.\nIn the case of a low-default portfolio (LDP), defaults are rare, making it difficult to assess whether constructing a model is even possible. Rather than building a model blindly and relying on luck for acceptable performance, it is crucial to first determine the minimum number of defaults required to justify model development.\nThis determination must satisfy both statistical and regulatory constraints. From a statistical perspective, the sample must be representative, with a distribution that reflects the underlying population. On the regulatory side, authorities typically require that a rating model include at least seven distinct grades (e.g., AAA, AA, A, BBB, BB, B, C).\nFailing to meet these conditions can lead to regulatory sanctions, delays due to corrective actions, and financial costs for the institution.\nThis brings us to the question, What is the optimal number of defaults required to build a seven-grade rating model for a low-default portfolio that satisfies both statistical rigor and regulatory expectations, while ensuring heterogeneity between rating grades?"
  },
  {
    "objectID": "Others/number_obs_for_model.html#which-data-should-be-used",
    "href": "Others/number_obs_for_model.html#which-data-should-be-used",
    "title": "Optimal Number of default for constructing a model",
    "section": "1. Which data should be used ?",
    "text": "1. Which data should be used ?\nIt is important to define the perimeter over which data will be collected. This is done by considering several criteria. If we consider a portfolio made up of large corporations, we can define the perimeter based on company size—for example, a turnover above a certain threshold (100 million euros), the region (Europe, North America, etc.), or the sector (agriculture, industry, services, etc.).\nWhat characterizes large companies is that they rarely default—that is, they generally meet their financial obligations to creditors. As a result, if we assign them ratings on a seven-grade scale (AAA, AA, A, BBB, BB, B, C), very few companies would receive a C rating. Most companies would fall within the intermediate grades (A, BBB, BB), and only a few would receive the highest rating (AAA). Therefore, the distribution of ratings tends to resemble a normal distribution.\nGoing forward, we will assume that our portfolio consists of large corporations and that the rating scale follows a normal distribution.\nIf you’re interested in reproducing this work, you can use a portfolio of your choice and define your own distribution of ratings across the grades."
  },
  {
    "objectID": "Others/number_obs_for_model.html#statistical-contraints-heterogeneity-between-grades.",
    "href": "Others/number_obs_for_model.html#statistical-contraints-heterogeneity-between-grades.",
    "title": "Optimal Number of default for constructing a model",
    "section": "2. Statistical contraints : heterogeneity between grades.",
    "text": "2. Statistical contraints : heterogeneity between grades.\nTo effectively assess the credit risk of counterparties, it’s important that the rating scale is both homogeneous within each category and heterogeneous across categories. In other words, observations within the same rating grade should share similar characteristics — meaning they represent the same level of risk — while observations in different rating grades should reflect distinct risk profiles.\nWe won’t address within-grade homogeneity in this post.\nTo ensure heterogeneity across rating categories, we’ll use the binomial test to compare the proportions of observations assigned to each grade. The difference in proportions will be evaluated under the assumption that it follows a normal distribution with a mean of zero — which serves as the null hypothesis.\nBy dividing the difference by its standard deviation, we obtain a standardized value that follows the standard normal distribution. \\[\nZ = \\frac{p_1 - p_2}{\\sqrt{\\frac{p_1(1-p_1)}{n_1} + \\frac{p_2(1-p_2)}{n_2}}}.\n\\] Where \\(p_1\\) and \\(p_2\\) are the proportions of observations in the two rating categories being compared, and \\(n_1\\) and \\(n_2\\) are the number of observations in each category."
  },
  {
    "objectID": "Others/number_obs_for_model.html#methodology",
    "href": "Others/number_obs_for_model.html#methodology",
    "title": "Optimal Number of default for constructing a model",
    "section": "Methodology",
    "text": "Methodology\nThis section outlines the procedure used to determine the optimal number of defaults when constructing a default model with seven rating grades.\n\nStep 1: Estimating the Number of Observations per Grade\nFirst, we need to determine how many observations fall into each rating grade. Given a total number of observations \\(N\\) and assuming a seven-grade scale (AAAA, AA, A, BBB, BB, B, C), we compute the number of observations per grade using the normal distribution. That means calculating the probability associated with each grade and multiplying it by \\(N\\).\nThere are many ways to assign probabilities across grades, but we choose this method because it is deterministic and replicable. Other approaches, including random sampling techniques, could be used as well. In our case, the number of observations for each grade is generated based on the following methodology:\n\nComputing the probability density function (PDF) of the normal distribution for each grade.\n\nFor the first grade (AAAA), the probability is:\n\n\\[\nP(\\text{AAAA}) = \\frac{F(\\text{AAAA} + \\epsilon)}{2\\epsilon},\n\\]\nwhere \\(F\\) is the cumulative distribution function (CDF) of the normal distribution and \\(\\epsilon \\to 0\\). We assume \\(F(\\text{AAAA} - \\epsilon) = 0\\).\n\nFor middle grades (AA, A, BBB, BB, B), the probability is calculated as:\n\n\\[\nP(\\text{Grade}) = \\frac{F(\\text{Grade} + \\epsilon) - F(\\text{Grade} - \\epsilon)}{2\\epsilon},\n\\]\n\nFor the last grade (C), since \\(F(\\text{C} + \\epsilon) = 1\\), the probability becomes:\n\n\\[\nP(\\text{C}) = \\frac{1 - F(\\text{C} - \\epsilon)}{2\\epsilon}.\n\\]\nThe number of observations in each grade is then computed as:\n\n\\[\nN_{\\text{grade}} = N \\cdot P(\\text{grade}).\n\\]\n\n\nStep 2: Estimating the Number of Defaults per Grade\nNext, we determine the number of defaults for each grade while ensuring heterogeneity between them. In the context of low-default portfolios (LDP), the highest rating (AAAA) is expected to have very few defaults. So, we begin by fixing the number of defaults for grade AAAA at 1.\nTo compute the number of defaults for grade \\(i+1\\) based on grade \\(i\\), we follow this approach:\n\n1. Ensuring Heterogeneity Between Grades\nThe two grades must be heterogeneous, meaning the null hypothesis of the binomial test (that the default rates are equal) must be rejected at a significance level \\(\\alpha\\). This leads to the following statistical condition:\n\\[\nP(Z = \\frac{|p_i - p_{i+1}|}{\\sqrt{\\frac{p_i(1 - p_i)}{N_i} + \\frac{p_{i+1}(1 - p_{i+1})}{N_{i+1}}}} \\geq Z_\\alpha) = \\alpha,\n\\]\nwhere \\(p_i\\) and \\(p_{i+1}\\) are the default rates for grades \\(i\\) and \\(i+1\\), and \\(N_i\\), \\(N_{i+1}\\) are the respective numbers of observations. \\(Z_\\alpha = \\Phi^{-1}(1 - \\alpha/2)\\), where \\(\\Phi^{-1}\\) is the inverse of the cumulative distribution function of the standard normal distribution.\nis the critical value of the standard normal distribution for the chosen confidence level.\nWe can rewrite this condition using the pooled default rate:\n\\[\np = \\frac{N_i p_i + N_{i+1} p_{i+1}}{N_i + N_{i+1}}.\n\\]\nThen the \\(Z\\)-statistic becomes:\n\\[\nP(Z = \\frac{|p_i - p_{i+1}|}{\\sqrt{p(1 - p)\\left(\\frac{1}{N_i} + \\frac{1}{N_{i+1}}\\right)}} \\geq Z_\\alpha ) = \\alpha,\n\\]\nNow, let \\(e = p_{i+1} - p_i\\). Then:\n\\[\np_{i+1} = p_i + e.\n\\]\n\n\n2. Computing the Number of Defaults for Grade \\(i+1\\)\nOnce \\(p_{i+1}\\) is known, the number of defaults for grade \\(i+1\\) is:\n\\[\nn_{i+1} = p_{i+1} \\cdot N_{i+1}.\n\\]\nTo find the value of the optimal \\(e\\) that satisfies the heterogeneity condition, we solve the equation:\n\\[\nZ = \\frac{e}{\\sqrt{p(1 - p)\\left(\\frac{1}{N_i} + \\frac{1}{N_{i+1}}\\right)}} = Z_\\alpha,\n\\]\nThis leads to a second-degree equation of the form:\n\\[\nae^2 + be + c = 0.\n\\]\nwith coefficients:\n\\[\na = -\\left[\\left(\\frac{N_{i+1}}{N_i + N_{i+1}}\\right)^2 + \\frac{1}{Z_\\alpha^2\\left(\\frac{1}{N_i} + \\frac{1}{N_{i+1}}\\right)}\\right],\n\\]\n\\[\nb = (1 - 2p_i)\\left(\\frac{N_{i+1}}{N_i + N_{i+1}}\\right),\n\\]\n\\[\nc = p_i(1 - p_i),\n\\]\nSince \\(a &lt; 0\\) and \\(c &gt; 0\\), the quadratic equation has two real roots — one negative and one positive. We select the positive solution:\n\\[\ne = \\frac{-b - \\sqrt{b^2 - 4ac}}{2a}.\n\\]\n\n\nimport openpyxl\nimport numpy as np\nimport pandas as pd\nfrom scipy.stats import norm\nfrom IPython.display import Markdown\nfrom tabulate import tabulate\nimport seaborn as sns\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "Others/number_obs_for_model.html#results",
    "href": "Others/number_obs_for_model.html#results",
    "title": "Optimal Number of default for constructing a model",
    "section": "Results",
    "text": "Results\nTo implement the method, we use Python. The number of defaults for the highest grade (AAAA) is fixed at 1. A simulation is then conducted by varying the total number of observations from 1,000 to 10,000 in steps of 1,000. The optimal number of defaults is defined as the average number of defaults across all simulations.\n\nSimulating the Distribution of Observations per Grade\nThe code below simulates how observations are distributed across rating grades based on the normal distribution. We assume a mean of 4 (corresponding to grade BBB) and a standard deviation of 1. The number of observations in each grade is determined by multiplying the total number of observations by the probability density function (PDF) of the normal distribution centered at each grade (see Figure 1).\n\ndef generate_obs_pdf(N, mu, sigma,espilon=0.5):\n    grades = ['AAAA', 'AA', 'A', 'BBB', 'BB', 'B', 'C']\n    grades_positions = np.arange(1,len(grades)+1)\n    prob = np.zeros(len(grades))\n    prob[0] = norm.cdf(grades_positions[0] + espilon, mu, sigma) \n    prob[6] = 1 - norm.cdf(grades_positions[6] - espilon, mu, sigma)\n    for i in range(1, len(grades)-1):\n        prob[i] = norm.cdf(grades_positions[i] + espilon, mu, sigma) - norm.cdf(grades_positions[i] - espilon, mu, sigma)\n\n    obs_count = (prob * N).round().astype(int)\n    count_int = np.floor(obs_count).astype(int)\n    remainder = N - count_int.sum()\n    fractional_part = obs_count - count_int\n    sorted_indices = np.argsort(-fractional_part)\n    for i in range(remainder):\n        count_int[sorted_indices[i]] += 1\n\n    obs_count = pd.Series(obs_count, index=grades)\n    return obs_count\n# Define the parameters\nN = 5001\nmu = 4\nsigma = 1\n# Generate the number of observations per grade\nobs_count = generate_obs_pdf(N, mu, sigma)\n\n\n# Plot the results and add the number of observations in each grade\nplt.figure(figsize=(8, 5))\nsns.barplot(x=obs_count.index, y=obs_count.values)\nplt.title(f'Number of Observations per Grade (N={N}, mu={mu}, sigma={sigma})')\nfor i, v in enumerate(obs_count.values):\n    plt.text(i, v + 0.5, str(v), ha='center', va='bottom')\nplt.xlabel('Grade')\nplt.ylabel('Number of Observations')\nplt.xticks(rotation=45)\nplt.show()\n\n\n\n\n\n\n\nFigure 1: Distribution of Observations per Grade\n\n\n\n\n\n\n\nEstimating the Number of Defaults per Grade\nThe following code estimates the number of defaults for each grade, given the total number of observations, while ensuring heterogeneity between consecutive grades. This is done using the binomial test at a significance level \\(\\alpha = 0.05\\) (see Table 1).\n\n\n\nTable 1: Number of Defaults per Grade\n\n\ndef generate_defaut_per_grade(N, mu, sigma, alpha):\n    df_obs = generate_obs_pdf(N, mu, sigma)\n    grades = df_obs.index\n    n = df_obs.values\n\n    # Initialisation des vecteurs\n    z_alpha = norm.ppf(alpha/2)\n    k = len(n)\n    d_rate = np.zeros(k)\n    ecart = np.zeros(k)\n    p_moy = np.zeros(k)\n    stats = np.zeros(k)\n    p_vals = np.zeros(k)\n    n_def = np.zeros(k, dtype=int)\n\n    # Conditions initiales\n    n_def[0] = 1\n    d_rate[0] = 1 / n[0]\n\n    for i in range(1, k):\n        n1, n2 = n[i-1], n[i]\n        prev_rate = d_rate[i-1]\n        frac = n2 / (n1 + n2)\n\n        # Coefficients quadratiques\n        a = - (frac**2 + 1 / (z_alpha**2 * (1/n1 + 1/n2)))\n        b = (1 - 2 * prev_rate) * frac\n        c = prev_rate * (1 - prev_rate)\n        delta = b**2 - 4*a*c\n\n        if delta &lt; 0:\n            raise ValueError(f\"No real solution at i={i} (delta &lt; 0)\")\n\n        ecart[i] = (-b - np.sqrt(delta)) / (2 * a)\n        d_rate[i] = prev_rate + ecart[i]\n        n_def[i] = int(round(d_rate[i] * n[i]))\n        p_moy[i] = prev_rate + ecart[i] * frac\n\n        var = p_moy[i] * (1 - p_moy[i]) * (1/n1 + 1/n2)\n        stats[i] = ecart[i] / np.sqrt(var)\n        p_vals[i] = 2 * (1 - norm.cdf(abs(stats[i])))\n\n    return pd.DataFrame({\n        'Grade': grades,\n        'nb obs': n,\n        '#defaut': n_def,\n        'pct defaut': d_rate,\n        'écart pct defaut': ecart,\n        'proba moyenne': p_moy,\n        'statistic': stats,\n        'p-value': p_vals\n    })\n\n# Exemple d'appel\nN = 5001\nmu = 4\nsigma = 1\nalpha = 0.05\ndf_defaut = generate_defaut_per_grade(N, mu, sigma, alpha)\n# Display the results\nprint(tabulate(df_defaut, headers='keys', tablefmt='psql', showindex=False))\ndf_defaut.to_excel('df_defaut.xlsx', index=False)\n\n+---------+----------+-----------+--------------+--------------------+-----------------+-------------+-----------+\n| Grade   |   nb obs |   #defaut |   pct defaut |   écart pct defaut |   proba moyenne |   statistic |   p-value |\n|---------+----------+-----------+--------------+--------------------+-----------------+-------------+-----------|\n| AAAA    |       31 |         1 |    0.0322581 |          0         |        0        |     0       |      0    |\n| AA      |      303 |        50 |    0.165283  |          0.133025  |        0.152936 |     1.95996 |      0.05 |\n| A       |     1209 |       261 |    0.216208  |          0.0509256 |        0.206003 |     1.95996 |      0.05 |\n| BBB     |     1915 |       472 |    0.246731  |          0.0305224 |        0.234918 |     1.95996 |      0.05 |\n| BB      |     1209 |       336 |    0.278268  |          0.0315377 |        0.258936 |     1.95996 |      0.05 |\n| B       |      303 |       102 |    0.335389  |          0.0571204 |        0.289715 |     1.95996 |      0.05 |\n| C       |       31 |        16 |    0.511876  |          0.176487  |        0.351769 |     1.95996 |      0.05 |\n+---------+----------+-----------+--------------+--------------------+-----------------+-------------+-----------+"
  },
  {
    "objectID": "Others/number_obs_for_model.html#simulating-the-optimal-number-of-defaults",
    "href": "Others/number_obs_for_model.html#simulating-the-optimal-number-of-defaults",
    "title": "Optimal Number of default for constructing a model",
    "section": "Simulating the Optimal Number of Defaults",
    "text": "Simulating the Optimal Number of Defaults\nThis final simulation estimates the optimal number of defaults by computing the average number of defaults over multiple runs. The number of observations varies from 5,000 to 1,000,000 in increments of 5,000(see Figure 2).\n\nplt.figure(figsize=(8, 5))\n\nN_values = np.arange(5000, 1000000, 5000)\noptimal_defaults = [generate_defaut_per_grade(N, mu, sigma, alpha)['#defaut'].sum() for N in N_values]\nmean_defaults = np.mean(optimal_defaults)\nplt.plot(N_values, optimal_defaults, marker='o')\nplt.axhline(mean_defaults, color='r', linestyle='--', label=f'Mean Defaults: {mean_defaults:.2f}')\n\n########################################\n# Ajout de texte pour afficher la valeur de la moyenne\n##########################################\nplt.text(N_values[-1], mean_defaults, f'Mean: {mean_defaults:.2f}', color='red', fontsize=10, ha='left', va='center')\nplt.title('Optimal Number of Defaults vs. Number of Observations')\nplt.xlabel('Number of Observations')\nplt.ylabel('Optimal Number of Defaults')\nplt.xticks(rotation=45)\nplt.legend()\nplt.grid(False)\nplt.show()\n\n\n\n\n\n\n\nFigure 2: Optimal Number of Defaults vs. Number of Observations\n\n\n\n\n\n\nConclusion\nThis study addressed a fundamental challenge in credit risk modeling: determining the optimal number of defaults required to develop a reliable and regulatory-compliant rating model, particularly for low-default portfolios (LDPs). Building such models without sufficient defaults not only compromises statistical robustness but also risks non-compliance with regulatory expectations, especially the requirement of a seven-grade rating scale. Through a structured and reproducible methodology based on the normal distribution of credit grades and binomial statistical tests, the research demonstrated how one can estimate the number of observations and defaults per grade necessary to ensure heterogeneity between rating categories. A key insight from the simulation was that, even with increasing total observations, the number of defaults required stabilizes - highlighting that data quantity alone does not compensate for data quality or risk signal strength. The implications of these findings are significant for risk managers and model validators. Institutions should not only focus on collecting more data but also ensure they meet a minimum threshold of defaults (estimated at 1,419) to build a statistically valid model. This threshold acts as a benchmark for initiating model development, validating its structure, and anticipating regulatory scrutiny. Nevertheless, the approach is not without limitations. The model assumes a normal distribution of credit grades and does not explore within-grade homogeneity or other sources of model uncertainty such as macroeconomic shocks, structural breaks, or portfolio shifts. Additionally, the focus on large corporate portfolios may limit generalizability to other segments like SMEs or retail. Future research could refine the simulation under alternative rating distributions.In conclusion, this research provides a quantitative foundation for institutions managing low-default portfolios to assess their readiness for model development. It encourages a shift from ad hoc modeling toward data-driven thresholds, reinforcing both statistical credibility and regulatory alignment in credit risk modeling."
  },
  {
    "objectID": "Meduim/RegressionLogistique/HosmerL.html",
    "href": "Meduim/RegressionLogistique/HosmerL.html",
    "title": "Evaluation of the Logistic Regression : The Hosmer-Lemeshow test",
    "section": "",
    "text": "Model performance is commonly evaluated based on two main criteria. Discrimination and calibration. The discrimination describes the ability of the model to assign higher probabilities of the outcomes to those observations that actually experience the outcome. A recognized metric for assessing a model’s discrimination is the area under the receiver operating characteristic (ROC) curve. Calibration or fit, on the other hand, captures how accurately the predicted probabilities is close to the actual occurence of the outcome. Several tests and graphical methods have been proposed to assess the calibration of a model, which is often referred to as “goodness of fit.” Among the goodness of fit tests, the Hosmer-Lemeshow (HL) test is the most widely applied approach (Nattino, 2020).\nCentral idea of the test : The main idea of the test is to divide the observations into groups and compute a chi2 statistic that reflects the overall mismatch between the observed number of events and the expected number of events in each group-outcome category.\nAs with most goodness of fit tests, the HL test is designed to decide between a null hypothesis of perfect fit, where the probabilities assumed by the model are hypothesized to coincide with the real probabilities, and a general alternative hypothesis of nonperfect fit. Let’s present the framework of the test.\nThe value of the test statistic is : \\[\nHL  = \\sum_{g=1}^{G} \\frac{(O_{D,k} - E_{D,k})^2}{E_{D,k}} + \\frac{(O_{ND,k} - E_{ND,k})^2}{E_{ND,k}}\n\\]\nWhere :\n\n\\(O_{D,k}\\) and \\(E_{D,k}\\) are respectively the number of observed events (default for example) and the number of expected events in the group k.\n\\(O_{ND,k}\\) and \\(E_{ND,k}\\) are respectively the number of observed non-events (non-default for example) and the number of expected non-events in the group k.\nG is the number of groups (typically 10)."
  },
  {
    "objectID": "Meduim/RegressionLogistique/HosmerL.html#logistic-regression-model",
    "href": "Meduim/RegressionLogistique/HosmerL.html#logistic-regression-model",
    "title": "Evaluation of the Logistic Regression : The Hosmer-Lemeshow test",
    "section": "Logistic Regression Model",
    "text": "Logistic Regression Model\n\nLet Y be the dependent variable: \\(Y = \\begin{cases}\n1 & \\text{if default} \\\\\n0 & \\text{if not default}\n\\end{cases}\\)\nLet \\(( X_1, \\ldots, X_p )\\) represent the set of p variables.\nThe logistic regression equation can be written as: \\[\n\\text{logit}(P(Y = 1 \\mid X)) = \\beta_0 + \\beta_1 X_1 + \\ldots + \\beta_p X_p\n\\]\nSolving for P: \\[\nP(Y = 1 \\mid X) = \\frac{1}{1 + \\exp{-(\\beta_0 + \\beta_1 X_1 + \\ldots + \\beta_p X_p)}}\n\\]\nHere, $_0, _1, , _p $ are parameters to be estimated."
  },
  {
    "objectID": "Meduim/RegressionLogistique/HosmerL.html#computing-probabilities",
    "href": "Meduim/RegressionLogistique/HosmerL.html#computing-probabilities",
    "title": "Evaluation of the Logistic Regression : The Hosmer-Lemeshow test",
    "section": "Computing Probabilities",
    "text": "Computing Probabilities\nFor a dataset with N individuals:\n\nFor each individual i, compute the probability of success: \\[\nP_i = P(Y_i = 1 \\mid X_1^i, \\ldots, X_p^i) = \\frac{1}{1 + \\exp{-(\\beta_0 + \\beta_1 X_1^i + \\ldots + \\beta_p X_p^i)}}\n\\]\n\\(P_i\\) represents the expected probability for individual i .\nCreate a table of individuals with their observed outcomes Y and predicted probabilities \\(P_i\\).\n\n\nExample Table\nAfter computing \\(P_i\\) for all individuals, the results can be summarized in a table:\n\n\n\nIndividual\nEvent (\\(Y\\))\n\\(P_i\\)\n\n\n\n\n1\n1\n0.8\n\n\n2\n0\n0.2\n\n\n3\n1\n0.9\n\n\n4\n0\n0.1\n\n\n…\n…\n…\n\n\nN\n1\n0.95\n\n\n\nIf the logistic regression fits well, the predicted probability \\(P_i\\) for each individual should align closely with the observed outcome Y Specifically:\n\nWhen Y=1 (the event occurred), \\(P_i\\) should be close to 1, reflecting high confidence in predicting the event.\nWhen Y = 0 (the event did not occur), \\(P_i\\) should be close to 0, reflecting high confidence in predicting non-occurrence.\n\n\n\nPerforming the Hosmer-Lemeshow Test\nAfter this stage, it is not difficult to carry out the Hosmer-Lemeshow test. What is necessary is ordering and grouping individuals. The Hosmer-Lemeshow test can be performed by dividing the predicted probabilities (Pi) into deciles (10 groups based on percentile ranks) and then computing the Chi-square statistic that compares the predicted to the observed frequencies (Hyeoun-AE, 2013).\nThe value of the Hosmer-Lemeshow statistic is given by:\n\\[\nH = \\sum_{g=1}^{G} \\frac{(O_g - E_g)^2}{E_g} + \\frac{(n_g - O_g - (n_g - E_g))^2}{n_g - E_g}\n\\]\nWhere:\n\nG : Number of groups (typically 10).\n\\(O_g\\) : Observed number of events in group g.\n\\(E_g\\) : Expected number of events in group g or the sum of predicted probabilities for the group g (\\(\\sum_{i =1}^{n_g}P_i\\)).\n\\(n_g\\) : Total number of individuals in group g.\n\n\n\nExample of Computing Each Element in One Group\nTo illustrate how the statistic is computed for a single group g :\n\nSuppose the group contains \\(n_g = 100\\) individuals.\nOut of these, \\(O_g = 20\\) individuals experienced the event (e.g., default).\nThe sum of predicted probabilities for the group is \\(E_g = 18.5\\).\n\nUsing the formula:\n\\[\nH_g = \\frac{(O_g - E_g)^2}{E_g} + \\frac{(n_g - O_g - (n_g - E_g))^2}{n_g - E_g}\n\\]\nSubstitute the values:\n\nFirst term: \\(\\frac{(20 - 18.5)^2}{18.5}\\)\nSecond term: \\(\\frac{(100 - 20 - (100 - 18.5))^2}{100 - 18.5}\\)\n\nCalculate each term and sum them to obtain the contribution of group g to the overall Hosmer-Lemeshow statistic.\n\n\nInterpreting the Hosmer-Lemeshow Test\nUnder the null hypothesis (the observed default numbers correspond to the expected default numbers), the test statistic asymptotically follows the $^2 $ distribution with G - 2 degrees of freedom, where G is the number of groups.\n\nIf the p-value is higher than 5%, it indicates a small statistic and thus a limited gap between observed defaults and expected defaults, suggesting a good model fit.\nIf the p-value is lower than 5%, it indicates a significant discrepancy between observed and expected values, suggesting a poor model fit.\n\nCaution: Hosmer and Lemeshow recommend avoiding the use of this test when there is a small number of observations (less than 400), as the test may yield unreliable results."
  },
  {
    "objectID": "index_gdr.html",
    "href": "index_gdr.html",
    "title": "Spearman Correlation Matrix",
    "section": "",
    "text": "Compte tenu de ma spécialisation en gestion des risques, les contenus que je prévois de partager dans cette section dédiée à la 3ème année (3A) porteront principalement sur les enseignements spécifiques à cette spécialisation. Je m’efforcerai de rendre ces partages aussi pertinents et enrichissants que possible, en espérant qu’ils serviront de guide précieux pour ceux qui suivront une voie similaire."
  },
  {
    "objectID": "index_gdr.html#the-prediction-risk",
    "href": "index_gdr.html#the-prediction-risk",
    "title": "Spearman Correlation Matrix",
    "section": "The prediction risk",
    "text": "The prediction risk\nThe risk prediction is the expected loss between the predicted value and the true value on new, unseen data. In formula form, it can be expressed as: \\[\n\\text{R(X)} = \\mathbb{E}[(Y - \\hat{Y})^2] = \\mathbb{E}[(Y - \\mathbb{text{E}}(Y|X))^2]\n\\] where \\(Y\\) is the true value, \\(\\hat{Y}\\) is the predicted value, and \\(X\\) is the set of covariates used for prediction.\n\nMay 28, 2025"
  },
  {
    "objectID": "index_gdr.html#observed-default-rate-odr",
    "href": "index_gdr.html#observed-default-rate-odr",
    "title": "Spearman Correlation Matrix",
    "section": "Observed Default Rate (ODR)",
    "text": "Observed Default Rate (ODR)\nHow can we estimate the observed default rate (ODR) of a portfolio of loans?\nThis is example of a database where we want to estimate the ODR and the yearly ODR as the the average of the quarterly ODRs.\n\nimport pandas as pd\nimport numpy as np\n\n\n# Étape 1 : Création de la table \"frequentist_PD\"\n# Simulons 6 trimestres de données pour 2 clusters (A et B)\nclusters = ['A'] * 6 + ['B'] * 6\ndates = pd.date_range('2020-01-01', periods=6, freq='QE').tolist() * 2\nodrQ = [0.020, 0.025, 0.030, 0.028, 0.027, 0.029,\n        0.030, 0.032, 0.031, 0.033, 0.035, 0.034]\n\nfrequentist_PD = pd.DataFrame({\n    'cluster': clusters,\n    'hpm_arret': dates,\n    'odrQ': odrQ\n})\n\n# On ajoute des colonnes d'index pour simuler \"obs\"\nfrequentist_PD['obs'] = frequentist_PD.groupby('cluster').cumcount()\n\n# On trie pour garantir l'ordre temporel par cluster\nfrequentist_PD = frequentist_PD.sort_values(['cluster', 'hpm_arret']).reset_index(drop=True)\n\nfrequentist_PD\n\n\n\n\n\n\n\n\ncluster\nhpm_arret\nodrQ\nobs\n\n\n\n\n0\nA\n2020-03-31\n0.020\n0\n\n\n1\nA\n2020-06-30\n0.025\n1\n\n\n2\nA\n2020-09-30\n0.030\n2\n\n\n3\nA\n2020-12-31\n0.028\n3\n\n\n4\nA\n2021-03-31\n0.027\n4\n\n\n5\nA\n2021-06-30\n0.029\n5\n\n\n6\nB\n2020-03-31\n0.030\n0\n\n\n7\nB\n2020-06-30\n0.032\n1\n\n\n8\nB\n2020-09-30\n0.031\n2\n\n\n9\nB\n2020-12-31\n0.033\n3\n\n\n10\nB\n2021-03-31\n0.035\n4\n\n\n11\nB\n2021-06-30\n0.034\n5\n\n\n\n\n\n\n\n\n# Étape 2 : Simuler les frequentist_PD_2_j (décalages pour j = i+1, i = 1, 2, 3)\n\n# On crée un dictionnaire pour stocker les décalages\nfrequentist_PD_2 = {}\n\n# On prépare une copie de base à laquelle on ajoutera les colonnes décalées\nbase = frequentist_PD.copy()\n\n# Pour chaque i (1 to 3), on génère le j = i + 1 et on décale les colonnes\nfor i in range(1, 4):\n    j = i + 1\n    shifted = base.copy()\n    shifted['obs'] = shifted['obs'] - i  # décale l'observation comme point=obs+i en SAS\n    shifted = shifted.rename(columns={\n        'cluster': f'cluster{j}',\n        'hpm_arret': f'hpm_arret{j}',\n        'odrQ': f'odrQ{j}'\n    })\n    frequentist_PD_2[j] = shifted[['obs', f'cluster{j}', f'hpm_arret{j}', f'odrQ{j}']]\n\n# On merge progressivement pour simuler les frequentist_PD_3_j\nmerged = base.copy()\nfor j in range(2, 5):\n    merged = pd.merge(merged, frequentist_PD_2[j], on='obs', how='left')\n\nmerged\n\n\n\n\n\n\n\n\ncluster\nhpm_arret\nodrQ\nobs\ncluster2\nhpm_arret2\nodrQ2\ncluster3\nhpm_arret3\nodrQ3\ncluster4\nhpm_arret4\nodrQ4\n\n\n\n\n0\nA\n2020-03-31\n0.020\n0\nA\n2020-06-30\n0.025\nA\n2020-09-30\n0.030\nA\n2020-12-31\n0.028\n\n\n1\nA\n2020-03-31\n0.020\n0\nA\n2020-06-30\n0.025\nA\n2020-09-30\n0.030\nB\n2020-12-31\n0.033\n\n\n2\nA\n2020-03-31\n0.020\n0\nA\n2020-06-30\n0.025\nB\n2020-09-30\n0.031\nA\n2020-12-31\n0.028\n\n\n3\nA\n2020-03-31\n0.020\n0\nA\n2020-06-30\n0.025\nB\n2020-09-30\n0.031\nB\n2020-12-31\n0.033\n\n\n4\nA\n2020-03-31\n0.020\n0\nB\n2020-06-30\n0.032\nA\n2020-09-30\n0.030\nA\n2020-12-31\n0.028\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n57\nB\n2020-12-31\n0.033\n3\nB\n2021-03-31\n0.035\nA\n2021-06-30\n0.029\nNaN\nNaT\nNaN\n\n\n58\nB\n2020-12-31\n0.033\n3\nB\n2021-03-31\n0.035\nB\n2021-06-30\n0.034\nNaN\nNaT\nNaN\n\n\n59\nB\n2021-03-31\n0.035\n4\nA\n2021-06-30\n0.029\nNaN\nNaT\nNaN\nNaN\nNaT\nNaN\n\n\n60\nB\n2021-03-31\n0.035\n4\nB\n2021-06-30\n0.034\nNaN\nNaT\nNaN\nNaN\nNaT\nNaN\n\n\n61\nB\n2021-06-30\n0.034\n5\nNaN\nNaT\nNaN\nNaN\nNaT\nNaN\nNaN\nNaT\nNaN\n\n\n\n\n62 rows × 13 columns\n\n\n\nThe SAS script is given below:\n%macro test1;\n\n  %do i=1 %to 3 %by 1;\n    %let j = %sysevalf(&i.+1);\n\n    /* Crée une table contenant les colonnes odrQ décalées */\n    data frequentist_PD_2_&j.;\n      obs1 = 1;\n      do while (obs1 &lt; nobs);\n        set frequentist_PD nobs=nobs;\n        obs&j. = obs1 + &i.;\n\n        set frequentist_PD (\n          rename=(\n            cluster=cluster&j.\n            lb=lb&j.\n            ub=ub&j.\n            hpm_arret=hpm_arret&j.\n            def=defQ&j.\n            n=nQ&j.\n            odrQ=odrQ&j.\n          )\n        ) point=obs&j.;\n\n        output;\n        keep cluster lb ub hpm_arret def defQ&j. n nQ&j. odrQ odrQ&j.;\n        obs1 + 1;\n      end;\n    run;\n\n    /* Initialisation au premier tour */\n    %if &i. = 1 %then %do;\n      data frequentist_PD_3_&j.;\n        set frequentist_PD_2_&j.;\n      run;\n    %end;\n\n    /* Jointure gauche avec la table précédente pour empiler les odrQ */\n    %else %do;\n      proc sql;\n        create table frequentist_PD_3_&j. as\n        select a.*,\n               b.defQ&j.,\n               b.nQ&j.,\n               b.odrQ&j.\n        from frequentist_PD_3_&i. as a\n        left join frequentist_PD_2_&j. as b\n          on a.cluster = b.cluster\n          and a.hpm_arret = b.hpm_arret;\n      quit;\n    %end;\n\n  %end;\n\n  /* Résultat final : table contenant odrQ, odrQ2, odrQ3, odrQ4 */\n  data frequentist_PD_4;\n    set frequentist_PD_3_&j.;\n    \n    /* Calcul du ODR annuel glissant comme moyenne des 4 trimestres */\n    odrY = mean(of odrQ odrQ2 odrQ3 odrQ4);\n  run;\n\n  /* Décale hpm_arret de 9 mois (équivalent intnx) */\n  data frequentist_PD_4;\n    set frequentist_PD_4;\n    hpm_arret = intnx('month', hpm_arret, 9, 'same');\n    if year(hpm_arret) = 2019 then delete; /* Exclusion des années trop anciennes */\n  run;\n\n  /* Sauvegarde finale */\n  data frequentist_PD_5;\n    set frequentist_PD_4;\n  run;\n\n%mend;\n\n/* Exécution de la macro */\n%test1;\n\nMay 27, 2025"
  },
  {
    "objectID": "index_gdr.html#model-selection",
    "href": "index_gdr.html#model-selection",
    "title": "Spearman Correlation Matrix",
    "section": "Model Selection",
    "text": "Model Selection\nWe have a data with many covariates. But we want to include only the covariates that are relevant to the response variable. This allows us to have a parsimonious model, with fewer covariates, which is easier to interpret and to use for prediction.\nGenerally, when we add more covariates to a model, the bias of the model decreases, but the variance increases.\n\nMay 26, 2025"
  },
  {
    "objectID": "index_gdr.html#the-adulterous-woman",
    "href": "index_gdr.html#the-adulterous-woman",
    "title": "Spearman Correlation Matrix",
    "section": "The adulterous woman",
    "text": "The adulterous woman\nA woman was caught in adultery. The Pharisees brought her to Jesus and asked him what should be done with her. They said that according the law of Moses, she should be stoned to death. Jesus replied, “Let anyone among you who is without sin be the first to throw a stone at her.”\nI like this story because it shows that Jesus is a feminist.\n\nMay 25, 2025"
  },
  {
    "objectID": "index_gdr.html#isaac-asimovs-three-laws-of-robotics",
    "href": "index_gdr.html#isaac-asimovs-three-laws-of-robotics",
    "title": "Spearman Correlation Matrix",
    "section": "Isaac Asimov’s “Three Laws of Robotics”",
    "text": "Isaac Asimov’s “Three Laws of Robotics”\n\nA robot may not injure a human being or, through inaction, allow a human being to come to harm.\nA robot must obey the orders given it by human beings except where such orders would conflict with the First Law.\nA robot must protect its own existence as long as such protection does not conflict with the First or Second Law.\n\n\nMay 24, 2025"
  },
  {
    "objectID": "index_gdr.html#density-probability-function-using-plotly",
    "href": "index_gdr.html#density-probability-function-using-plotly",
    "title": "Spearman Correlation Matrix",
    "section": "Density probability function using plotly",
    "text": "Density probability function using plotly\n\nimport pandas as pd\nimport numpy as np\n\n# Simuler les données\ndf = pd.DataFrame({\n    '2012': np.random.randn(200),\n    '2013': np.random.randn(200) + 1\n})\n\n# Calculer les statistiques\nstats_df = pd.DataFrame({\n    'min': df.min(),\n    'mean': df.mean(),\n    'median': df.median(),\n    'max': df.max(),\n    'std': df.std()\n})\n\n# Affichage\nprint(stats_df)\n\n           min     mean    median       max       std\n2012 -2.399943  0.07926  0.075151  2.206767  0.880948\n2013 -1.076208  1.02229  0.963917  3.607347  0.920831\n\n\n\nimport plotly.express as px\nimport pandas as pd\nimport numpy as np\n\n# Simuler les données\ndf = pd.DataFrame({\n    '2012': np.random.randn(200),\n    '2013': np.random.randn(200) + 1\n})\n\n# Convertir au format long\ndf_long = df.melt(var_name='Year', value_name='Value')\n\n# Créer le boxplot\nfig = px.box(df_long, x='Year', y='Value', points=False, title=\"Boxplot for 2012 and 2013\", color='Year',\n             labels={'Year': 'Year', 'Value': 'Values'})\nfig.show()\n\n        \n        \n        \n\n\n                            \n                                            \n\n\n\nimport numpy as np\nimport pandas as pd\nimport plotly.graph_objects as go\nfrom scipy.stats import gaussian_kde\n\n# Example data (you can replace this with your real df[\"age\"])\nage = np.array([15, 16, 16, 17, 18, 19, 20, 21, 22, 22, 23, 24, 25, 25, 26, 27, 28])\n\n# Remove NA if needed\nage = age[~np.isnan(age)]\n\n# KDE estimate\nkde = gaussian_kde(age)\n\n# Define range of x values\nx_vals = np.linspace(age.min(), age.max(), 200)\ny_vals = kde(x_vals)\n\n# Plot using Plotly\nfig = go.Figure()\nfig.add_trace(go.Scatter(x=x_vals, y=y_vals, mode='lines', name='Density', line=dict(width=2)))\nfig.update_layout(title=\"Density of age\", xaxis_title=\"Age\", yaxis_title=\"Density\")\nfig.show()"
  },
  {
    "objectID": "index_gdr.html#exploring-variable-relationships-in-python",
    "href": "index_gdr.html#exploring-variable-relationships-in-python",
    "title": "Spearman Correlation Matrix",
    "section": "Exploring Variable Relationships in Python",
    "text": "Exploring Variable Relationships in Python\nThe graphic analysis is a tool to understand the relationship between the covariates and the response variable. It is very important when we want to perform a linear regression.\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\nfrom statsmodels.api import OLS, add_constant\n\n# Load the dataset\ndf = pd.read_csv('data/Multiple_Regression_Dataset.csv')\ndf.head()\n\n\n\n\n\n\n\n\nR\nAge\nS\nEd\nEx0\nEx1\nLF\nM\nN\nNW\nU1\nU2\nW\nX\n\n\n\n\n0\n79.1\n151\n1\n91\n58\n56\n510\n950\n33\n301\n108\n41\n394\n261\n\n\n1\n163.5\n143\n0\n113\n103\n95\n583\n1012\n13\n102\n96\n36\n557\n194\n\n\n2\n57.8\n142\n1\n89\n45\n44\n533\n969\n18\n219\n94\n33\n318\n250\n\n\n3\n196.9\n136\n0\n121\n149\n141\n577\n994\n157\n80\n102\n39\n673\n167\n\n\n4\n123.4\n141\n0\n121\n109\n101\n591\n985\n18\n30\n91\n20\n578\n174\n\n\n\n\n\n\n\n\n# Create a new figure\n\n# Extract response variable and covariates\nresponse = 'R'\ncovariates = [col for col in df.columns if col != response]\n\nfig, axes = plt.subplots(nrows=4, ncols=4, figsize=(5, 5), sharex=False, sharey=True)\naxes = axes.flatten()\n\n# Plot boxplot for binary variable 'S'\nsns.boxplot(data=df, x='S', y='R', ax=axes[0])\naxes[0].set_title('Boxplot of R by S')\naxes[0].set_xlabel('S')\naxes[0].set_ylabel('R')\n\n# Plot regression lines for all other covariates\nplot_index = 1\nfor cov in covariates:\n    if cov != 'S':\n        sns.regplot(data=df, x=cov, y='R', ax=axes[plot_index], scatter=True, line_kws={\"color\": \"red\"})\n        axes[plot_index].set_title(f'{cov} vs R')\n        axes[plot_index].set_xlabel(cov)\n        axes[plot_index].set_ylabel('R')\n        plot_index += 1\n\n# Hide unused subplots\nfor i in range(plot_index, len(axes)):\n    fig.delaxes(axes[i])\n\nfig.tight_layout()\nplt.show()"
  },
  {
    "objectID": "index_gdr.html#when-several-variables-are-correlated",
    "href": "index_gdr.html#when-several-variables-are-correlated",
    "title": "Spearman Correlation Matrix",
    "section": "When several variables are correlated",
    "text": "When several variables are correlated\nWhen several variables are correlated with each other, we keep only one of them. The one the most correlated with the response variable.\n\n# Step 2: Correlation of each variable with response R\nspearman_corr_with_R = spearman_corr['R'].drop('R')  # exclude R-R\n\n# Step 3: Identify pairs of covariates with strong inter-correlation (e.g., &gt; 0.9)\nstrong_pairs = []\nthreshold = 0.6\ncovariates = spearman_corr_with_R.index\n\nfor i, var1 in enumerate(covariates):\n    for var2 in covariates[i+1:]:\n        if abs(spearman_corr.loc[var1, var2]) &gt; threshold:\n            strong_pairs.append((var1, var2))\n\n# Step 4: From each correlated pair, keep only the variable most correlated with R\nto_keep = set()\nto_discard = set()\n\nfor var1, var2 in strong_pairs:\n    if abs(spearman_corr_with_R[var1]) &gt;= abs(spearman_corr_with_R[var2]):\n        to_keep.add(var1)\n        to_discard.add(var2)\n    else:\n        to_keep.add(var2)\n        to_discard.add(var1)\n\n# Final selection: all covariates excluding the ones to discard due to redundancy\nfinal_selected_variables = [var for var in covariates if var not in to_discard]\n\nfinal_selected_variables\n\n['Ex1', 'LF', 'M', 'N', 'NW', 'U2']"
  },
  {
    "objectID": "index_gdr.html#fit-a-linear-regression-model-on-a-six-variables-after-standardization-not-split-data-into-train-and-test",
    "href": "index_gdr.html#fit-a-linear-regression-model-on-a-six-variables-after-standardization-not-split-data-into-train-and-test",
    "title": "Spearman Correlation Matrix",
    "section": "Fit a linear regression model on a six variables after standardization not split data into train and test",
    "text": "Fit a linear regression model on a six variables after standardization not split data into train and test\n\n# Variables\nX = df[final_selected_variables]\ny = df['R']\n\n# Standardisation des variables explicatives\nscaler = StandardScaler()\nX_scaled_vars = scaler.fit_transform(X)\n\n# ➕ Remettre les noms des colonnes (après standardisation)\nX_scaled_df = pd.DataFrame(X_scaled_vars, columns=final_selected_variables)\n\n# ➕ Ajouter l'intercept (constante)\nX_scaled_df = add_constant(X_scaled_df)\n\n# Régression avec noms conservés\nmodel = OLS(y, X_scaled_df).fit()\nprint(model.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      R   R-squared:                       0.568\nModel:                            OLS   Adj. R-squared:                  0.503\nMethod:                 Least Squares   F-statistic:                     8.773\nDate:                Sat, 31 May 2025   Prob (F-statistic):           4.07e-06\nTime:                        16:51:47   Log-Likelihood:                -218.24\nNo. Observations:                  47   AIC:                             450.5\nDf Residuals:                      40   BIC:                             463.4\nDf Model:                           6                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst         90.5085      3.975     22.767      0.000      82.474      98.543\nEx1           25.3077      5.008      5.054      0.000      15.187      35.429\nLF             4.9155      5.631      0.873      0.388      -6.465      16.296\nM              9.9027      5.681      1.743      0.089      -1.579      21.384\nN              2.6733      5.647      0.473      0.639      -8.741      14.087\nNW            11.1950      4.432      2.526      0.016       2.238      20.152\nU2             3.1268      4.928      0.634      0.529      -6.834      13.088\n==============================================================================\nOmnibus:                        1.619   Durbin-Watson:                   2.089\nProb(Omnibus):                  0.445   Jarque-Bera (JB):                1.126\nSkew:                           0.045   Prob(JB):                        0.569\nKurtosis:                       2.247   Cond. No.                         3.06\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
  },
  {
    "objectID": "index_gdr.html#dont-trust-chatgpt",
    "href": "index_gdr.html#dont-trust-chatgpt",
    "title": "Spearman Correlation Matrix",
    "section": "Don’t Trust ChatGPT",
    "text": "Don’t Trust ChatGPT\nEach year, we see rapid advancements in artificial language tools, especially from companies like OpenAI. These models are evolving fast. Today, we can generate images with ease, solve complex problems in computer science, mathematics, physics, and more. But can we really trust these tools?\nI don’t think so.\nToday, I was writing an article about regression to the mean. The equation is given by:\n\n\\(\\mathbb{E}(Y|X) = \\alpha + \\beta X\\)\n\nMy goal was to explain the meaning of the regression coefficient and offer an intuitive technique to predict the value of \\(Y\\) given a value of \\(X\\). I assumed that the variances of \\(X\\) and \\(Y\\) were both equal to one, and that \\(X\\) was centered (i.e., had mean zero). I then stated that under these conditions, the slope \\(\\beta\\) is equal to the correlation between \\(X\\) and \\(Y\\).\nBut ChatGPT told me this was incorrect. It insisted that this only holds true if \\(Y\\) is also centered.\nYet, we know that if both \\(X\\) and \\(Y\\) are scaled to have unit variance, then the slope of the regression is indeed equal to the correlation between them—even if \\(Y\\) is not centered. That’s a basic identity in linear regression under standardized variables.\nSo while these AI tools can be helpful, they’re not always right. You still need to think critically, check the math, and trust your own understanding.\n\nMay 11, 2025"
  },
  {
    "objectID": "index_gdr.html#sports-bring-us-together",
    "href": "index_gdr.html#sports-bring-us-together",
    "title": "Spearman Correlation Matrix",
    "section": "Sports Bring Us Together",
    "text": "Sports Bring Us Together\nToday, like most days, I went for a run and stopped at the park to do some exercises—push-ups, a few pull-ups, and some jump rope.\nEvery time, I find it amazing. I meet people from all walks of life—different backgrounds, all genders, all ages. Some look wealthy, others look like they’re struggling. Some seem strong and healthy, others clearly carry the weight of illness. There are professionals and amateurs. Some are trying sports for the very first time—and probably won’t be back. Others stick with it for a while before giving up. And then, of course, there are the regulars who keep showing up.\nBut what stays constant is the atmosphere. It’s always welcoming. When you greet someone, they greet you back—with kindness. That simple act of saying hello when you arrive creates a sense of safety. You never know what might happen out there, but that greeting—it’s a small sign of trust, of connection. It reminds me of Marcel Mauss’s idea of the gift and counter-gift: you offer a smile, and in return, you receive not just a smile, but a sense of protection.\n\nMay 11, 2025"
  },
  {
    "objectID": "index_gdr.html#why-certain-couples-marry-and-others-do-not",
    "href": "index_gdr.html#why-certain-couples-marry-and-others-do-not",
    "title": "Spearman Correlation Matrix",
    "section": "Why certain couples marry and others do not?",
    "text": "Why certain couples marry and others do not?\nToday, saturday, May 10 2025, I attended the wedding of a friend–a girl I met in high school. We were in the same grade from ninth grade all the way through senior year.\nShe was smart, almost among the top students in our class. She was attentive, displined, serious and hardworking. She rarely laughted out loud, but she always had a warm smile. Given her academic achievements and her work ethic, I expected her to pursue advanced studies and build a successful career. And that’s exactly what happened–she went on to study at ENSAE, one of the most prestigious schools in France, and she is now an actuary.\nNow she is married too, and I hope she will find the same success in her marriage as she did in her studies and career. But this made me wonder: why do some people marry and others do not? What are the factors that influence the decision to marry?\nAt her wedding, I began to reflect on this question using the story of my friend as a starting point. First, she and her husband come from similar social backgrounds—which I believe played a stabilizing role in their relationship.\nThen, there was distance. After her master’s, she earned a scholarship to continue her studies at ENSAE and moved to Paris, while her husband remained in Cameroon. They had to maintain a long-distance relationship for two years and waited until the third year to get married. Despite the distance, they managed to stay connected and committed.\nLastly, their social circle made a real difference. The groom repeatedly thanked his friends during the ceremony for supporting their relationship. He said they reassured his bride during moments of doubt and helped keep them grounded and hopeful.\nBy the end of the ceremony, I had identified three key factors that seemed to have supported their union: shared social status, the ability to overcome physical distance, and the strength of their support network. I believe there are many other factors yet to discover, and I’m curious to explore what else might influence the decision to marry.\n\nMay 10, 2025"
  },
  {
    "objectID": "index_gdr.html#regression-to-the-mean",
    "href": "index_gdr.html#regression-to-the-mean",
    "title": "Spearman Correlation Matrix",
    "section": "Regression to the mean",
    "text": "Regression to the mean\nRegression to the mean was discovered and named by Sir Francis Galton in the 19th century. It refers to the phenomenon where extreme observations tend to be followed by more moderate ones, and moving closer to the average.\nThis concept is often misunderstood, and interpreted in terms of causality. Take this proposition: &gt; “Highly intelligent women tend to marry men who are less intelligent than they are.” What is the explanation?\nSome may think of highly intelligent women wanting to avoid the competition equally intelligent men, or being forced to compromise in their choice of partner because intelligent men do not want to compete with intelligent women. This explanation is wrong.\nThe correlation between the intelligence scores of spouces is less than perfect. If the correlation between the intelligence scores of spouses is not perfect( and if men and women on average do not differ in intelligence), then it is mathematically inevitable that the higly intelligent women will be married to men who are on overage less intelligent than they are.\n\nMay 9, 2025"
  },
  {
    "objectID": "index_gdr.html#should-you-invest-save-or-keep-your-money-under-the-mattress",
    "href": "index_gdr.html#should-you-invest-save-or-keep-your-money-under-the-mattress",
    "title": "Spearman Correlation Matrix",
    "section": "Should You Invest, Save, or Keep Your Money Under the Mattress?",
    "text": "Should You Invest, Save, or Keep Your Money Under the Mattress?\nA quick read of the bible can help you to answer this question. I recommend reading Mathieu 25:14, the parable of the servants and the master’s reward.\nIn this parable, a master gives three servants different amounts of money to manage during his absence. The first servant receives five talents, the second two talents, and the third one talent. The first two servants invest their money and double it, while the third servant hides his talent in the ground.\nWhen the master returned from his journey, the first two servants presented their profits. The master was proud of them and rewarded them. But the third servant explained he knew that his master was hard and demanding. Because he was afraid of losing the money, he hid the talent in the ground. The master was angry and disappointed, and he took the talent. He told him that if he was so afraid, the least he could have done was put the money in a bank to earn interest.\nThis parable teaches us that the most important to get money is to invest it wisely. If you are afraid of losing your money, you should at least put it in a bank to earn interest. Keep money under a pillow is not a good idea, and should be used as the last resort.\n\nMay 8, 2025"
  },
  {
    "objectID": "index_gdr.html#how-can-we-choose-to-modeling-time-series-using-ardl",
    "href": "index_gdr.html#how-can-we-choose-to-modeling-time-series-using-ardl",
    "title": "Spearman Correlation Matrix",
    "section": "How can we choose to modeling time series using ARDL?",
    "text": "How can we choose to modeling time series using ARDL?\nWhat ultimately determines the choice of a model is the data. This assumes that we’ve already carried out a preliminary analysis to understand the nature of the risk, define its scope, and identify the relevant risk factors.\nThat’s why it’s misguided to say, for example, that we should—or should not—use an ARDL model to analyze a time series. What truly matters is ensuring that the residuals of the model are not autocorrelated.\n\nMay 7, 2025"
  },
  {
    "objectID": "index_gdr.html#what-drives-stock-prices",
    "href": "index_gdr.html#what-drives-stock-prices",
    "title": "Spearman Correlation Matrix",
    "section": "What drives stock prices?",
    "text": "What drives stock prices?\nIt’s been about a week since I invested in Eutelsat stock at €3.59 through my PEA (Plan d’Épargne en Actions). When I woke up on Monday, May 5th, I was surprised to see that the stock had jumped to €4.20. Suddenly, it started dropping, and I was tempted to sell. Fortunately, I didn’t—because by the end of the day, the stock had climbed to over €4.65.\nThat’s a 29.5% increase relative to my investment:\n\\[\n\\frac{4.65 - 3.59}{3.59} \\times 100 ≈ 29.5\\%\n\\]\nNaturally, I started wondering what could have caused such a sharp rise. I looked it up on Google and checked the news on TV—but found nothing.\nLater that afternoon—maybe by chance, maybe because I had been actively searching—I stumbled upon an article in Les Échos. It mentioned that Eutelsat’s CEO, Eva Berneke, had been replaced by Jean-François Fallacher, the former CEO of Orange.\nThat was likely one reason for the stock’s spike. Another possible explanation is the French government’s involvement with the company. In the context of European and French national defense, Eutelsat is seen as a strategic alternative to Starlink, SpaceX’s satellite internet service.\nIt’s incredibly hard to predict stock prices.\nOn a similar note, I also noticed that oil prices had dropped recently. And again, there’s a potential explanation: the number of oil producers within OPEC has increased by three. This rise in oil production leads to a greater supply, which in turn explains the drop in oil prices.\n\nMay 6, 2025"
  },
  {
    "objectID": "index_gdr.html#we-are-champions-the-champions-the-first-trophy-of-harry-kane-in-his-career.",
    "href": "index_gdr.html#we-are-champions-the-champions-the-first-trophy-of-harry-kane-in-his-career.",
    "title": "Spearman Correlation Matrix",
    "section": "We are champions the champions : The first trophy of Harry Kane in his career.",
    "text": "We are champions the champions : The first trophy of Harry Kane in his career.\nYesterday, after Bayer Leverkusen’s draw, Bayern Munich officially clinched the Bundesliga title. This marks the very first trophy of Harry Kane’s career.\nOne question comes to mind: Is this title more meaningful to Harry Kane than it is to other Bayern players like Thomas Müller or Manuel Neuer, who’ve already won countless trophies?I want to take it even further: Is Kane happier about this title than a die-hard Bayern Munich fan might be? To answer those questions properly, we’d have to consider several factors—but let me just share my opinion.\nIt’s true that after a long, demanding season, winning a title is always a great source of satisfaction for any player. But for a club like Bayern Munich, winning the Bundesliga is important, yes—but it’s also expected. The fans and the club’s leadership invest a lot of money in top-tier talent to win the Champions League. So when you’re a Bayern player, lifting the Bundesliga trophy isn’t enough—you need to win the Champions League to feel truly fulfilled.\nNow, for someone like Harry Kane, who’s never won a single trophy in his career and who’s been through a lot of disappointment and heartbreak—with Tottenham and even with England—this title must feel incredibly rewarding. Let’s not forget: he lost the Champions League final in 2019 with Tottenham against Liverpool, and the Euro 2020 final with England against Italy. People even started saying he was cursed. Let’s hope he fully enjoys this title—and that it’s just the beginning of many more to come with Bayern Munich.\n\nMay 5, 2025"
  },
  {
    "objectID": "index_gdr.html#warren-buffett-retires-at-94",
    "href": "index_gdr.html#warren-buffett-retires-at-94",
    "title": "Spearman Correlation Matrix",
    "section": "Warren Buffett retires at 94",
    "text": "Warren Buffett retires at 94\nThe most famous investor in the world, Warren Buffett, has announced his retirement at the age of 94 in the end of the year. He has announced this decision during the annual meeting of Berkshire Hathaway, the company he founded in 1965.\nWhat explains his success? He is known for his long-term investment strategy, which focuses on buying and holding quality companies. He has also been a strong advocate of value investing, which involves looking for undervalued stocks with strong fundamentals.\nDo all these elements fully explain his success? I don’t think so. To explain his success, we also need to take luck into account. He wrote a book, and many others have written about him. But not many people have achieved the kind of success he has. That’s why I want to propose a model to predict Warren Buffett’s success, which will be formulated as follows:\nSuccess = f(strategy, long-term investment, value investing, …) + luck\n\nMay 4, 2025"
  },
  {
    "objectID": "index_gdr.html#diagnostic-de-performance-énergétique-dpe",
    "href": "index_gdr.html#diagnostic-de-performance-énergétique-dpe",
    "title": "Spearman Correlation Matrix",
    "section": "Diagnostic de performance énergétique (DPE)",
    "text": "Diagnostic de performance énergétique (DPE)\nLe DPE renseigne sur les performances énergétiques et environnementales d’un logement et d’un bâtiment, en évaluant ses émissions de gaz à effet de serre (GHG).\nLe DPE contient les informations suivantes : - Les informations sur les caractéristiques du bâtiment telles que la surface, les orientations, les mûrs, les fenêtres, les matériaux, etc. - Les informations sur les équipements du logement tels que le chauffage, la climatisation, l’eau chaude sanitaire, la ventilation, etc.\nLe contenu et les modalités du DPE sont réglementés. Ainsi, les données sur les DPE peuvent être utilisées comme facteur de risque ESG (environnemental, social et de gouvernance).\nPour plus d’informations, veuillez consulter le site de l’ademe ici\n\nMay 3, 2025"
  },
  {
    "objectID": "index_gdr.html#aimé-césaire-inspired-by-the-bible",
    "href": "index_gdr.html#aimé-césaire-inspired-by-the-bible",
    "title": "Spearman Correlation Matrix",
    "section": "Aimé Césaire inspired by the Bible",
    "text": "Aimé Césaire inspired by the Bible\nWhen we take time to reflect on the condition of the oppressed, the poor, and the suffering, we see that the Bible has inspired many well-known writers—among them, Aimé Césaire.\nIn Proverbs 31:8–9, it is written: “Speak up for those who cannot speak for themselves, for the rights of all who are abandoned.Speak up and judge fairly; defend the rights of the poor and needy.”\nWith this in mind, we can believe that Aimé Césaire—a powerful French poet—was deeply influenced by this text when he wrote: “My mouth will be the mouth of those who have no mouth, my voice, the freedom of those who sink into the dungeons of despair.”\n\nMay 2, 2025"
  },
  {
    "objectID": "index_gdr.html#la-clé-lamine-yamal",
    "href": "index_gdr.html#la-clé-lamine-yamal",
    "title": "Spearman Correlation Matrix",
    "section": "La clé Lamine Yamal",
    "text": "La clé Lamine Yamal\nHier soir, le barça affrontait l’Inter Milan dans le cadre du match aller de la demi-finale de la Ligue des champions. Ce match opposait la meilleure attaque de la compétition, le barça, à la meilleure défense, l’Inter Milan. On s’attendait donc à un match difficile et fermé avec peu de buts. Cependant, on a assisté à spectable incroyable, magnifique et inoubliable.\nL’Inter Milan a ouvert le score grâce à un but superbe de Marcus Thuram au tout début du match. L’inter de Milan a même doublé la mise grâce à un but de Dunfries. On s’est dit en ce moment que c’est fini pour le barça. L’Inter Milan a commencé à défendre, à mettre le bus. Mais il a fallu que Yamal trouve la clé pour ouvrir le cadenas mis en place par l’Inter.\nAprès avoir effacé Henrikh Mkhitaryan avec une facilité déconcertante, le joyau de la Masia a fixé Alessandro Bastoni puis décoché un remarquable tir du pied gauche qui est aller fracasser le poteau droit de Yann Sommer avant de franchir la ligne. On a vu en lui du Ronaldinho, du Neymar et même du Messi. On se pose même la question de savoir si c’est le nouveau Messi du Barça. Ce qui est sûr, c’est que Lamine Yamal est un génie, comme l’a dit son entraîneur, ancien entraîneur du Bayern Munich, Hans-Dieter Flick.\nIl est difficile de croire mais cet enfant sort de l’adolescence, mais il a une grande maturité. Il n’a que 17 ans et ce match est son 100e match avec le Barça. Il est le plus jeune joueur de l’histoire a avoir marqué en demi-finale de la Ligue des champions, dépassant ainsi le record d’un certain Kylian Mbappé.\nGrâce à lui, le Barça a pu arracher un match nul 3-3. Nous sommes impatients de voir Yamal briller lors du match retour à Milan."
  },
  {
    "objectID": "index_gdr.html#le-livre-des-réponses",
    "href": "index_gdr.html#le-livre-des-réponses",
    "title": "Spearman Correlation Matrix",
    "section": "Le livre des réponses",
    "text": "Le livre des réponses\nJ’adore la bible parce que tu peux y trouver des réponses à toutes tes questions. Par exemple, si tu est quelqu’un qui travaille beaucoup et dort peu, si ton entourage te dit qu’il faut dormir, que le sommeil est réparateur. Si tu es fatigué par eux, tu peux leur répondre que d’après Proverbes 20:13, “N’aime pas le sommeil, tu risquerais de t’appauvrir. Garde les yeux ouverts et tu seras rassasié de pain.”\nLire la bible, notamment le livre des proverbes ou de l’Ecclésiaste, écrit par Salomon, te donnera de l’intelligence et de la sagesse."
  },
  {
    "objectID": "index_gdr.html#categories",
    "href": "index_gdr.html#categories",
    "title": "Spearman Correlation Matrix",
    "section": "Categories",
    "text": "Categories\nHow tall is Junior? If Junior is 1.5m. Your answer is a function of his age. He is very tall if I tell you that he is 6 years old. Very short if he is 20 years old. Your brain automatically returns the relevant norm, which allows you to make a quick decision.\nWe are also able to match intensity across categories and answer the question: “How expensive is a restaurant that matches Junior’s height?”\nOur world is broken into categories for which we have a norm. And those norms allow us to make quick decisions."
  },
  {
    "objectID": "index_gdr.html#what-evokes-you-the-100-days-of-donald-trump",
    "href": "index_gdr.html#what-evokes-you-the-100-days-of-donald-trump",
    "title": "Spearman Correlation Matrix",
    "section": "What evokes you the 100 days of Donald Trump?",
    "text": "What evokes you the 100 days of Donald Trump?\nEven if Trump sticks to his program, the first 100 days have nonetheless been marked by a major disruption in the global economy, which has had a negative impact on various markets, including Wall Street, the European market, and the Asian market.\nFurthermore, we were shocked by his stances on the war in Ukraine, his attack on the Chairman of the Federal Reserve, Jerome Powell, as well as the increase in tariffs — especially in the context of the trade war with China.\n\nMay 1, 2025"
  },
  {
    "objectID": "index_gdr.html#la-puissance-de-la-parole.",
    "href": "index_gdr.html#la-puissance-de-la-parole.",
    "title": "Spearman Correlation Matrix",
    "section": "La puissance de la parole.",
    "text": "La puissance de la parole.\nLa bible met en évidence la puissance de la parole. Par la parole, Dieu à créé le monde. Pour des non croyants, ceci peut être ridicule. Je pense qu’ils conviennent avec moi que ceux qui maitrisent la parole ont un pouvoir. Ils peuvent séduire, enchanter, persuader, parfois ils peuvent même manipuler.\nDans la bible, notamment dans prophète, l’auteur préconise l’usage adéquat de la parole. Il dit que la langue, qui permet de parler, a un pouvoir de vie et de mort; ceux qui aiment parler en goûteront les fruits. Ensuite, il conseille de refléchir avant de parler. Celui qui répond avant d’avoir écouté fait preuve de folie et se couvre de honte.\nPlusieurs écrivaints ont souligné cette importance de la parole, notamment les mots. Jean Paul Sartre, un écrivain français, disait que “Les mots sont des pistolets chargés.”\n\nApril 30, 2025"
  },
  {
    "objectID": "00_tds/3_proportional_ordinal_regression.html",
    "href": "00_tds/3_proportional_ordinal_regression.html",
    "title": "Proportional Odds Model for Ordinal Logistic Regression",
    "section": "",
    "text": "The proportional odds model for ordinal logistic regression was first introduced by McCullagh (1980). This model extends binary logistic regression to situations where the dependent variable is ordinal—that is, it consists of ordered categorical values. The proportional odds model is built on several assumptions, including independence of observations, linearity of the log-odds, absence of multicollinearity among predictors, and, most importantly, the proportional odds assumption. This last assumption states that the regression coefficients are constant across all thresholds of the ordinal dependent variable. Ensuring the proportional odds assumption holds is crucial for the validity and interpretability of the model.\nA variety of methods have been proposed in the literature to assess model fit and, in particular, to test the proportional odds assumption. In this paper, we focus on two approaches developed by Brant in his article Brant (1990), “Assessing Proportionality in the Proportional Odds Model for Ordinal Logistic Regression.” We also demonstrate how to implement these techniques in Python, applying them to real-world data. Whether you come from a background in data science, machine learning, or statistics, this article aims to help your understand how to evaluate model fit in ordinal logistic regression.\nThis paper is organized into four main sections:"
  },
  {
    "objectID": "00_tds/3_proportional_ordinal_regression.html#introduction-to-the-proportional-odds-model",
    "href": "00_tds/3_proportional_ordinal_regression.html#introduction-to-the-proportional-odds-model",
    "title": "Proportional Odds Model for Ordinal Logistic Regression",
    "section": "Introduction to the Proportional Odds Model",
    "text": "Introduction to the Proportional Odds Model\nBefore presenting the model, we introduce the data structure. We assume we have \\(N\\) independent observations. Each observation is represented by a vector of \\(p\\) explanatory variables \\(X_i = (X_{i1}, X_{i2}, \\ldots, X_{ip})\\), along with a dependent or response variable \\(Y\\) that takes ordinal values from \\(1\\) to \\(K\\). The proportional odds model specifically models the cumulative distribution probabilities of the response variable \\(Y\\), defined as \\(\\gamma_j = P(Y \\leq j \\mid X_i)\\) for \\(j = 1, 2, \\dots, K-1\\), as functions of the explanatory variables \\(X_i\\). The model is formulated as follows:\n\\[\n\\text{logit}(\\gamma_j) = \\log\\left(\\frac{\\gamma_j}{1 - \\gamma_j}\\right) = \\theta_j - \\beta^\\top \\mathbf{X}\n\\tag{1}\\]\nWhere \\(\\theta_j\\) are the intercepts for each category j and respect the condition \\(\\theta_1 &lt; \\theta_2 &lt; ... &lt; \\theta_{K-1}\\), and \\(\\beta\\) is the vector of regression coefficients which are the same for all categories. We observe a monotonic trend in the coefficients \\(\\theta_j\\) across the categories of the response variable Y.\nThis model is also known as the grouped continuous model, as it can be derived by assuming the existence of a continuous latent variable \\(Y^*\\). This latent variable follows a linear regression model with conditional mean \\(\\eta = \\boldsymbol{\\beta}^{\\top} \\mathbf{X}\\), and it relates to the observed ordinal variable \\(Y\\) through thresholds \\(\\theta_j\\) defined as follows: \\[\ny^* = {\\beta}^{T}\\mathbf{X} + \\epsilon\n\\tag{2}\\]\nwhere \\(\\epsilon\\) is an error term (random noise), generally assumed to follow a standard logistic distribution in the proportional odds model.\nThe latent variable \\(Y^*\\) is unobserved and partitioned into intervals defined by thresholds \\(\\theta_1, \\theta_2, \\dots, \\theta_{K-1}\\), generating the observed ordinal variable \\(Y\\) as follows:\n\\[\nY = \\begin{cases}\n1 & \\text{if } Y^* \\leq \\theta_1 \\\\\n2 & \\text{if } \\theta_1 &lt; Y^* \\leq \\theta_2 \\\\\n\\vdots & \\\\\nK & \\text{if } Y^* &gt; \\theta_{K-1}\n\\end{cases}\n\\tag{3}\\]\nIn the next section, we introduce the various approaches proposed by Brant (1990) for assessing the proportional odds assumption. These methods evaluate whether the regression coefficients remain constant across the categories defined by the ordinal response variable."
  },
  {
    "objectID": "00_tds/3_proportional_ordinal_regression.html#assessing-the-proportional-odds-assumption-the-likelihood-ratio-test",
    "href": "00_tds/3_proportional_ordinal_regression.html#assessing-the-proportional-odds-assumption-the-likelihood-ratio-test",
    "title": "Proportional Odds Model for Ordinal Logistic Regression",
    "section": "Assessing the Proportional Odds Assumption: The Likelihood Ratio Test",
    "text": "Assessing the Proportional Odds Assumption: The Likelihood Ratio Test\nTo assess the proportional odds assumption in an ordinal logistic regression model, Brant (1990) proposes the use of the likelihood ratio test. This approach begins by fitting a less restrictive model in which the regression coefficients are allowed to vary across categories. This model is expressed as: \\[\n\\text{logit}(\\gamma_j) = \\theta_j - \\beta_j^\\top \\mathbf{X}\n\\tag{4}\\]\nwhere \\(\\beta_j\\) is the vector of regression coefficients for each category j. Here the coefficients \\(\\beta_j\\) are allowed to vary across categories, which means that the proportional odds assumption is not satisfied. We then use the conventionnel likelihood ratio test to assess the hypothesis : \\[\nH_0: \\beta_j = \\beta \\quad \\text{for all } j = 1, 2, \\ldots, K-1\n\\tag{5}\\]\nTo perform this test, we conduct a likelihood ratio test comparing the unconstrained (non-proportional or satured) model with the constrained (proportional odds or reduced) model.\nBefore proceeding further, we briefly recall how to use the likelihood ratio test in hypothesis testing. Suppose we want to evaluate the null hypothesis \\(H_0 : \\theta \\in \\Theta_0\\) against the alternative \\(H_1 : \\theta \\in \\Theta_1\\),\nThe likelihood ratio statistic is defined as: \\[\n\\lambda = 2 \\log\\left(\\frac{\\displaystyle\\sup_{\\theta \\in \\Theta}\\mathcal{L}(\\theta)}{\\displaystyle\\sup_{\\theta \\in \\Theta_0}\\mathcal{L}(\\theta)}\\right)\n= 2\\log\\left(\\frac{\\mathcal{L}(\\hat{\\theta})}{\\mathcal{L}(\\hat{\\theta}_0)}\\right),\n\\tag{6}\\]\nwhere \\(\\mathcal{L}(\\theta)\\) is the likelihood function, \\(\\hat{\\theta}\\) is the maximum likelihood estimate (MLE) under the full model, and \\(\\hat{\\theta}_0\\) is the MLE under the constrained model. The test statistic \\(\\lambda\\) follows a chi-square distribution with degrees of freedom equal to the difference in the number of parameters between the full and constrained models.\nHere, \\(\\hat{\\theta}\\) is the maximum likelihood estimate (MLE) under the full (unconstrained) model, and \\(\\hat{\\theta}_0\\) is the MLE under the constrained model where the proportional odds assumption holds. The test statistic \\(\\lambda\\) follows a chi-square distribution under the null hypothesis.\nIn a general setting, suppose the full parameter space is denoted by\n\\[\n\\Theta = (\\theta_1, \\theta_2, \\ldots, \\theta_q, \\ldots, \\theta_p),\n\\]\nand the restricted parameter space under the null hypothesis is\n\\[\n\\Theta_0 = (\\theta_1, \\theta_2, \\ldots, \\theta_q).\n\\]\n(Note: These parameters are generic and should not be confused with the \\(K - 1\\) thresholds or intercepts in the proportional odds model.), the likelihood ratio test statistic \\(\\lambda\\) follows a chi-square distribution with \\(p - q\\) degrees of freedom. Where \\(p\\) represents the total number of parameters in the full (unconstrained or “saturated”) model, while \\(K - 1\\) corresponds to the number of parameters in the reduced (restricted) model.\nNow, let us apply this approach to the ordinal logistic regression model with the proportional odds assumption. Assume that our response variable has \\(K\\) ordered categories and that we have \\(p\\) predictor variables. To use the likelihood ratio test to evaluate the proportional odds assumption, we need to compare two models:\n\n1. Unconstrained Model (non-proportional odds):\nThis model allows each outcome threshold to have its own set of regression coefficients, meaning that we do not assume the regression coefficients are equal across all thresholds. The model is defined as:\n\\[\n\\text{logit}(\\mathbb{P}(Y \\leq j \\mid \\mathbf{X})) = \\theta_j - \\boldsymbol{\\beta}_j^\\top \\mathbf{X}\n\\]\n\nThere are \\(K - 1\\) threshold (intercept) parameters: \\(\\theta_1, \\theta_2, \\ldots, \\theta_{K-1}\\)\nEach threshold has its own vector of slope coefficients \\({\\beta}_j\\) of dimension \\(p\\)\n\nThus, the total number of parameters in the unconstrained model is:\n\\[\n(K - 1) \\text{ thresholds} + (K - 1) \\times p \\text{ slopes} = (K - 1)(p + 1)\n\\]\n\n\n2. Proportional Odds Model:\nThis model assumes a single set of regression coefficients for all thresholds:\n\\[\n\\text{logit}(\\mathbb{P}(Y \\leq j \\mid \\mathbf{X})) = \\theta_j - {\\beta}^\\top \\mathbf{X}\n\\]\n\nThere are \\(K - 1\\) threshold parameters\nThere is one common slope vector \\({\\beta}\\) for all \\(j\\)\n\nThus, the total number of parameters in the proportional odds model is:\n\\[\n(K - 1) \\text{ thresholds} + p \\text{ slopes} = (K - 1) + p\n\\]\nThus, the likelihood ratio test statistic follows a chi-square distribution with degrees of freedom:\n\\[\n\\text{df} = [(K - 1) \\times (p+1)] - [(K - 1) + p] = (K - 2) \\times p\n\\tag{7}\\]\nThis test provides a formal way to assess whether the proportional odds assumption holds for the given data. At a significance level of 1%, 5%, or any other conventional threshold, the proportional odds assumption is rejected if the test statistic \\(\\lambda\\) exceeds the critical value from the chi-square distribution with \\((K - 2) \\times p\\) degrees of freedom.\nIn other words, we reject the null hypothesis\n\\[\nH_0 : {\\beta}_1 = {\\beta}_2 = \\cdots = {\\beta}_{K-1} = {\\beta},\n\\]\nwhich states that the regression coefficients are equal across all cumulative logits. This test has the advantage of being straightforward to implement and provides an overall assessment of the proportional odds assumption.\nIn the next section, we introduce the proportional odds test based on separate fits.\n\nAssessing the Proportional Odds Assumption: The Separate Fits Approach\n\nTo understand this part, you must understand the Mahalanobis distance and its properties. The Mahalanobis distance can be used to measure the dissimilarity between two vectors \\(x=(x_1, x_2, \\ldots, x_p)^\\top\\) and \\(y=(y_1, y_2, \\ldots, y_p)^\\top\\) in a multivariate space with the same distribution. It is defined as: \\[\nD_M(x, y) = \\sqrt{(x - y)^\\top \\Sigma^{-1} (x - y)}\n\\tag{8}\\]\nwhere \\(\\Sigma\\) is the covariance matrix of the distribution. The Mahalanobis distance is linked with the \\(\\chi^2\\) distribution, specifically, if \\(X \\sim N(\\mu, \\Sigma)\\) is a p-dimensional normal random vector, with the mean \\(\\mu\\) and covariance matrix \\(\\Sigma\\), then the Mahalanobis distance \\(D_M(X, \\mu)\\) follows a \\(\\chi^2\\) distribution with \\(p\\) degrees of freedom. This step is essential for understanding how to assess proportionality using separate fits. You will see why shortly.\nIn fact, the author notes that the natural approach to evaluating the proportional odds assumption is to fit a set of \\(K-1\\) binary logistic regression models (where \\(K\\) is the number of categories of the response variable), and then use the statistical properties of the estimated parameters to construct a test statistic for the proportional odds hypothesis.\nThe procedure is as follows:\nFirst, we construct separate binary logistic regression models for each threshold \\(j = 1, 2, \\ldots, K-1\\) of the ordinal response variable \\(Y\\). For each threshold \\(j\\), we define a binary variable \\(Z_j\\), which takes the value 1 if the observation exceeds threshold \\(j\\), and 0 otherwise. Specifically, we have: \\[\nZ_j = \\begin{cases}\n0 & \\text{if } Y &gt; j \\\\\n0 & \\text{if } Y \\leq j\n\\end{cases}\n\\tag{9}\\]\nWith the probaility, \\(\\pi_j = P(Z_j = 1 \\mid \\mathbf{X}) = 1 - \\gamma_j\\) satisfying the logistic regression model: \\[\n\\text{logit}(\\pi_j) = \\theta_j - \\beta_j^\\top \\mathbf{X}.\n\\tag{10}\\]\nThen, assessing the proportional odds assumption in this context involves testing the hypothesis that the regression coefficients \\(\\beta_j\\) are equal across all \\(K-1\\) models. This is equivalent to testing the hypothesis:\n\\[\nH_0 : \\beta_1 = \\beta_2 = \\cdots = \\beta_{K-1} = \\beta\n\\tag{11}\\]\nLet \\(\\hat{\\beta}_j\\) denote the maximum likelihood estimators of the regression coefficients for each binary model, and let \\(\\hat{\\beta} = (\\hat{\\beta}_1^\\top, \\hat{\\beta}_2^\\top, \\ldots, \\hat{\\beta}_{K-1}^\\top)^\\top\\) represent the global vector of estimators. This vector is asymptotically normally distributed, such that \\(\\mathbb{E}(\\hat{\\beta}_j) \\approx \\beta\\), with variance-covariance matrix \\(\\mathbb{V}(\\hat{\\beta}_j)\\). The general term of this matrix, \\(\\text{cov}(\\hat{\\beta}_j, \\hat{\\beta}_k)\\), needs to be determined and is given by:\n\\[\n\\widehat{V}(\\hat{\\boldsymbol{\\beta}}) =\n\\begin{bmatrix}\n\\text{Cov}(\\hat{\\boldsymbol{\\beta}}_1, \\hat{\\boldsymbol{\\beta}}_1) & \\text{Cov}(\\hat{\\boldsymbol{\\beta}}_1, \\hat{\\boldsymbol{\\beta}}_2) & \\cdots & \\text{Cov}(\\hat{\\boldsymbol{\\beta}}_1, \\hat{\\boldsymbol{\\beta}}_{K-1}) \\\\\n\\text{Cov}(\\hat{\\boldsymbol{\\beta}}_2, \\hat{\\boldsymbol{\\beta}}_1) & \\text{Cov}(\\hat{\\boldsymbol{\\beta}}_2, \\hat{\\boldsymbol{\\beta}}_2) & \\cdots & \\text{Cov}(\\hat{\\boldsymbol{\\beta}}_2, \\hat{\\boldsymbol{\\beta}}_{K-1}) \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\text{Cov}(\\hat{\\boldsymbol{\\beta}}_{K-1}, \\hat{\\boldsymbol{\\beta}}_1) & \\text{Cov}(\\hat{\\boldsymbol{\\beta}}_{K-1}, \\hat{\\boldsymbol{\\beta}}_2) & \\cdots & \\text{Cov}(\\hat{\\boldsymbol{\\beta}}_{K-1}, \\hat{\\boldsymbol{\\beta}}_{K-1})\n\\end{bmatrix}\n\\in \\mathbb{R}^{(K-1)p \\times (K-1)p}\n\\tag{12}\\]\nwhere \\(\\text{Cov}(\\hat{\\boldsymbol{\\beta}}_j, \\hat{\\boldsymbol{\\beta}}_k)\\) is the covariance between the estimated coefficients of the \\(j\\)-th and \\(k\\)-th binary models. To evaluate the proportional odds assumption, Brant constructs a matrix \\(\\mathbf{D}\\) that captures the differences between the coefficients \\(\\hat{\\beta}_j\\). Recall that each vector \\(\\hat{\\beta}_j\\) has dimension \\(p\\). The matrix \\(\\mathbf{D}\\) is defined as follows:\n\\[\n\\mathbf{D} =\n\\begin{bmatrix}\nI & -I & 0 & \\cdots & 0 \\\\\nI & 0 & -I & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\nI & 0 & 0 & \\cdots & -I \\\\\n\\end{bmatrix}\n\\in \\mathbb{R}^{(K-2)p \\times (K-1)p}\n\\tag{13}\\]\nwhere \\(I\\) is the identity matrix of size \\(p \\times p\\). The first row of the matrix D corresponds to the difference between the first and second coefficients, the second row corresponds to the difference between the second and third coefficients, and so on, until the last row which corresponds to the difference between the \\((K-2)\\)-th and \\((K-1)\\)-th coefficients. We can notice that the product \\(\\mathbf{D} \\hat{{\\beta}}\\) will yield a vector of differences between the coefficients \\(\\hat{\\beta_j}\\).\nOnce the matrix \\(\\mathbf{D}\\) is constructed, Brant defines the Wald statistic \\(X^2\\) to test the proportional odds assumption. This statistic can be interpreted as the Mahalanobis distance between the vector \\(\\mathbf{D} \\hat{\\boldsymbol{\\beta}}\\) and the zero vector. The Wald statistic is defined as follows:\n\\[\nX^2 = (\\mathbf{D} \\hat{{\\beta}})^\\top \\left[ \\mathbf{D} \\widehat{V}(\\hat{{\\beta}}) \\mathbf{D}^\\top \\right]^{-1} (\\mathbf{D} \\hat{{\\beta}})\n\\tag{14}\\]\nwhich will be asymptotically \\(\\chi^2\\) distributed with \\((K - 2)p\\) degrees of freedom under the null hypothesis. The challenging part here is to determine the variance-covariance matrix \\(\\widehat{V}(\\hat{\\beta})\\). In his article, Brant provides an explicit estimator for this variance-covariance matrix, which is based on the maximum likelihood estimators \\(\\hat{\\beta}_j\\) from each binary model.\nIn the following sections, we implement these approaches in Python, using the statsmodels package for the regressions and statistical tests."
  },
  {
    "objectID": "00_tds/3_proportional_ordinal_regression.html#example",
    "href": "00_tds/3_proportional_ordinal_regression.html#example",
    "title": "Proportional Odds Model for Ordinal Logistic Regression",
    "section": "Example",
    "text": "Example\nThe data for this example comes from the “Wine Quality” dataset, which contains information about red wine samples and their quality ratings. The dataset includes 1,599 observations and 12 variables. The target variable, “quality,” is ordinal and originally ranges from 3 to 8. To ensure enough observations in each group, we combine categories 3 and 4 into a single category (labeled 4), and categories 7 and 8 into a single category (labeled 7), so the response variable has four levels. We then handle outliers in the explanatory variables using the Interquartile Range (IQR) method. Finally, we select three predictors—volatile acidity, free sulfur dioxide, and total sulfur dioxide—to use in our ordinal logistic regression model, and we standardize these variables to have a mean of 0 and a standard deviation of 1.\nTables 1 and 2 present the results of the three binary logistic regression models and the proportional odds model, respectively. Several discrepancies can be seen in these tables, particularly in the “volatile acidity” coefficients. For instance, the difference in the “volatile acidity” coefficient between the first and second binary models is -0.280, while the difference between the second and third models is 0.361. These differences—especially when compared alongside the standard errors—suggest that the proportional odds assumption may not hold.\nTo assess the overall significance of the proportional odds assumption, we perform the likelihood ratio test, which yields a test statistic of \\(\\mathrm{LR} = 53.207\\) and a p-value of \\(1.066 \\times 10^{-9}\\) when compared to the chi-square distribution with 6 degrees of freedom. This result indicates that the proportional odds assumption is violated at the 5% significance level, suggesting that the model may not be appropriate for the data. We also use the separate fits approach to further investigate this assumption. The Wald test statistic is computed as \\(X^2 = 41.880\\), with a p-value of \\(1.232 \\times 10^{-7}\\), also based on the chi-square distribution with 6 degrees of freedom. This further confirms that the proportional odds assumption is violated at the 5% significance level."
  },
  {
    "objectID": "00_tds/3_proportional_ordinal_regression.html#conclusion",
    "href": "00_tds/3_proportional_ordinal_regression.html#conclusion",
    "title": "Proportional Odds Model for Ordinal Logistic Regression",
    "section": "Conclusion",
    "text": "Conclusion\nThis paper had two main goals: first, to illustrate how to test the proportional odds assumption in the context of ordinal logistic regression, and second, to encourage readers to explore Brant (1990)’s article for a deeper understanding of the topic.\nBrant’s work extends beyond assessing the proportional odds assumption—it also provides methods for evaluating the overall adequacy of the ordinal logistic regression model. For instance, he discusses how to test whether the latent variable \\(Y^*\\) truly follows a logistic distribution or whether an alternative link function might be more appropriate.\nIn this article, we focused on a global assessment of the proportional odds assumption, without investigating which specific coefficients may be responsible for any violations. Brant also addresses this finer-grained analysis, which is why we strongly encourage you to read his 1990 article in full.\nWe welcome any comments or suggestions. Happy reading!\n\nimport pandas as pd\n\ndata = pd.read_csv(\"data/winequality-red.csv\", sep=\";\")\ndata.head()\n\n# Repartition de la variable cible quality \n\ndata['quality'].value_counts(normalize=False).sort_index()\n\n# I want to regroup modalities 3, 4 and the modalities 7 and 8\ndata['quality'] = data['quality'].replace({3: 4, 8: 7})\ndata['quality'].value_counts(normalize=False).sort_index()\nprint(\"Number of observations:\", data.shape[0])\n\nNumber of observations: 1599\n\n\n\n# Traitons les outliers des variables privées de la variable cible quality par IQR.\n\ndef remove_outliers_iqr(df, column):\n    Q1 = df[column].quantile(0.25)\n    Q3 = df[column].quantile(0.75)\n    IQR = Q3 - Q1\n    lower_bound = Q1 - 1.5 * IQR\n    upper_bound = Q3 + 1.5 * IQR\n    return df[(df[column] &gt;= lower_bound) & (df[column] &lt;= upper_bound)]\nfor col in data.columns:\n    if col != 'quality':\n        data = remove_outliers_iqr(data, col)\n\n\nvar_names_without_quality = [col for col in data.columns if col != 'quality']\n\n##  Create the boxplot of each variable per group of quality\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.figure(figsize=(15, 10))\nfor i, var in enumerate(var_names_without_quality):\n    plt.subplot(3, 4, i + 1)\n    sns.boxplot(x='quality', y=var, data=data)\n    plt.title(f'Boxplot of {var} by quality')\n    plt.xlabel('Quality')\n    plt.ylabel(var)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n# Implement the ordered logistic regression to variables 'volatile acidity', 'free sulfur dioxide', and 'total sulfur dioxide'\nfrom statsmodels.miscmodels.ordinal_model import OrderedModel\nfrom sklearn.preprocessing import StandardScaler\nexplanatory_vars = ['volatile acidity', 'free sulfur dioxide', 'total sulfur dioxide']\n# Standardize the explanatory variables\ndata[explanatory_vars] = StandardScaler().fit_transform(data[explanatory_vars])\n\ndef fit_ordered_logistic_regression(data, response_var, explanatory_vars):\n    model = OrderedModel(\n        data[response_var],\n        data[explanatory_vars],\n        distr='logit'\n    )\n    result = model.fit(method='bfgs', disp=False)\n    return result\nresponse_var = 'quality'\n\nresult = fit_ordered_logistic_regression(data, response_var, explanatory_vars)\nprint(result.summary())\n# Compute the log-likelihood of the model\nlog_reduced = result.llf\nprint(f\"Log-likelihood of the reduced model: {log_reduced}\")\n\n                             OrderedModel Results                             \n==============================================================================\nDep. Variable:                quality   Log-Likelihood:                -1130.1\nModel:                   OrderedModel   AIC:                             2272.\nMethod:            Maximum Likelihood   BIC:                             2302.\nDate:                Sun, 05 Oct 2025                                         \nTime:                        21:51:49                                         \nNo. Observations:                1135                                         \nDf Residuals:                    1129                                         \nDf Model:                           3                                         \n========================================================================================\n                           coef    std err          z      P&gt;|z|      [0.025      0.975]\n----------------------------------------------------------------------------------------\nvolatile acidity        -0.7180      0.064    -11.302      0.000      -0.842      -0.593\nfree sulfur dioxide      0.3627      0.076      4.770      0.000       0.214       0.512\ntotal sulfur dioxide    -0.5903      0.080     -7.406      0.000      -0.747      -0.434\n4/5                     -3.8601      0.182    -21.153      0.000      -4.218      -3.502\n5/6                      1.3002      0.050     25.863      0.000       1.202       1.399\n6/7                      0.8830      0.042     20.948      0.000       0.800       0.966\n========================================================================================\nLog-likelihood of the reduced model: -1130.0713953351503\n\n\n\nnum_of_thresholds = len(result.params) - len(explanatory_vars)  # Number of thresholds is total params minus explanatory vars\nOrderedModel(\n        data[response_var],\n        data[explanatory_vars],\n        distr='logit'\n    ).transform_threshold_params(result.params[-num_of_thresholds:])\n\narray([       -inf, -3.86010874, -0.19012621,  2.2279648 ,         inf])\n\n\n\n# The likelihood ratio test\n# Compute the full multinomial model\nimport statsmodels.api as sm\n\ndata_sm = sm.add_constant(data[explanatory_vars])\nmodel_full = sm.MNLogit(data[response_var], data_sm)\nresult_full = model_full.fit(method='bfgs', disp=False)\n#summary\nprint(result_full.summary())\n# Commpute the log-likelihood of the full model\nlog_full = result_full.llf\nprint(f\"Log-likelihood of the full model: {log_full}\")\n\n# Compute the likelihood ratio statistic\n\nLR_statistic = 2 * (log_full - log_reduced)\nprint(f\"Likelihood Ratio Statistic: {LR_statistic}\")\n\n# Compute the degrees of freedom\ndf1 = (num_of_thresholds - 1) * len(explanatory_vars)\ndf2 = result_full.df_model - OrderedModel(\n        data[response_var],\n        data[explanatory_vars],\n        distr='logit'\n    ).fit().df_model\nprint(f\"Degrees of Freedom: {df1}\")\nprint(f\"Degrees of Freedom for the full model: {df2}\")\n\n# Compute the p-value\nfrom scipy.stats import chi2\nprint(\"The LR statistic :\", LR_statistic)\np_value = chi2.sf(LR_statistic, df1)\nprint(f\"P-value: {p_value}\")\nif p_value &lt; 0.05:\n    print(\"Reject the null hypothesis: The proportional odds assumption is violated.\")\nelse:\n    print(\"Fail to reject the null hypothesis: The proportional odds assumption holds.\")\n\n                          MNLogit Regression Results                          \n==============================================================================\nDep. Variable:                quality   No. Observations:                 1135\nModel:                        MNLogit   Df Residuals:                     1123\nMethod:                           MLE   Df Model:                            9\nDate:                Sun, 05 Oct 2025   Pseudo R-squ.:                  0.1079\nTime:                        21:51:51   Log-Likelihood:                -1103.5\nconverged:                      False   LL-Null:                       -1236.9\nCovariance Type:            nonrobust   LLR p-value:                 2.753e-52\n========================================================================================\n           quality=5       coef    std err          z      P&gt;|z|      [0.025      0.975]\n----------------------------------------------------------------------------------------\nconst                    3.2418      0.269     12.034      0.000       2.714       3.770\nvolatile acidity        -0.6541      0.180     -3.624      0.000      -1.008      -0.300\nfree sulfur dioxide      0.2494      0.323      0.772      0.440      -0.384       0.882\ntotal sulfur dioxide     0.6314      0.310      2.037      0.042       0.024       1.239\n----------------------------------------------------------------------------------------\n           quality=6       coef    std err          z      P&gt;|z|      [0.025      0.975]\n----------------------------------------------------------------------------------------\nconst                    3.2549      0.269     12.089      0.000       2.727       3.783\nvolatile acidity        -1.0838      0.184     -5.880      0.000      -1.445      -0.723\nfree sulfur dioxide      0.6269      0.325      1.930      0.054      -0.010       1.264\ntotal sulfur dioxide     0.0723      0.315      0.230      0.818      -0.544       0.689\n----------------------------------------------------------------------------------------\n           quality=7       coef    std err          z      P&gt;|z|      [0.025      0.975]\n----------------------------------------------------------------------------------------\nconst                    1.4139      0.302      4.684      0.000       0.822       2.006\nvolatile acidity        -1.8364      0.214     -8.596      0.000      -2.255      -1.418\nfree sulfur dioxide      1.0125      0.358      2.830      0.005       0.311       1.714\ntotal sulfur dioxide    -0.9086      0.389     -2.337      0.019      -1.671      -0.147\n========================================================================================\nLog-likelihood of the full model: -1103.467809036406\nLikelihood Ratio Statistic: 53.20717259748881\nDegrees of Freedom: 6\nDegrees of Freedom for the full model: 6.0\nThe LR statistic : 53.20717259748881\nP-value: 1.0658102529671109e-09\nReject the null hypothesis: The proportional odds assumption is violated.\n\n\n/Users/juniorjumbong/Desktop/personal-website/.venv/lib/python3.13/site-packages/statsmodels/base/model.py:607: ConvergenceWarning:\n\nMaximum Likelihood optimization failed to converge. Check mle_retvals\n\n/Users/juniorjumbong/Desktop/personal-website/.venv/lib/python3.13/site-packages/statsmodels/base/optimizer.py:737: RuntimeWarning:\n\nMaximum number of iterations has been exceeded.\n\n/Users/juniorjumbong/Desktop/personal-website/.venv/lib/python3.13/site-packages/statsmodels/base/model.py:607: ConvergenceWarning:\n\nMaximum Likelihood optimization failed to converge. Check mle_retvals\n\n\n\n\nimport numpy as np\nimport statsmodels.api as sm\nimport pandas as pd\n\ndef fit_binary_models(data, explanatory_vars, y):\n    \"\"\"\n    - data : DataFrame pandas original (doit contenir toutes les variables)\n    - explanatory_vars : liste des variables explicatives\n    - y : array-like, cible ordinale (n,) (ex: 4, 5, 6, 7)\n\n    Retourne :\n      - binary_models : liste d'objets Logit results (statsmodels)\n      - beta_hat : array (K-1, p+1) (coeffs incluant l'intercept)\n      - var_hat : liste de matrices (p+1, p+1) (variance-covariance complète)\n      - z_mat : DataFrame des variables binaires z_j (pour debug/inspection)\n      - thresholds : liste des seuils utilisés\n    \"\"\"\n    qualities = np.sort(np.unique(y))   # toutes les modalités, triées\n    thresholds = qualities[:-1]         # seuils pour les modèles binaires (K-1)\n    p = len(explanatory_vars)\n    n = len(y)\n    K_1 = len(thresholds)\n\n    binary_models = []\n    beta_hat = np.full((K_1, p+1), np.nan)\n    p_values_beta_hat = np.full((K_1, p+1), np.nan)  # pour les p-values\n    var_hat = []\n    z_mat = pd.DataFrame(index=np.arange(n))\n    X_with_const = sm.add_constant(data[explanatory_vars])\n\n    # Construction et estimation des modèles binaires pour chaque seuil\n    for j, t in enumerate(thresholds):\n        z_j = (y &gt; t).astype(int)\n        z_mat[f'z&gt;{t}'] = z_j\n        model = sm.Logit(z_j, X_with_const)\n        res = model.fit(disp=0)\n        binary_models.append(res)\n        beta_hat[j, :] = res.params.values           # Incluant intercept\n        p_values_beta_hat[j, :] = res.pvalues.values  # P-values des coefficients\n        var_hat.append(res.cov_params().values)      # Covariance complète (p+1, p+1)\n\n    return binary_models, beta_hat, X_with_const, var_hat, z_mat, thresholds\nbinary_models, beta_hat,X_with_const, var_hat, z_mat, thresholds = fit_binary_models(data, explanatory_vars, data[response_var])\n# Afficher les coefficients estimés\nprint(\"Estimated coefficients (beta_hat):\")\nprint(beta_hat)\n# Afficher les p-values des coefficients\nprint(\"P-values of coefficients (p_values_beta_hat):\")\nprint(X_with_const)\n# Afficher les seuils\nprint(\"Thresholds:\")\nprint(thresholds)   \nprint(\"z_mat (variables binaires créées) :\\n\", z_mat.head())\n\nEstimated coefficients (beta_hat):\n[[ 4.09606917 -0.88743434  0.63477387  0.20921617]\n [ 0.15729349 -0.60735704  0.4339553  -0.65663161]\n [-2.60302245 -0.9677302   0.60691768 -1.30246297]]\nP-values of coefficients (p_values_beta_hat):\n      const  volatile acidity  free sulfur dioxide  total sulfur dioxide\n0       1.0          1.080055            -0.441353             -0.282198\n1       1.0          2.173545             1.189601              1.058458\n2       1.0          1.444552             0.024634              0.530321\n3       1.0         -1.471421             0.257627              0.774077\n4       1.0          1.080055            -0.441353             -0.282198\n...     ...               ...                  ...                   ...\n1594    1.0          0.472561             2.005078              0.124061\n1595    1.0          0.168814             2.820555              0.408443\n1596    1.0         -0.074184             1.655588             -0.038443\n1597    1.0          0.745933             2.005078              0.124061\n1598    1.0         -1.289172             0.374124              0.042809\n\n[1135 rows x 4 columns]\nThresholds:\n[4 5 6]\nz_mat (variables binaires créées) :\n    z&gt;4  z&gt;5  z&gt;6\n0  1.0  0.0  0.0\n1  1.0  0.0  0.0\n2  1.0  0.0  0.0\n3  1.0  1.0  0.0\n4  1.0  0.0  0.0\n\n\n\ndef compute_pi_hat(binary_models, X_with_const):\n    \"\"\"\n    - binary_models : liste d'objets Logit results (statsmodels)\n    - X_with_const  : matrice (n, p+1) des variables explicatives AVEC constante\n\n    Retourne :\n      - pi_hat : array (n, K-1) des fitted values pour chaque modèle binaire\n    \"\"\"\n    n = X_with_const.shape[0]\n    K_1 = len(binary_models)\n    pi_hat = np.full((n, K_1), np.nan)\n    for m, model in enumerate(binary_models):\n        pi_hat[:, m] = model.predict(X_with_const)\n    return pi_hat\n\n# Supposons que tu as :\n# - binary_models (liste)\n# - X_with_const (matrice numpy (n, p+1) créée dans la fonction précédente)\n\npi_hat = compute_pi_hat(binary_models, X_with_const)\nprint(\"Shape de pi_hat :\", pi_hat.shape)  # (n, K-1)\nprint(\"Aperçu de pi_hat :\\n\", pi_hat[:5, :])\n\nShape de pi_hat : (1135, 3)\nAperçu de pi_hat :\n [[0.94258882 0.37638681 0.02796232]\n [0.95866233 0.20724576 0.00466477]\n [0.94982271 0.25776823 0.00922353]\n [0.99675485 0.65802083 0.11599334]\n [0.94258882 0.37638681 0.02796232]]\n\n\n\nimport numpy as np\n\ndef assemble_varBeta(pi_hat, X_with_const):\n    \"\"\"\n    Construit la matrice de variance-covariance globale varBeta pour les estimateurs des modèles binaires.\n    - pi_hat : array (n, K-1), chaque colonne = fitted proba du modèle binaire j\n    - X_with_const : array (n, p+1), matrice de design AVEC constante\n\n    Retourne :\n      - varBeta : array ((K-1)*p, (K-1)*p) [sans l'intercept]\n    \"\"\"\n    # Assure que tout est en numpy\n    if hasattr(X_with_const, 'values'):\n        X = X_with_const.values\n    else:\n        X = np.asarray(X_with_const)\n    n, p1 = X.shape  # p1 = p + 1 (avec intercept)\n    p = p1 - 1\n    K_1 = pi_hat.shape[1]\n\n    # Initialisation de la matrice globale\n    varBeta = np.zeros(((K_1)*p, (K_1)*p))\n\n    # Pour chaque bloc (j, l)\n    for j in range(K_1):\n        pi_j = pi_hat[:, j]\n        Wj = np.diag(pi_j * (1 - pi_j))\n        X_j = X\n        Xt = X_j.T\n\n        # Diagonale principale (variance de beta_j)\n        inv_XtWjX = np.linalg.pinv(Xt @ Wj @ X_j)\n        # On enlève la première ligne/colonne (intercept)\n        inv_XtWjX_no_const = inv_XtWjX[1:, 1:]\n\n        row_start = j * p\n        row_end = (j + 1) * p\n        varBeta[row_start:row_end, row_start:row_end] = inv_XtWjX_no_const\n\n        # Blocs hors diagonale (covariances entre beta_j et beta_l)\n        for l in range(j + 1, K_1):\n            pi_l = pi_hat[:, l]\n            Wml = np.diag(pi_l - pi_j * pi_l)\n            Wl = np.diag(pi_l * (1 - pi_l))\n            # Termes croisés\n            inv_XtWlX = np.linalg.pinv(Xt @ Wl @ X_j)\n            block_vars = (\n                inv_XtWjX @ (Xt @ Wml @ X_j) @ inv_XtWlX\n            )[1:, 1:]  # Retirer intercept\n            # Place les blocs (symétriques)\n            col_start = l * p\n            col_end = (l + 1) * p\n            varBeta[row_start:row_end, col_start:col_end] = block_vars\n            varBeta[col_start:col_end, row_start:row_end] = block_vars.T  # symétrie\n\n    return varBeta\n\nvarBeta = assemble_varBeta(pi_hat, X_with_const)\nprint(\"Shape de varBeta :\", varBeta.shape)  # ((K-1)*p, (K-1)*p)\nprint(\"Aperçu de varBeta :\\n\", varBeta[:5, :5])  # Afficher un aperçu\n\nShape de varBeta : (9, 9)\nAperçu de varBeta :\n [[ 2.87696584e-02  8.96840197e-04 -9.43834509e-04  1.80729535e-03\n   2.32011452e-04]\n [ 8.96840197e-04  1.09112963e-01 -5.78619412e-02  3.21424832e-04\n   3.10295243e-03]\n [-9.43834509e-04 -5.78619412e-02  7.73627725e-02 -4.69694598e-04\n  -1.48837502e-03]\n [ 1.80729535e-03  3.21424832e-04 -4.69694598e-04  4.61543407e-03\n   1.04759944e-04]\n [ 2.32011452e-04  3.10295243e-03 -1.48837502e-03  1.04759944e-04\n   7.48753786e-03]]\n\n\n\ndef fill_varBeta_diagonal(varBeta, var_hat):\n    K_1 = len(var_hat)\n    p = var_hat[0].shape[0] - 1  # -1 car on enlève l'intercept\n    for m in range(K_1):\n        block = var_hat[m][1:, 1:]  # enlève intercept\n        row_start = m * p\n        row_end = (m + 1) * p\n        varBeta[row_start:row_end, row_start:row_end] = block\n    return varBeta\n\n# betaStar : concaténation des coefficients sans intercept\nbetaStar = beta_hat[:, 1:].flatten()\n\n# Compléter les blocs diagonaux de varBeta\nvarBeta = fill_varBeta_diagonal(varBeta, var_hat)\nprint(\"Shape de varBeta après remplissage diagonal :\", varBeta.shape)  # ((K-1)*p, (K-1)*p)\nprint(\"Aperçu de varBeta après remplissage diagonal :\\n\", varBeta[:5, :5])  # Afficher un aperçu    \n\nShape de varBeta après remplissage diagonal : (9, 9)\nAperçu de varBeta après remplissage diagonal :\n [[ 2.87696584e-02  8.96840197e-04 -9.43834509e-04  1.80729535e-03\n   2.32011452e-04]\n [ 8.96840197e-04  1.09112963e-01 -5.78619412e-02  3.21424832e-04\n   3.10295243e-03]\n [-9.43834509e-04 -5.78619412e-02  7.73627725e-02 -4.69694598e-04\n  -1.48837502e-03]\n [ 1.80729535e-03  3.21424832e-04 -4.69694598e-04  4.61543407e-03\n   1.04759944e-04]\n [ 2.32011452e-04  3.10295243e-03 -1.48837502e-03  1.04759944e-04\n   7.48753786e-03]]\n\n\n\nimport numpy as np\n\ndef construct_D(K_1, p):\n    \"\"\"\n    Construit la matrice D de taille ((K-2)*p, (K-1)*p) pour le test de Wald.\n    K_1 : nombre de seuils (K-1)\n    p   : nombre de variables explicatives (hors intercept)\n    \"\"\"\n    D = np.zeros(((K_1-1)*p, K_1*p))\n    I = np.eye(p)\n    for i in range(K_1-1):  # i = 0 à K-2\n        for j in range(K_1):\n            if j == 0:\n                temp = I\n            elif j == i+1:\n                temp = -I\n            else:\n                temp = np.zeros((p, p))\n            col_start = j*p\n            col_end = (j+1)*p\n            row_start = i*p\n            row_end = (i+1)*p\n            D[row_start:row_end, col_start:col_end] += temp\n    return D\nD = construct_D(len(thresholds), len(explanatory_vars))\nprint(\"Shape de D :\", D.shape)  # ((K-2)*p, (K-1)*p)\nprint(\"Aperçu de D :\\n\", D[:5, :5])  # Afficher un aperçu\n\nShape de D : (6, 9)\nAperçu de D :\n [[ 1.  0.  0. -1.  0.]\n [ 0.  1.  0.  0. -1.]\n [ 0.  0.  1.  0.  0.]\n [ 1.  0.  0.  0.  0.]\n [ 0.  1.  0.  0.  0.]]\n\n\n\ndef wald_statistic(D, betaStar, varBeta):\n    \"\"\"\n    Calcule la statistique de Wald X^2 pour le test de proportionnalité.\n    \"\"\"\n    Db = D @ betaStar\n    V = D @ varBeta @ D.T\n    # Symétriser V pour stabilité\n    #V = 0.5 * (V + V.T)\n    # Utilise le pseudo-inverse par sécurité numérique\n    inv_V = np.linalg.inv(V)\n    X2 = float(Db.T @ inv_V @ Db)\n    return X2\n\n\n# Supposons que tu as K_1, p, betaStar, varBeta\nK_1 = len(binary_models)\np = len(explanatory_vars)  # Nombre de variables explicatives (hors intercept)\nD = construct_D(K_1, p)\nX2 = wald_statistic(D, betaStar, varBeta)\nddl = (K_1-1)*p\n\nfrom scipy.stats import chi2\npval = 1 - chi2.cdf(X2, ddl)\n\nprint(f\"Statistique X² = {X2:.4f}\")\nprint(f\"Degrés de liberté = {ddl}\")\nprint(f\"p-value = {pval:.4g}\")\n\nStatistique X² = 42.8803\nDegrés de liberté = 6\np-value = 1.232e-07"
  },
  {
    "objectID": "00_tds/4_psi_cramer_v.html",
    "href": "00_tds/4_psi_cramer_v.html",
    "title": "Representiveness",
    "section": "",
    "text": "The issue of data representativeness arises frequently in modeling projects, whether academic or professional. Beyond its apparent simplicity, it raises a fundamental question: are the data used to build a model representative of the data the model will face in the real world?\nBy representativeness, we mean the extent to which a sample reflects the characteristics of the population it is supposed to model. In practice, this means that the distributions, proportions, and patterns observed in the training data should be consistent with those the model will encounter once implemented or deployed.\nThe first time I encountered this issue was while building a credit scoring model for a bank’s business clients. Like many others, I had been taught to split data into train, test, and sometimes out-of-sample sets. However, I realized that the real challenge was not simply achieving strong performance on the training data, but ensuring that the model remained reliable when applied to a different dataset or a new population of clients. Addressing this required comparing the distributions of key variables across datasets and verifying that they followed the same distribution.\nIn practice, comparing distributions is not just a theoretical exercise; it’s a critical step throughout the entire lifecycle of a model:\nTo address this, we can use a variety of tools. Visual methods—such as histograms, boxplots, and density plots—provide quick intuition, while statistical methods allow us to quantify differences and make the analysis more objective.\nIn this article, we will focus on two particularly useful and complementary indicators:\nOur goal is twofold:"
  },
  {
    "objectID": "00_tds/4_psi_cramer_v.html#i.-comparing-distributions-a-key-challenge-throughout-the-lifecycle-of-a-credit-model",
    "href": "00_tds/4_psi_cramer_v.html#i.-comparing-distributions-a-key-challenge-throughout-the-lifecycle-of-a-credit-model",
    "title": "Representiveness",
    "section": "I. Comparing Distributions: A Key Challenge Throughout the Lifecycle of a Credit Model",
    "text": "I. Comparing Distributions: A Key Challenge Throughout the Lifecycle of a Credit Model\nFrom a statistician’s perspective, the lifecycle of a credit model can be summed up in three main stages.\nThe construction phase: this is where it all begins. You gather the data, clean it, split it into training, test, and out-of-time samples, estimate the parameters, and carefully document every decision. You ensure that the test and the out-of-time samples are representative of the training data.\nThe application phase: once the model is built, it must be confronted with reality. And here a crucial question arises: do the new datasets truly resemble the ones used during construction? If not, much of the previous work may quickly lose its value.\nThe monitoring phase, or backtesting: over time, populations evolve. The model must therefore be regularly challenged. Do its predictions remain valid? Is the representativeness of the target portfolio still ensured?\nAt each of these stages, the same recurring question plays like a refrain: Is the model still representative of the population it is supposed to score?\n\nPractical Illustrations\nTo make this more concrete, let me share two situations I’ve encountered—or could easily have encountered in practice.\nCase 1: A change in scope\nImagine a bank developing a scoring model for small businesses. The model performs well and is recognized internally. Encouraged by this success, the leadership decides to extend its use to large corporations. But here every statistician would pause to ask: do the characteristic variables of large corporations follow the same distributions as those of small businesses? If not, the model risks becoming fragile.\nCase 2: A banking merger\nNow consider Bank A, equipped with a proven model to assess client default risk. It merges with Bank B and seeks to harmonize its tools. The challenge: Bank B operates in a different economic environment, with a portfolio of clients that may not share the same structure. Before transferring the model, the distributions of key variables across the two portfolios must be compared. Once again, representativeness is at stake."
  },
  {
    "objectID": "00_tds/4_psi_cramer_v.html#ii.-comparing-distributions-to-assess-representativeness-between-datasets",
    "href": "00_tds/4_psi_cramer_v.html#ii.-comparing-distributions-to-assess-representativeness-between-datasets",
    "title": "Representiveness",
    "section": "II. Comparing Distributions to Assess Representativeness Between Datasets",
    "text": "II. Comparing Distributions to Assess Representativeness Between Datasets\nThese examples show that behind every strategic decision; whether changing scope, merging models, or tracking their evolution over time—there lies the same underlying question: are the variable distributions sufficiently similar to guarantee the model’s robustness?\nTo answer it, statisticians can rely on a wide range of tools. Some are visual and intuitive:\n\noverlaying histograms to instantly spot divergent densities,\nusing boxplots to compare medians and spreads,\nplotting cumulative distribution functions or estimated densities to get a global view.\n\nOthers are statistical and more formalized:\n\nthe Kolmogorov-Smirnov test, which measures the maximum gap between two cumulative distributions,\nthe Chi-square test, well suited for categorical variables,\nthe Anderson-Darling test, particularly sensitive to differences in the tails.\n\nThese methods form a solid foundation, and Matteo Courthoud offers a clear and comprehensive overview of them in his article How to Compare Two (or More) Distributions.\nBut in credit risk, I’ve found that two indicators deserve special attention because they are simple, easy to use, and they address representativeness issues very directly:\n\nthe Population Stability Index (PSI), widely used to detect distribution shifts over time, and\nCramér’s V, which measures the strength of association between two categorical variables and helps assess the coherence between populations.\n\nThe next part of this article will focus on these two tools, showing how they complement classical approaches and illustrating their use with a concrete example."
  },
  {
    "objectID": "00_tds/4_psi_cramer_v.html#iii.-two-indicators-to-assess-representativeness-psi-and-cramérs-v",
    "href": "00_tds/4_psi_cramer_v.html#iii.-two-indicators-to-assess-representativeness-psi-and-cramérs-v",
    "title": "Representiveness",
    "section": "III. Two Indicators to Assess Representativeness: PSI and Cramér’s V",
    "text": "III. Two Indicators to Assess Representativeness: PSI and Cramér’s V\nWhen comparing distributions, we often start by examining the basic elements: cumulative curves or density graphs. These visualizations provide an initial overview of the differences and help to form an intuition. But intuition is not always enough: in practice, decision-makers also expect quantified measurements, not just graphs or pictures.\nThat’s where two tools come in: the Population Stability Index (PSI) and Cramér’s V. They’re easy to calculate, easy to read, and most importantly, they turn what a chart suggests into clear numbers. Instead of just saying “these distributions look different”, they tell us “this is how different they really are.” Whether you’re checking for risk drift or following portfolio stability, they make comparison both precise and practical.\n\n1. The Population Stability Index (PSI)\nThe PSI is a fundamental tool in the credit industry. It measures the difference between two distributions of the same variable:\n\nfor example, between the training dataset and a more recent application dataset,\nor between a reference dataset at time \\(T_0\\) and another at time \\(T_1\\).\n\nIn other words, the PSI quantifies how much a population has drifted over time or across different scopes.\nHere’s how it works in practice:\n\nFor a categorical variable, we compute the proportion of observations in each category for both datasets.\nFor a continuous variable, we first discretize it into bins. In practice, deciles are often used to obtain a balanced distribution.\n\nThe PSI then compares, bin by bin, the proportions observed in the reference dataset versus the target dataset. The final indicator aggregates these differences using a logarithmic formula:\n\\[\nPSI = \\sum_{i=1}^{k} (p_i - q_i) \\cdot \\ln\\!\\left(\\frac{p_i}{q_i}\\right)\n\\]\nwhere \\(p_i\\) and \\(q_i\\) represent the proportions in bin \\(i\\) for the reference dataset and the target dataset, respectively.\nThe interpretation is highly intuitive:\n\nA smaller PSI means the two distributions are closer.\nA PSI of 0 means the distributions are identical.\nA very large PSI (tending toward infinity) means the two distributions are fundamentally different.\n\nIn practice, industry guidelines often use the following thresholds:\n\nPSI &lt; 0.1: the population is stable,\n0.1 ≤ PSI &lt; 0.25: the shift is noticeable—monitor closely,\nPSI ≥ 0.25: the shift is significant—the model may no longer be reliable."
  },
  {
    "objectID": "00_tds/4_psi_cramer_v.html#cramérs-v",
    "href": "00_tds/4_psi_cramer_v.html#cramérs-v",
    "title": "Representiveness",
    "section": "2. Cramér’s V",
    "text": "2. Cramér’s V\nWhen assessing the representativeness of a categorical variable (or a discretized continuous variable) between two datasets, a natural starting point is the Chi-square test of independence.\nWe build a contingency table crossing:\n\nthe categories (modalities) of the variable of interest, and\nan indicator variable for dataset membership (Dataset 1 / Dataset 2).\n\nThe test is based on the following statistic:\n\\[\n\\chi^2 = \\sum_{i=1}^{r}\\sum_{j=1}^{c} \\frac{(O_{ij} - E_{ij})^2}{E_{ij}}\n\\]\nwhere \\(O_{ij}\\) are the observed counts and \\(E_{ij}\\) are the expected counts under the assumption of independence.\n\nNull hypothesis \\(H_0\\): the variable has the same distribution in both datasets (independence).\nAlternative hypothesis \\(H_1\\): the distributions differ.\n\nIf \\(H_0\\) is rejected, we conclude that the variable does not follow the same distribution across the two datasets.\nHowever, the Chi-square test has a major limitation: it only provides a binary answer (reject / do not reject), and its power is highly sensitive to sample size. With very large datasets, even tiny differences can appear statistically significant.\nTo address this limitation, we use Cramér’s V, which rescales the Chi-square statistic to produce a normalized measure of association bounded between 0 and 1:\n\\[\nV = \\sqrt{\\frac{\\chi^2}{n \\cdot \\min(r-1,\\,c-1)}}\n\\]\nwhere $n$ is the total sample size, $r$ is the number of rows, and $c$ is the number of columns in the contingency table.\nThe interpretation is intuitive:\n\n\\(V \\approx 0\\) → the distributions are very similar; representativeness is strong.\n\\(V\\) close to 1 → the difference between distributions is large; the datasets are structurally different.\n\nUnlike the Chi-square test, which simply answers “yes” or “no,” Cramér’s V provides a graded measure of the strength of the difference. This allows us to assess whether the difference is negligible, moderate, or substantial."
  },
  {
    "objectID": "00_tds/4_psi_cramer_v.html#why-compare-a-fold-to-the-global-dataset",
    "href": "00_tds/4_psi_cramer_v.html#why-compare-a-fold-to-the-global-dataset",
    "title": "Representiveness",
    "section": "Why Compare a Fold to the Global Dataset?",
    "text": "Why Compare a Fold to the Global Dataset?\nIf a fold is representative of the overall dataset, then working with this subsample—and aggregating the results across all folds—should yield reliable conclusions at the full-dataset level.\nIn this article, we’ll look at a concrete example: checking whether fold 1 is representative of the global dataset."
  },
  {
    "objectID": "00_tds/4_psi_cramer_v.html#step-1-start-with-the-target-variable",
    "href": "00_tds/4_psi_cramer_v.html#step-1-start-with-the-target-variable",
    "title": "Representiveness",
    "section": "Step 1: Start with the Target Variable",
    "text": "Step 1: Start with the Target Variable\nWe begin with the target variable. The idea is simple: compare its distribution between fold 1 and the entire dataset. To quantify this difference, we’ll use two complementary indicators:\n\nthe Population Stability Index (PSI), which measures distributional shifts,\nCramér’s V, which measures the strength of association between two categorical variables."
  },
  {
    "objectID": "00_tds/4_psi_cramer_v.html#step-2-automating-the-analysis-for-all-variables",
    "href": "00_tds/4_psi_cramer_v.html#step-2-automating-the-analysis-for-all-variables",
    "title": "Representiveness",
    "section": "Step 2: Automating the Analysis for All Variables",
    "text": "Step 2: Automating the Analysis for All Variables\nAfter illustrating the approach with the target, we extend it to all features. We’ll build a Python function that computes PSI and Cramér’s V for each of the 19 explanatory variables, as well as for the target variable.\nTo make the results easy to interpret, we’ll export everything into an Excel file with:\n\none sheet per variable, showing the detailed comparison by segment,\na Summary tab, aggregating results across all variables."
  },
  {
    "objectID": "00_tds/4_psi_cramer_v.html#comparing-the-target-variable-violentcrimesperpop-between-the-global-dataset-reference-and-fold-1-target",
    "href": "00_tds/4_psi_cramer_v.html#comparing-the-target-variable-violentcrimesperpop-between-the-global-dataset-reference-and-fold-1-target",
    "title": "Representiveness",
    "section": "Comparing the target variable ViolentCrimesPerPop between the global dataset (reference) and fold 1 (target)",
    "text": "Comparing the target variable ViolentCrimesPerPop between the global dataset (reference) and fold 1 (target)\nBefore applying statistical tests or building decision indicators, it is essential to conduct a descriptive and graphical analysis. There are not just formalities; they provide an early intuition about the differences between populations and help interpreting the results. In practice, a well-chosen chart often reveals the conclusions that indicators like PSI or Cramér’s V will later confirm (or challenge).\nFor visualization, we proceed in three steps:\n1. Comparing continuous distributions We begin with graphical tools such as boxplots, cumulative distribution functions, and probability density plots. These visualizations provide an intuitive way to examine differences in the target variable’s distribution between the two datasets.\n2. Discretization into quantiles Next, we discretize the variable in the reference dataset using quartiles (Q1, Q2, Q3, Q4), which creates five classes (Q1 through Q5). We then apply the exact same cut-off points to the target dataset, ensuring that each observation is mapped to intervals defined from the reference. This guarantees comparability between the two distributions.\n3. Comparing categorical distributions Finally, once the variable has been discretized, we can use visualization methods suited for categorical data — such as bar charts — to compare how frequencies are distributed across the two datasets.\nThe process depends on the type of variable:\nFor a continuous variable:\n\nStart with standard visualizations (boxplots, cumulative distributions, and density plots).\nNext, split the variable into segments (Q1 to Q5) based on the reference dataset’s quantiles.\nFinally, treat these segments as categories and compare their distributions.\n\nFor a categorical variable:\n\nNo discretization is needed — it’s already in categorical form.\nGo straight to comparing category distributions, for example with a bar chart.\n\nThe code below prepares the two datasets we want to compare and then visualizes the target variable with a boxplot, showing its distribution in both the global dataset and in fold 1.\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy.stats import chi2_contingency, ks_2samp\n\ndata = pd.read_csv(\"data/communities_data.csv\")\n# filter sur fold =1\n\ndata_ref = data\ndata_target = data[data[\"fold\"] == 1]\n\n# compare the two distribution of \"ViolentCrimesPerPop\" in the reference and target datasets with boxplots\n\n\n\n# Build datasets with a \"Group\" column\ndf_ref = pd.DataFrame({\n    \"ViolentCrimesPerPop\": data_ref[\"ViolentCrimesPerPop\"],\n    \"Group\": \"Reference\"\n})\n\ndf_target = pd.DataFrame({\n    \"ViolentCrimesPerPop\": data_target[\"ViolentCrimesPerPop\"],\n    \"Group\": \"Target\"\n})\n\n# Merge them\ndf_all = pd.concat([df_ref, df_target])\n\n\nplt.figure(figsize=(8, 6))\n\n# Boxplot with both distributions overlayed\nsns.boxplot(\n    x=\"Group\", \n    y=\"ViolentCrimesPerPop\", \n    data=df_all,\n    palette=\"Set2\",\n    width=0.6,\n    fliersize=3\n)\n\n\n# Add mean points\nmeans = df_all.groupby(\"Group\")[\"ViolentCrimesPerPop\"].mean()\nfor i, m in enumerate(means):\n    plt.scatter(i, m, color=\"red\", marker=\"D\", s=50, zorder=3, label=\"Mean\" if i == 0 else \"\")\n\n# Title tells the story\nplt.title(\"Violent Crimes Per Population by Group\", fontsize=14, weight=\"bold\")\nplt.suptitle(\"Both groups show nearly identical distributions\", \n             fontsize=10, color=\"gray\")\n\nplt.ylabel(\"Violent Crimes (Per Pop)\", fontsize=12)\nplt.xlabel(\"\")\n\n# Cleaner look\nsns.despine()\nplt.grid(visible=False)\nplt.legend()\n\nplt.show()\n\n\nprint(len(data.columns))\n\n/var/folders/v8/l5q0bw4s2ln17s59y7cc86rm0000gn/T/ipykernel_57553/1831008403.py:35: FutureWarning:\n\n\n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n\n\n\n\n\n\n\n\n\n\n18\n\n\nThe figure above suggests that both groups share similar distributions for the ViolentCrimesPerPop variable. To take a closer look, we can use Kernel Density Estimation (KDE) plots, which provide a smooth view of the underlying distribution and make it easier to spot subtle differences.\n\nplt.figure(figsize=(8, 6))\n\n# KDE plots with better styling\nsns.kdeplot(\n    data=df_all,\n    x=\"ViolentCrimesPerPop\",\n    hue=\"Group\",\n    fill=True,         # use shading for overlap\n    alpha=0.4,         # transparency to show overlap\n    common_norm=False,\n    palette=\"Set2\",\n    linewidth=2\n)\n\n# KS-test for distribution difference\ng1 = df_all[df_all[\"Group\"] == df_all[\"Group\"].unique()[0]][\"ViolentCrimesPerPop\"]\ng2 = df_all[df_all[\"Group\"] == df_all[\"Group\"].unique()[1]][\"ViolentCrimesPerPop\"]\nstat, pval = ks_2samp(g1, g2)\n\n# Add annotation\nplt.text(df_all[\"ViolentCrimesPerPop\"].mean(),\n         plt.ylim()[1]*0.9,\n         f\"KS-test p-value = {pval:.3f}\\nNo significant difference observed\",\n         ha=\"center\", fontsize=10, color=\"black\")\n\n# Titles with story\nplt.title(\"Kernel Density Estimation of Violent Crimes Per Population\", fontsize=14, weight=\"bold\")\nplt.suptitle(\"Distributions overlap almost completely between groups\", fontsize=10, color=\"gray\")\n\nplt.xlabel(\"Violent Crimes (Per Pop)\")\nplt.ylabel(\"Density\")\n\nsns.despine()\nplt.grid(False)\nplt.show()\n\n\n\n\n\n\n\n\nThe KDE graph confirms that the two distributions are very similar, showing a high degree of overlap. The Kolmogorov-Smirnov (KS) statistical test of 0.976 also indicates that there is no significant difference between the two groups. To extend the analysis, we can now examine the cumulative distribution of the target variable.\n\n# Cumulative distribution\nplt.figure(figsize=(9, 6))\nsns.histplot(\n    data=df_all,\n    x=\"ViolentCrimesPerPop\",\n    hue=\"Group\",\n    stat=\"density\",\n    common_norm=False,\n    fill=False,\n    element=\"step\",\n    bins=len(df_all),\n    cumulative=True,\n)\n\n# Titles tell the story\nplt.title(\"Cumulative Distribution of Violent Crimes Per Population\", fontsize=14, weight=\"bold\")\nplt.suptitle(\"ECDFs overlap extensively; central tendencies are nearly identical\", fontsize=10)\n\n# Labels & cleanup\nplt.xlabel(\"Violent Crimes (Per Pop)\")\nplt.ylabel(\"Cumulative proportion\")\nplt.grid(visible=False)\nplt.show()\n\n\n\n\n\n\n\n\nThe cumulative distribution plot provides additional evidence that the two groups are very similar. The curves overlap almost completely, suggesting that their distributions are nearly identical in both central tendency and spread.\nAs a next step, we’ll discretize the variable into quantiles in the reference dataset and then apply the same cut-off points to the target dataset (fold 1). The code below demonstrates how to do this. Finally, we’ll compare the resulting distributions using a bar chart.\n\ndef bin_numeric(ref, tgt, n_bins=5):\n    \"\"\"\n    Discretize a numeric variable into quantile bins (ex: quintiles).\n    - Quantile thresholds are computed only on the reference dataset.\n    - Extend bins with -inf and +inf to cover all possible values.\n    - Returns:\n        * ref binned\n        * tgt binned\n        * bin labels (Q1, Q2, ...)\n    \"\"\"\n    edges = np.unique(ref.dropna().quantile(np.linspace(0, 1, n_bins + 1)).values)\n    if len(edges) &lt; 3:  # if variable is almost constant\n        edges = np.array([-np.inf, np.inf])\n    else:\n        edges[0], edges[-1] = -np.inf, np.inf\n    labels = [f\"Q{i}\" for i in range(1, len(edges))]\n    return (\n        pd.cut(ref, bins=edges, labels=labels, include_lowest=True),\n        pd.cut(tgt, bins=edges, labels=labels, include_lowest=True),\n        labels\n    )\n\n# Apply binning\nref_binned, tgt_binned, bin_labels = bin_numeric(data_ref[\"ViolentCrimesPerPop\"], data_target[\"ViolentCrimesPerPop\"], n_bins=5)\n\n\n\n\n# Effectifs par segment pour Reference et Target\nref_counts = ref_binned.value_counts().reindex(bin_labels, fill_value=0)\ntgt_counts = tgt_binned.value_counts().reindex(bin_labels, fill_value=0)\n\n# Convertir en proportions\nref_props = ref_counts / ref_counts.sum()\ntgt_props = tgt_counts / tgt_counts.sum()\n\n# Construire un DataFrame pour seaborn\ndf_props = pd.DataFrame({\n    \"Segment\": bin_labels,\n    \"Reference\": ref_props.values,\n    \"Target\": tgt_props.values\n})\n\n# Restructurer en format long\ndf_long = df_props.melt(id_vars=\"Segment\", \n                        value_vars=[\"Reference\", \"Target\"], \n                        var_name=\"Source\", \n                        value_name=\"Proportion\")\n\n# Style sobre\nsns.set_theme(style=\"whitegrid\")\n\n# Barplot avec proportions\nplt.figure(figsize=(8,6))\nsns.barplot(\n    x=\"Segment\", y=\"Proportion\", hue=\"Source\",\n    data=df_long, palette=[\"#4C72B0\", \"#55A868\"]  # bleu & vert sobres\n)\n\n# Titre et légende\n# Titles with story\nplt.title(\"Proportion Comparison by Segment (ViolentCrimesPerPop)\", fontsize=14, weight=\"bold\")\nplt.suptitle(\"Across all quantile segments (Q1–Q5), proportions are nearly identical\", fontsize=10, color=\"gray\")\n\nplt.xlabel(\"Quantile Segment (Q1 - Q5)\")\nplt.ylabel(\"Proportion\")\nplt.legend(title=\"Dataset\", loc=\"upper right\")\nplt.grid(False)\nplt.show()\n\n\n\n\n\n\n\n\nAs before, we reach the same conclusion: the distributions in the reference and target datasets are very similar. To move beyond visual inspection, we will now compute the Population Stability Index (PSI) and Cramér’s V statistic. These metrics allow us to quantify the differences between distributions; both for all variables in general and for the target variable ViolentCrimesPerPop in particular.\n\nEPS = 1e-12  # A very small constant to avoid division by zero or log(0)\n\n# ============================================================\n# 1. Basic functions\n# ============================================================\n\ndef safe_proportions(counts):\n    \"\"\"\n    Convert raw counts into proportions in a safe way.\n    - If the total count = 0, return all zeros (to avoid division by zero).\n    - Clip values so no proportion is exactly 0 or 1 (numerical stability).\n    \"\"\"\n    total = counts.sum()\n    if total == 0:\n        return np.zeros_like(counts, dtype=float)\n    p = counts / total\n    return np.clip(p, EPS, 1.0)\n\ndef calculate_psi(p_ref, p_tgt):\n    \"\"\"\n    Compute the Population Stability Index (PSI) between two distributions.\n\n    PSI = sum( (p_ref - p_tgt) * log(p_ref / p_tgt) )\n\n    Interpretation:\n    - PSI &lt; 0.1  → stable\n    - 0.1–0.25   → moderate shift\n    - &gt; 0.25     → major shift\n    \"\"\"\n    p_ref = np.clip(p_ref, EPS, 1.0)\n    p_tgt = np.clip(p_tgt, EPS, 1.0)\n    return float(np.sum((p_ref - p_tgt) * np.log(p_ref / p_tgt)))\n\ndef calculate_cramers_v(contingency):\n    \"\"\"\n    Compute Cramér's V statistic for association between two categorical variables.\n    - Input: a 2 x K contingency table (counts).\n    - Uses Chi² test.\n    - Normalizes the result to [0, 1].\n      * 0   → no association\n      * 1   → perfect association\n    \"\"\"\n    chi2, _, _, _ = chi2_contingency(contingency, correction=False)\n    n = contingency.sum()\n    r, c = contingency.shape\n    if n == 0 or min(r - 1, c - 1) == 0:\n        return 0.0\n    return np.sqrt(chi2 / (n * (min(r - 1, c - 1))))\n\n# ============================================================\n# 2. Preparing variables\n# ============================================================\n\ndef bin_numeric(ref, tgt, n_bins=5):\n    \"\"\"\n    Discretize a numeric variable into quantile bins (ex: quintiles).\n    - Quantile thresholds are computed only on the reference dataset.\n    - Extend bins with -inf and +inf to cover all possible values.\n    - Returns:\n        * ref binned\n        * tgt binned\n        * bin labels (Q1, Q2, ...)\n    \"\"\"\n    edges = np.unique(ref.dropna().quantile(np.linspace(0, 1, n_bins + 1)).values)\n    if len(edges) &lt; 3:  # if variable is almost constant\n        edges = np.array([-np.inf, np.inf])\n    else:\n        edges[0], edges[-1] = -np.inf, np.inf\n    labels = [f\"Q{i}\" for i in range(1, len(edges))]\n    return (\n        pd.cut(ref, bins=edges, labels=labels, include_lowest=True),\n        pd.cut(tgt, bins=edges, labels=labels, include_lowest=True),\n        labels\n    )\n\ndef prepare_counts(ref, tgt, n_bins=5):\n    \"\"\"\n    Prepare frequency counts for one variable.\n    - If numeric: discretize into quantile bins.\n    - If categorical: take all categories present in either dataset.\n    Returns:\n      segments, counts in reference, counts in target\n    \"\"\"\n    if pd.api.types.is_numeric_dtype(ref) and pd.api.types.is_numeric_dtype(tgt):\n        ref_b, tgt_b, labels = bin_numeric(ref, tgt, n_bins)\n        segments = labels\n    else:\n        segments = sorted(set(ref.dropna().unique()) | set(tgt.dropna().unique()))\n        ref_b, tgt_b = ref.astype(str), tgt.astype(str)\n\n    ref_counts = ref_b.value_counts().reindex(segments, fill_value=0)\n    tgt_counts = tgt_b.value_counts().reindex(segments, fill_value=0)\n    return segments, ref_counts, tgt_counts\n\n# ============================================================\n# 3. Analysis per variable\n# ============================================================\n\ndef analyze_variable(ref, tgt, n_bins=5):\n    \"\"\"\n    Analyze a single variable between two datasets.\n    Steps:\n    - Build counts by segment (bin for numeric, category for categorical).\n    - Compute PSI by segment and Global PSI.\n    - Compute Cramér's V from the contingency table.\n    - Return:\n        DataFrame with details\n        Summary dictionary (psi, v_cramer)\n    \"\"\"\n    segments, ref_counts, tgt_counts = prepare_counts(ref, tgt, n_bins)\n    p_ref, p_tgt = safe_proportions(ref_counts.values), safe_proportions(tgt_counts.values)\n\n    # PSI\n    psi_global = calculate_psi(p_ref, p_tgt)\n    psi_by_segment = (p_ref - p_tgt) * np.log(p_ref / p_tgt)\n\n    # Cramér's V\n    contingency = np.vstack([ref_counts.values, tgt_counts.values])\n    v_cramer = calculate_cramers_v(contingency)\n\n    # Build detailed results table\n    df = pd.DataFrame({\n        \"Segment\": segments,\n        \"Count Reference\": ref_counts.values,\n        \"Count Target\": tgt_counts.values,\n        \"Percent Reference\": p_ref,\n        \"Percent Target\": p_tgt,\n        \"PSI by Segment\": psi_by_segment\n    })\n\n    # Add summary lines at the bottom of the table\n    df.loc[len(df)] = [\"Global PSI\", np.nan, np.nan, np.nan, np.nan, psi_global]\n    df.loc[len(df)] = [\"Cramer's V\", np.nan, np.nan, np.nan, np.nan, v_cramer]\n\n    return df, {\"psi\": psi_global, \"v_cramer\": v_cramer}\n\n# ============================================================\n# 4. Excel reporting utilities\n# ============================================================\n\ndef apply_traffic_light(ws, wb, first_row, last_row, col, low, high):\n    \"\"\"\n    Apply conditional formatting (traffic light colors) to a numeric column in Excel:\n    - green  if value &lt; low\n    - orange if low &lt;= value &lt;= high\n    - red    if value &gt; high\n\n    Note: first_row, last_row, and col are zero-based indices (xlsxwriter convention).\n    \"\"\"\n    green  = wb.add_format({\"bg_color\": \"#C6EFCE\", \"font_color\": \"#006100\"})\n    orange = wb.add_format({\"bg_color\": \"#FCD5B4\", \"font_color\": \"#974706\"})\n    red    = wb.add_format({\"bg_color\": \"#FFC7CE\", \"font_color\": \"#9C0006\"})\n\n    if last_row &lt; first_row:\n        return  # nothing to color\n\n    ws.conditional_format(first_row, col, last_row, col,\n        {\"type\": \"cell\", \"criteria\": \"&lt;\", \"value\": low, \"format\": green})\n    ws.conditional_format(first_row, col, last_row, col,\n        {\"type\": \"cell\", \"criteria\": \"between\", \"minimum\": low, \"maximum\": high, \"format\": orange})\n    ws.conditional_format(first_row, col, last_row, col,\n        {\"type\": \"cell\", \"criteria\": \"&gt;\", \"value\": high, \"format\": red})\n\ndef representativity_report(ref_df, tgt_df, variables, output=\"representativity.xlsx\",\n                            n_bins=5, psi_thresholds=(0.10, 0.25),\n                            v_thresholds=(0.10, 0.25), color_summary=True):\n    \"\"\"\n    Build a representativity report across multiple variables and export to Excel.\n\n    For each variable:\n      - Create a sheet with detailed PSI by segment, Global PSI, and Cramer's V.\n      - Apply traffic light colors for easier interpretation.\n\n    Create one \"Résumé\" sheet with overall Global PSI and Cramer's V for all variables.\n    \"\"\"\n    summary = []\n\n    with pd.ExcelWriter(output, engine=\"xlsxwriter\") as writer:\n        wb = writer.book\n        fmt_header = wb.add_format({\"bold\": True, \"bg_color\": \"#0070C0\",\n                                    \"font_color\": \"white\", \"align\": \"center\"})\n        fmt_pct   = wb.add_format({\"num_format\": \"0.00%\"})\n        fmt_ratio = wb.add_format({\"num_format\": \"0.000\"})\n        fmt_int   = wb.add_format({\"num_format\": \"0\"})\n\n        for var in variables:\n            # Analyze variable\n            df, meta = analyze_variable(ref_df[var], tgt_df[var], n_bins)\n            sheet = var[:31]  # Excel sheet names are limited to 31 characters\n            df.to_excel(writer, sheet_name=sheet, index=False)\n            ws = writer.sheets[sheet]\n\n            # Format headers and columns\n            for j, col in enumerate(df.columns):\n                ws.write(0, j, col, fmt_header)\n            ws.set_column(0, 0, 18)\n            ws.set_column(1, 2, 16, fmt_int)\n            ws.set_column(3, 4, 20, fmt_pct)\n            ws.set_column(5, 5, 18, fmt_ratio)\n\n            nrows = len(df)   # number of data rows (excluding header)\n            col_psi = 5       # \"PSI by Segment\" column index\n\n            # PSI by Segment rows\n            apply_traffic_light(ws, wb, first_row=1, last_row=max(1, nrows-2),\n                                col=col_psi, low=psi_thresholds[0], high=psi_thresholds[1])\n\n            # Global PSI row (second to last)\n            apply_traffic_light(ws, wb, first_row=nrows-1, last_row=nrows-1,\n                                col=col_psi, low=psi_thresholds[0], high=psi_thresholds[1])\n\n            # Cramer's V row (last row) \n            apply_traffic_light(ws, wb, first_row=nrows, last_row=nrows,\n                                col=col_psi, low=v_thresholds[0], high=v_thresholds[1])\n\n            # Add summary info for Résumé sheet\n            summary.append({\"Variable\": var,\n                            \"Global PSI\": meta[\"psi\"],\n                            \"Cramer's V\": meta[\"v_cramer\"]})\n\n        # Résumé sheet\n        df_sum = pd.DataFrame(summary)\n        df_sum.to_excel(writer, sheet_name=\"Résumé\", index=False)\n        ws = writer.sheets[\"Résumé\"]\n        for j, col in enumerate(df_sum.columns):\n            ws.write(0, j, col, fmt_header)\n        ws.set_column(0, 0, 28)\n        ws.set_column(1, 2, 16, fmt_ratio)\n\n        # Apply traffic light to summary sheet\n        if color_summary and len(df_sum) &gt; 0:\n            last = len(df_sum)\n            # PSI column\n            apply_traffic_light(ws, wb, 1, last, 1, psi_thresholds[0], psi_thresholds[1])\n            # Cramer's V column\n            apply_traffic_light(ws, wb, 1, last, 2, v_thresholds[0], v_thresholds[1])\n\n    return output\n\n# ============================================================\n# Example\n# ============================================================\n\nif __name__ == \"__main__\":\n    # columns namees privées de fold\n    columns = [x for x in data.columns if x != \"fold\"]\n    print(columns)\n    # Generate the report\n    path = representativity_report(data_ref, data_target, columns, output=\"representativity.xlsx\")\n    print(f\" Report generated: {path}\")\n\n['PctKids2Par', 'PctWorkMom', 'LandArea', 'PopDens', 'MedYrHousBuilt', 'PctVacantBoarded', 'LemasPctOfficDrugUn', 'AsianPerCap', 'PctUsePubTrans', 'MedOwnCostPctIncNoMtg', 'PctSameHouse85', 'PctVacMore6Mos', 'PctEmplProfServ', 'PctEmplManu', 'PctImmigRec10', 'MedRentPctHousInc', 'ViolentCrimesPerPop']\n Report generated: representativity.xlsx\n\n\nAs mentioned earlier, the results of the distribution comparisons for each variable between the two datasets, calculated using PSI and Cramér’s V, are presented in separate sheets within a single Excel file.\nTo illustrate, we will first look at the results for the target variable. This example shows how both PSI and Cramér’s V are computed and how their values can be interpreted in practice.\nFinally, we will introduce the last sheet of the file, titled Summary, which consolidates the results for all variables of interest. This synthesis provides a global view of representativeness between the two datasets and makes interpretation and decision-making much easier.\n\nimport os, time, requests, pandas as pd\n\n# --- paramètres de base ---\nAPI_URL = \"https://api.football-data.org/v4/competitions/PL/matches\"\nSEASONS = [2023, 2024]  # 2023-24 et 2024-25\n\n# --- hardcoder la clé pour un test local ---\nTOKEN = \"46ff0dc946b740059274353d6889b27b\"  # &lt;-- colle ici ta clé brute\n\n# hardcoder la clé\nif not TOKEN:\n    raise RuntimeError(\"Variable d'environnement FOOTBALL_DATA_TOKEN absente.\")\n\nheaders = {\"X-Auth-Token\": TOKEN}\nrows = []\n\nfor season in SEASONS:\n    params = {\"season\": season}\n    r = requests.get(API_URL, params=params, headers=headers, timeout=30)\n    if r.status_code == 429:\n        # rate-limit: petite pause puis retry\n        time.sleep(10)\n        r = requests.get(API_URL, params=params, headers=headers, timeout=30)\n    r.raise_for_status()\n\n    for m in r.json().get(\"matches\", []):\n        # On garde les matches terminés ou attribués\n        if m.get(\"status\") in {\"FINISHED\", \"AWARDED\"}:\n            md = m.get(\"matchday\")\n            score = m.get(\"score\", {}).get(\"fullTime\", {})\n            hg, ag = score.get(\"home\"), score.get(\"away\")\n            if hg is None or ag is None or md is None:\n                continue\n            rows.append({\n                \"season\": season,\n                \"match_id\": m.get(\"id\"),\n                \"utcDate\": m.get(\"utcDate\"),\n                \"matchday\": int(md),\n                \"homeTeam\": m.get(\"homeTeam\", {}).get(\"name\"),\n                \"awayTeam\": m.get(\"awayTeam\", {}).get(\"name\"),\n                \"homeGoals\": int(hg),\n                \"awayGoals\": int(ag),\n                \"totalGoals\": int(hg) + int(ag),\n                \"venue\": (m.get(\"area\") or {}).get(\"name\"),  # parfois vide\n                \"status\": m.get(\"status\"),\n            })\n\n# --- ton export \"par match\" (inchangé) ---\ndf = pd.DataFrame(rows).sort_values([\"season\", \"matchday\", \"utcDate\"])\noutfile = \"data/premier_league_goals_by_match_2023_2024.csv\"\ndf.to_csv(outfile, index=False, encoding=\"utf-8\")\nprint(f\"Écrit {len(df)} lignes dans {outfile}\")\n\n# --- agrégat par journée (inchangé) ---\nagg = (df.groupby([\"season\",\"matchday\"], as_index=False)[\"totalGoals\"]\n         .sum()\n         .rename(columns={\"totalGoals\":\"goals_by_matchday\"}))\nagg.to_csv(\"data/premier_league_goals_by_matchday_2023_2024.csv\", index=False, encoding=\"utf-8\")\nprint(\"Agrégat par journée écrit dans premier_league_goals_by_matchday_2023_2024.csv\")\n\n# --- NOUVEAU : table large (DRY/orthogonale) -&gt; matchday, goals_2023, goals_2024 ---\nwide = (agg.pivot(index=\"matchday\", columns=\"season\", values=\"goals_by_matchday\")\n           .sort_index())\n\n# renommer colonnes -&gt; goals_&lt;season&gt;\nwide.columns = [f\"goals_{s}\" for s in wide.columns]\n\n# (option) remplir les journées manquantes par 0 (cas saison en cours)\nif not wide.empty:\n    full_index = range(1, int(wide.index.max()) + 1)\n    wide = wide.reindex(full_index)\n\nwide = wide.fillna(0).astype(int).reset_index().rename(columns={\"index\": \"matchday\"})\n\n# garantir l'ordre des colonnes : matchday, goals_2023, goals_2024, ...\ncols = [\"matchday\"] + sorted([c for c in wide.columns if c != \"matchday\"])\nwide = wide[cols]\n\nwide_outfile = \"data/goals_by_matchday_wide_2023_2024.csv\"\nwide.to_csv(wide_outfile, index=False, encoding=\"utf-8\")\nprint(f\"Table large écrite dans {wide_outfile}\")\nprint(wide.head())\n\nÉcrit 760 lignes dans data/premier_league_goals_by_match_2023_2024.csv\nAgrégat par journée écrit dans premier_league_goals_by_matchday_2023_2024.csv\nTable large écrite dans data/goals_by_matchday_wide_2023_2024.csv\n   matchday  goals_2023  goals_2024\n0         1          28          21\n1         2          30          32\n2         3          31          30\n3         4          41          23\n4         5          24          30\n\n\n\n# Compute the total number of goals in 2023 and 2024\ntotal_goals_2023 = wide[\"goals_2023\"].sum()\ntotal_goals_2024 = wide[\"goals_2024\"].sum()\n\nprint(f\"Total goals in 2023: {total_goals_2023}\")\nprint(f\"Total goals in 2024: {total_goals_2024}\")\n\nTotal goals in 2023: 1246\nTotal goals in 2024: 1115\n\n\n\n# Create global data.\n\n# Build datasets with a \"Group\" column\ndata_2023 = pd.DataFrame({\n    \"Goals\": wide[\"goals_2023\"],\n    \"Season\": \"2023/2024\"\n})\n\ndata_2024 = pd.DataFrame({\n    \"Goals\": wide[\"goals_2024\"],\n    \"Season\": \"2024/2025\"\n})\n\n# Merge them\ndata_2_seasons = pd.concat([data_2023, data_2024])\ndata_2_seasons\nprint(\"Data for 2023/2024:\")\ndata_2023\n\nData for 2023/2024:\n\n\n\n\n\n\n\n\n\nGoals\nSeason\n\n\n\n\n0\n28\n2023/2024\n\n\n1\n30\n2023/2024\n\n\n2\n31\n2023/2024\n\n\n3\n41\n2023/2024\n\n\n4\n24\n2023/2024\n\n\n5\n30\n2023/2024\n\n\n6\n29\n2023/2024\n\n\n7\n27\n2023/2024\n\n\n8\n33\n2023/2024\n\n\n9\n30\n2023/2024\n\n\n10\n30\n2023/2024\n\n\n11\n37\n2023/2024\n\n\n12\n34\n2023/2024\n\n\n13\n38\n2023/2024\n\n\n14\n30\n2023/2024\n\n\n15\n27\n2023/2024\n\n\n16\n28\n2023/2024\n\n\n17\n23\n2023/2024\n\n\n18\n39\n2023/2024\n\n\n19\n35\n2023/2024\n\n\n20\n30\n2023/2024\n\n\n21\n39\n2023/2024\n\n\n22\n45\n2023/2024\n\n\n23\n37\n2023/2024\n\n\n24\n34\n2023/2024\n\n\n25\n34\n2023/2024\n\n\n26\n36\n2023/2024\n\n\n27\n30\n2023/2024\n\n\n28\n30\n2023/2024\n\n\n29\n32\n2023/2024\n\n\n30\n29\n2023/2024\n\n\n31\n35\n2023/2024\n\n\n32\n33\n2023/2024\n\n\n33\n40\n2023/2024\n\n\n34\n32\n2023/2024\n\n\n35\n36\n2023/2024\n\n\n36\n33\n2023/2024\n\n\n37\n37\n2023/2024\n\n\n\n\n\n\n\n\n# Boxplot des buts par saison\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(6, 4))\n\n# Boxplot with both distributions overlayed\nsns.boxplot(\n    x=\"Season\", \n    y=\"Goals\", \n    data=data_2_seasons,\n    palette=\"Set2\",\n    width=0.6,\n    fliersize=3\n)\n\n\n# Add mean points\nmedian = data_2_seasons.groupby(\"Season\")[\"Goals\"].median()\nfor i, m in enumerate(median):\n    plt.scatter(i, m, color=\"red\", marker=\"D\", s=50, zorder=3, label=\"Median\" if i == 0 else \"\")\n\n# Title tells the story\nplt.title(\"Goals Per Season\", fontsize=14, weight=\"bold\")\nplt.suptitle(\"Goals were higher in 2023/2024 than in 2024/2025.\",\n             fontsize=10, color=\"gray\")\n\nplt.ylabel(\"Goals\", fontsize=12)\nplt.xlabel(\"\")\n\n# Cleaner look\nsns.despine()\nplt.grid(visible=False)\nplt.legend()\n\nplt.show()\n\n/var/folders/v8/l5q0bw4s2ln17s59y7cc86rm0000gn/T/ipykernel_57553/619544620.py:8: FutureWarning:\n\n\n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n\n\n\n\n\n\n\n\n\n\nTo take a closer look, we can use Kernel Density Estimation (KDE) plots, which provide a smooth view of the underlying distribution and make it easier to spot subtle differences.\n\nplt.figure(figsize=(7.24, 4.07), dpi=100)  # ~724x407 px\n\nsns.kdeplot(\n    data=data_2_seasons,\n    x=\"Goals\",\n    hue=\"Season\",\n    fill=True,\n    alpha=0.4,\n    common_norm=False,\n    palette=\"Set2\",\n    linewidth=2\n)\n\nplt.title(\"Kernel Density Estimation of Goals Per Season\", fontsize=14, weight=\"bold\")\nplt.suptitle(\"2023/2024 shows greater variability, while 2024/2025 is more concentrated around 30 goals.\", \n             fontsize=10, color=\"gray\")\n\nplt.xlabel(\"Goals\")\nplt.ylabel(\"Density\")\n\nsns.despine()\nplt.grid(False)\n\n  # ≈ 50 KB\nplt.show()\n\n\n\n\n\n\n\n\nThe KDE graph confirms that the two distributions are very similar, showing a high degree of overlap. The Kolmogorov-Smirnov (KS) statistical test of 0.976 also indicates that there is no significant difference between the two groups. To extend the analysis, we can now examine the cumulative distribution of the target variable.\n\n# Cumulative distribution\nplt.figure(figsize=(7.24, 4.07), dpi=100)  # ~724x407 px\nsns.histplot(\n    data=data_2_seasons,\n    x=\"Goals\",\n    hue=\"Season\",\n    stat=\"density\",\n    common_norm=False,\n    fill=False,\n    element=\"step\",\n    bins=len(df_all),\n    cumulative=True,\n    palette=\"Set2\"\n)\n\n# Titles tell the story\nplt.title(\"Cumulative Distribution of Goals\", fontsize=14, weight=\"bold\")\nplt.suptitle(\"Comparing goal distributions between 2023/2024 and 2024/2025\", fontsize=10)\n\n# Labels & cleanup\nplt.xlabel(\"Goals\")\nplt.ylabel(\"Cumulative proportion\")\nplt.grid(visible=False)\nplt.show()\n\n\n\n\n\n\n\n\n\ndef bin_numeric(ref, tgt, n_bins=5):\n    \"\"\"\n    Discretize a numeric variable into quantile bins (ex: quintiles).\n    - Quantile thresholds are computed only on the reference dataset.\n    - Extend bins with -inf and +inf to cover all possible values.\n    - Returns:\n        * ref binned\n        * tgt binned\n        * bin labels (Q1, Q2, ...)\n    \"\"\"\n    edges = np.unique(ref.dropna().quantile(np.linspace(0, 1, n_bins + 1)).values)\n    if len(edges) &lt; 3:  # if variable is almost constant\n        edges = np.array([-np.inf, np.inf])\n    else:\n        edges[0], edges[-1] = -np.inf, np.inf\n    labels = [f\"Q{i}\" for i in range(1, len(edges))]\n    return (\n        pd.cut(ref, bins=edges, labels=labels, include_lowest=True),\n        pd.cut(tgt, bins=edges, labels=labels, include_lowest=True),\n        labels\n    )\n\n# Apply binning\nref_binned, tgt_binned, bin_labels = bin_numeric(data_2023[\"Goals\"], data_2024[\"Goals\"], n_bins=5)\n\n# Effectifs par segment pour Reference et Target\nref_counts = ref_binned.value_counts().reindex(bin_labels, fill_value=0)\ntgt_counts = tgt_binned.value_counts().reindex(bin_labels, fill_value=0)\n\n# Convertir en proportions\nref_props = ref_counts / ref_counts.sum()\ntgt_props = tgt_counts / tgt_counts.sum()\n\n# Construire un DataFrame pour seaborn\ndf_props = pd.DataFrame({\n    \"Segment\": bin_labels,\n    \"2023/2024\": ref_props.values,\n    \"2024/2025\": tgt_props.values\n})\n\n# Restructurer en format long\ndf_long = df_props.melt(id_vars=\"Segment\", \n                        value_vars=[\"2023/2024\", \"2024/2025\"], \n                        var_name=\"Source\", \n                        value_name=\"Proportion\")\n\n# Style sobre\nsns.set_theme(style=\"whitegrid\")\n\n# Barplot avec proportions\nplt.figure(figsize=(7.24, 4.07), dpi=100)  # ~724x407 px\nsns.barplot(\n    x=\"Segment\", y=\"Proportion\", hue=\"Source\",\n    data=df_long, palette=\"Set2\"\n)\n\n# Titre et légende\n# Titles with story\nplt.title(\"Proportion Comparison by Segment (Goals)\", fontsize=14, weight=\"bold\")\nplt.suptitle(\"Across all quantile segments (Q1–Q5), proportions are different\", fontsize=10, color=\"gray\")\n\nplt.xlabel(\"Quantile Segment (Q1 - Q5)\")\nplt.ylabel(\"Proportion\")\nplt.legend(title=\"Dataset\", loc=\"upper right\")\nplt.grid(False)\nplt.show()\n\n\n\n\n\n\n\n\n\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Example: goals data\nx = np.sort(data_2023[\"Goals\"].dropna().values)\ny = np.sort(data_2024[\"Goals\"].dropna().values)\n\n# Compute common quantiles\nquantiles = np.linspace(0, 1, min(len(x), len(y)))\nqx = np.quantile(x, quantiles)\nqy = np.quantile(y, quantiles)\n\n# Apply seaborn style\nsns.set_style(\"white\")          # removes background grid\npalette = sns.color_palette(\"Set2\", 2)\n\n# Scatter\nsns.scatterplot(x=qx, y=qy, color=palette[0], s=60, label=\"Q-Q Points\")\n\n# 45° reference line\nplt.plot([min(qx.min(), qy.min()), max(qx.max(), qy.max())],\n         [min(qx.min(), qy.min()), max(qx.max(), qy.max())],\n         color=palette[1], linestyle=\"--\", label=\"2024 = 2023\")\n\n# Titles and labels\nplt.title(\"Q-Q Plot of Goals (2023/2024 vs 2024/2025)\", fontsize=14)\nplt.xlabel(\"Quantiles of 2023/2024\", fontsize=12)\nplt.ylabel(\"Quantiles of 2024/2025\", fontsize=12)\n\n# No grid\nplt.grid(False)\n\n# Legend\nplt.legend()\n\nplt.show()\n\n\n\n\n\n\n\n\n\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\n\n# Extract the two series\nx = data_2023[\"Goals\"].dropna().values\ny = data_2024[\"Goals\"].dropna().values\n\n# Compute QQ plot\nplt.figure(figsize=(7.24, 4.07), dpi=100)  # ~724x407 px\nsm.qqplot_2samples(x, y, line='45')   # '45' draws y=x line\nplt.title(\"Q-Q Plot of Goals: 2023/2024 vs 2024/2025\")\nplt.xlabel(\"Quantiles of 2023/2024\")\nplt.ylabel(\"Quantiles of 2024/2025\")\nplt.show()\n\n&lt;Figure size 724x407 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n\ncolumns = [x for x in data_2023.columns if x != \"Season\"]\nprint(data_2023[\"Goals\"].value_counts())\n    # Generate the report\npath = representativity_report(data_2023, data_2024, columns, output=\"representativity_plot.xlsx\")\n\nGoals\n30    8\n34    3\n37    3\n33    3\n32    2\n36    2\n35    2\n39    2\n28    2\n27    2\n29    2\n38    1\n23    1\n24    1\n45    1\n41    1\n31    1\n40    1\nName: count, dtype: int64"
  }
]