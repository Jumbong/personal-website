---
title : "Exercices sur les Vecteurs Aléatoires Discrets"
---

# 7. Vecteurs Aléatoires Discrets et continus

## Exercice 1

La loi de probabilité jointe de deux variables aléatoires X et Y est

$$
\mathbb{P}(X = x, Y = y) = \frac{x + 1}{12},
\qquad x \in \{0,1\}, \; y \in \{0,1,2,3\}.
$$

1. Les variables aléatoires X et Y sont-elles indépendantes ?

2. Déterminer les lois marginales de X et Y, et identifier lesquelles correspondent à des distributions classiques étudiées en cours.

3. Calculer E(XY) et
   $$
   E\!\left( \frac{X}{Y+1} \right).
   $$


## Correction de l’Exercice 1

On considère la loi jointe :

$$
\mathbb{P}(X = x, Y = y) = \frac{x + 1}{12},
\qquad x \in \{0,1\},\; y \in \{0,1,2,3\}.
$$

---

1. Indépendance de X et Y

Nous calculons d'abord les marginales.

**Marginale de X**
$$
\mathbb{P}(X = x)
= \sum_{y=0}^3 \frac{x+1}{12}
= 4 \cdot \frac{x+1}{12}
= \frac{x+1}{3}.
$$

Donc :

$$
\mathbb{P}(X=0)=\frac{1}{3},\qquad
\mathbb{P}(X=1)=\frac{2}{3}.
$$

**Marginale de Y**

$$
\mathbb{P}(Y = y)
= \sum_{x=0}^1 \frac{x+1}{12}
= \frac{1}{12} + \frac{2}{12}
= \frac{3}{12}
= \frac{1}{4}.
$$

Donc Y est uniforme sur {0,1,2,3}.

**Vérification de l’indépendance**

soient $x \in \{0,1\}$ et $y \in \{0,1,2,3\}$. On a :
$$
\mathbb{P}(X=x)\,\mathbb{P}(Y=y) = \frac{x+1}{3} \cdot \frac{1}{4} = \frac{x+1}{12} = \mathbb{P}(X=x, Y=y).
$$

Ainsi :

$$
\boxed{X \text{ et } Y \text{ sont indépendantes.}}
$$

---

2. Lois marginales de X et Y

- Le support de X est donné par $\{0,1\}$, et X suit une loi de Bernoulli de paramètre $p=\frac23$ :

  $$
  \mathbb{P}(X=1)=\frac{2}{3},\quad \mathbb{P}(X=0)=\frac{1}{3}.
  $$

- Y a pour support $\{0,1,2,3\}$, et chaque valeur est equiprobable :

  $$
  \mathbb{P}(Y=y)=\frac14,\quad y=0,1,2,3.
  $$

Donc Y suit une loi uniforme discrète sur $\{0,1,2,3\}$.

---

3. Calcul de E(XY)

Comme X et Y sont indépendantes :

$$
E(XY)=E(X)\,E(Y).
$$

**Calcul de E(X)**

$$
E(X)=0 \cdot \frac13 + 1 \cdot \frac23 = \frac23.
$$

**Calcul de E(Y)**

$$
E(Y) = \frac{0+1+2+3}{4} = \frac{6}{4} = \frac32.
$$

#### Produit

$$
E(XY)=\frac23 \cdot \frac32 = 1.
$$

---

4. Calcul de $E\left(\frac{X}{Y+1}\right)$

Grâce à l’indépendance :

$$
E\left(\frac{X}{Y+1}\right)
=E(X)\,E\left(\frac{1}{Y+1}\right)
=\frac23\,E\left(\frac{1}{Y+1}\right).
$$

Calculons :

$$
E\left(\frac{1}{Y+1}\right)
= \frac14\left(1 + \frac12 + \frac13 + \frac14\right)
= \frac14 \cdot \frac{25}{12}
= \frac{25}{48}.
$$

Donc :

$$
E\left(\frac{X}{Y+1}\right)
= \frac23 \cdot \frac{25}{48}
= \frac{50}{144}
= \frac{25}{72}.
$$

---

## Exercice 2

Soient X et Y deux variables aléatoires à valeurs dans $\mathbb{N}$, ayant la densité jointe suivante :

$$
\mathbb{P}(X = k,\, Y = n)
=
\begin{cases}
\binom{n}{k} p^k (1-p)^{\,n-k} \, e^{-\lambda} \dfrac{\lambda^n}{n!}, & 0 \le k \le n, \\[6pt]
0, & \text{sinon},
\end{cases}
$$

avec $p \in ]0,1[$ et $\lambda > 0$ deux paramètres fixés.

1. Déterminer les densités marginales de $X$ et de $Y$, et montrer que chacune suit une loi de Poisson en précisant le paramètre.

2. Déterminer la loi conditionnelle de $X$ sachant $Y$.  
   En déduire :
   - la valeur de $E(XY)$,
   - puis de $E(X)$,
   - puis de $\mathrm{Cov}(X,Y)$.


## Correction de l’Exercice 2



1) Lois marginales de $X$ et $Y$

a) Loi marginale de $X$

Pour tout $k \in \mathbb{N}$ :

$$
\mathbb{P}(X = k) = \sum_{n=0}^{+\infty} \mathbb{P}(X=k, Y=n).
$$

$$
= \sum_{n=0}^{k-1} \mathbb{P}({X=k, Y=n}) + \sum_{n=k}^{+\infty} \mathbb{P}(X=k, Y=n).
$$  

La première somme est nulle car $\mathbb{P}(X=k, Y=n)=0$ pour $n<k$.
Donc :
$$
\mathbb{P}(X = k)
= \sum_{n=k}^{+\infty} \binom{n}{k} p^k (1-p)^{\,n-k} e^{-\lambda} \frac{\lambda^n}{n!}.
$$

Par changement de variable, on écrit :

$$
\mathbb{P}(X = k) = \sum_{n=0}^{+\infty} \binom{n+k}{k} p^k (1-p)^{\,n} e^{-\lambda} \frac{\lambda^{n+k}}{(n+k)!}.
$$

Or $\binom{n+k}{k} = \dfrac{(n+k)!}{k! \, n!}$.
On obtient :

$$
\mathbb{P}(X=k)
= \sum_{n=0}^{+\infty} \frac{(n+k)!}{k! \, n!} p^k (1-p)^{\,n} e^{-\lambda} \frac{\lambda^{n+k}}{(n+k)!}.
$$

On simplifie et on factorise par tout ce qui ne dépend pas de $n$ :

$$
\mathbb{P}(X = k)
= e^{-\lambda} \frac{(p\lambda)^k}{k!} \sum_{n=0}^{+\infty} \frac{\{(1-p)\lambda\}^n}{n!}.
$$

La somme est :

$$
\sum_{m=0}^{+\infty} \frac{\{(1-p)\lambda\}^m}{m!}
= e^{(1-p)\lambda}.
$$

Donc :

$$
\mathbb{P}(X = k)
= e^{-p\lambda}\frac{(p\lambda)^k}{k!}.
$$

Ainsi :

$$
X \sim \mathrm{Pois}(p\lambda).
$$

---

b) Loi marginale de Y

Pour $n \in \mathbb{N}$ :

$$
\mathbb{P}(Y = n)
= \sum_{k=0}^{+ \infty} \mathbb{P}(X = k, Y = n)
= \sum_{k=0}^{n} \mathbb{P}(X = k, Y = n) + \sum_{k=n+1}^{+\infty} \mathbb{P}(X = k, Y = n).
$$

La deuxième somme est nulle car $\mathbb{P}(X=k, Y=n)=0$ pour $k>n$.
Donc :

$$
\mathbb{P}(Y = n)
= \sum_{k=0}^{n} \binom{n}{k} p^k (1-p)^{\,n-k} e^{-\lambda} \frac{\lambda^n}{n!}.
$$

On factorise par tout ce qui ne dépend pas de $k$ :

$$
\mathbb{P}(Y = n)
= e^{-\lambda} \frac{\lambda^n}{n!} \sum_{k=0}^n \binom{n}{k} p^k (1-p)^{\,n-k}.    
$$


Mais :

$$
\sum_{k=0}^n \binom{n}{k} p^k (1-p)^{\,n-k} = (p + (1-p))^n = 1.
$$

Donc :

$$
\mathbb{P}(Y = n)
= e^{-\lambda}\frac{\lambda^n}{n!},
$$

ainsi :

$$
Y \sim \mathrm{Pois}(\lambda).
$$

---

2) Loi conditionnelle de X sachant Y

Pour $0 \le k \le n$ :

$$
\mathbb{P}(X = k \mid Y = n)
= \frac{\mathbb{P}(X=k,Y=n)}{\mathbb{P}(Y=n)}
= \binom{n}{k} p^k (1-p)^{\,n-k}.
$$

Ainsi :

$$
X \mid (Y=n) \sim \mathrm{Bin}(n, p).
$$

---

**Conséquences sur les moments**

- Espérance conditionnelle :

  $$
  E(X \mid Y = n) = np.
  $$

Donc :
$$
E(X \mid Y) = p Y.
$$

- Deuxième moment conditionnel :

  $$
  E(XY \mid Y = n) = n E(X \mid Y=n) = n^2 p.
  $$

Ainsi :
$$
E(XY \mid Y) = p Y^2.
$$

---

a) Calcul de E(XY)

On utilise la loi totale :

$$
E(XY) = E\big( E(XY \mid Y) \big)
= E(p Y^2)
= p E(Y^2).
$$

Or, pour une Poisson($\lambda$) :

$$
E(Y)=\lambda, \qquad Var(Y)=\lambda,
$$

donc :

$$
E(Y^2)=Var(Y)+[E(Y)]^2=\lambda+\lambda^2.
$$

Donc :

$$
E(XY)= p(\lambda+\lambda^2).
$$

---

b) Calcul de E(X)

On sait :

$$
X \sim \mathrm{Pois}(p\lambda),
$$

donc :

$$
E(X) = p\lambda.
$$

---

c) Covariance $\mathrm{Cov}(X,Y)$

Définition :

$$
\mathrm{Cov}(X,Y)
= E(XY) - E(X)E(Y).
$$

On remplace :

$$
E(XY)=p(\lambda+\lambda^2),
\qquad
E(X)=p\lambda,
\qquad
E(Y)=\lambda.
$$

Donc :

$$
\mathrm{Cov}(X,Y)
= p(\lambda+\lambda^2) - (p\lambda)(\lambda)
= p\lambda.
$$

---

## Exercice 3

Dans une maison de retraite, le nombre de chutes X au cours d’une année suit une loi de Poisson de paramètre $\lambda$.
Chaque chute peut, indépendamment des autres chutes, conduire à une fracture de la hanche avec une probabilité $p$.
Soit $Y$ le nombre total de fractures de la hanche au cours de l’année.

1. Donner la loi conditionnelle de $Y$ sachant $X$.
2. En déduire l’espérance du nombre de fractures sachant le nombre de chutes, puis l’espérance du nombre total de fractures.


## Correction Exercice 3

1) Loi conditionnelle de $Y$ sachant $X$

Conditionnellement à $X = x$, il y a $x$ chutes indépendantes.
Chaque chute entraîne une fracture avec une probabilité $p$.

Ainsi, conditionnellement à $X = x$, la variable $Y$ suit une loi binomiale :

$$
Y \mid X = x \sim \text{Binomial}(x, p).
$$

Donc, pour tout $k = 0, \ldots, x$,

$$
\mathbb{P}(Y = k \mid X = x)
= \binom{x}{k} p^{k} (1 - p)^{x - k}.
$$

---

2) Espérance conditionnelle et espérance totale

L'espérance d'une loi binomiale donne :

$$
E(Y \mid X = x) = px.
$$

Donc : $$E(Y \mid X) = pX.$$

En appliquant la loi de l'espérance totale :

$$
E(Y) = E(E(Y \mid X)) = E(pX).
$$

Comme $X \sim \text{Poisson}(\lambda)$, on a $E(X) = \lambda$.

Donc :

$$
E(Y) = p \, E(X) = \lambda p.
$$

---

## Exercice 4

Durant une journée, Jean reçoit des coups de téléphone et des textos.  
Le nombre de coups de téléphone $X$ suit une loi de Poisson de paramètre $\lambda$,  
et le nombre de textos $Y$ suit une loi de Poisson de paramètre $\mu$.  
Les variables $X$ et $Y$ sont indépendantes.

1. Sachant que Jean a reçu au total $n$ coups de téléphone ou textos, donner la distribution conditionnelle du nombre de textos.  
   Vous montrerez que la loi conditionnelle est celle d’une loi binomiale et vous préciserez ses paramètres.

2. En déduire le nombre moyen de textos reçus, conditionnellement à $n$.


## Correction Exercice 4

Nous savons que $X \sim \text{Pois}(\lambda)$, $Y \sim \text{Pois}(\mu)$, et qu’ils sont indépendants.  
La somme $X + Y$ suit alors une loi de Poisson de paramètre $\lambda + \mu$.

---

1) Loi conditionnelle de $Y$ sachant $X + Y = n$

Pour $k = 0, \ldots, n$,

$$
P(Y = k \mid X + Y = n)
= \frac{P(Y = k,\, X + Y = n )}{P(X + Y = n)}.
$$

$$
= \frac{P(Y = k) P(X+ Y = n \mid Y = k)}{P(X + Y = n)}
= \frac{P(Y = k) P(X = n-k \mid Y = k)}{P(X + Y = n)}.
$$

Comme $X$ et $Y$ sont indépendantes,
$$
P(X = n-k \mid Y = k) = P(X = n-k).
$$

Donc :

$$
P(Y = k \mid X + Y = n)
= \frac{P(X = n-k) P(Y = k)}{P(X + Y = n)}.
$$

Or,

$$
P(X = n-k) = e^{-\lambda} \frac{\lambda^{n-k}}{(n-k)!},
\qquad
P(Y = k) = e^{-\mu} \frac{\mu^k}{k!}.
$$

Le dénominateur est

$$
P(X + Y = n) = e^{-(\lambda+\mu)} \frac{(\lambda+\mu)^n}{n!}.
$$

En combinant :

$$
P(Y = k \mid X + Y = n)
= \binom{n}{k}
\left( \frac{\mu}{\lambda+\mu} \right)^k
\left( \frac{\lambda}{\lambda+\mu} \right)^{n-k}.
$$

Ce qui donne encore :

$$
P(Y = k \mid X + Y = n ) 
= \binom{n}{k} 
\left( \frac{\mu}{\lambda+\mu} \right)^k
\left( 1 - \frac{\mu}{\lambda+\mu} \right)^{n-k}.
$$

Ainsi :
$$
Y \mid (X+Y = n)
\sim \text{Binomial}\left(n,\; \frac{\mu}{\lambda+\mu}\right).
$$

---

2) Espérance conditionnelle

Pour une loi binomiale $\text{Bin}(n,p)$, l'espérance vaut $np$.  
Donc :

$$
E(Y \mid X+Y = n)
= n \cdot \frac{\mu}{\lambda+\mu}.
$$

D'ou :
$$
E(Y \mid X+Y) = (X + Y) \cdot \frac{\mu}{\lambda+\mu}.
$$

---


## Exercice 5

Soit $\mathbf{X} = (X, Y)^\top$ un vecteur aléatoire de densité

$$
f_X(x, y) =
\begin{cases}
c + 2xy, & \text{si } x, y \in [0,1], \\
0, & \text{sinon}.
\end{cases}
$$

1. Donner la valeur de $c$ pour que $f_X$ corresponde à une densité.

2. Est-ce que les variables $X$ et $Y$ sont indépendantes ?  
   Expliquer pourquoi elles ont la même distribution marginale, et calculer cette distribution.

3. Calculer la fonction de répartition de $\mathbf{X}$.

## Correction — Exercice 5

On considère la densité

$$
f_{X}(x,y)=
\begin{cases}
c + 2xy, & x,y \in [0,1],\\[4pt]
0, & \text{sinon}.
\end{cases}
$$

---

1) Détermination de la constante c.

Pour que $f_X$ soit une densité, il faut :

$$
\iint_{\mathbb{R}^2} f_X(x,y)\,dxdy = 1.
$$

Comme la densité est nulle hors de $[0,1]^2$, on calcule :

$$  
\int_0^1 \int_0^1 (c + 2xy)\,dxdy.
$$

On sépare les termes :

$$
\int_0^1\int_0^1 c\,dxdy = c,
$$

$$  
\int_0^1\int_0^1 2xy\,dxdy 
= 2\left(\int_0^1 x\,dx\right)\left(\int_0^1 y\,dy\right)
= 2\left(\frac12\right)\left(\frac12\right)
= \frac12.
$$

Ainsi :

$$
c + \frac12 = 1 \quad \Rightarrow \quad c = \frac12.
$$

---

2) Les variables $X$ et $Y$ sont-elles indépendantes ?
On observe que :

$$
f_{X}(x,y) = \frac12 + 2xy.
$$

Il n’est **pas possible** d’écrire cette fonction comme un produit :

$$
f_X(x,y) = g(x)\,h(y),
$$

car la présence simultanée :

- d’un terme *constant* $\frac12$,
- et d’un terme *multiplicatif* $2xy$

rend impossible toute factorisation de la forme $g(x)h(y)$.

Ainsi :

$$  
\boxed{X \text{ et } Y \text{ ne sont pas indépendantes}.}
$$

Comme la densité est **symétrique en $x$ et $y$**, leurs marginales sont identiques. On peut vérifier que :

$$
f_X(x) = \frac12 + x, \qquad f_Y(y)=\frac12+y.
$$

---

3) Fonction de répartition de $\mathbf{X}$

On cherche :

$$
F_X(t_1,t_2)=\mathbb{P}(X \le t_1,\ Y \le t_2)
= \int_0^{t_1}\int_0^{t_2} \left(\frac12 + 2xy\right) dy\,dx.
$$

On examine les différents cas :

---

 **Cas 1 :** $t_1 < 0$ ou $t_2 < 0$

$$
F_X(t_1,t_2)=0.
$$
---

**Cas 2 :** $t_1 > 1$ et $t_2 > 1$

$$
F_X(t_1,t_2)=1.
$$
---

**Cas 3 :** $t_1 \in [0,1]$ et $t_2 > 1$

Symétriquement, la probabilité vaut :

$$
F_X(t_1,t_2)=\frac{t_1(t_1+1)}{2}.
$$
---

**Cas 4 :** $t_1 > 1$ et $t_2 \in [0,1]$

De façon analogue :

$$
F_X(t_1,t_2)=\frac{t_2(t_2+1)}{2}.
$$

---

**Cas 5 :** $t_1,t_2 \in [0,1]$
On calcule explicitement :

$$
F_X(t_1,t_2)
= \int_0^{t_1} \left( \int_0^{t_2}
\left(\frac12 + 2xy\right) dy \right) dx.
$$

Intégrale intérieure :

$$
\int_0^{t_2} \left(\frac12 + 2xy\right) dy
= \frac{t_2}{2} + x\,t_2^2.
$$

Puis intégration en $x$ :

$$
F_X(t_1,t_2)
= \int_0^{t_1} \left( \frac{t_2}{2} + xt_2^2 \right) dx
= \frac{t_1t_2}{2} + \frac{t_1^2 t_2^2}{2}.
$$

---

**Forme finale de la fonction de répartition**

$$
F_X(t_1,t_2)=
\begin{cases}
0, & t_1 < 0 \text{ ou } t_2 < 0, \\[6pt]
\frac{t_1t_2}{2} + \frac{t_1^2t_2^2}{2}, & t_1,t_2 \in [0,1],\\[6pt]
\frac{t_1(t_1+1)}{2}, & t_1 \in [0,1],\ t_2>1, \\[6pt]
\frac{t_2(t_2+1)}{2}, & t_2 \in [0,1],\ t_1>1, \\[6pt]
1, & t_1>1,\ t_2>1.
\end{cases}
$$


## Exercice 7

Soit $\mathbf{X} = (X, Y, Z)^\top$ un vecteur aléatoire de densité

$$
f_X(x,y,z) =
\begin{cases}
c \, e^{-x - y - 3z}, & \text{si } x, y, z > 0, \\[4pt]
0, & \text{sinon}.
\end{cases}
$$

1. Donner la valeur de c pour que $f_X$ corresponde bien à une densité.

2. Les variables aléatoires $X$, $Y$ et $Z$ sont-elles indépendantes ?  
   Déduire directement leurs densités de l’écriture de $f_X$, et utiliser vos connaissances sur la loi exponentielle.

3. Déterminer la densité du vecteur aléatoire
$$
\mathbf{Y} = \left( \frac{X}{X+Y},\ X+Y \right)^\top.
$$

En déduire que \(\frac{X}{X+Y}\) et \(X+Y\) sont indépendantes, et donner leurs densités marginales.

## Correction — Exercice 7

1) Détermination de la constante c

Pour que $f_X$ soit une densité, il faut :

$$
\iiint_{\mathbb{R}^3} f_X(x,y,z)\,dx\,dy\,dz = 1.
$$

Comme la densité est nulle hors du domaine \(x>0, y>0, z>0\), on calcule :

$$
\int_0^{+\infty}\!\!\int_0^{+\infty}\!\!\int_0^{+\infty}
c e^{-x - y - 3z}\, dx\,dy\,dz.
$$

On sépare les intégrales :

$$
c \left( \int_0^{+\infty} e^{-x}\,dx \right)
\left( \int_0^{+\infty} e^{-y}\,dy \right)
\left( \int_0^{+\infty} e^{-3z}\,dz \right).
$$

Or,

$$
\int_0^{+\infty} e^{-x}\,dx = 1,\qquad
\int_0^{+\infty} e^{-y}\,dy = 1,\qquad
\int_0^{+\infty} e^{-3z}\,dz = \frac{1}{3}.
$$

Ainsi :

$$
c \cdot 1 \cdot 1 \cdot \frac{1}{3} = 1
\quad\Rightarrow\quad
c = 3.
$$

---

 2) Indépendance de X, Y, Z

La densité jointe s’écrit :

$$
f_X(x,y,z) = 3 e^{-x} e^{-y} e^{-3z}.
$$

Cette écriture est de la forme :

$$
f_{X,Y,Z}(x,y,z) = f_X(x)\, f_Y(y)\, f_Z(z),
$$

où :

$$
f_X(x) = e^{-x} \mathbf{1}_{x>0}, 
\qquad f_Y(y) = e^{-y}\mathbf{1}_{y>0}, 
\qquad f_Z(z) = 3 e^{-3z}\mathbf{1}_{z>0}.
$$

Conclusion :

$$
\boxed{X,\ Y,\ Z \text{ sont indépendants}.}
$$

De plus :

- $X \sim \mathrm{Exp}(1)$
- $Y \sim \mathrm{Exp}(1)$
- $Z \sim \mathrm{Exp}(3)$

---

3) Densité du vecteur transformé 

$$
\mathbf{Y} = \left( T = \frac{X}{X+Y},\ U= X+Y \right).
$$

On considère l’application :

$$
h : (x,y) \mapsto (t,u)= \left( \frac{x}{x+y},\, x+y \right),
\qquad x,y>0.
$$

**a) Inversion du changement de variables**

À partir de :

$$
t = \frac{x}{x+y}, \qquad u = x+y,
$$

on résout :

$$
x = tu,\qquad y = u - tu = u(1-t),
$$

avec :

$$
t \in (0,1),\quad u>0.
$$

**b) Jacobien**

Le jacobien vaut :

$$
J = 
\begin{vmatrix}
\frac{\partial x}{\partial t} & \frac{\partial x}{\partial u}\\[3pt]
\frac{\partial y}{\partial t} & \frac{\partial y}{\partial u}
\end{vmatrix}
=
\begin{vmatrix}
u & t\\[3pt]
-u & 1-t
\end{vmatrix}
= u(1-t) + ut = u.
$$

Donc :

$$
|J| = u.
$$

**c) Densité jointe de (T,U)**

Puisque $X$ et $Y$ sont indépendants exponentiels :

$$
f_{X,Y}(x,y) = e^{-x} e^{-y} = e^{-u}.
$$

On applique le changement de variables :

$$
f_{T,U}(t,u)
= f_{X,Y}(tu, u(1-t)) \cdot |J|
= e^{-u} \cdot u,
\qquad t\in(0,1),\ u>0.
$$

On observe que la densité ne dépend **pas** de $t$.

---

 **d) Indépendance de $T$ et $U$**

On note :

$$
f_{T,U}(t,u) = 
\underbrace{1_{0<t<1}}_{f_T(t)}
\cdot
\underbrace{u e^{-u}}_{f_U(u)}.
$$

Ainsi :

$$
f_{T,U}(t,u) = f_T(t) f_U(u).
$$

Donc :

$$
\boxed{T = \frac{X}{X+Y} \text{ et } U = X+Y \text{ sont indépendants}.}
$$

**Lois marginales**

- T a pour densité :

$$
f_T(t) = 1,\qquad t\in(0,1),
$$

donc :

$$
T \sim \mathrm{Uniform}(0,1).
$$

- $U$ a pour densité :

$$
f_U(u) = u e^{-u},\qquad u>0,
$$
c'est-à-dire :

$$
U \sim \Gamma(2,1).
$$

## Exercice 8

Soient X et Y deux variables aléatoires indépendantes suivant une loi exponentielle 
de paramètre $\lambda > 0$. Nous considérons les variables aléatoires :

$$
U = X + Y, 
\qquad
V = \frac{X}{Y}.
$$

1. Donner la densité du vecteur $\mathbf{X} = (X, Y)^\top$.

2. Montrer que l’application

$$
h : ]0,+\infty[ \times ]0,+\infty[ \;\longrightarrow\; I_1 \times I_2,
\qquad
(x,y) \longmapsto \bigl( x + y,\; \frac{x}{y} \bigr)
$$
est bijective, en déterminant les intervalles $I_1$ et $I_2$.  
En déduire son application réciproque $h^{-1}$, ainsi que son Jacobien.

3. rocéder par changement de variables pour obtenir la densité du vecteur
$$
\mathbf{Y} = (U, V)^\top.
$$
Caractériser la loi de $U$, et en déduire que $U$ et $V$ sont indépendantes.


## Correction — Exercice 8

1) Densité du vecteur X,Y.

Comme X et Y sont indépendantes et exponentielles de paramètre $\lambda$, on a :

$$
f_{X,Y}(x,y)
= \lambda e^{-\lambda x} \, \lambda e^{-\lambda y}
= \lambda^2 e^{-\lambda(x+y)}, 
\qquad x>0,\ y>0.
$$

---

2) Bijectivité de l’application h

L’application considérée est :

$$
h(x,y) = (u,v) = (x+y,\ \frac{x}{y}),
\qquad x>0,\ y>0.
$$

**Détermination des intervalles $I_1, I_2$**

- $u = x+y > 0 \Rightarrow I_1 = ]0,+\infty[$
- $v = x/y > 0 \Rightarrow I_2 = ]0,+\infty[$

Ainsi :

$$
h : ]0,+\infty[\times ]0,+\infty[ \longrightarrow ]0,+\infty[\times ]0,+\infty[.
$$

 **Détermination de l’application réciproque**

À partir de :

$$
u = x+y, \qquad v = \frac{x}{y},
$$

on résout :

$$
x = \frac{uv}{1+v}, 
\qquad
y = \frac{u}{1+v}.
$$

Donc :

$$
h^{-1}(u,v)
= \left( \frac{uv}{1+v},\ \frac{u}{1+v} \right),
\qquad u>0,\ v>0.
$$

**Calcul du Jacobien**

On calcule la matrice jacobienne de $h^{-1}$ :

$$
J_{h^{-1}}(u,v) = 
\begin{pmatrix}
\frac{\partial x}{\partial u} & \frac{\partial x}{\partial v} \\
\frac{\partial y}{\partial u} & \frac{\partial y}{\partial v}
\end{pmatrix}
=
\begin{pmatrix}
\frac{v}{1+v} &
\frac{u}{(1+v)^2} \\[6pt]
\frac{1}{1+v} &
-\frac{u}{(1+v)^2}
\end{pmatrix}.
$$

Son déterminant vaut :

$$
\det J_{h^{-1}}(u,v)
= - \frac{u}{(1+v)^2}.
$$

On utilisera sa valeur absolue :

$$
\left| \det J_{h^{-1}}(u,v) \right| = \frac{u}{(1+v)^2}.
$$

---

3) Densité du vecteur (U,V)

Par changement de variables :

$$
f_{U,V}(u,v)
= f_{X,Y}\bigl(h^{-1}(u,v)\bigr)\,
\left|\det J_{h^{-1}}(u,v)\right|.
$$

On remplace :

- $x = \dfrac{uv}{1+v},$
- $y = \dfrac{u}{1+v}$,
- $x+y = u$,

donc

$$
f_{X,Y}(x,y) = \lambda^2 e^{-\lambda u}.
$$

Ainsi :

$$
f_{U,V}(u,v)
= \lambda^2 e^{-\lambda u} \cdot \frac{u}{(1+v)^2},
\qquad u>0,\ v>0.
$$  

---

**Marginales et indépendance**

**Loi marginale de U**

$$
f_U(u)
= \int_0^{\infty} \lambda^2 e^{-\lambda u} \frac{u}{(1+v)^2}\,dv.
$$

Or :

$$
\int_0^\infty \frac{1}{(1+v)^2}\,dv = 1.
$$

Donc :

$$
f_U(u) = \lambda^2 u e^{-\lambda u},\qquad u>0.
$$

C’est la densité d’une loi Gamma de paramètres \((2,\lambda)\) :

$$
U \sim \Gamma(2,\lambda).
$$

**Loi marginale de V**

$$
f_V(v)
= \int_0^\infty \lambda^2 e^{-\lambda u} \frac{u}{(1+v)^2}\,du.
$$

Mais :

$$
\int_0^\infty \lambda^2 u e^{-\lambda u}\,du = 1,
$$

donc :

$$
f_V(v) = \frac{1}{(1+v)^2},\qquad v>0.
$$

Il s’agit d’une **loi de Pareto (type II)**.

---

**Indépendance**

On a :

$$
f_{U,V}(u,v)
= 
\underbrace{\lambda^2 u e^{-\lambda u}}_{f_U(u)}
\cdot
\underbrace{\frac{1}{(1+v)^2}}_{f_V(v)}.
$$

Donc :

$$
f_{U,V}(u,v) = f_U(u)\, f_V(v).
$$

Ainsi :

$$
\boxed{U \text{ et } V \text{ sont indépendantes}.}
$$

--- 

## Exercice 9

Soient X et Y deux variables aléatoires indépendantes suivant une loi uniformesur]0,1[.

1. Montrer que la densité de $X + Y$ est

$$
f_{X+Y}(t) =
\begin{cases}
t, & t \in ]0,1[,\\[4pt]
2 - t, & t \in ]1,2[,\\[4pt]
0, & \text{sinon}.
\end{cases}
$$

2. Montrer que la densité de $X - Y$ est

$$
f_{X-Y}(t) =
\begin{cases}
1 + t, & t \in ]-1,0[,\\[4pt]
1 - t, & t \in ]0,1[,\\[4pt]
0, & \text{sinon}.
\end{cases}
$$

3. Montrer que la densité de $X/Y$ est

$$
f_{X/Y}(t) =
\begin{cases}
\frac{1}{2}, & t \in ]0,1[,\\[6pt]
\frac{1}{2 t^{2}}, & t \in ]1,+\infty[,\\[4pt]
0, & \text{sinon}.
\end{cases}
$$
