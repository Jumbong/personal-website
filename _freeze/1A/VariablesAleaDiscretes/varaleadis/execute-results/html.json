{
  "hash": "126c638b6f81150ac47bcb37852a2a83",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Exercices sur les variables aléatoires discrètes\"\n---\n\n\n\n\n\n\n# 4. Variables aléatoires discrètes\n\n## Exercice 1\n\nDans chacune des situations, identifier si $X$ suit une loi binomiale. Si oui, donner les paramètres $n$ et $p$ correspondants. Sinon, expliquer pourquoi et proposer une modélisation alternative :\n\n- Chaque jour, Dean va déjeuner et il y a 25% de chances qu’il prenne une pizza. Soit $X$ le nombre de fois où il a pris une pizza la semaine dernière.\n\n- Jessica joue au basketball, et elle a 60% de chances de réussir un lancer franc. Soit $X$ le nombre de lancers francs réussis pendant le dernier match.\n\n- Une boîte contient 300 bonbons, dont 100 carambars et 200 chocolats. Sam prend un échantillon de 10 bonbons dans la boîte. Soit $X$ le nombre de carambars dans son échantillon.\n\n- Marie lit un livre de 600 pages. Sur les pages paires, il y a 1% de chances d’avoir une faute d’orthographe. Sur les pages impaires, il y a 2% de chances d’avoir une faute d’orthographe. Soit $X$ le nombre total de fautes d’orthographe dans le livre.\n\n- John lit un livre de 600 pages. Sur chaque page, le nombre de fautes d’orthographe est distribué selon une loi de Bernouilli de paramètre $0.01$. Soit $X$ le nombre total de fautes d’orthographe dans le livre.\n\n\n## Correction Exercice 1\n\nDans chaque situation, on indique si $X$ suit une loi binomiale. Si oui, on précise les paramètres $n$ et $p$. Sinon, on propose une modélisation alternative.\n\n---\n\n1. **Dean et les pizzas**\n\n- Chaque jour, Dean prend une pizza avec probabilité $p = 0{,}25$, pendant une semaine ($n = 7$ jours). \n- Ensuite, il faut se poser la question : la décision de prendre une pizza un jour est-elle indépendante des autres jours ? Si oui, alors :\n- Le nombre de jours $X$ où il prend une pizza suit donc une loi binomiale :\n\n$$X \\sim \\mathcal{B}(n = 7,\\; p = 0{,}25).$$\n\n---\n\n2. **Jessica et les lancers francs**\n\n- Jessica réussit un lancer franc avec probabilité $p = 0{,}6$, mais le nombre de lancers tentés pendant le match **n’est pas fixé à l’avance**.\n- Il ne s’agit donc pas d’une loi binomiale (condition nécessaire : nombre d’essais fixé). Dans une modélisation binomiale, le nombre d’essais $n$ doit être fixé et connu à l’avance. \n\nUne modélisation possible est :\n\n- On note $N$ le nombre de lancers francs tentés pendant le dernier match. Il y a plusieurs possibilités : Soit $N$ est une constante connue, soit $N$ est une variable aléatoire (par exemple, on peut modéliser $N$ par une loi de Poisson.). Ensuite, on considère le nombre de lancers réussis $X$ conditionnellement à $N$.\n\n- Conditionnellement à $N$, le nombre de lancers réussis suit une loi binomiale :\n\n$$X \\mid N \\sim \\mathcal{B}(N,\\; 0{,}6).$$\n\nsi les lancers sont indépendants.\n\n---\n\n3. **Boîte de bonbons (carambars/chocolats)**\n\n- La boîte contient $300$ bonbons dont $100$ carambars et $200$ chocolats.\n- Sam tire un échantillon de $n = 10$ bonbons **sans remise**.\n- Les tirages ne sont pas indépendants (sans remise), donc $X$ **ne suit pas** une loi binomiale.\n\nLa loi adaptée est l’hypergéométrique :\n\n- Taille de la population : $N = 300$.\n- Nombre de “succès” (carambars) : $K = 100$.\n- Taille de l’échantillon : $n = 10$.\n\nOn a alors :\n\n$$X \\sim \\mathcal{H}(N = 300,\\; K = 100,\\; n = 10).$$\n\n---\n\n4. **Marie et les fautes d’orthographe (1 % / 2 %)**\n\n- Le livre a $600$ pages.\n- Sur les pages paires (300 pages), probabilité de faute $p_1 = 0{,}01$.\n- Sur les pages impaires (300 pages), probabilité de faute $p_2 = 0{,}02$.\n- La probabilité de “succès” (faute) **n’est pas la même** sur tous les essais (pages), donc $X$ **ne suit pas** une loi binomiale.\n\nUne modélisation naturelle est de décomposer :\n\n- On suppose l'indépendance entre les pages.\n- $X_{\\text{paires}}$ : nombre de fautes sur les pages paires,\n- $X_{\\text{impaires}}$ : nombre de fautes sur les pages impaires.\n\nOn a :\n\n$$\nX_{\\text{paires}} \\sim \\mathcal{B}(300,\\; 0{,}01), \\qquad\nX_{\\text{impaires}} \\sim \\mathcal{B}(300,\\; 0{,}02),\n$$\n\net\n\n$$\nX = X_{\\text{paires}} + X_{\\text{impaires}}.\n$$\n\n---\n\n5. **John et les fautes d’orthographe (Bernoulli 0.01)**\n\n- Le livre a $600$ pages.\n- Sur chaque page, le nombre de fautes suit une loi de Bernoulli de paramètre $p = 0{,}01$ (on suppose indépendance entre pages).\n- Cette fois, la probabilité de faute est la même pour toutes les pages, et le nombre total de pages $n = 600$ est fixé.\n\nAinsi, $X$ suit une loi binomiale :\n\n$$X \\sim \\mathcal{B}(n = 600,\\; p = 0{,}01).$$\n\n\n## Exercice 2\n\nSoit $X_1, \\dots, X_n$ une suite de variables aléatoires indépendantes telles que, pour tout $k = 1, \\dots, n$,\n\n$$\\mathbb{P}(X = \\pm 1) = \\frac{1}{2}.$$\n\nSoit\n$$S = \\sum_{i=1}^n X_i.$$\nOn parle de **marche aléatoire symétrique**, où le point de départ est $0$ avec un déplacement aléatoire à gauche ou à droite à chaque temps.\n\n1. Calculer $E(S)$ et $V(S)$.\n\nNous supposons maintenant que\n\n$$\\mathbb{P}(X = 1) = p \\quad \\text{et} \\quad \\mathbb{P}(X = -1) = 1 - p.$$\n\nSi $p > \\dfrac{1}{2}$, on parle de **marche aléatoire à dérive positive**.\n\n2. Montrer qu’on peut réécrire $X = aY + b$, avec $Y \\sim B(p)$, et $a$ et $b$ deux constantes à déterminer.  \n3. En déduire $E(S)$ et $V(S)$.\n\n\n## Correction — Exercice 2\n\nOn considère une suite de variables aléatoires indépendantes $X_1, \\dots, X_n$ telles que\n$$\\mathbb{P}(X = \\pm 1) = \\frac{1}{2}.$$\n\nOn pose :\n$$S = \\sum_{i=1}^n X_i.$$\n\n---\n\n#### 1) Calculer $E(S)$ et $V(S)$ dans le cas symétrique\n\nPour chaque $X_i$ :\n\n- $\\mathbb{E}(X_i) = 1 \\cdot \\frac{1}{2} + (-1) \\cdot \\frac{1}{2} = 0$,\n\n- $\\mathbb{E}(X_i^2) = 1^2 = 1$ d’où $\\operatorname{Var}(X_i) = \\mathbb{E}(X_i^2) - \\mathbb{E}(X_i)^2 = 1$.\n\n\nPar linéarité de l’espérance :\n$$\\mathbb{E}(S) = \\sum_{i=1}^n \\mathbb{E}(X_i) = 0,$$\n\nComme les $X_i$ sont indépendantes :\n$$\\operatorname{Var}(S) = \\sum_{i=1}^n \\operatorname{Var}(X_i) = n.$$\n\n---\n\nNous supposons maintenant que\n$$\\mathbb{P}(X = 1) = p \\quad \\text{et} \\quad \\mathbb{P}(X = -1) = 1 - p,$$\navec $p > \\frac{1}{2}$ (marche aléatoire à dérive positive).\n\n---\n\n#### 2) Montrer que l’on peut réécrire $X = aY + b$, avec $Y \\sim B(p)$\n\nIl faut jouer ici avec le support des variables aléatoires. L'idée est de partir d'une variable de Bernoulli $Y$ qui prend les valeurs $0$ et $1$, et de la transformer linéairement pour obtenir une variable $X$ qui prend les valeurs $-1$ et $1$. Comme $P(X = 1) = p$ et $P(X = -1) = 1 - p$, on peut définir $Y$ comme une variable de Bernoulli telle que $P(Y = 1) = p$ et $P(Y = 0) = 1 - p$.\n\nOn cherche $a$ et $b$ tels que :\n- si $Y = 1$, alors $X = 1$,\n- si $Y = 0$, alors $X = -1$.\n\nOn résout :\n$$\n\\begin{cases}\na \\cdot 1 + b = 1,\\\\\na \\cdot 0 + b = -1.\n\\end{cases}\n$$\n\nD'où $b = -1$ et $a = 2$.\n\nAinsi :\n$$X = 2Y - 1 \\quad \\text{avec} \\quad Y \\sim \\text{Bernoulli}(p).$$\n\n---\n\n####  3) En déduire $E(S)$ et $V(S)$ dans le cas général\n\nComme $S = \\sum_{i=1}^n X_i$ et $X_i = 2Y_i - 1$ avec $Y_i \\sim B(p)$ indépendantes :\n\nPar linéarité de l’espérance :\n\n$$\n\\mathbb{E}(X_i) = \\mathbb{E}{(2Y_i - 1)} =\n2\\,\\mathbb{E}(Y_i) - 1 = 2p - 1.\n$$\n\nCalculons la variance :\n\nOn sait que pour toute variable aléatoire $X = aY + b$, on a $\\operatorname{Var}(X) = a^2\\,\\operatorname{Var}(Y)$. Donc :\n$$\n\\operatorname{Var}(X_i) = \\operatorname{Var}(2Y_i - 1) = 2^2\\,\\operatorname{Var}(Y_i) =\n   4\\,\\operatorname{Var}(Y_i) = 4\\,p(1 - p).\n$$\nDonc :\n\n$$\n\\mathbb{E}(S) = \\sum_{i=1}^n \\mathbb{E}(X_i) = n(2p - 1),\n$$\n\n$$\n\\operatorname{Var}(S) = \\sum_{i=1}^n \\operatorname{Var}(X_i) = 4n\\,p(1 - p).\n$$\n\n---\n\n####  Résumé final\n\n| Type de marche | $\\mathbb{E}(S)$ | $\\operatorname{Var}(S)$ |\n|---------------|----------------|-------------------------|\n| Symétrique ($p = \\tfrac12$) | $0$ | $n$ |\n| Dérive positive ($p > \\tfrac12$) | $n(2p - 1)$ | $4n\\,p(1 - p)$ |\n\n## Exercice 3\n\nIdentifier les lois des variables aléatoires suivantes, en se basant sur leur fonction génératrice des moments :\n\n1. $M_X(t) = 0.8 e^t + 0.2$\n\n2. $M_Y(t) = \\dfrac{0.1 e^t}{1 - 0.9 e^t}$\n\n3. $M_Z(t) = (0.3 e^t + 0.7)^{14}$\n\n## Correction — Exercice 3\n### Rappel : fonctions génératrices des moments\n\n1. **Loi de Bernoulli de paramètre $p$**\n\nOn a\n$$\n\\mathbb{P}(X = 1) = p, \\qquad \\mathbb{P}(X = 0) = 1 - p.\n$$\n\nAlors la fonction génératrice des moments est :\n$$\nM_X(t) = \\mathbb{E}(e^{tX})\n= (1-p) e^{t \\cdot 0} + p e^{t \\cdot 1}\n= (1 - p) + p e^t.\n$$\n\n2. **Loi géométrique de paramètre $p$ sur $\\{1,2,\\dots\\}$**\n\nOn prend la convention\n$$\n\\mathbb{P}(Y = k) = (1 - p)^{k-1} p, \\quad k \\ge 1.\n$$\n\nAlors\n$$\nM_Y(t) = \\mathbb{E}(e^{tY})\n= \\sum_{k=1}^{\\infty} e^{tk} (1-p)^{k-1} p\n= p e^t \\sum_{k=0}^{\\infty} \\big((1-p)e^t\\big)^k.\n$$\n\nPour $| (1-p)e^t | < 1$, c’est une série géométrique :\n$$\n\\sum_{k=0}^{\\infty} r^k = \\frac{1}{1-r}.\n$$\n\nDonc\n$$\nM_Y(t) = \\frac{p e^t}{1 - (1-p) e^t}.\n$$\n\n3. **Loi binomiale $\\mathcal{B}(n,p)$**\n\nSoit $Z \\sim \\mathcal{B}(n,p)$. On peut écrire\n$$\nZ = X_1 + \\cdots + X_n,\n$$\noù les $X_i$ sont indépendantes et suivent toutes une Bernoulli$(p)$.\n\nOn sait que, pour une Bernoulli$(p)$,\n$$\nM_{X_i}(t) = (1-p) + p e^t.\n$$\n\nPar indépendance,\n$$\nM_Z(t) = \\mathbb{E}(e^{tZ})\n= \\mathbb{E}\\big(e^{t(X_1 + \\cdots + X_n)}\\big)\n= \\prod_{i=1}^n \\mathbb{E}(e^{tX_i})\n= \\big((1-p) + p e^t\\big)^n.\n$$\n\n---\n\n### Identification des lois\n\n1. On a\n$$\nM_X(t) = 0.8 e^t + 0.2 = 0.2 + 0.8 e^t.\n$$\n\nEn comparant avec $(1-p) + p e^t$, on obtient $p = 0.8$.\n\n> Donc $X$ suit une loi de Bernoulli de paramètre $0.8$ :\n> $$\n> X \\sim \\text{Bernoulli}(0.8).\n> $$\n\n2. On a\n$$\nM_Y(t) = \\dfrac{0.1 e^t}{1 - 0.9 e^t}.\n$$\n\nEn comparant avec\n$$\nM_Y(t) = \\frac{p e^t}{1 - (1-p) e^t},\n$$\non lit $p = 0.1$ et $1-p = 0.9$.\n\n> Donc $Y$ suit une loi géométrique de paramètre $p = 0.1$ (sur $\\{1,2,\\dots\\}$) :\n> $$\n> Y \\sim \\text{Géométrique}(p = 0.1).\n> $$\n\n3. On a\n$$\nM_Z(t) = (0.3 e^t + 0.7)^{14}\n= \\big( (1 - 0.3) + 0.3 e^t \\big)^{14}.\n$$\n\nEn comparant avec\n$$\nM_Z(t) = \\big( (1-p) + p e^t \\big)^n,\n$$\non obtient $p = 0.3$ et $n = 14$.\n\n> Donc $Z$ suit une loi binomiale $\\mathcal{B}(14, 0.3)$ :\n> $$\n> Z \\sim \\mathcal{B}(n = 14, p = 0.3).\n> $$\n\n## Exercice 4\n\nSoit $X$ une variable aléatoire telle que\n$$\\mathbb{P}(X = k) = \\frac{k}{10} \\quad \\text{pour } k = 1, 2, 3, 4.$$\n\nSoit $Y$ une variable aléatoire indépendante de $X$, et suivant la même distribution.\n\nCalculer la loi de probabilité de $X + Y$.\n\n## Correction — Exercice 4\n\n\nOn a\n$$\n\\mathbb{P}(X = k) = \\frac{k}{10}, \\quad k = 1,2,3,4,\n$$\net $Y$ est indépendante de $X$ et de même loi.\n\n\n|       | Y = 1 | Y = 2 | Y = 3 | Y = 4 |\n| ----- | ----- | ----- | ----- | ----- |\n| X = 1 | 2     | 3     | 4     | 5     |\n| X = 2 | 3     | 4     | 5     | 6     |\n| X = 3 | 4     | 5     | 6     | 7     |\n| X = 4 | 5     | 6     | 7     | 8     |\n\n\nLa variable $S = X + Y$ prend des valeurs entières de $2$ à $8$.\n\n\nLa variable aléatoire $X + Y$ prend ses valeurs dans $\\{2, \\dots, 8\\}$.  \n\nPour $k \\in \\{2, \\dots, 8\\}$, nous avons en utilisant la formule des probabilités totales :\n\n$$\n\\mathbb{P}(X + Y = k)\n= \\sum_{l=1}^{4} \\mathbb{P}(X + Y = k \\mid Y = l)\\,\\mathbb{P}(Y = l)\n$$\n\n$$\n= \\sum_{l=1}^{4} \\mathbb{P}(Y = k - l \\mid Y = l)\\,\\mathbb{P}(Y = l)\n$$\n\n$$\n= \\sum_{l=1}^{4} \\mathbb{P}(X = k - l)\\,\\mathbb{P}(Y = l) \\quad \\text{car $X$ et $Y$ sont indépendantes.}\n$$\n\noù la somme porte sur les $k$ tels que $1 \\le k \\le 4$ et $1 \\le k- l \\le 4$.\n\nOn note\n$$\np_1 = 0.1,\\quad p_2 = 0.2,\\quad p_3 = 0.3,\\quad p_4 = 0.4.\n$$\n\nOn calcule alors, cas par cas :\n\n- Pour $k = 2$  \n  $$(X,Y) = (1,1) \\quad\\Rightarrow\\quad \\mathbb{P}(S=2) = p_1 p_1 = 0.1\\times 0.1 = 0.01.$$\n\n- Pour $k = 3$  \n  $$(X,Y) = (1,2),(2,1)$$\n  $$\n  \\mathbb{P}(S=3) = p_1 p_2 + p_2 p_1 = 0.1\\times 0.2 + 0.2\\times 0.1 = 0.04.\n  $$\n\n- Pour $s = 4$  \n  $$(X,Y) = (1,3),(2,2),(3,1)$$\n  $$\n  \\mathbb{P}(S=4) = p_1 p_3 + p_2 p_2 + p_3 p_1\n  = 0.1\\times 0.3 + 0.2\\times 0.2 + 0.3\\times 0.1\n  = 0.10.\n  $$\n\n- Pour $k = 5$  \n  $$(X,Y) = (1,4),(2,3),(3,2),(4,1)$$\n  $$\n  \\mathbb{P}(S=5) = p_1 p_4 + p_2 p_3 + p_3 p_2 + p_4 p_1\n  = 0.04 + 0.06 + 0.06 + 0.04 = 0.20.\n  $$\n\n- Pour $k = 6$  \n  $$(X,Y) = (2,4),(3,3),(4,2)$$\n  $$\n  \\mathbb{P}(S=6) = p_2 p_4 + p_3 p_3 + p_4 p_2\n  = 0.2\\times 0.4 + 0.3\\times 0.3 + 0.4\\times 0.2\n  = 0.08 + 0.09 + 0.08 = 0.25.\n  $$\n\n- Pour $k = 7$  \n  $$(X,Y) = (3,4),(4,3)$$\n  $$\n  \\mathbb{P}(S=7) = p_3 p_4 + p_4 p_3\n  = 0.3\\times 0.4 + 0.4\\times 0.3\n  = 0.24.\n  $$\n\n- Pour $k = 8$  \n  $$(X,Y) = (4,4) \\quad\\Rightarrow\\quad \\mathbb{P}(S=8) = p_4 p_4 = 0.4\\times 0.4 = 0.16.$$\n\nOn obtient donc la loi de $S = X+Y$ :\n\n$$\n\\begin{array}{c|ccccccc}\ns      & 2    & 3    & 4    & 5    & 6    & 7    & 8    \\\\ \\hline\n\\mathbb{P}(S=s) & 0.01 & 0.04 & 0.10 & 0.20 & 0.25 & 0.24 & 0.16\n\\end{array}\n$$\n\n(On vérifie que la somme vaut bien $1$: $0.01 + 0.04 + 0.10 + 0.20 + 0.25 + 0.24 + 0.16 = 1$.)\n\n## Exercice 5\n\nUn statisticien a modélisé le nombre de mots d’une recherche sur internet en utilisant une loi de Poisson. Supposons que la longueur moyenne est de 3 mots, et soit $X$ le nombre de mots d’une recherche. Comme une recherche ne peut pas être vide, nous utilisons une modélisation par une loi de probabilité restreinte définie par\n\n$$\\mathbb{P}(X = k) = \\mathbb{P}(Y = k \\mid Y \\neq 0) \\quad \\text{où } Y \\sim \\text{Pois}(\\lambda).$$\n\n1) Trouver la loi de $X$.  \n2) Donner la valeur de $\\lambda$ correspondant à une longueur moyenne de 3 mots.  \n3) Quelle est la probabilité d’avoir une recherche de 6 mots ou plus ?\n\n## Correction — Exercice 5\n\n1) Trouver la loi de $X$\n\nPour déterminer la loi d'une variable aléatoire $X$ , il faut d'abord définir son support, c'est-à-dire l'ensemble des valeurs que $X$ peut prendre avec une probabilité non nulle. Dans ce cas, le support de $X$ est l'ensemble des entiers naturels positifs, car une recherche ne peut pas être vide.\n\nNous notons le support de $X$ par $\\Omega_X = \\{1, 2, 3, \\ldots\\}$ = $\\mathbb{N}^*$.\n\nUne fois le support identifié, nous pouvons calculer la probabilité associée à chaque valeur du support de $X$. \n\nRappelons nous que l'ensemble ${X \\in A} = {w \\in \\Omega : X(w) \\in A} = X^-1(A)$.\nPour tout $k \\in \\Omega_X$, nous avons :\n\n${X = k} = {w \\in \\Omega : X(w) = k} = X^{-1}(\\{k\\})$. où $\\Omega$ est l'ensemble des issues possibles de l'expérience aléatoire.\n\nDonc, pour tout $k \\in \\Omega_X$, nous avons :\n\nOn part de la définition de $X$ comme loi de Poisson **conditionnée à être non nulle** :\n\n1. **Définition de $X$ comme loi restreinte :**\n   $$\n   \\mathbb{P}(X = k)\n   = \\mathbb{P}(Y = k \\mid Y \\neq 0).\n   $$\n   C’est donné dans l’énoncé : $X$ est la loi de $Y$ sachant que $Y$ ne vaut pas 0.\n\n2. **Formule de probabilité conditionnelle :**\n   $$\n   \\mathbb{P}(Y = k \\mid Y \\neq 0)\n   = \\frac{\\mathbb{P}(Y = k \\cap Y \\neq 0)}{\\mathbb{P}(Y \\neq 0)}.\n   $$\n   Par définition :  \n   $\\displaystyle \\mathbb{P}(A \\mid B) = \\frac{\\mathbb{P}(A \\cap B)}{\\mathbb{P}(B)}$.\n\n3. **Simplification de l'intersection :**\n   $$\n   \\mathbb{P}(Y = k \\cap Y \\neq 0) = \\mathbb{P}(Y = k)\n   $$\n   car si $k \\ge 1$, alors l'événement « $Y = k$ » implique automatiquement « $Y \\neq 0$ ».  \n   Donc l’intersection ne change rien.\n\n4. **Remplacement du dénominateur :**\n   $$\n   \\mathbb{P}(Y \\neq 0) = 1 - \\mathbb{P}(Y = 0).\n   $$\n   C’est la propriété générale :  \n   $\\displaystyle \\mathbb{P}(B^c) = 1 - \\mathbb{P}(B)$.\n\n5. **Utilisation de la formule de la loi de Poisson :**\n   $$\n   \\mathbb{P}(Y = k) = e^{-\\lambda} \\frac{\\lambda^k}{k!},\n   \\qquad\n   \\mathbb{P}(Y = 0) = e^{-\\lambda}.\n   $$\n\n6. **Substitution dans la formule :**\n   $$\n   \\mathbb{P}(X = k)\n   = \\frac{e^{-\\lambda} \\frac{\\lambda^k}{k!}}{1 - e^{-\\lambda}}.\n   $$\n\n\n\n\n\n###  Résultat final :\n$$\n\\boxed{\\mathbb{P}(X = k) = \\frac{e^{-\\lambda} }{1 - e^{-\\lambda}} \\cdot \\frac{\\lambda^k}{k!}, \\quad k = 1, 2, 3, \\ldots}\n$$\n\n\n### 2) Calcul de l’espérance de $X$\n\nNous avons la loi :\n$$\n\\mathbb{P}(X = k)\n= \\frac{e^{-\\lambda} \\lambda^k}{k!\\,(1 - e^{-\\lambda})}, \\qquad k \\ge 1.\n$$\n\nL’espérance vaut en utilisant le théorème de transfert :\n$$\n\\mathbb{E}(X)\n= \\sum_{k=1}^{+\\infty} k \\, \\mathbb{P}(X = k)\n= \\sum_{k=1}^{\\infty} k \\, \\frac{e^{-\\lambda} \\lambda^k}{k!\\,(1 - e^{-\\lambda})}.\n$$\n\n**On factorise les constantes** :\n$$\n\\mathbb{E}(X)\n= \\frac{e^{-\\lambda}}{1 - e^{-\\lambda}}\n   \\sum_{k=1}^{\\infty} k \\frac{\\lambda^{k}}{k!}.\n$$\n\n---\n\nJustification du passage suivant\n\nOn réécrit :\n$$\nk\\frac{\\lambda^{k}}{k!}\n= \\lambda \\frac{\\lambda^{k-1}}{(k-1)!}.\n$$\n\nEn effet :\n$$\n\\frac{k}{k!} = \\frac{1}{(k-1)!}.\n$$\n\nDonc :\n$$\n\\sum_{k=1}^{\\infty} k \\frac{\\lambda^k}{k!}\n= \\lambda \\sum_{k=1}^{\\infty} \\frac{\\lambda^{k-1}}{(k-1)!}.\n$$\n\nOn effectue le **changement d’indice**  $j = k - 1$ :\n\n- quand $k = 1$, $j = 0$,\n- quand $k \\to \\infty$, $j \\to \\infty$.\n\nD’où :\n$$\n\\sum_{k=1}^{\\infty} \\frac{\\lambda^{k-1}}{(k-1)!}\n= \\sum_{j=0}^{\\infty} \\frac{\\lambda^{j}}{j!}.\n$$\n\nOr cette somme est la **série de Taylor de l’exponentielle** :\n$$\n\\sum_{j=0}^{\\infty} \\frac{\\lambda^{j}}{j!} = e^{\\lambda}.\n$$\n\n\n\nOn revient à l’expression de l’espérance\n\nOn obtient donc :\n$$\n\\mathbb{E}(X)\n= \\frac{e^{-\\lambda}}{1 - e^{-\\lambda}} \\cdot \\lambda e^{\\lambda}.\n$$\nPuis en simplifiant \\( e^{-\\lambda} e^{\\lambda} = 1 \\) :\n$$    \n\\boxed{\n\\mathbb{E}(X) = \\frac{\\lambda}{1 - e^{-\\lambda}}.\n}\n$$    \n\n---\n\n### Détermination de $\\lambda$ pour une espérance de 3\n\nEn résolvant :\n$$\n\\frac{\\lambda}{1 - e^{-\\lambda}} = 3,\n$$\n\n::: {#b08cfb59 .cell execution_count=1}\n``` {.python .cell-code}\n#Utilisons Newton-Raphson pour résoudre l'équation\n\nfrom scipy.optimize import newton\nimport numpy as np\n\n# Définir la fonction\ndef equation(lmbda):\n    return lmbda / (1 - np.exp(-lmbda)) - 3\n\n# Résoudre avec Newton-Raphson\nlambda_solution = newton(equation, x0=1.0)\nprint(lambda_solution)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n2.8214393721220787\n```\n:::\n:::\n\n\non obtient numériquement :\n$$\n\\boxed{\\lambda \\approx 2.82}.\n$$\n\n3) Probabilité d’avoir une recherche de 6 mots ou plus\n\nOn cherche :\n$$\n\\mathbb{P}(X \\ge 6).\n$$\n\nOr :\n$$\n\\mathbb{P}(X \\ge 6) = 1 - \\mathbb{P}(X \\le 5).\n$$\n\nEt comme \\(X\\) est la loi de Poisson tronquée :\n$$\n\\mathbb{P}(X \\le 5)\n= \\sum_{k=1}^{5} \\mathbb{P}(X = k)\n= \\sum_{k=1}^{5}\n\\frac{e^{-\\lambda} \\lambda^k}{k!\\,(1 - e^{-\\lambda})}.\n$$\n\nOn factorise pour simplifier :\n$$  \n\\mathbb{P}(X \\le 5)\n= \\frac{e^{-\\lambda}}{1 - e^{-\\lambda}}\n\\sum_{k=1}^{5} \\frac{\\lambda^k}{k!}.\n$$\n\nDonc :\n\n$$\n\\boxed{\n\\mathbb{P}(X \\ge 6)\n= 1 -\n\\frac{e^{-\\lambda}}{1 - e^{-\\lambda}}\n\\sum_{k=1}^{5} \\frac{\\lambda^k}{k!}\n}.\n$$\n\n---\n\n Avec $\\lambda \\approx 2.82$, on calcule :\n\nLe calcul numérique donne :\n$$\n\\boxed{\\mathbb{P}(X \\ge 6) \\approx 0.07}\n$$\n\nsoit **environ 7%**.\n\n## Exercice 6\n\nLa loi de probabilité jointe de deux variables $X$ et $Y$ est définie par\n\n$$\n\\mathbb{P}(X = x, Y = y)\n= \\frac{1}{e^{2} \\, y! \\, (x - y)!}\n\\qquad \\text{pour } x \\in \\mathbb{N} \\text{ et } y = 0, \\ldots, x.\n$$\n\n1) Trouver la loi de probabilité de $X$.  \n2) En déduire la loi de probabilité de $Y$ sachant que $X = x$.\n\n## Correction — Exercice 6\n\nAvant de résoudre l’exercice, rappelons les formules de pascal :\n\n$$\n\\binom{x}{y} = \\frac{x!}{y! (x-y)!},\n$$\net\n$$\n(a + b)^x = \\sum_{y=0}^{x} \\binom{x}{y} a^y b^{x-y}.\n$$\n\nPour a = 1 et b = 1, on obtient :\n$$\n2^x = \\sum_{y=0}^{x} \\binom{x}{y}.\n$$\n\n1) Trouver la loi de probabilité de $X$\n\nLe support de X est $\\mathbb{N}$.\n\nPour tout $x \\in \\mathbb{N}$, on utilise la formule des probabilités totales :\n$$\n\\mathbb{P}(X = x)\n= \\mathbb{P}(X = x, \\Omega)\n= \\mathbb{P}(X = x, \\bigcup_{y=0}^{x} \\{Y = y\\})\n= \\sum_{y=0}^{x} \\mathbb{P}(X = x, Y = y).\n$$\n\n- $\\Omega$ est l’événement certain.\n- Les événements {Y=0}, \\ldots, {Y=x} sont disjoints et forment une partition des valeurs possibles de Y quand X=x.\n- La formule générale de probabilité totale dit :\n  $$\n  \\mathbb{P}(A) = \\sum_i \\mathbb{P}(A \\cap B_i)\n  \\quad \\text{si les } B_i \\text{ forment une partition}.\n  $$\n\n\nEn théorie de probabilité, on écrit souvent $P(A, B)$ pour $P(A \\cap B)$.\n\nOn remplace par la loi jointe donnée :\n$$\n\\mathbb{P}(X = x)\n= \\sum_{y=0}^{x} \\frac{1}{e^{2} \\, y! \\, (x - y)!}.\n$$\n\nOn factorise :\n$$\n\\mathbb{P}(X = x)\n= \\frac{1}{e^{2}} \\sum_{y=0}^{x} \\frac{1}{y! \\, (x - y)!}.\n$$\n\nOn utilise la formule de pascal pour réécrire la somme :\nOn utilise l’identité :\n$$\n\\binom{x}{y} = \\frac{x!}{y! (x-y)!}.\n$$\n\nD’où :\n$$\n\\frac{1}{y!(x-y)!} = \\frac{1}{x!}\\binom{x}{y}.\n$$\n\nAinsi :\n$$\n\\sum_{y=0}^{x} \\frac{1}{y!(x-y)!}\n= \\frac{1}{x!} \\sum_{y=0}^{x} \\binom{x}{y}.\n$$\n\nOr :\n$$\n\\sum_{y=0}^{x} \\binom{x}{y} = 2^x,\n$$\ncar c’est le développement de \\((1+1)^x\\).\n\nOn obtient donc :\nOn obtient :\n$$\n\\mathbb{P}(X = x)\n= \\frac{1}{e^{2}} \\cdot \\frac{2^x}{x!}\n= e^{-2} \\frac{2^x}{x!}.\n$$\n\n\nNous reconnaissons la loi de Poisson de paramètre $\\lambda = 2$.\n\n\n$$\n\\boxed{X \\sim \\text{Poisson}(2)}.\n$$\n\n2) Soient $x, y \\in \\mathbb{N}$. Nous avons\n\n$$\n\\mathbb{P}(Y = y \\mid X = x)\n= \\frac{\\mathbb{P}(X = x, Y = y)}{\\mathbb{P}(X = x)},\n$$\n\net cette probabilité est nulle si $y > x$.  \nSi $y \\le x$, nous avons :\n\n$$\n\\mathbb{P}(Y = y \\mid X = x)\n= \\frac{1}{e^{2} y! (x - y)!} \\times \\frac{e^{2} x!}{2^{x}}\n$$\n\n$$\n= \\binom{x}{y} \\left(\\frac{1}{2}\\right)^{y} \\left(\\frac{1}{2}\\right)^{x - y}.\n$$\n\nNous reconnaissons la densité d’une loi binomiale de paramètres $m = x$ et  \n$p = \\frac{1}{2}$.\n\nDonc :\n$$\n\\boxed{Y \\mid X = x \\sim \\mathcal{B}\\left(x, \\frac{1}{2}\\right)}.\n$$\n\n## Exercice 7\n\nSoient $X_1, \\ldots, X_n$ une suite de variables aléatoires i.i.d. suivant une loi $\\mathcal{L}$.\nNous supposons que les paramètres de la loi $\\mathcal{L}$ sont entièrement caractérisés\npar les moments $\\mu_1 \\equiv \\mathbb{E}(X), \\ldots, \\mu_k \\equiv \\mathbb{E}(X^k)$.\nLa *méthode des moments* consiste à estimer les paramètres de la loi $\\mathcal{L}$\nen remplaçant les moments inconnus par leur estimateur empirique\n\n$$\n\\hat{\\mu}_k = \\frac{1}{n} \\sum_{i=1}^n (X_i)^k.\n$$\n\nNous supposons que les variables aléatoires $X_1, \\ldots, X_n$ suivent une loi de Bernoulli\nde paramètre $p$.\n\n1) Exprimer $p$ en fonction du premier moment de $X$.  \n2) En déduire une estimation de $p$ basée sur l’échantillon.\n\nNous supposons que les variables aléatoires $X_1, \\ldots, X_n$ suivent une loi binomiale\nde paramètres $m$ et $p$.\n\n3) Exprimer $m$ et $p$ en fonction des deux premiers moments de $X$.  \n4) En déduire une estimation de $m$ et de $p$ basée sur l’échantillon.\n\n### Correction Exercice 7\n\n1) Nous avons  \n$$\np = \\mathbb{E}(X) = \\mu_1.\n$$\n\n2) Le paramètre \\(p\\) est donc estimé par\n$$\n\\hat{p} = \\hat{\\mu}_1 = \\frac{1}{n} \\sum_{i=1}^n X_i.\n$$\n\nNotons que dans ce cas précis, il s’agit d’un estimateur sans biais.\n\n---\n\n3) Nous avons, pour une loi binomiale \\(\\mathcal{B}(m,p)\\) :\n$$\n\\mathbb{E}(X) = mp\n\\qquad\\text{et}\\qquad\n\\operatorname{Var}(X) = \\mathbb{E}(X^2) - \\{\\mathbb{E}(X)\\}^2 = mp(1 - p).\n$$\n\nNous en déduisons que\n$$\n1 - p = \\frac{\\mu_2}{\\mu_1} - \\mu_1,\n$$\n\npuis\n$$\np = 1 - \\frac{\\mu_2}{\\mu_1} + \\mu_1,\n$$\n\net\n$$\nm = \\frac{\\mu_1}{1 - \\frac{\\mu_2}{\\mu_1} + \\mu_1}.\n$$\n\n---\n\n4) Nous obtenons les estimateurs par la méthode des moments :\n\n$$\n\\hat{p} = 1 - \\frac{\\hat{\\mu}_2}{\\hat{\\mu}_1} + \\hat{\\mu}_1,\n$$\n\n$$\n\\hat{m} = \n\\frac{\\hat{\\mu}_1}{1 - \\frac{\\hat{\\mu}_2}{\\hat{\\mu}_1} + \\hat{\\mu}_1}.\n$$\n\nÀ noter que ces estimateurs n’ont pas de raison particulière d’être sans biais.\n\n## Exercice 8\n\nSoit $X \\sim \\text{Pois}(\\lambda)$.\n\n1) Donner la fonction génératrice des moments de $X$ et sa fonction caractéristique.  \n\n2) Calculer $\\mathbb{E}(X^3)$.  \n3) Calculer la probabilité que $X$ soit impair.  \n   Utiliser un développement en série entière de $e^\\lambda$ et $e^{-\\lambda}$.  \n\n4) Calculer $\\mathbb{E}(X!)$.\n\n## Correction — Exercice 8\n\nAvant de commencer, rappelons que :\n\n{X impair} qui s'écrit en latex $X \\text{ impair}$ est l'ensemble des issues où la variable aléatoire $X$ prend des valeurs impaires. Cet ensemble peut être représenté comme l'union des événements disjoints {X = 1}, {X = 3}, {X = 5}, etc.\n\nOn peut donc écrire :\n\nL’événement $X$ est impair s’écrit :\n\n$$\n\\{X \\text{ impair}\\}\n= \\{\\omega \\in \\Omega : X(\\omega) \\text{ est impair}\\}\n$$\n\nPar définition d’un entier impair :\n\n$$  \n\\{X \\text{ impair}\\}\n= \\{\\omega : X(\\omega) = 1\\}\n  \\cup \\{\\omega : X(\\omega) = 3\\}\n  \\cup \\{\\omega : X(\\omega) = 5\\}\n  \\cup \\cdots\n$$\n\nC’est-à-dire :\n\n$$\n\\{X \\text{ impair}\\}\n= \\{\\omega \\in \\Omega : \\exists\\, k \\in \\mathbb{N}, \\; X(\\omega) = 2k + 1\\}.\n$$\n\nEn notation abrégée :\n\n$$\n\\{X \\text{ impair}\\}\n= \\{X = 1\\} \\cup \\{X = 3\\} \\cup \\{X = 5\\} \\cup \\cdots\n$$\n\net de manière compacte :\n\n$$\n\\boxed{\n\\{X \\text{ impair}\\}\n= \\bigcup_{k=0}^{\\infty} \\{X = 2k + 1\\}.\n}\n$$\n\n\n1) Fonction génératrice des moments et fonction caractéristique\n\n$\\textbf{1)}$ Soit $t > 0$. Nous avons\n\n$$\nM_X(t)\n= \\mathbb{E}\\{\\exp(tX)\\}\n= e^{-\\lambda} \\sum_{k=0}^{+\\infty} \\exp(tk)\\, \\frac{\\lambda^k}{k!}\n$$\n\n$$ \n= e^{-\\lambda} \\sum_{k=0}^{+\\infty} \\frac{\\{\\lambda \\exp(t)\\}^k}{k!}\n= e^{-\\lambda} \\times \\exp\\{\\lambda \\exp(t)\\}\n$$\n\n$$\n= \\exp\\{\\lambda(\\exp(t) - 1)\\}.\n$$\n\nOn vérifie au passage que la fonction génératrice des moments est ici bien\ndéfinie pour tout $t \\in \\mathbb{R}$.\n\nLa fonction caractéristique se calcule de façon analogue :\n\n$$\n\\varphi_X(t)\n= \\mathbb{E}\\{\\exp(itX)\\}\n= e^{-\\lambda} \\sum_{k=0}^{+\\infty} \\exp(itk)\\, \\frac{\\lambda^k}{k!}\n$$\n\n$$\n= e^{-\\lambda} \\sum_{k=0}^{+\\infty} \\frac{\\{\\lambda \\exp(it)\\}^k}{k!}\n= e^{-\\lambda} \\times \\exp\\{\\lambda \\exp(it)\\}\n$$\n\n$$\n= \\exp\\{\\lambda(\\exp(it) - 1)\\}.\n$$\n\n2) Calcul de $\\mathbb{E}(X^3)$\n\nNous utilisons la fonction génératrice des moments, qui est ici indéfiniment dérivable.\n\nRappel :\n$$\nM_X(t) = \\exp\\{\\lambda(e^t - 1)\\}.\n$$\n\nNous calculons ses dérivées successives.\n\n#### Première dérivée\n\n$$\nM_X'(t)\n= \\lambda e^t \\exp\\{\\lambda(e^t - 1)\\}.\n$$\n#### Deuxième dérivée\n\n$$\nM_X''(t)\n= \\left(\\lambda e^t + \\lambda^2 e^{2t}\\right)\n  \\exp\\{\\lambda(e^t - 1)\\}.\n$$\n#### Troisième dérivée\n\n$$\nM_X^{(3)}(t)\n= \\left(\\lambda e^t + 3\\lambda^2 e^{2t} + \\lambda^3 e^{3t}\\right)\n  \\exp\\{\\lambda(e^t - 1)\\}.\n$$\nNous pouvons maintenant obtenir le troisième moment.\n\n---\n\n### Calcul de  $\\mathbb{E}(X^3)$\n\nPar définition :\n$$\n\\mathbb{E}(X^3) = M_X^{(3)}(0).\n$$\n\nComme $e^{0}=1$, nous obtenons :\n$$\nM_X^{(3)}(0)\n= \\left(\\lambda + 3\\lambda^2 + \\lambda^3\\right)\\exp\\{0\\}\n= \\lambda^3 + 3\\lambda^2 + \\lambda.\n$$\n\n$$\n\\boxed{\\mathbb{E}(X^3)=\\lambda^3 + 3\\lambda^2 + \\lambda.}\n$$\n\n3) Calcul de la probabilité que $X$ soit impair\n\nNous avons\n\n$$\n\\mathbb{P}(X \\text{ impair})\n= \\mathbb{P}\\left(\\bigcup_{k=0}^{\\infty} \\{X = 2k + 1\\}\\right)\n= \\sum_{k=0}^{\\infty} \\mathbb{P}(X = 2k + 1)\n$$\n\n$$\n= \\sum_{k=0}^{\\infty} e^{-\\lambda} \\frac{\\lambda^{2k + 1}}{(2k + 1)!}\n= e^{-\\lambda} \\sum_{k=0}^{\\infty} \\frac{\\lambda^{2k + 1}}{(2k + 1)!}.\n$$ \n\n\nD’autre part,\n$$\ne^\\lambda = \\sum_{k=0}^{+\\infty} \\frac{\\lambda^k}{k!}\n= 1 + \\lambda + \\frac{\\lambda^2}{2} + \\frac{\\lambda^3}{3!} + \\frac{\\lambda^4}{4!} + \\cdots\n$$  \n\net\n$$\ne^{-\\lambda} = \\sum_{k=0}^{+\\infty} \\frac{(-\\lambda)^k}{k!}\n= 1 - \\lambda + \\frac{\\lambda^2}{2} - \\frac{\\lambda^3}{3!} + \\frac{\\lambda^4}{4!} + \\cdots\n$$\n\nDonc,\n$$\ne^\\lambda - e^{-\\lambda}\n= \\sum_{k=0}^{+\\infty} \\frac{\\lambda^k}{k!}\n+ \\sum_{k=0}^{+\\infty} \\frac{(-1)^{k+1}\\lambda^k}{k!}\n= 2 \\sum_{k=0}^{+\\infty} \\frac{\\lambda^{2k+1}}{(2k+1)!}.\n$$\n\nEt finalement,\n$$\n\\mathbb{P}(X \\text{ impair})\n= \\frac{e^{-\\lambda}(e^\\lambda - e^{-\\lambda})}{2}\n= \\frac{1 - e^{-2\\lambda}}{2}.\n$$\n\n4) Nous avons\n\n$$\nE(X!) = \\sum_{k=0}^{+\\infty} e^{-\\lambda} \\frac{\\lambda^k}{k!} \\times k!\n$$\n\n$$\n= e^{-\\lambda} \\sum_{k=0}^{+\\infty} \\lambda^k\n= \n\\begin{cases}\n\\dfrac{e^{-\\lambda}}{1 - \\lambda}, & \\text{si } |\\lambda| < 1, \\\\[6pt]\n+\\infty, & \\text{si } |\\lambda| \\ge 1.\n\\end{cases}\n$$\n\n## Exercice 9\n\nSoient \\(X_1, \\ldots, X_n\\) une suite de variables aléatoires i.i.d. suivant une loi de Bernoulli de paramètre \\(p\\).  \nTrouver la loi de \\(X_1\\) sachant que \\(X_1 + \\cdots + X_n = k\\).\n\n---\n\n## Correction Exercice 9\n\nSoit \\(x \\in \\{0,1\\}\\). Nous avons, en utilisant la formule de Bayes :\n\n$$\n\\mathbb{P}(X_1 = x \\mid X_1 + \\cdots + X_n = k)\n= \\frac{\\mathbb{P}(X_1 + \\cdots + X_n = k \\mid X_1 = x)\\mathbb{P}(X_1 = x)}\n{\\mathbb{P}(X_1 + \\cdots + X_n = k)}.\n$$\n\nOr :\n\n$$\n\\mathbb{P}(X_1 + \\cdots + X_n = k) = \\binom{n}{k} p^k (1-p)^{\\,n-k},\n$$\n\npuisque \\(X_1 + \\cdots + X_n \\sim \\mathcal{B}(n,p)\\).\n\n---\n\nEnsuite :\n\n$$\nX_2 + \\cdots + X_n \\sim \\mathcal{B}(n-1,p),\n$$\n\ndonc\n\n$$\n\\mathbb{P}(X_2 + \\cdots + X_n = k - x)\n= \\binom{n-1}{k-x} p^{\\,k-x} (1-p)^{\\,n-1-k+x}.\n$$\n\nDe plus :\n\n$$\n\\mathbb{P}(X_1 = x) = p^x (1-p)^{\\,1-x}.\n$$\n\n---\n\nNous obtenons finalement :\n\n$$\n\\mathbb{P}(X_1 = x \\mid X_1 + \\cdots + X_n = k)\n= \\frac{\\binom{n-1}{k-x} p^{\\,k-x}(1-p)^{\\,n-1-k+x} \\, p^x (1-p)^{\\,1-x}}\n{\\binom{n}{k} p^k (1-p)^{\\,n-k}}.\n$$\n\nEn simplifiant, on trouve :\n\n$$\n\\mathbb{P}(X_1 = x \\mid X_1 + \\cdots + X_n = k)\n= \\frac{\\binom{n-1}{k-x}}{\\binom{n}{k}}.\n$$\n\nOn calcule les deux cas :\n- Si $x = 0$ :\n  $$\n  \\mathbb{P}(X_1 = 0 \\mid X_1 + \\cdots + X_n = k)\n  = \\frac{\\binom{n-1}{k}}{\\binom{n}{k}}\n  = \\frac{(n-1)! \\, k! \\, (n-k)!}{(n-k)! \\, (n)! \\, k!}\n  = \\frac{n-k}{n}.\n  $$\n- Si $x = 1$ :\n  $$\n  \\mathbb{P}(X_1 = 1 \\mid X_1 + \\cdots + X_n = k)\n  = \\frac{\\binom{n-1}{k-1}}{\\binom{n}{k}}\n  = \\frac{(n-1)! \\, (k-1)! \\, (n-k)!}{(n-k)! \\, (n)! \\, k!}\n  = \\frac{k}{n}.\n  $$\n\nDonc \n\n$$\n\\boxed{X_1 \\mid (X_1 + \\cdots + X_n = k) \\sim \\mathcal{B}\\left(1, \\frac{k}{n}\\right)}.\n$$\n\n",
    "supporting": [
      "varaleadis_files"
    ],
    "filters": [],
    "includes": {}
  }
}