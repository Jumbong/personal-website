{
  "hash": "d3873e14b43656f8a606ac9a640a5ac9",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Variables aléatoires générales\" \n---\n\n\n\n\n\n# 3 Variables aléatoires générales\n\n\n## Exercice 1\n\nNous considérons l’expérience aléatoire consistant à lancer 3 fois une pièce de monnaie. Soit $X$ la variable aléatoire donnant le nombre de « Pile » obtenu.\n\n1) Donner la loi de la variable aléatoire $X$.\n2) Donner la fonction de répartition de $X$, et la représenter graphiquement.\n3) Calculer l’espérance, la variance et le coefficient de variation de $X$.\n\n## Correction de l'exercice 1\n\n1) Nous considérons l'espace probabilisé $(\\Omega, \\mathcal{F}, P)$ associé à l'expérience aléatoire de lancer 3 fois une pièce de monnaie. L'ensemble des résultats possibles $\\Omega$ est donné par :\n\n$$\n\\Omega = \\{PPP, PPF, PFP, PFF, FPP, FPF, FFP, FFF\\} = \\{P, F\\}^3\n$$\n\nNous munissons cet espace de la tribu $\\mathcal{F}$ des parties de $\\Omega$ et de la probabilité uniforme $P$.\n\nLa variable aléatoire $X$ est définie comme le nombre de « Pile » obtenus lors des 3 lancers. Ainsi, $X$ peut prendre les valeurs suivantes :\n- $X = 0$ : lorsque le résultat est FFF\n- $X = 1$ : lorsque le résultat est PFF, FPF, ou FFP\n- $X = 2$ : lorsque le résultat est PPF, PFP, ou FPP\n- $X = 3$ : lorsque le résultat est PPP\n\nCette variable est donc définie comme suit :\n\n\n$$\nX : \\Omega^3 \\longrightarrow \\Omega' = \\{0,1,2,3\\}\n$$\n\n$$\n(\\omega_1, \\omega_2, \\omega_3) \\longmapsto\n\\begin{cases}\n0 & \\text{si } (\\omega_1, \\omega_2, \\omega_3) = (FFF),\\\\[4pt]\n1 & \\text{si } (\\omega_1, \\omega_2, \\omega_3) \\in \\{(PFF),(FPF),(FFP)\\},\\\\[4pt]\n2 & \\text{si } (\\omega_1, \\omega_2, \\omega_3) \\in \\{(PPF),(PFP),(FPP)\\},\\\\[4pt]\n3 & \\text{si } (\\omega_1, \\omega_2, \\omega_3) = (PPP).\n\\end{cases}\n$$\n\nComme la variable aléatoire X est **discrète**, il suffit de déterminer la probabilité $\\mathbb{P}_X$ sur les singletons.\n\n$$\n\\begin{aligned}\n\\mathbb{P}_X(0) &= \\frac{1}{8},\\\n\\mathbb{P}_X(1) &= \\frac{3}{8},\\\n\\mathbb{P}_X(2) &= \\frac{3}{8},\\\n\\mathbb{P}_X(3) &= \\frac{1}{8}.\n\\end{aligned}\n$$\n\nAinsi, ( X ) suit une **loi binomiale** :\n$$\nX \\sim \\mathcal{B}(n=3,, p=\\tfrac{1}{2}).\n$$\n\n\n\n2. Fonction de répartition\n\nÀ partir de la définition de la fonction de répartition :\n$$\nF_X(x) = \\mathbb{P}(X \\le x),\n$$\non obtient :\n\n$$\nF_X(x) =\n\\begin{cases}\n0 & \\text{si } x < 0,\\\\[4pt]\n\\frac{1}{8} & \\text{si } 0 \\le x < 1,\\\\[4pt]\n\\frac{4}{8} = \\frac{1}{2} & \\text{si } 1 \\le x < 2,\\\\[4pt]\n\\frac{7}{8} & \\text{si } 2 \\le x < 3,\\\\[4pt]\n1 & \\text{si } 3 \\le x.\n\\end{cases}\n$$\n\nRéprésentation graphique de la fonction de répartition :\n\n::: {#5f05a953 .cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef F(x):\n    x = np.asarray(x)\n    return np.where(\n        x < 0, 0,\n        np.where(\n            x < 1, 1/8,\n            np.where(\n                x < 2, 4/8,\n                np.where(\n                    x < 3, 7/8,\n                    1\n                )\n            )\n        )\n    )\n\n# Segments horizontaux de la fonction de répartition\nsegments = [\n    (-1, 0, 0),   # de x=-1 à x=0 : F=0\n    (0, 1, 1/8),\n    (1, 2, 4/8),\n    (2, 3, 7/8),\n    (3, 4, 1)\n]\n\nplt.figure(figsize=(6,4))\n\n# Tracé des segments horizontaux uniquement\nfor x_start, x_end, y_val in segments:\n    plt.hlines(y_val, x_start, x_end, colors=\"blue\")\n\n# Points ouverts (limite à gauche non incluse)\nx_open  = [0,   1,   2,   3]\ny_open  = [0, 1/8, 4/8, 7/8]\nplt.scatter(x_open, y_open, facecolors=\"none\", edgecolors=\"black\", s=60, zorder=3)\n\n# Points fermés (valeur incluse)\nx_closed = [0,   1,   2,   3]\ny_closed = [1/8, 4/8, 7/8, 1]\nplt.scatter(x_closed, y_closed, color=\"black\", s=60, zorder=3)\n\n# Format graphique\nplt.xlabel(\"x\")\nplt.ylabel(\"F(x)\")\nplt.title(\"Fonction de répartition \")\nplt.grid(True, linestyle=\"--\", alpha=0.4)\nplt.xlim(-1, 4)\nplt.ylim(-0.05, 1.05)\n\nplt.show()\n\n```\n\n::: {.cell-output .cell-output-display}\n![](varalegenerale_files/figure-html/cell-2-output-1.png){width=519 height=377}\n:::\n:::\n\n\n3. Espérance, variance et coefficient de variation\n\nPour calculer l'espérance, la variance et le coefficient de variation de la variable aléatoire $X$, nous allons utiliser la formule de transfert en temps discret :\n\n$$\n\\mathbb{E}[g(X)] = \\sum_{i} g(x_i) \\cdot \\mathbb{P}_X(x_i),\n$$\noù $g$ est une fonction mésurable.\n\nAinsi, pour l'espérance, nous avons :\n$$\n\\mathbb{E}[X] = \\sum_{x=0}^{3} x \\cdot \\mathbb{P}_X(x) = 0 \\cdot \\frac{1}{8} + 1 \\cdot \\frac{3}{8} + 2 \\cdot \\frac{3}{8} + 3 \\cdot \\frac{1}{8} = \\frac{12}{8} = np =1.5.\n$$\n\nPour la variance, nous utilisons la formule :\n\n$$\n\\text{Var}(X) = \\sum_{x=0}^{3} (x - \\mathbb{E}[X])^2 \\cdot \\mathbb{P}_X(x) = (0 - 1.5)^2 \\cdot \\frac{1}{8} + (1 - 1.5)^2 \\cdot \\frac{3}{8} + (2 - 1.5)^2 \\cdot \\frac{3}{8} + (3 - 1.5)^2 \\cdot \\frac{1}{8} = \\frac{6}{8} = np(1-p) = 0.75.\n$$\n\nEnfin, le coefficient de variation est donné par la formule :\n\n$$\nCV(X) = \\frac{\\sigma_X}{\\mathbb{E}[X]} = \\frac{\\sqrt{\\text{Var}(X)}}{\\mathbb{E}[X]} = \\frac{\\sqrt{0.75}}{1.5} = \\frac{\\sqrt{3}}{3} \\approx 0.577.\n$$\n\n## Exercice 2\n\nNous considérons l’expérience aléatoire consistant à jeter un dé jusqu’à obtenir un **« 6 »**. Soit $X$ la variable aléatoire donnant le nombre de jets.\n\n1. Donner la loi de la variable aléatoire $X$.\n2. Donner la fonction de répartition de $X$, et la représenter graphiquement.\n3. Montrer que la fonction génératrice des moments de $X$ vaut :\n\n   $$\n   M_X(t) = \\frac{pe^t}{1 - e^t(1 - p)},\n   $$\n\n   avec $p = \\frac{1}{6}$.\n    \n4. En déduire l’espérance et la variance de $X$, puis son coefficient de variation.\n\n## Correction de l'exercice 2\n\n1) Loi de la variable aléatoire X\n\nNous considérons l'espace probabilisé $(\\Omega^{\\mathbb{N}}, \\mathcal{P}(\\Omega^{\\mathbb{N}}), \\mathbb{P})$ avec $\\Omega = \\{1,2,3,4,5,6\\}$ muni de la tribu produit et de la probabilité uniforme $\\mathbb{P}$.\n\nL'univers $\\Omega^{\\mathbb{N}}$ correspond à l'ensemble des suites infinies de résultats de lancers de dé. Autrement dit on peut l'écrire comme l'ensemble des suites infinies :\n$$\n\\Omega^{\\mathbb{N}} = \\{(\\omega_1, \\omega_2, \\ldots) \\mid \\omega_i \\in \\Omega = \\{1,2,3,4,5,6\\} \\text{ pour tout } i \\in \\mathbb{N}\\}.\n$$\n\n La variable aléatoire étudiée est\n\n$$\nX : \\Omega^{\\mathbb{N}} \\longrightarrow \\Omega' = \\mathbb{N}^*\n$$\n\n$$\n(\\omega_1, \\omega_2, \\ldots) \\longmapsto\n\\begin{cases}\n1 & \\text{si } \\omega_1 = \\{6\\},\\\\[4pt]\n2 & \\text{si } \\omega_1 \\neq \\{6\\} \\text{ et } \\omega_2 = \\{6\\},\\\\[4pt]\n& \\cdots\\\\[4pt]\nk & \\text{si } \\omega_1, \\ldots, \\omega_{k-1} \\neq \\{6\\} \\text{ et } \\omega_k = \\{6\\},\\\\[4pt]\n& \\cdots\n\\end{cases}\n$$\n\nPour tout $k \\in \\mathbb{N}^*$, on a :\n\n$$\n\\mathbb{P}(X = k) = \\mathbb{P}(\\omega_1, \\ldots, \\omega_{k-1} \\neq \\{6\\} \\text{ et } \\omega_k = \\{6\\})\n$$\n\n$$\n= \\prod_{i=1}^{k-1} \\mathbb{P}(\\omega_i \\neq \\{6\\}) \\times \\mathbb{P}(\\omega_k = \\{6\\})\n$$\n\n$$\n= (1 - p)^{k-1}p,\n$$\n\navec $p = \\frac{1}{6}$ la probabilité d'obtenir un \"6\" lors d'un lancer. Il s'agit d'une distribution géométrique de paramètre $p$. \n\n2) Fonction de répartition\n\nSoit $k \\geq 1$. Nous avons :\n\n$$\n\\mathbb{P}(X \\leq k) = \\sum_{j=1}^{k} (1-p)^{j-1}p\n$$\n\n$$\n= p\\sum_{j=0}^{k-1} (1-p)^{j} = p\\frac{1-(1-p)^k}{1-(1-p)} = 1-(1-p)^k.\n$$\n\nA partir de la définition de la fonction de répartition, nous obtenons\n\n$$\nF_X(x) = \n\\begin{cases}\n0 & \\text{si } x < 1,\\\\[4pt]\np & \\text{si } 1 \\leq x < 2,\\\\[4pt]\n& \\cdots,\\\\[4pt]\n1-(1-p)^k & \\text{si } k \\leq x < k+1,\\\\[4pt]\n& \\cdots.\n\\end{cases}\n$$\n\nReprésentation graphique de la fonction de répartition :\n\n::: {#d3a888ae .cell execution_count=2}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef plot_cdf_geometrique(p=0.3, k_max=8):\n    \"\"\"\n    Trace la fonction de répartition\n        F_X(x) = 0                si x < 1\n               = 1 - (1-p)^k      si k <= x < k+1   pour k = 1,2,...\n    avec une troncature à k_max.\n    \"\"\"\n    if not (0 < p < 1):\n        raise ValueError(\"p doit être dans l'intervalle (0, 1).\")\n\n    plt.figure(figsize=(6, 4))\n\n    # Segment avant 1 : F(x) = 0 pour 0 <= x < 1\n    plt.hlines(0, 0, 1, linewidth=2)\n\n    # Segments horizontaux pour k = 1,...,k_max\n    for k in range(1, k_max + 1):\n        level = 1 - (1 - p)**k\n        plt.hlines(level, k, k + 1, linewidth=2)\n\n    # Points ouverts (limite à gauche, non incluse)\n    x_open, y_open = [], []\n    # Points fermés (valeur de la FDR)\n    x_closed, y_closed = [], []\n\n    for k in range(1, k_max + 1):\n        # Limite à gauche en k :\n        if k == 1:\n            left_limit = 0\n        else:\n            left_limit = 1 - (1 - p)**(k - 1)\n\n        value_at_k = 1 - (1 - p)**k\n\n        x_open.append(k)\n        y_open.append(left_limit)\n\n        x_closed.append(k)\n        y_closed.append(value_at_k)\n\n    # Cercles ouverts (non remplis) : limite à gauche\n    plt.scatter(\n        x_open, y_open,\n        facecolors=\"none\",\n        edgecolors=\"black\",\n        s=60,\n        zorder=3\n    )\n\n    # Cercles fermés (remplis) : valeur de F_X en k\n    plt.scatter(\n        x_closed, y_closed,\n        color=\"black\",\n        s=60,\n        zorder=3\n    )\n\n    # Mise en forme\n    plt.xlabel(\"x\")\n    plt.ylabel(\"F(x)\")\n    plt.title(f\"Fonction de répartition géométrique (p = {p})\")\n    plt.xlim(0, k_max + 1.5)\n    plt.ylim(-0.05, 1.05)\n    plt.grid(True, linestyle=\"--\", alpha=0.4)\n\n    plt.show()\n\n# Exemple d'appel\nplot_cdf_geometrique(p=0.25, k_max=8)\n```\n\n::: {.cell-output .cell-output-display}\n![](varalegenerale_files/figure-html/cell-3-output-1.png){width=514 height=377}\n:::\n:::\n\n\n3) Fonction génératrice des moments\n\nEn utilisant la formule de transfert en temps discret, nous avons :\n$$\nM_X(t) = \\int e^{tX} d\\mathbb{P} = \\sum_{x=1}^{\\infty} e^{tx}\\mathbb{P}_X(x)\n$$\n\n$$\n= p\\sum_{j=1}^{\\infty} e^{tj}(1-p)^{j-1}\n$$\n\n$$\n= pe^t\\sum_{j=0}^{\\infty} e^{tj}(1-p)^j\n$$\n\n$$\n= pe^t\\sum_{j=0}^{\\infty} q^j \\text{ avec } q = e^t(1-p)\n$$\n\n$$\n= \\frac{pe^t}{1-q} = \\frac{pe^t}{1-e^t(1-p)}.\n$$\n\nLa série converge si et seulement si $|q| < 1$, c'est-à-dire si $|e^t(1-p)| < 1$, soit $e^t < \\frac{1}{1-p}$, ce qui donne $t < \\ln\\left(\\frac{1}{1-p}\\right) = -\\ln(1-p)$.\n\n4) Espérance, variance et coefficient de variation\n\nNous savons que la fonction $M_X$ est indéfiniment dérivable dans un voisinage de 0, et que $M'_X(0) = E(X)$ et $M''_X(0) = E(X^2)$. Nous avons après calcul\n\n$$\nM'_x(t) = \\frac{pe^t}{\\{1-e^t(1-p)\\}^2},\n$$\n\n$$\nM''_x(t) = \\frac{pe^t\\{1-(1-p)^2e^{2t}\\}}{\\{1-e^t(1-p)\\}^4},\n$$\n\ndont nous déduisons que\n\n$$\nM'_x(0) = \\frac{1}{p} \\equiv E(X),\n$$\n\n$$\nM''_x(0) = \\frac{p\\{1-(1-p)^2\\}}{\\{1-(1-p)\\}^4} = \\frac{2-p}{p^2} \\equiv E(X^2),\n$$\n\npuis\n\n$$\nV(X) = \\frac{1-p}{p^2},\n$$\n\n$$\nCV(X) = \\sqrt{1-p}.\n$$\n\nNumériquement, nous obtenons\n\n$$\nE(X) = 6, \\quad V(X) = 30, \\quad CV(X) = 0.91.\n$$\n\n\n## Exercice 3\n\nSoit $X$ une variable aléatoire admettant un moment d'ordre 2. Nous notons $m = E(X)$ et $\\sigma^2 = V(X)$. Soient $X_1, \\ldots, X_n$ des variables aléatoires indépendantes et de même loi que $X$. Nous notons\n\n$$\n\\overline{X}_n = \\frac{1}{n}\\sum_{i=1}^{n} X_i\n$$\n\nla moyenne empirique des $X_i$, $i = 1, \\ldots, n$, et\n\n$$\ns_X^2 = \\frac{1}{n-1}\\sum_{i=1}^{n} (X_i - \\overline{X}_n)^2\n$$\n\nla dispersion des $X_i$, $i = 1, \\ldots, n$.\n\n1) Montrer que $E(\\overline{X}_n) = m$.\n\n2) Montrer que $E(s_X^2) = \\sigma^2$. Vous pourrez utiliser l'identité\n\n$$\ns_X^2 = \\frac{1}{2n(n-1)} \\sum_{i=1}^{n} \\sum_{\\substack{j=1\\\\j \\neq i}}^{n} (X_i - X_j)^2.\n$$\n\n3) Utiliser ces résultats pour obtenir une approximation de Monte-Carlo de l'espérance, la variance et le coefficient de variation de la variable aléatoire $X$ de l'exercice 2.\n\n- Démonstration de l'équivalence des formules de variance\n\nNous voulons démontrer que :\n\n$$\\frac{1}{n-1}\\sum_{i=1}^{n}(X_i - \\bar{X}_n)^2 = \\frac{1}{2n(n-1)}\\sum_{i=1}^{n}\\sum_{\\substack{j=1\\\\j\\neq i}}^{n}(X_i - X_j)^2$$\n\n- Étape 1 : Développement de la somme double\n\nCommençons par développer le membre de droite :\n\n$$\\sum_{i=1}^{n}\\sum_{\\substack{j=1\\\\j\\neq i}}^{n}(X_i - X_j)^2$$\n\nDéveloppons $(X_i - X_j)^2$ :\n\n$$= \\sum_{i=1}^{n}\\sum_{\\substack{j=1\\\\j\\neq i}}^{n}(X_i^2 - 2X_iX_j + X_j^2)$$\n\n$$= \\sum_{i=1}^{n}\\sum_{\\substack{j=1\\\\j\\neq i}}^{n}X_i^2 - 2\\sum_{i=1}^{n}\\sum_{\\substack{j=1\\\\j\\neq i}}^{n}X_iX_j + \\sum_{i=1}^{n}\\sum_{\\substack{j=1\\\\j\\neq i}}^{n}X_j^2$$\n\n- Étape 2 : Simplification de chaque terme\n\nPremier terme :\n$$\\sum_{i=1}^{n}\\sum_{\\substack{j=1\\\\j\\neq i}}^{n}X_i^2 = \\sum_{i=1}^{n}X_i^2 \\cdot (n-1) = (n-1)\\sum_{i=1}^{n}X_i^2$$\n\nCar pour chaque $i$ fixé, $X_i^2$ est sommé $(n-1)$ fois (pour tous les $j \\neq i$).\n\nTroisième terme :\nPar symétrie, le troisième terme donne le même résultat :\n$$\\sum_{i=1}^{n}\\sum_{\\substack{j=1\\\\j\\neq i}}^{n}X_j^2 = (n-1)\\sum_{j=1}^{n}X_j^2 = (n-1)\\sum_{i=1}^{n}X_i^2$$\n\nDeuxième terme :\n$$\\sum_{i=1}^{n}\\sum_{\\substack{j=1\\\\j\\neq i}}^{n}X_iX_j$$\n\nPour chaque paire $(i,j)$ avec $i \\neq j$, le produit $X_iX_j$ apparaît dans cette double somme. On peut réécrire :\n\n$$= \\sum_{i=1}^{n}X_i\\sum_{\\substack{j=1\\\\j\\neq i}}^{n}X_j = \\sum_{i=1}^{n}X_i\\left(\\sum_{j=1}^{n}X_j - X_i\\right)$$\n\n$$= \\sum_{i=1}^{n}X_i \\cdot n\\bar{X}_n - \\sum_{i=1}^{n}X_i^2$$\n\n$$= n\\bar{X}_n \\cdot n\\bar{X}_n - \\sum_{i=1}^{n}X_i^2 = n^2\\bar{X}_n^2 - \\sum_{i=1}^{n}X_i^2$$\n\n- Étape 3 : Assemblage\n\nEn rassemblant les trois termes :\n\n$$\\sum_{i=1}^{n}\\sum_{\\substack{j=1\\\\j\\neq i}}^{n}(X_i - X_j)^2 = (n-1)\\sum_{i=1}^{n}X_i^2 - 2\\left(n^2\\bar{X}_n^2 - \\sum_{i=1}^{n}X_i^2\\right) + (n-1)\\sum_{i=1}^{n}X_i^2$$\n\n$$= (n-1)\\sum_{i=1}^{n}X_i^2 - 2n^2\\bar{X}_n^2 + 2\\sum_{i=1}^{n}X_i^2 + (n-1)\\sum_{i=1}^{n}X_i^2$$\n\n$$= 2(n-1)\\sum_{i=1}^{n}X_i^2 + 2\\sum_{i=1}^{n}X_i^2 - 2n^2\\bar{X}_n^2$$\n\n$$= 2n\\sum_{i=1}^{n}X_i^2 - 2n^2\\bar{X}_n^2$$\n\n$$= 2n\\left(\\sum_{i=1}^{n}X_i^2 - n\\bar{X}_n^2\\right)$$\n\n- Étape 4 : Lien avec la variance classique\n\nRappelons la formule classique de la variance :\n\n$$\\sum_{i=1}^{n}(X_i - \\bar{X}_n)^2 = \\sum_{i=1}^{n}(X_i^2 - 2X_i\\bar{X}_n + \\bar{X}_n^2)$$\n\n$$= \\sum_{i=1}^{n}X_i^2 - 2\\bar{X}_n\\sum_{i=1}^{n}X_i + n\\bar{X}_n^2$$\n\n$$= \\sum_{i=1}^{n}X_i^2 - 2\\bar{X}_n \\cdot n\\bar{X}_n + n\\bar{X}_n^2$$\n\n$$= \\sum_{i=1}^{n}X_i^2 - n\\bar{X}_n^2$$\n\n- Étape 5 : Conclusion\n\nD'après l'étape 3 :\n$$\n\\sum_{i=1}^{n}\\sum_{\\substack{j=1\\\\j\\neq i}}^{n}(X_i - X_j)^2 = 2n\\left(\\sum_{i=1}^{n}X_i^2 - n\\bar{X}_n^2\\right)\n$$\n\n\nD'après l'étape 4 :\n$$\n\\sum_{i=1}^{n}(X_i - \\bar{X}_n)^2 = \\sum_{i=1}^{n}X_i^2 - n\\bar{X}_n^2\n$$\n\nDonc :\n$$\n\\sum_{i=1}^{n}\\sum_{\\substack{j=1\\\\j\\neq i}}^{n}(X_i - X_j)^2 = 2n\\sum_{i=1}^{n}(X_i - \\bar{X}_n)^2\n$$\n\nEn divisant par $2n(n-1)$ :\n\n$$\n\\frac{1}{2n(n-1)}\\sum_{i=1}^{n}\\sum_{\\substack{j=1\\\\j\\neq i}}^{n}(X_i - X_j)^2 = \\frac{1}{n-1}\\sum_{i=1}^{n}(X_i - \\bar{X}_n)^2\n$$\n\n## Correction de l'exercice 3\n\n1) Calcul de $E(\\overline{X}_n)$\n\nPar linéarité de l'espérance, nous avons :\n\n$$\nE(\\overline{X}_n) = E\\left(\\frac{1}{n}\\sum_{i=1}^{n} X_i\\right) = \\frac{1}{n}\\sum_{i=1}^{n} E(X_i) = \\frac{1}{n} \\cdot n \\cdot m = m.\n$$\n\n\n\n2) Calcul de $E(s_X^2)$\n\nEn utilisant l'identité donnée, nous avons en utilisant la linéarité de l'espérance :\n\n$$\nE(s_X^2) = E\\left(\\frac{1}{2n(n-1)} \\sum_{i=1}^{n} \\sum_{\\substack{j=1\\\\j \\neq i}}^{n} (X_i - X_j)^2\\right)\n$$\n\n$$\n= \\frac{1}{2n(n-1)} \\sum_{i=1}^{n} \\sum_{\\substack{j=1\\\\j \\neq i}}^{n} E\\left((X_i - X_j)^2\\right).\n$$\n\nIci on peut utiliser deux méthodes de calcul qui sont équivalentes.\n\nLa première consiste à remarquer que pour $i \\neq j$ :\n\n$$\nE\\left((X_i - X_j)^2\\right) = E\\left(X_i^2\\right) - 2E(X_i X_j) + E\\left(X_j^2\\right)\n$$\n\nComme les $X_i$ sont indépendantes et de même loi, nous avons :\n\n$$\nE\\left(X_i^2\\right) = E\\left(X_1^2\\right) \\quad \\text{et} \\quad E(X_i X_j) = E(X_1)E(X_2) \\quad \\text{pour } i \\neq j.\n$$\n\nDe ce fait, nous obtenons :\n\n$$\nE\\left((X_i - X_j)^2\\right) = E\\left(X_1^2\\right) - 2E(X_1)E(X_2) + E\\left(X_2^2\\right)\n$$\n\n$$\n= E\\left(X_1^2\\right) - 2E(X_1X_2) + E\\left(X_2^2\\right)\n$$\n\nCar $X_1$ et $X_2$ sont indépendantes et de même loi.\n\nDonc finalement :\n\n$$\nE\\left((X_i - X_j)^2\\right) = E\\left((X_1 - X_2)^2\\right).\n$$\n\nUne autre méthode consiste à utiliser la formule de Koenig-Huygens pour la variance. Nous avons :\n\n$$\nE\\left((X_i - X_j)^2\\right) = E\\left(X_1^2\\right) - 2E(X_1)E(X_2) + E\\left(X_2^2\\right)\n$$\n\n$$\n= 2\\left(E\\left(X_1^2\\right) - E(X_1)^2\\right) = 2V(X) = 2\\sigma^2.\n$$\n\nLorsque nous utilisons la première méthode, nous devons calculer $E\\left((X_1 - X_2)^2\\right)$ en développant :\n\n$$\nE\\left((X_1 - X_2)^2\\right) = E[{(X_1 - m) - (X_2 - m)}^2]\n$$\n\n$$\n= E\\left((X_1 - m)^2\\right) + E\\left((X_2 - m)^2\\right) - 2E\\left((X_1 - m)(X_2 - m)\\right)\n$$\n\n$$\n= E\\left((X_1 - m)^2\\right) + E\\left((X_2 - m)^2\\right) - 2E(X_1 - m)\\times E(X_2 - m)\n$$\nCar $X_1$ et $X_2$ sont indépendantes.\n\n$$\n= \\sigma^2 + \\sigma^2 - 2 \\cdot 0 \\cdot 0 = 2\\sigma^2.\n$$\n\nDonc, nous obtenons finalement :\n$$\nE(s_X^2) = \\frac{1}{2} \\cdot 2\\sigma^2 = \\sigma^2.\n$$\n\n3) Approximation de Monte-Carlo\n\nCe que nous avons montré aux points précédents, est que nous pouvons estimer l'espérance et la variance de la variable aléatoire $X$ en utilisant les statistiques d'échantillon $\\overline{X}_n$ et $s_X^2$. C'est-à-dire en générant un grand nombre de réalisations indépendantes de $X$ et en calculant la moyenne empirique et la dispersion de ces réalisations.\n\nVoici un exemple de code R pour effectuer cette approximation de Monte-Carlo pour la variable aléatoire $X$ de l'exercice 2 :\n\n\n```default\n# Monte Carlo approximation in R\n\n# Répétition de l'expérience aléatoire\n# Simulation du nombre de lancers nécessaires pour obtenir un 6\n\nset.seed(360)\nn <- 50000\nctr <- 0\nsimlist <- numeric(n)\n\nwhile (ctr < n) {\n  ctr <- ctr + 1\n  \n  # Valeur du lancer (initialisation)\n  trial <- 0\n  \n  # Nombre de tentatives (initialisation)\n  nb_tent <- 0\n  \n  while (trial != 6) {\n    nb_tent <- nb_tent + 1\n    trial <- sample(1:6, 1)\n  }\n  \n  simlist[ctr] <- nb_tent\n}\n\n# Affichage des résultats\nmean(simlist)\nvar(simlist)\n\n# Statistiques supplémentaires\ncat(\"\\n=== Résultats de la simulation ===\\n\")\ncat(\"Nombre de simulations:\", n, \"\\n\")\ncat(\"Moyenne du nombre de lancers:\", mean(simlist), \"\\n\")\ncat(\"Variance:\", var(simlist), \"\\n\")\ncat(\"Écart-type:\", sd(simlist), \"\\n\")\ncat(\"Minimum:\", min(simlist), \"\\n\")\ncat(\"Maximum:\", max(simlist), \"\\n\")\ncat(\"Médiane:\", median(simlist), \"\\n\")\n\n# Histogramme\nhist(simlist, \n     breaks = 30, \n     col = \"lightblue\", \n     border = \"white\",\n     main = \"Distribution du nombre de lancers pour obtenir un 6\",\n     xlab = \"Nombre de lancers\",\n     ylab = \"Fréquence\",\n     xlim = c(0, max(simlist)))\n\n# Ajout de la moyenne théorique\nabline(v = 6, col = \"red\", lwd = 2, lty = 2)\nlegend(\"topright\", \n       legend = c(\"Moyenne observée\", \"Moyenne théorique = 6\"),\n       col = c(\"blue\", \"red\"), \n       lty = c(1, 2), \n       lwd = 2)\n```\n\n::: {#b9d5e0eb .cell execution_count=3}\n``` {.python .cell-code}\nimport random\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Répétition de l'expérience\nrandom.seed(360)\nn = 50000\nsimlist = []\n\nfor _ in range(n):\n    trial = 0        # valeur du lancer\n    nb_tent = 0      # nombre de tentatives\n    \n    while trial != 6:\n        nb_tent += 1\n        trial = random.randint(1, 6)  # équivalent de sample(1:6,1)\n    \n    simlist.append(nb_tent)\n\nsimlist = np.array(simlist)\n\n# Résultats\nprint(\"\\n=== Résultats de la simulation ===\")\nprint(\"Nombre de simulations:\", n)\nprint(\"Moyenne du nombre de lancers:\", simlist.mean())\nprint(\"Variance:\", simlist.var(ddof=1))\nprint(\"Écart-type:\", simlist.std(ddof=1))\nprint(\"Minimum:\", simlist.min())\nprint(\"Maximum:\", simlist.max())\nprint(\"Médiane:\", np.median(simlist))\n\n# Calcul des moyennes cumulées\nmean_values = np.cumsum(simlist) / np.arange(1, n + 1)\n\n# Tracé\nplt.figure(figsize=(7, 4))\nplt.plot(mean_values, label=\"Moyenne empirique\")\nplt.axhline(6, color=\"red\", linestyle=\"--\", label=\"Espérance théorique = 6\")\n\nplt.xlabel(\"n (nombre d'expériences)\")\nplt.ylabel(\"Moyenne empirique\")\nplt.title(\"Convergence de la moyenne empirique vers l'espérance théorique\")\nplt.legend()\nplt.grid(alpha=0.4)\nplt.show() \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n=== Résultats de la simulation ===\nNombre de simulations: 50000\nMoyenne du nombre de lancers: 6.00114\nVariance: 30.1522217448349\nÉcart-type: 5.4911038730691395\nMinimum: 1\nMaximum: 65\nMédiane: 4.0\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](varalegenerale_files/figure-html/cell-4-output-2.png){width=583 height=378}\n:::\n:::\n\n\n```default\n# Utilisation de la distribution géométrique\n# Alternative plus efficace à la simulation par boucle\n\nset.seed(360)\nn <- 50000\np <- 1/6\nctr <- 0\nsimlist <- numeric(n)\n\nwhile (ctr < n) {\n  ctr <- ctr + 1\n  \n  # Génération d'une réalisation d'une loi géométrique\n  # Attention, rgeom(n,p) donne le nombre d'échecs avant\n  # le premier succès\n  simlist[ctr] <- rgeom(1, p) + 1\n}\n\n# Affichage des résultats\nmean(simlist)\nvar(simlist)\n\n# Statistiques détaillées\ncat(\"\\n=== Résultats de la simulation (méthode géométrique) ===\\n\")\ncat(\"Nombre de simulations:\", n, \"\\n\")\ncat(\"Probabilité de succès (p):\", p, \"\\n\")\ncat(\"Moyenne observée:\", mean(simlist), \"\\n\")\ncat(\"Moyenne théorique:\", 1/p, \"\\n\")\ncat(\"Variance observée:\", var(simlist), \"\\n\")\ncat(\"Variance théorique:\", (1-p)/p^2, \"\\n\")\ncat(\"Écart-type observé:\", sd(simlist), \"\\n\")\ncat(\"Médiane:\", median(simlist), \"\\n\")\n\n# Comparaison graphique\npar(mfrow = c(1, 2))\n\n# Histogramme\nhist(simlist, \n     breaks = 50, \n     col = \"lightgreen\", \n     border = \"white\",\n     main = \"Distribution du nombre de lancers\\n(méthode rgeom)\",\n     xlab = \"Nombre de lancers\",\n     ylab = \"Fréquence\",\n     probability = TRUE)\n\n# Ajout de la moyenne\nabline(v = mean(simlist), col = \"blue\", lwd = 2)\nabline(v = 1/p, col = \"red\", lwd = 2, lty = 2)\nlegend(\"topright\", \n       legend = c(\"Moyenne obs.\", \"Moyenne théo.\"),\n       col = c(\"blue\", \"red\"), \n       lty = c(1, 2), \n       lwd = 2,\n       cex = 0.8)\n\n# QQ-plot pour vérifier la distribution\nqqplot(qgeom(ppoints(n), p) + 1, simlist,\n       main = \"QQ-Plot : Théorique vs Observé\",\n       xlab = \"Quantiles théoriques (Géométrique)\",\n       ylab = \"Quantiles observés\",\n       col = \"darkgreen\",\n       pch = 20,\n       cex = 0.5)\nabline(0, 1, col = \"red\", lwd = 2)\n\npar(mfrow = c(1, 1))\n\n# Méthode encore plus efficace (vectorisée)\ncat(\"\\n=== Méthode vectorisée (plus rapide) ===\\n\")\nset.seed(360)\nsimlist_vec <- rgeom(n, p) + 1\ncat(\"Moyenne:\", mean(simlist_vec), \"\\n\")\ncat(\"Variance:\", var(simlist_vec), \"\\n\")\n```\n\n::: {#09114036 .cell execution_count=4}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport scipy.stats as st\n\n# Paramètres\nnp.random.seed(360)\nn = 50000\np = 1/6\n\n# La loi géométrique de R donne \"nombre d'échecs avant succès\"\n# => équivalent python : st.geom(p).rvs() renvoie déjà nb_tentatives\n# MAIS Python compte à partir de 1, donc c'est ce qu’on veut !\nsimlist = st.geom(p).rvs(size=n)\n\n# Affichage des résultats\nprint(\"\\n=== Résultats de la simulation (méthode géométrique) ===\")\nprint(\"Nombre de simulations:\", n)\nprint(\"Probabilité de succès p:\", p)\nprint(\"Moyenne observée:\", simlist.mean())\nprint(\"Moyenne théorique:\", 1/p)\nprint(\"Variance observée:\", simlist.var(ddof=1))\nprint(\"Variance théorique:\", (1 - p) / p**2)\nprint(\"Écart-type observé:\", simlist.std(ddof=1))\nprint(\"Médiane:\", np.median(simlist))\n\n\n# Moyennes cumulées\nmean_values = np.cumsum(simlist) / np.arange(1, n + 1)\n\n# Tracé\nplt.figure(figsize=(7, 4))\nplt.plot(mean_values, label=\"Moyenne empirique\")\nplt.axhline(1/p, color=\"red\", linestyle=\"--\", label=f\"Espérance théorique = {1/p:.0f}\")\n\nplt.xlabel(\"n (nombre d'expériences)\")\nplt.ylabel(\"Moyenne empirique\")\nplt.title(\"Convergence de la moyenne empirique de la loi géométrique\")\nplt.legend()\nplt.grid(alpha=0.4)\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n=== Résultats de la simulation (méthode géométrique) ===\nNombre de simulations: 50000\nProbabilité de succès p: 0.16666666666666666\nMoyenne observée: 6.04552\nMoyenne théorique: 6.0\nVariance observée: 29.974807425748512\nVariance théorique: 30.000000000000004\nÉcart-type observé: 5.474925335175678\nMédiane: 4.0\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](varalegenerale_files/figure-html/cell-5-output-2.png){width=585 height=378}\n:::\n:::\n\n\n## Exercice 4\n\nSoient X et Y deux variables aléatoires de carré $\\mathbb{P}$-intégrable.\n\n1. Soit $a \\in \\mathbb{R}$.  \n   Écrire  \n   $$\n   E\\big((|X| - a|Y|)^2\\big)\n   $$\n   sous la forme d’un polynôme en a, et calculer son discriminant $\\Delta$.\n\n2. Expliquer pourquoi $\\Delta \\le 0$, et en déduire l’inégalité de Hölder :\n   $$\n   E(|XY|) \\le \\sqrt{E(X^2)} \\, \\sqrt{E(Y^2)}.\n   $$\n\n3. En appliquant l’inégalité précédente à des variables bien choisies, en déduire que :\n   $$\n   |\\mathrm{Cov}(X,Y)| \\le \\sqrt{V(X)} \\, \\sqrt{V(Y)}.\n   $$\n\nL’inégalité de Hölder se généralise sous la forme suivante (admise dans la suite).  \nSoient deux nombres réels $p, q \\ge 1$ conjugués, c’est-à-dire tels que :\n$$\n\\frac{1}{p} + \\frac{1}{q} = 1.\n$$\n\nSoient X et Y deux variables aléatoires telles que  \n$\\int |X|^p \\, d\\mathbb{P} < \\infty$ et $\\int |Y|^q \\, d\\mathbb{P} < \\infty$.  \nAlors :\n$$\nE(|XY|) \\le \\big(E(|X|^p)\\big)^{1/p} \\, \\big(E(|Y|^q)\\big)^{1/q}.\n$$\n\nSoient maintenant deux réels $r$ et $s$ tels que $1 < r < s < \\infty$.  \nSoit $Z$ une variable aléatoire telle que $\\int |Z|^s \\, d\\mathbb{P} < \\infty$.\n\n4. Vérifier que les réels $\\frac{s}{r}$ et $\\frac{s}{s-r}$ sont conjugués.  \n   En appliquant l’inégalité de Hölder avec des variables aléatoires $X$ et $Y$ bien choisies, montrer que :\n   $$\n   E(|Z|^r) \\le \\big(E(|Z|^s)\\big)^{r/s}.\n   $$\n\n5. En déduire que si une variable aléatoire $Z$ admet un moment d’ordre $s > 1$, alors elle admet un moment d’ordre $r$ pour tous les réels $1 < r < s$.\n\n## Correction de l'exercice 4\n\n1) Calcul de $E\\big((|X| - a|Y|)^2\\big)$\n\nNous avons en utilisant la linéarité de l'espérance :\n\n$$\nE\\big((|X| - a|Y|)^2\\big) = E(|X|^2) - 2aE(|X||Y|) + a^2E(|Y|^2).\n$$\n\nest sa forme de polynôme en $a$.\n\nSon discriminant est donc donné par :\n\n$$\n\\Delta = (-2E(|X||Y|))^2 - 4E(|X|^2)E(|Y|^2) = 4\\big(E(|X||Y|)^2 - E(|X|^2)E(|Y|^2)\\big).\n$$\n\nComme $E\\big((|X| - a|Y|)^2\\big) \\geq 0$ pour tout $a \\in \\mathbb{R}$, le polynôme ne peut pas avoir deux racines réelles distinctes, donc $\\Delta \\leq 0$.\n\n3) En appliquant l’inégalité précédente en remplaçant $X$ par $X - E(X)$, et\n$Y$ par $Y - E(Y)$, nous obtenons :\n\n$$\nE \\big( \\{ X - E(X) \\} \\{ Y - E(Y) \\} \\big)\n\\le \\sqrt{E \\big( \\{ X - E(X) \\}^2 \\big)} \\, \\sqrt{E \\big( \\{ X - E(X) \\}^2 \\big)}.\n$$\n\nNous avons ensuite\n\n$$\n|\\mathrm{Cov}(X,Y)|\n  = \\big| E \\big( \\{ X - E(X) \\} \\{ Y - E(Y) \\} \\big) \\big|\n  \\le E \\big( \\{ X - E(X) \\} \\{ Y - E(Y) \\} \\big),\n$$\n\nce qui donne le résultat voulu.\n\n---\n\n4) Nous avons bien\n\n$$\n\\frac{r}{s} + \\frac{s - r}{s} = 1.\n$$\n\nEn appliquant l’inégalité de Hölder générale aux variables aléatoires $X = Z^r$\net $Y = 1$, et avec les nombres conjugués $\\frac{s}{r}$ et $\\frac{s}{s-r}$,\nnous obtenons\n\n$$\nE \\big( |Z^r \\times 1| \\big)\n\\le \\Big( E \\big( |Z^r|^{\\frac{s}{r}} \\big) \\Big)^{\\frac{r}{s}}\n     \\Big( E \\big( |1|^{\\frac{s}{s-r}} \\big) \\Big)^{\\frac{s-r}{s}}\n     = \\Big( E(|Z|^s) \\Big)^{\\frac{r}{s}}.\n$$\n\n$$\n\\Rightarrow \\quad\nE(|Z|^r) \\le \\big( E(|Z|^s) \\big)^{\\frac{r}{s}}.\n$$\n\n\n\n5) Cela découle de l’inégalité précédente.\n\n",
    "supporting": [
      "varalegenerale_files"
    ],
    "filters": [],
    "includes": {}
  }
}