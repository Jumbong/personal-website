{
  "hash": "c75ede2429830f4f23a578c1b7285e30",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"\"\n---\n\n\n\n\n\n<div class=\"postcard-grid\">\n\nVous trouverez ici certains exercices de la fiche de TD, qui vous permettront de mieux comprendre les concepts abordés par le professeur pendant les cours. N'hésitez pas à me contacter si vous avez des questions ou des suggestions. \n\n\n\n::: {.postcard}\n## 1. Exercices sur les espaces probabilisés\n- [Espaces probabilisés](1A/EspacesProbabilises/espaceproba.qmd)\n:::\n\n\n::: {.postcard}\n## 2. Exercice sur la probabilité conditionnelle et indépendance\n- [Probabilité conditionnelle et indépendance](1A/ProbaCondi/probacondi.qmd)\n:::\n\n::: {.postcard}\n## 3. Variables aléatoires générales\n- [Variables aléatoires générales](1A/VariablesAleaGen/varalegenerale.qmd)\n:::\n\n# 4. Variables aléatoires discrètes\n\n## Exercice 1\n\nDans chacune des situations, identifier si $X$ suit une loi binomiale. Si oui, donner les paramètres $n$ et $p$ correspondants. Sinon, expliquer pourquoi et proposer une modélisation alternative :\n\n- Chaque jour, Dean va déjeuner et il y a 25% de chances qu’il prenne une pizza. Soit $X$ le nombre de fois où il a pris une pizza la semaine dernière.\n\n- Jessica joue au basketball, et elle a 60% de chances de réussir un lancer franc. Soit $X$ le nombre de lancers francs réussis pendant le dernier match.\n\n- Une boîte contient 300 bonbons, dont 100 carambars et 200 chocolats. Sam prend un échantillon de 10 bonbons dans la boîte. Soit $X$ le nombre de carambars dans son échantillon.\n\n- Marie lit un livre de 600 pages. Sur les pages paires, il y a 1% de chances d’avoir une faute d’orthographe. Sur les pages impaires, il y a 2% de chances d’avoir une faute d’orthographe. Soit $X$ le nombre total de fautes d’orthographe dans le livre.\n\n- John lit un livre de 600 pages. Sur chaque page, le nombre de fautes d’orthographe est distribué selon une loi de Bernouilli de paramètre $0.01$. Soit $X$ le nombre total de fautes d’orthographe dans le livre.\n\n\n## Correction Exercice 1\n\nDans chaque situation, on indique si $X$ suit une loi binomiale. Si oui, on précise les paramètres $n$ et $p$. Sinon, on propose une modélisation alternative.\n\n---\n\n1. **Dean et les pizzas**\n\n- Chaque jour, Dean prend une pizza avec probabilité $p = 0{,}25$, pendant une semaine ($n = 7$ jours). \n- Ensuite, il faut se poser la question : la décision de prendre une pizza un jour est-elle indépendante des autres jours ? Si oui, alors :\n- Le nombre de jours $X$ où il prend une pizza suit donc une loi binomiale :\n\n$$X \\sim \\mathcal{B}(n = 7,\\; p = 0{,}25).$$\n\n---\n\n2. **Jessica et les lancers francs**\n\n- Jessica réussit un lancer franc avec probabilité $p = 0{,}6$, mais le nombre de lancers tentés pendant le match **n’est pas fixé à l’avance**.\n- Il ne s’agit donc pas d’une loi binomiale (condition nécessaire : nombre d’essais fixé). Dans une modélisation binomiale, le nombre d’essais $n$ doit être fixé et connu à l’avance. \n\nUne modélisation possible est :\n\n- On note $N$ le nombre de lancers francs tentés pendant le dernier match. Il y a plusieurs possibilités : Soit $N$ est une constante connue, soit $N$ est une variable aléatoire (par exemple, on peut modéliser $N$ par une loi de Poisson.). Ensuite, on considère le nombre de lancers réussis $X$ conditionnellement à $N$.\n\n- Conditionnellement à $N$, le nombre de lancers réussis suit une loi binomiale :\n\n$$X \\mid N \\sim \\mathcal{B}(N,\\; 0{,}6).$$\n\nsi les lancers sont indépendants.\n\n---\n\n3. **Boîte de bonbons (carambars/chocolats)**\n\n- La boîte contient $300$ bonbons dont $100$ carambars et $200$ chocolats.\n- Sam tire un échantillon de $n = 10$ bonbons **sans remise**.\n- Les tirages ne sont pas indépendants (sans remise), donc $X$ **ne suit pas** une loi binomiale.\n\nLa loi adaptée est l’hypergéométrique :\n\n- Taille de la population : $N = 300$.\n- Nombre de “succès” (carambars) : $K = 100$.\n- Taille de l’échantillon : $n = 10$.\n\nOn a alors :\n\n$$X \\sim \\mathcal{H}(N = 300,\\; K = 100,\\; n = 10).$$\n\n---\n\n4. **Marie et les fautes d’orthographe (1 % / 2 %)**\n\n- Le livre a $600$ pages.\n- Sur les pages paires (300 pages), probabilité de faute $p_1 = 0{,}01$.\n- Sur les pages impaires (300 pages), probabilité de faute $p_2 = 0{,}02$.\n- La probabilité de “succès” (faute) **n’est pas la même** sur tous les essais (pages), donc $X$ **ne suit pas** une loi binomiale.\n\nUne modélisation naturelle est de décomposer :\n\n- On suppose l'indépendance entre les pages.\n- $X_{\\text{paires}}$ : nombre de fautes sur les pages paires,\n- $X_{\\text{impaires}}$ : nombre de fautes sur les pages impaires.\n\nOn a :\n\n$$\nX_{\\text{paires}} \\sim \\mathcal{B}(300,\\; 0{,}01), \\qquad\nX_{\\text{impaires}} \\sim \\mathcal{B}(300,\\; 0{,}02),\n$$\n\net\n\n$$\nX = X_{\\text{paires}} + X_{\\text{impaires}}.\n$$\n\n---\n\n5. **John et les fautes d’orthographe (Bernoulli 0.01)**\n\n- Le livre a $600$ pages.\n- Sur chaque page, le nombre de fautes suit une loi de Bernoulli de paramètre $p = 0{,}01$ (on suppose indépendance entre pages).\n- Cette fois, la probabilité de faute est la même pour toutes les pages, et le nombre total de pages $n = 600$ est fixé.\n\nAinsi, $X$ suit une loi binomiale :\n\n$$X \\sim \\mathcal{B}(n = 600,\\; p = 0{,}01).$$\n\n\n## Exercice 2\n\nSoit $X_1, \\dots, X_n$ une suite de variables aléatoires indépendantes telles que, pour tout $k = 1, \\dots, n$,\n\n$$\\mathbb{P}(X = \\pm 1) = \\frac{1}{2}.$$\n\nSoit\n$$S = \\sum_{i=1}^n X_i.$$\nOn parle de **marche aléatoire symétrique**, où le point de départ est $0$ avec un déplacement aléatoire à gauche ou à droite à chaque temps.\n\n1. Calculer $E(S)$ et $V(S)$.\n\nNous supposons maintenant que\n\n$$\\mathbb{P}(X = 1) = p \\quad \\text{et} \\quad \\mathbb{P}(X = -1) = 1 - p.$$\n\nSi $p > \\dfrac{1}{2}$, on parle de **marche aléatoire à dérive positive**.\n\n2. Montrer qu’on peut réécrire $X = aY + b$, avec $Y \\sim B(p)$, et $a$ et $b$ deux constantes à déterminer.  \n3. En déduire $E(S)$ et $V(S)$.\n\n\n## Correction — Exercice 2\n\nOn considère une suite de variables aléatoires indépendantes $X_1, \\dots, X_n$ telles que\n$$\\mathbb{P}(X = \\pm 1) = \\frac{1}{2}.$$\n\nOn pose :\n$$S = \\sum_{i=1}^n X_i.$$\n\n---\n\n#### 1) Calculer $E(S)$ et $V(S)$ dans le cas symétrique\n\nPour chaque $X_i$ :\n\n- $\\mathbb{E}(X_i) = 1 \\cdot \\frac{1}{2} + (-1) \\cdot \\frac{1}{2} = 0$,\n\n- $\\mathbb{E}(X_i^2) = 1^2 = 1$ d’où $\\operatorname{Var}(X_i) = \\mathbb{E}(X_i^2) - \\mathbb{E}(X_i)^2 = 1$.\n\n\nPar linéarité de l’espérance :\n$$\\mathbb{E}(S) = \\sum_{i=1}^n \\mathbb{E}(X_i) = 0,$$\n\nComme les $X_i$ sont indépendantes :\n$$\\operatorname{Var}(S) = \\sum_{i=1}^n \\operatorname{Var}(X_i) = n.$$\n\n---\n\nNous supposons maintenant que\n$$\\mathbb{P}(X = 1) = p \\quad \\text{et} \\quad \\mathbb{P}(X = -1) = 1 - p,$$\navec $p > \\frac{1}{2}$ (marche aléatoire à dérive positive).\n\n---\n\n#### 2) Montrer que l’on peut réécrire $X = aY + b$, avec $Y \\sim B(p)$\n\nIl faut jouer ici avec le support des variables aléatoires. L'idée est de partir d'une variable de Bernoulli $Y$ qui prend les valeurs $0$ et $1$, et de la transformer linéairement pour obtenir une variable $X$ qui prend les valeurs $-1$ et $1$. Comme $P(X = 1) = p$ et $P(X = -1) = 1 - p$, on peut définir $Y$ comme une variable de Bernoulli telle que $P(Y = 1) = p$ et $P(Y = 0) = 1 - p$.\n\nOn cherche $a$ et $b$ tels que :\n- si $Y = 1$, alors $X = 1$,\n- si $Y = 0$, alors $X = -1$.\n\nOn résout :\n$$\n\\begin{cases}\na \\cdot 1 + b = 1,\\\\\na \\cdot 0 + b = -1.\n\\end{cases}\n$$\n\nD'où $b = -1$ et $a = 2$.\n\nAinsi :\n$$X = 2Y - 1 \\quad \\text{avec} \\quad Y \\sim \\text{Bernoulli}(p).$$\n\n---\n\n####  3) En déduire $E(S)$ et $V(S)$ dans le cas général\n\nComme $S = \\sum_{i=1}^n X_i$ et $X_i = 2Y_i - 1$ avec $Y_i \\sim B(p)$ indépendantes :\n\nPar linéarité de l’espérance :\n\n$$\n\\mathbb{E}(X_i) = \\mathbb{E}{(2Y_i - 1)} =\n2\\,\\mathbb{E}(Y_i) - 1 = 2p - 1.\n$$\n\nCalculons la variance :\n\nOn sait que pour toute variable aléatoire $X = aY + b$, on a $\\operatorname{Var}(X) = a^2\\,\\operatorname{Var}(Y)$. Donc :\n$$\n\\operatorname{Var}(X_i) = \\operatorname{Var}(2Y_i - 1) = 2^2\\,\\operatorname{Var}(Y_i) =\n   4\\,\\operatorname{Var}(Y_i) = 4\\,p(1 - p).\n$$\nDonc :\n\n$$\n\\mathbb{E}(S) = \\sum_{i=1}^n \\mathbb{E}(X_i) = n(2p - 1),\n$$\n\n$$\n\\operatorname{Var}(S) = \\sum_{i=1}^n \\operatorname{Var}(X_i) = 4n\\,p(1 - p).\n$$\n\n---\n\n####  Résumé final\n\n| Type de marche | $\\mathbb{E}(S)$ | $\\operatorname{Var}(S)$ |\n|---------------|----------------|-------------------------|\n| Symétrique ($p = \\tfrac12$) | $0$ | $n$ |\n| Dérive positive ($p > \\tfrac12$) | $n(2p - 1)$ | $4n\\,p(1 - p)$ |\n\n## Exercice 3\n\nIdentifier les lois des variables aléatoires suivantes, en se basant sur leur fonction génératrice des moments :\n\n1. $M_X(t) = 0.8 e^t + 0.2$\n\n2. $M_Y(t) = \\dfrac{0.1 e^t}{1 - 0.9 e^t}$\n\n3. $M_Z(t) = (0.3 e^t + 0.7)^{14}$\n\n## Correction — Exercice 3\n### Rappel : fonctions génératrices des moments\n\n1. **Loi de Bernoulli de paramètre $p$**\n\nOn a\n$$\n\\mathbb{P}(X = 1) = p, \\qquad \\mathbb{P}(X = 0) = 1 - p.\n$$\n\nAlors la fonction génératrice des moments est :\n$$\nM_X(t) = \\mathbb{E}(e^{tX})\n= (1-p) e^{t \\cdot 0} + p e^{t \\cdot 1}\n= (1 - p) + p e^t.\n$$\n\n2. **Loi géométrique de paramètre $p$ sur $\\{1,2,\\dots\\}$**\n\nOn prend la convention\n$$\n\\mathbb{P}(Y = k) = (1 - p)^{k-1} p, \\quad k \\ge 1.\n$$\n\nAlors\n$$\nM_Y(t) = \\mathbb{E}(e^{tY})\n= \\sum_{k=1}^{\\infty} e^{tk} (1-p)^{k-1} p\n= p e^t \\sum_{k=0}^{\\infty} \\big((1-p)e^t\\big)^k.\n$$\n\nPour $| (1-p)e^t | < 1$, c’est une série géométrique :\n$$\n\\sum_{k=0}^{\\infty} r^k = \\frac{1}{1-r}.\n$$\n\nDonc\n$$\nM_Y(t) = \\frac{p e^t}{1 - (1-p) e^t}.\n$$\n\n3. **Loi binomiale $\\mathcal{B}(n,p)$**\n\nSoit $Z \\sim \\mathcal{B}(n,p)$. On peut écrire\n$$\nZ = X_1 + \\cdots + X_n,\n$$\noù les $X_i$ sont indépendantes et suivent toutes une Bernoulli$(p)$.\n\nOn sait que, pour une Bernoulli$(p)$,\n$$\nM_{X_i}(t) = (1-p) + p e^t.\n$$\n\nPar indépendance,\n$$\nM_Z(t) = \\mathbb{E}(e^{tZ})\n= \\mathbb{E}\\big(e^{t(X_1 + \\cdots + X_n)}\\big)\n= \\prod_{i=1}^n \\mathbb{E}(e^{tX_i})\n= \\big((1-p) + p e^t\\big)^n.\n$$\n\n---\n\n### Identification des lois\n\n1. On a\n$$\nM_X(t) = 0.8 e^t + 0.2 = 0.2 + 0.8 e^t.\n$$\n\nEn comparant avec $(1-p) + p e^t$, on obtient $p = 0.8$.\n\n> Donc $X$ suit une loi de Bernoulli de paramètre $0.8$ :\n> $$\n> X \\sim \\text{Bernoulli}(0.8).\n> $$\n\n2. On a\n$$\nM_Y(t) = \\dfrac{0.1 e^t}{1 - 0.9 e^t}.\n$$\n\nEn comparant avec\n$$\nM_Y(t) = \\frac{p e^t}{1 - (1-p) e^t},\n$$\non lit $p = 0.1$ et $1-p = 0.9$.\n\n> Donc $Y$ suit une loi géométrique de paramètre $p = 0.1$ (sur $\\{1,2,\\dots\\}$) :\n> $$\n> Y \\sim \\text{Géométrique}(p = 0.1).\n> $$\n\n3. On a\n$$\nM_Z(t) = (0.3 e^t + 0.7)^{14}\n= \\big( (1 - 0.3) + 0.3 e^t \\big)^{14}.\n$$\n\nEn comparant avec\n$$\nM_Z(t) = \\big( (1-p) + p e^t \\big)^n,\n$$\non obtient $p = 0.3$ et $n = 14$.\n\n> Donc $Z$ suit une loi binomiale $\\mathcal{B}(14, 0.3)$ :\n> $$\n> Z \\sim \\mathcal{B}(n = 14, p = 0.3).\n> $$\n\n## Exercice 4\n\nSoit $X$ une variable aléatoire telle que\n$$\\mathbb{P}(X = k) = \\frac{k}{10} \\quad \\text{pour } k = 1, 2, 3, 4.$$\n\nSoit $Y$ une variable aléatoire indépendante de $X$, et suivant la même distribution.\n\nCalculer la loi de probabilité de $X + Y$.\n\n## Correction — Exercice 4\n\n\nOn a\n$$\n\\mathbb{P}(X = k) = \\frac{k}{10}, \\quad k = 1,2,3,4,\n$$\net $Y$ est indépendante de $X$ et de même loi.\n\n\n|       | Y = 1 | Y = 2 | Y = 3 | Y = 4 |\n| ----- | ----- | ----- | ----- | ----- |\n| X = 1 | 2     | 3     | 4     | 5     |\n| X = 2 | 3     | 4     | 5     | 6     |\n| X = 3 | 4     | 5     | 6     | 7     |\n| X = 4 | 5     | 6     | 7     | 8     |\n\n\nLa variable $S = X + Y$ prend des valeurs entières de $2$ à $8$.\n\n\nLa variable aléatoire $X + Y$ prend ses valeurs dans $\\{2, \\dots, 8\\}$.  \n\nPour $k \\in \\{2, \\dots, 8\\}$, nous avons en utilisant la formule des probabilités totales :\n\n$$\n\\mathbb{P}(X + Y = k)\n= \\sum_{l=1}^{4} \\mathbb{P}(X + Y = k \\mid Y = l)\\,\\mathbb{P}(Y = l)\n$$\n\n$$\n= \\sum_{l=1}^{4} \\mathbb{P}(Y = k - l \\mid Y = l)\\,\\mathbb{P}(Y = l)\n$$\n\n$$\n= \\sum_{l=1}^{4} \\mathbb{P}(X = k - l)\\,\\mathbb{P}(Y = l) \\quad \\text{car $X$ et $Y$ sont indépendantes.}\n$$\n\noù la somme porte sur les $k$ tels que $1 \\le k \\le 4$ et $1 \\le k- l \\le 4$.\n\nOn note\n$$\np_1 = 0.1,\\quad p_2 = 0.2,\\quad p_3 = 0.3,\\quad p_4 = 0.4.\n$$\n\nOn calcule alors, cas par cas :\n\n- Pour $k = 2$  \n  $$(X,Y) = (1,1) \\quad\\Rightarrow\\quad \\mathbb{P}(S=2) = p_1 p_1 = 0.1\\times 0.1 = 0.01.$$\n\n- Pour $k = 3$  \n  $$(X,Y) = (1,2),(2,1)$$\n  $$\n  \\mathbb{P}(S=3) = p_1 p_2 + p_2 p_1 = 0.1\\times 0.2 + 0.2\\times 0.1 = 0.04.\n  $$\n\n- Pour $s = 4$  \n  $$(X,Y) = (1,3),(2,2),(3,1)$$\n  $$\n  \\mathbb{P}(S=4) = p_1 p_3 + p_2 p_2 + p_3 p_1\n  = 0.1\\times 0.3 + 0.2\\times 0.2 + 0.3\\times 0.1\n  = 0.10.\n  $$\n\n- Pour $k = 5$  \n  $$(X,Y) = (1,4),(2,3),(3,2),(4,1)$$\n  $$\n  \\mathbb{P}(S=5) = p_1 p_4 + p_2 p_3 + p_3 p_2 + p_4 p_1\n  = 0.04 + 0.06 + 0.06 + 0.04 = 0.20.\n  $$\n\n- Pour $k = 6$  \n  $$(X,Y) = (2,4),(3,3),(4,2)$$\n  $$\n  \\mathbb{P}(S=6) = p_2 p_4 + p_3 p_3 + p_4 p_2\n  = 0.2\\times 0.4 + 0.3\\times 0.3 + 0.4\\times 0.2\n  = 0.08 + 0.09 + 0.08 = 0.25.\n  $$\n\n- Pour $k = 7$  \n  $$(X,Y) = (3,4),(4,3)$$\n  $$\n  \\mathbb{P}(S=7) = p_3 p_4 + p_4 p_3\n  = 0.3\\times 0.4 + 0.4\\times 0.3\n  = 0.24.\n  $$\n\n- Pour $k = 8$  \n  $$(X,Y) = (4,4) \\quad\\Rightarrow\\quad \\mathbb{P}(S=8) = p_4 p_4 = 0.4\\times 0.4 = 0.16.$$\n\nOn obtient donc la loi de $S = X+Y$ :\n\n$$\n\\begin{array}{c|ccccccc}\ns      & 2    & 3    & 4    & 5    & 6    & 7    & 8    \\\\ \\hline\n\\mathbb{P}(S=s) & 0.01 & 0.04 & 0.10 & 0.20 & 0.25 & 0.24 & 0.16\n\\end{array}\n$$\n\n(On vérifie que la somme vaut bien $1$: $0.01 + 0.04 + 0.10 + 0.20 + 0.25 + 0.24 + 0.16 = 1$.)\n\n## Exercice 5\n\nUn statisticien a modélisé le nombre de mots d’une recherche sur internet en utilisant une loi de Poisson. Supposons que la longueur moyenne est de 3 mots, et soit $X$ le nombre de mots d’une recherche. Comme une recherche ne peut pas être vide, nous utilisons une modélisation par une loi de probabilité restreinte définie par\n\n$$\\mathbb{P}(X = k) = \\mathbb{P}(Y = k \\mid Y \\neq 0) \\quad \\text{où } Y \\sim \\text{Pois}(\\lambda).$$\n\n1) Trouver la loi de $X$.  \n2) Donner la valeur de $\\lambda$ correspondant à une longueur moyenne de 3 mots.  \n3) Quelle est la probabilité d’avoir une recherche de 6 mots ou plus ?\n\n## Correction — Exercice 5\n\n1) Trouver la loi de $X$\n\nPour déterminer la loi d'une variable aléatoire $X$ , il faut d'abord définir son support, c'est-à-dire l'ensemble des valeurs que $X$ peut prendre avec une probabilité non nulle. Dans ce cas, le support de $X$ est l'ensemble des entiers naturels positifs, car une recherche ne peut pas être vide.\n\nNous notons le support de $X$ par $\\Omega_X = \\{1, 2, 3, \\ldots\\}$ = $\\mathbb{N}^*$.\n\nUne fois le support identifié, nous pouvons calculer la probabilité associée à chaque valeur du support de $X$. \n\nRappelons nous que l'ensemble ${X \\in A} = {w \\in \\Omega : X(w) \\in A} = X^-1(A)$.\nPour tout $k \\in \\Omega_X$, nous avons :\n\n${X = k} = {w \\in \\Omega : X(w) = k} = X^{-1}(\\{k\\})$. où $\\Omega$ est l'ensemble des issues possibles de l'expérience aléatoire.\n\nDonc, pour tout $k \\in \\Omega_X$, nous avons :\n\nOn part de la définition de $X$ comme loi de Poisson **conditionnée à être non nulle** :\n\n1. **Définition de $X$ comme loi restreinte :**\n   $$\n   \\mathbb{P}(X = k)\n   = \\mathbb{P}(Y = k \\mid Y \\neq 0).\n   $$\n   C’est donné dans l’énoncé : $X$ est la loi de $Y$ sachant que $Y$ ne vaut pas 0.\n\n2. **Formule de probabilité conditionnelle :**\n   $$\n   \\mathbb{P}(Y = k \\mid Y \\neq 0)\n   = \\frac{\\mathbb{P}(Y = k \\cap Y \\neq 0)}{\\mathbb{P}(Y \\neq 0)}.\n   $$\n   Par définition :  \n   $\\displaystyle \\mathbb{P}(A \\mid B) = \\frac{\\mathbb{P}(A \\cap B)}{\\mathbb{P}(B)}$.\n\n3. **Simplification de l'intersection :**\n   $$\n   \\mathbb{P}(Y = k \\cap Y \\neq 0) = \\mathbb{P}(Y = k)\n   $$\n   car si $k \\ge 1$, alors l'événement « $Y = k$ » implique automatiquement « $Y \\neq 0$ ».  \n   Donc l’intersection ne change rien.\n\n4. **Remplacement du dénominateur :**\n   $$\n   \\mathbb{P}(Y \\neq 0) = 1 - \\mathbb{P}(Y = 0).\n   $$\n   C’est la propriété générale :  \n   $\\displaystyle \\mathbb{P}(B^c) = 1 - \\mathbb{P}(B)$.\n\n5. **Utilisation de la formule de la loi de Poisson :**\n   $$\n   \\mathbb{P}(Y = k) = e^{-\\lambda} \\frac{\\lambda^k}{k!},\n   \\qquad\n   \\mathbb{P}(Y = 0) = e^{-\\lambda}.\n   $$\n\n6. **Substitution dans la formule :**\n   $$\n   \\mathbb{P}(X = k)\n   = \\frac{e^{-\\lambda} \\frac{\\lambda^k}{k!}}{1 - e^{-\\lambda}}.\n   $$\n\n\n\n\n\n###  Résultat final :\n$$\n\\boxed{\\mathbb{P}(X = k) = \\frac{e^{-\\lambda} }{1 - e^{-\\lambda}} \\cdot \\frac{\\lambda^k}{k!}, \\quad k = 1, 2, 3, \\ldots}\n$$\n\n\n### 2) Calcul de l’espérance de $X$\n\nNous avons la loi :\n$$\n\\mathbb{P}(X = k)\n= \\frac{e^{-\\lambda} \\lambda^k}{k!\\,(1 - e^{-\\lambda})}, \\qquad k \\ge 1.\n$$\n\nL’espérance vaut en utilisant le théorème de transfert :\n$$\n\\mathbb{E}(X)\n= \\sum_{k=1}^{+\\infty} k \\, \\mathbb{P}(X = k)\n= \\sum_{k=1}^{\\infty} k \\, \\frac{e^{-\\lambda} \\lambda^k}{k!\\,(1 - e^{-\\lambda})}.\n$$\n\n**On factorise les constantes** :\n$$\n\\mathbb{E}(X)\n= \\frac{e^{-\\lambda}}{1 - e^{-\\lambda}}\n   \\sum_{k=1}^{\\infty} k \\frac{\\lambda^{k}}{k!}.\n$$\n\n---\n\nJustification du passage suivant\n\nOn réécrit :\n$$\nk\\frac{\\lambda^{k}}{k!}\n= \\lambda \\frac{\\lambda^{k-1}}{(k-1)!}.\n$$\n\nEn effet :\n$$\n\\frac{k}{k!} = \\frac{1}{(k-1)!}.\n$$\n\nDonc :\n$$\n\\sum_{k=1}^{\\infty} k \\frac{\\lambda^k}{k!}\n= \\lambda \\sum_{k=1}^{\\infty} \\frac{\\lambda^{k-1}}{(k-1)!}.\n$$\n\nOn effectue le **changement d’indice**  $j = k - 1$ :\n\n- quand $k = 1$, $j = 0$,\n- quand $k \\to \\infty$, $j \\to \\infty$.\n\nD’où :\n$$\n\\sum_{k=1}^{\\infty} \\frac{\\lambda^{k-1}}{(k-1)!}\n= \\sum_{j=0}^{\\infty} \\frac{\\lambda^{j}}{j!}.\n$$\n\nOr cette somme est la **série de Taylor de l’exponentielle** :\n$$\n\\sum_{j=0}^{\\infty} \\frac{\\lambda^{j}}{j!} = e^{\\lambda}.\n$$\n\n\n\nOn revient à l’expression de l’espérance\n\nOn obtient donc :\n$$\n\\mathbb{E}(X)\n= \\frac{e^{-\\lambda}}{1 - e^{-\\lambda}} \\cdot \\lambda e^{\\lambda}.\n$$\nPuis en simplifiant \\( e^{-\\lambda} e^{\\lambda} = 1 \\) :\n$$    \n\\boxed{\n\\mathbb{E}(X) = \\frac{\\lambda}{1 - e^{-\\lambda}}.\n}\n$$    \n\n---\n\n### Détermination de $\\lambda$ pour une espérance de 3\n\nEn résolvant :\n$$\n\\frac{\\lambda}{1 - e^{-\\lambda}} = 3,\n$$\n\n::: {#74468f88 .cell execution_count=1}\n``` {.python .cell-code}\n#Utilisons Newton-Raphson pour résoudre l'équation\n\nfrom scipy.optimize import newton\nimport numpy as np\n\n# Définir la fonction\ndef equation(lmbda):\n    return lmbda / (1 - np.exp(-lmbda)) - 3\n\n# Résoudre avec Newton-Raphson\nlambda_solution = newton(equation, x0=1.0)\nprint(lambda_solution)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n2.8214393721220787\n```\n:::\n:::\n\n\non obtient numériquement :\n$$\n\\boxed{\\lambda \\approx 2.82}.\n$$\n\n3) Probabilité d’avoir une recherche de 6 mots ou plus\n\nOn cherche :\n$$\n\\mathbb{P}(X \\ge 6).\n$$\n\nOr :\n$$\n\\mathbb{P}(X \\ge 6) = 1 - \\mathbb{P}(X \\le 5).\n$$\n\nEt comme \\(X\\) est la loi de Poisson tronquée :\n$$\n\\mathbb{P}(X \\le 5)\n= \\sum_{k=1}^{5} \\mathbb{P}(X = k)\n= \\sum_{k=1}^{5}\n\\frac{e^{-\\lambda} \\lambda^k}{k!\\,(1 - e^{-\\lambda})}.\n$$\n\nOn factorise pour simplifier :\n$$  \n\\mathbb{P}(X \\le 5)\n= \\frac{e^{-\\lambda}}{1 - e^{-\\lambda}}\n\\sum_{k=1}^{5} \\frac{\\lambda^k}{k!}.\n$$\n\nDonc :\n\n$$\n\\boxed{\n\\mathbb{P}(X \\ge 6)\n= 1 -\n\\frac{e^{-\\lambda}}{1 - e^{-\\lambda}}\n\\sum_{k=1}^{5} \\frac{\\lambda^k}{k!}\n}.\n$$\n\n---\n\n Avec $\\lambda \\approx 2.82$, on calcule :\n\nLe calcul numérique donne :\n$$\n\\boxed{\\mathbb{P}(X \\ge 6) \\approx 0.07}\n$$\n\nsoit **environ 7%**.\n\n## Exercice 6\n\nLa loi de probabilité jointe de deux variables $X$ et $Y$ est définie par\n\n$$\n\\mathbb{P}(X = x, Y = y)\n= \\frac{1}{e^{2} \\, y! \\, (x - y)!}\n\\qquad \\text{pour } x \\in \\mathbb{N} \\text{ et } y = 0, \\ldots, x.\n$$\n\n1) Trouver la loi de probabilité de $X$.  \n2) En déduire la loi de probabilité de $Y$ sachant que $X = x$.\n\n## Correction — Exercice 6\n\nAvant de résoudre l’exercice, rappelons les formules de pascal :\n\n$$\n\\binom{x}{y} = \\frac{x!}{y! (x-y)!},\n$$\net\n$$\n(a + b)^x = \\sum_{y=0}^{x} \\binom{x}{y} a^y b^{x-y}.\n$$\n\nPour a = 1 et b = 1, on obtient :\n$$\n2^x = \\sum_{y=0}^{x} \\binom{x}{y}.\n$$\n\n1) Trouver la loi de probabilité de $X$\n\nLe support de X est $\\mathbb{N}$.\n\nPour tout $x \\in \\mathbb{N}$, on utilise la formule des probabilités totales :\n$$\n\\mathbb{P}(X = x)\n= \\mathbb{P}(X = x, \\Omega)\n= \\mathbb{P}(X = x, \\bigcup_{y=0}^{x} \\{Y = y\\})\n= \\sum_{y=0}^{x} \\mathbb{P}(X = x, Y = y).\n$$\n\n- $\\Omega$ est l’événement certain.\n- Les événements {Y=0}, \\ldots, {Y=x} sont disjoints et forment une partition des valeurs possibles de Y quand X=x.\n- La formule générale de probabilité totale dit :\n  $$\n  \\mathbb{P}(A) = \\sum_i \\mathbb{P}(A \\cap B_i)\n  \\quad \\text{si les } B_i \\text{ forment une partition}.\n  $$\n\n\nEn théorie de probabilité, on écrit souvent $P(A, B)$ pour $P(A \\cap B)$.\n\nOn remplace par la loi jointe donnée :\n$$\n\\mathbb{P}(X = x)\n= \\sum_{y=0}^{x} \\frac{1}{e^{2} \\, y! \\, (x - y)!}.\n$$\n\nOn factorise :\n$$\n\\mathbb{P}(X = x)\n= \\frac{1}{e^{2}} \\sum_{y=0}^{x} \\frac{1}{y! \\, (x - y)!}.\n$$\n\nOn utilise la formule de pascal pour réécrire la somme :\nOn utilise l’identité :\n$$\n\\binom{x}{y} = \\frac{x!}{y! (x-y)!}.\n$$\n\nD’où :\n$$\n\\frac{1}{y!(x-y)!} = \\frac{1}{x!}\\binom{x}{y}.\n$$\n\nAinsi :\n$$\n\\sum_{y=0}^{x} \\frac{1}{y!(x-y)!}\n= \\frac{1}{x!} \\sum_{y=0}^{x} \\binom{x}{y}.\n$$\n\nOr :\n$$\n\\sum_{y=0}^{x} \\binom{x}{y} = 2^x,\n$$\ncar c’est le développement de \\((1+1)^x\\).\n\nOn obtient donc :\nOn obtient :\n$$\n\\mathbb{P}(X = x)\n= \\frac{1}{e^{2}} \\cdot \\frac{2^x}{x!}\n= e^{-2} \\frac{2^x}{x!}.\n$$\n\n\nNous reconnaissons la loi de Poisson de paramètre $\\lambda = 2$.\n\n\n$$\n\\boxed{X \\sim \\text{Poisson}(2)}.\n$$\n\n2) Soient $x, y \\in \\mathbb{N}$. Nous avons\n\n$$\n\\mathbb{P}(Y = y \\mid X = x)\n= \\frac{\\mathbb{P}(X = x, Y = y)}{\\mathbb{P}(X = x)},\n$$\n\net cette probabilité est nulle si $y > x$.  \nSi $y \\le x$, nous avons :\n\n$$\n\\mathbb{P}(Y = y \\mid X = x)\n= \\frac{1}{e^{2} y! (x - y)!} \\times \\frac{e^{2} x!}{2^{x}}\n$$\n\n$$\n= \\binom{x}{y} \\left(\\frac{1}{2}\\right)^{y} \\left(\\frac{1}{2}\\right)^{x - y}.\n$$\n\nNous reconnaissons la densité d’une loi binomiale de paramètres $m = x$ et  \n$p = \\frac{1}{2}$.\n\nDonc :\n$$\n\\boxed{Y \\mid X = x \\sim \\mathcal{B}\\left(x, \\frac{1}{2}\\right)}.\n$$\n\n## Exercice 7\n\nSoient $X_1, \\ldots, X_n$ une suite de variables aléatoires i.i.d. suivant une loi $\\mathcal{L}$.\nNous supposons que les paramètres de la loi $\\mathcal{L}$ sont entièrement caractérisés\npar les moments $\\mu_1 \\equiv \\mathbb{E}(X), \\ldots, \\mu_k \\equiv \\mathbb{E}(X^k)$.\nLa *méthode des moments* consiste à estimer les paramètres de la loi $\\mathcal{L}$\nen remplaçant les moments inconnus par leur estimateur empirique\n\n$$\n\\hat{\\mu}_k = \\frac{1}{n} \\sum_{i=1}^n (X_i)^k.\n$$\n\nNous supposons que les variables aléatoires $X_1, \\ldots, X_n$ suivent une loi de Bernoulli\nde paramètre $p$.\n\n1) Exprimer $p$ en fonction du premier moment de $X$.  \n2) En déduire une estimation de $p$ basée sur l’échantillon.\n\nNous supposons que les variables aléatoires $X_1, \\ldots, X_n$ suivent une loi binomiale\nde paramètres $m$ et $p$.\n\n3) Exprimer $m$ et $p$ en fonction des deux premiers moments de $X$.  \n4) En déduire une estimation de $m$ et de $p$ basée sur l’échantillon.\n\n### Correction Exercice 7\n\n1) Nous avons  \n$$\np = \\mathbb{E}(X) = \\mu_1.\n$$\n\n2) Le paramètre \\(p\\) est donc estimé par\n$$\n\\hat{p} = \\hat{\\mu}_1 = \\frac{1}{n} \\sum_{i=1}^n X_i.\n$$\n\nNotons que dans ce cas précis, il s’agit d’un estimateur sans biais.\n\n---\n\n3) Nous avons, pour une loi binomiale \\(\\mathcal{B}(m,p)\\) :\n$$\n\\mathbb{E}(X) = mp\n\\qquad\\text{et}\\qquad\n\\operatorname{Var}(X) = \\mathbb{E}(X^2) - \\{\\mathbb{E}(X)\\}^2 = mp(1 - p).\n$$\n\nNous en déduisons que\n$$\n1 - p = \\frac{\\mu_2}{\\mu_1} - \\mu_1,\n$$\n\npuis\n$$\np = 1 - \\frac{\\mu_2}{\\mu_1} + \\mu_1,\n$$\n\net\n$$\nm = \\frac{\\mu_1}{1 - \\frac{\\mu_2}{\\mu_1} + \\mu_1}.\n$$\n\n---\n\n4) Nous obtenons les estimateurs par la méthode des moments :\n\n$$\n\\hat{p} = 1 - \\frac{\\hat{\\mu}_2}{\\hat{\\mu}_1} + \\hat{\\mu}_1,\n$$\n\n$$\n\\hat{m} = \n\\frac{\\hat{\\mu}_1}{1 - \\frac{\\hat{\\mu}_2}{\\hat{\\mu}_1} + \\hat{\\mu}_1}.\n$$\n\nÀ noter que ces estimateurs n’ont pas de raison particulière d’être sans biais.\n\n## Exercice 8\n\nSoit $X \\sim \\text{Pois}(\\lambda)$.\n\n1) Donner la fonction génératrice des moments de $X$ et sa fonction caractéristique.  \n\n2) Calculer $\\mathbb{E}(X^3)$.  \n3) Calculer la probabilité que $X$ soit impair.  \n   Utiliser un développement en série entière de $e^\\lambda$ et $e^{-\\lambda}$.  \n\n4) Calculer $\\mathbb{E}(X!)$.\n\n## Correction — Exercice 8\n\nAvant de commencer, rappelons que :\n\n{X impair} qui s'écrit en latex $X \\text{ impair}$ est l'ensemble des issues où la variable aléatoire $X$ prend des valeurs impaires. Cet ensemble peut être représenté comme l'union des événements disjoints {X = 1}, {X = 3}, {X = 5}, etc.\n\nOn peut donc écrire :\n\nL’événement $X$ est impair s’écrit :\n\n$$\n\\{X \\text{ impair}\\}\n= \\{\\omega \\in \\Omega : X(\\omega) \\text{ est impair}\\}\n$$\n\nPar définition d’un entier impair :\n\n$$  \n\\{X \\text{ impair}\\}\n= \\{\\omega : X(\\omega) = 1\\}\n  \\cup \\{\\omega : X(\\omega) = 3\\}\n  \\cup \\{\\omega : X(\\omega) = 5\\}\n  \\cup \\cdots\n$$\n\nC’est-à-dire :\n\n$$\n\\{X \\text{ impair}\\}\n= \\{\\omega \\in \\Omega : \\exists\\, k \\in \\mathbb{N}, \\; X(\\omega) = 2k + 1\\}.\n$$\n\nEn notation abrégée :\n\n$$\n\\{X \\text{ impair}\\}\n= \\{X = 1\\} \\cup \\{X = 3\\} \\cup \\{X = 5\\} \\cup \\cdots\n$$\n\net de manière compacte :\n\n$$\n\\boxed{\n\\{X \\text{ impair}\\}\n= \\bigcup_{k=0}^{\\infty} \\{X = 2k + 1\\}.\n}\n$$\n\n\n1) Fonction génératrice des moments et fonction caractéristique\n\n$\\textbf{1)}$ Soit $t > 0$. Nous avons\n\n$$\nM_X(t)\n= \\mathbb{E}\\{\\exp(tX)\\}\n= e^{-\\lambda} \\sum_{k=0}^{+\\infty} \\exp(tk)\\, \\frac{\\lambda^k}{k!}\n$$\n\n$$ \n= e^{-\\lambda} \\sum_{k=0}^{+\\infty} \\frac{\\{\\lambda \\exp(t)\\}^k}{k!}\n= e^{-\\lambda} \\times \\exp\\{\\lambda \\exp(t)\\}\n$$\n\n$$\n= \\exp\\{\\lambda(\\exp(t) - 1)\\}.\n$$\n\nOn vérifie au passage que la fonction génératrice des moments est ici bien\ndéfinie pour tout $t \\in \\mathbb{R}$.\n\nLa fonction caractéristique se calcule de façon analogue :\n\n$$\n\\varphi_X(t)\n= \\mathbb{E}\\{\\exp(itX)\\}\n= e^{-\\lambda} \\sum_{k=0}^{+\\infty} \\exp(itk)\\, \\frac{\\lambda^k}{k!}\n$$\n\n$$\n= e^{-\\lambda} \\sum_{k=0}^{+\\infty} \\frac{\\{\\lambda \\exp(it)\\}^k}{k!}\n= e^{-\\lambda} \\times \\exp\\{\\lambda \\exp(it)\\}\n$$\n\n$$\n= \\exp\\{\\lambda(\\exp(it) - 1)\\}.\n$$\n\n2) Calcul de $\\mathbb{E}(X^3)$\n\nNous utilisons la fonction génératrice des moments, qui est ici indéfiniment dérivable.\n\nRappel :\n$$\nM_X(t) = \\exp\\{\\lambda(e^t - 1)\\}.\n$$\n\nNous calculons ses dérivées successives.\n\n#### Première dérivée\n\n$$\nM_X'(t)\n= \\lambda e^t \\exp\\{\\lambda(e^t - 1)\\}.\n$$\n#### Deuxième dérivée\n\n$$\nM_X''(t)\n= \\left(\\lambda e^t + \\lambda^2 e^{2t}\\right)\n  \\exp\\{\\lambda(e^t - 1)\\}.\n$$\n#### Troisième dérivée\n\n$$\nM_X^{(3)}(t)\n= \\left(\\lambda e^t + 3\\lambda^2 e^{2t} + \\lambda^3 e^{3t}\\right)\n  \\exp\\{\\lambda(e^t - 1)\\}.\n$$\nNous pouvons maintenant obtenir le troisième moment.\n\n---\n\n### Calcul de  $\\mathbb{E}(X^3)$\n\nPar définition :\n$$\n\\mathbb{E}(X^3) = M_X^{(3)}(0).\n$$\n\nComme $e^{0}=1$, nous obtenons :\n$$\nM_X^{(3)}(0)\n= \\left(\\lambda + 3\\lambda^2 + \\lambda^3\\right)\\exp\\{0\\}\n= \\lambda^3 + 3\\lambda^2 + \\lambda.\n$$\n\n$$\n\\boxed{\\mathbb{E}(X^3)=\\lambda^3 + 3\\lambda^2 + \\lambda.}\n$$\n\n3) Calcul de la probabilité que $X$ soit impair\n\nNous avons\n\n$$\n\\mathbb{P}(X \\text{ impair})\n= \\mathbb{P}\\left(\\bigcup_{k=0}^{\\infty} \\{X = 2k + 1\\}\\right)\n= \\sum_{k=0}^{\\infty} \\mathbb{P}(X = 2k + 1)\n$$\n\n$$\n= \\sum_{k=0}^{\\infty} e^{-\\lambda} \\frac{\\lambda^{2k + 1}}{(2k + 1)!}\n= e^{-\\lambda} \\sum_{k=0}^{\\infty} \\frac{\\lambda^{2k + 1}}{(2k + 1)!}.\n$$ \n\n\nD’autre part,\n$$\ne^\\lambda = \\sum_{k=0}^{+\\infty} \\frac{\\lambda^k}{k!}\n= 1 + \\lambda + \\frac{\\lambda^2}{2} + \\frac{\\lambda^3}{3!} + \\frac{\\lambda^4}{4!} + \\cdots\n$$  \n\net\n$$\ne^{-\\lambda} = \\sum_{k=0}^{+\\infty} \\frac{(-\\lambda)^k}{k!}\n= 1 - \\lambda + \\frac{\\lambda^2}{2} - \\frac{\\lambda^3}{3!} + \\frac{\\lambda^4}{4!} + \\cdots\n$$\n\nDonc,\n$$\ne^\\lambda - e^{-\\lambda}\n= \\sum_{k=0}^{+\\infty} \\frac{\\lambda^k}{k!}\n+ \\sum_{k=0}^{+\\infty} \\frac{(-1)^{k+1}\\lambda^k}{k!}\n= 2 \\sum_{k=0}^{+\\infty} \\frac{\\lambda^{2k+1}}{(2k+1)!}.\n$$\n\nEt finalement,\n$$\n\\mathbb{P}(X \\text{ impair})\n= \\frac{e^{-\\lambda}(e^\\lambda - e^{-\\lambda})}{2}\n= \\frac{1 - e^{-2\\lambda}}{2}.\n$$\n\n4) Nous avons\n\n$$\nE(X!) = \\sum_{k=0}^{+\\infty} e^{-\\lambda} \\frac{\\lambda^k}{k!} \\times k!\n$$\n\n$$\n= e^{-\\lambda} \\sum_{k=0}^{+\\infty} \\lambda^k\n= \n\\begin{cases}\n\\dfrac{e^{-\\lambda}}{1 - \\lambda}, & \\text{si } |\\lambda| < 1, \\\\[6pt]\n+\\infty, & \\text{si } |\\lambda| \\ge 1.\n\\end{cases}\n$$\n\n## Exercice 9\n\nSoient \\(X_1, \\ldots, X_n\\) une suite de variables aléatoires i.i.d. suivant une loi de Bernoulli de paramètre \\(p\\).  \nTrouver la loi de \\(X_1\\) sachant que \\(X_1 + \\cdots + X_n = k\\).\n\n---\n\n## Correction Exercice 9\n\nSoit \\(x \\in \\{0,1\\}\\). Nous avons, en utilisant la formule de Bayes :\n\n$$\n\\mathbb{P}(X_1 = x \\mid X_1 + \\cdots + X_n = k)\n= \\frac{\\mathbb{P}(X_1 + \\cdots + X_n = k \\mid X_1 = x)\\mathbb{P}(X_1 = x)}\n{\\mathbb{P}(X_1 + \\cdots + X_n = k)}.\n$$\n\nOr :\n\n$$\n\\mathbb{P}(X_1 + \\cdots + X_n = k) = \\binom{n}{k} p^k (1-p)^{\\,n-k},\n$$\n\npuisque \\(X_1 + \\cdots + X_n \\sim \\mathcal{B}(n,p)\\).\n\n---\n\nEnsuite :\n\n$$\nX_2 + \\cdots + X_n \\sim \\mathcal{B}(n-1,p),\n$$\n\ndonc\n\n$$\n\\mathbb{P}(X_2 + \\cdots + X_n = k - x)\n= \\binom{n-1}{k-x} p^{\\,k-x} (1-p)^{\\,n-1-k+x}.\n$$\n\nDe plus :\n\n$$\n\\mathbb{P}(X_1 = x) = p^x (1-p)^{\\,1-x}.\n$$\n\n---\n\nNous obtenons finalement :\n\n$$\n\\mathbb{P}(X_1 = x \\mid X_1 + \\cdots + X_n = k)\n= \\frac{\\binom{n-1}{k-x} p^{\\,k-x}(1-p)^{\\,n-1-k+x} \\, p^x (1-p)^{\\,1-x}}\n{\\binom{n}{k} p^k (1-p)^{\\,n-k}}.\n$$\n\nEn simplifiant, on trouve :\n\n$$\n\\mathbb{P}(X_1 = x \\mid X_1 + \\cdots + X_n = k)\n= \\frac{\\binom{n-1}{k-x}}{\\binom{n}{k}}.\n$$\n\nOn calcule les deux cas :\n- Si $x = 0$ :\n  $$\n  \\mathbb{P}(X_1 = 0 \\mid X_1 + \\cdots + X_n = k)\n  = \\frac{\\binom{n-1}{k}}{\\binom{n}{k}}\n  = \\frac{(n-1)! \\, k! \\, (n-k)!}{(n-k)! \\, (n)! \\, k!}\n  = \\frac{n-k}{n}.\n  $$\n- Si $x = 1$ :\n  $$\n  \\mathbb{P}(X_1 = 1 \\mid X_1 + \\cdots + X_n = k)\n  = \\frac{\\binom{n-1}{k-1}}{\\binom{n}{k}}\n  = \\frac{(n-1)! \\, (k-1)! \\, (n-k)!}{(n-k)! \\, (n)! \\, k!}\n  = \\frac{k}{n}.\n  $$\n\nDonc \n\n$$\n\\boxed{X_1 \\mid (X_1 + \\cdots + X_n = k) \\sim \\mathcal{B}\\left(1, \\frac{k}{n}\\right)}.\n$$\n\n# 5 Variables aléatoires continues\n\n## Exercice 1\n\nUne variable aléatoire $X$ possède la fonction de densité\n\n$$\nf(x) = c x \\quad \\text{pour } 0 < x < 1.\n$$\n\n1. Trouver la valeur de $c$.  \n2. Calculer $\\mathbb{P}(X < 0.5)$.  \n3. Calculer $\\mathbb{E}(X)$.  \n4. Calculer la fonction génératrice des moments de $X$.\n\n## Correction — Exercice 1\n\nOn considère la densité\n\n$$\nf(x) = c x \\quad \\text{pour } 0 < x < 1.\n$$\n\n---\n\n1. Détermination de $c$\n\n$f$ est une densité si et seulement si elle vérifie les deux conditions suivantes :\n\n1. $f(x) \\ge 0$ pour tout $x \\in \\mathbb{R}$.  \n   C’est le cas ici si $c \\ge 0$.\n2. L’intégrale de $f$ sur $\\mathbb{R}$ vaut 1.  \n   Ici, cela revient à :\n\n\n$$\n\\int_{-\\infty}^{+\\infty} f(x) \\, dx = 1.\n$$\n\nCalcul :\n\n$$\n\\int_{-\\infty}^{+\\infty} f(x) \\, dx\n= \\int_0^1 c x \\, dx\n= c \\left[ \\frac{x^2}{2} \\right]_0^1\n= \\frac{c}{2}.\n$$\n\nDonc :\n\n$$\n\\frac{c}{2} = 1 \\quad \\Longrightarrow \\quad c = 2.\n$$\n\n---\n\n2. Calcul de $\\mathbb{P}(X < 0.5)$\n\n$$\n\\mathbb{P}(X < 0.5) = \\int_{-\\infty}^{0.5} f(x) \\, dx\n= \\int_0^{0.5} 2x \\, dx.\n$$\n\n$$\n= 2 \\left[ \\frac{x^2}{2} \\right]_0^{0.5}\n= (0.5)^2 = 0.25.\n$$\n\n---\n\n3. Calcul de $\\mathbb{E}(X)$\n\n$$\n\\mathbb{E}(X) = \n= \\int_{-\\infty}^{+\\infty} x \\, f(x) \\, dx\n= \\int_0^1 x \\cdot 2x \\, dx\n= \\int_0^1 2x^2 \\, dx.\n$$\n\n$$\n\\int_0^1 2x^2 \\, dx = 2 \\left[ \\frac{x^3}{3} \\right]_0^1 = \\frac{2}{3}.\n$$\n\nDonc :\n\n$$\n\\mathbb{E}(X) = \\frac{2}{3}.\n$$\n\n---\n\n4. Fonction génératrice des moments (MGF)\n\nLa fonction génératrice des moments est :\n\nsoit $t \\in \\mathbb{R}$,\n$$\nM_X(t) = \\mathbb{E}(e^{tX})\n= \\int_{-\\infty}^{+\\infty} e^{tx} \\, f(x) \\, dx\n= \\int_0^1 2x e^{tx} \\, dx.\n$$\n\nOn calcule l’intégrale par parties.\n\nPosons :\n\n- $u = x$ donc $du = dx$\n- $dv = e^{tx} dx$ donc $v = \\frac{e^{tx}}{t}$\n\nAlors :\n\nSi $t \\neq 0$,\n$$\n\\int_0^1 x e^{tx} dx\n= \\left[ \\frac{x e^{tx}}{t} \\right]_0^1\n- \\int_0^1 \\frac{e^{tx}}{t} dx.\n$$\n\nOn obtient :\n\n$$\n\\int_0^1 x e^{tx} dx\n= \\frac{e^{t}}{t}\n- \\frac{1}{t} \\int_0^1 e^{tx} dx.\n$$\n\nOr :\n\n$$\n\\int_0^1 e^{tx} dx\n= \\left[ \\frac{e^{tx}}{t} \\right]_0^1\n= \\frac{e^{t} - 1}{t}.\n$$\n\nDonc :\n\n$$\n\\int_0^1 x e^{tx} dx\n= \\frac{e^{t}}{t}\n- \\frac{e^{t} - 1}{t^2}.\n$$\n\nDonc si $t \\neq 0$,\n\n$$\nM_X(t) = 2 \\left( \n\\frac{e^{t}}{t}\n- \\frac{e^{t} - 1}{t^2}\n\\right).\n$$\n\nEt si $t = 0$, on a :\n\n$$\nM_X(0) = \\mathbb{E}(e^{0}) = \\mathbb{E}(1) = 1.\n$$\n\n---\n\nDonc la fonction génératrice des moments est donnée par :\n$$\n\\boxed{\nM_X(t) =\n\\begin{cases}\n2 \\left( \n\\frac{e^{t}}{t}\n- \\frac{e^{t} - 1}{t^2}\n\\right), & \\text{si } t \\neq 0, \\\\[6pt]\n1, & \\text{si } t = 0.\n\\end{cases}\n}\n$$\n\n## Exercice 2\n\nSoit la fonction définie par  \n$$f(x) = c\\,x(1 - x)$$  \npour $x \\in [0,1]$, et $0$ sinon.\n\n1. Pour quelle valeur de $c$ est-ce une densité de probabilité ?\n\n2. Déterminer la fonction de répartition de cette loi et sa médiane.\n\n## Exercice 2 — Correction\n\nOn considère la fonction :\n\n$$\nf(x) = c\\,x(1-x), \\quad x \\in [0,1],\n$$\n\net $f(x)=0$ sinon.\n\n---\n\n1) Pour quelle valeur de $c$ est-ce une densité ?\n\nPour que $f$ soit une densité, il faut :\n\n$$\n\\int_0^1 c\\,x(1-x)\\,dx = 1.\n$$\n\nCalculons l’intégrale :\n\n$$\n\\int_0^1 x(1-x)\\,dx\n= \\int_0^1 (x - x^2)\\,dx\n= \\left[\\frac{x^2}{2} - \\frac{x^3}{3}\\right]_0^1\n= \\frac12 - \\frac13\n= \\frac16.\n$$\n\nDonc :\n\n$$\nc \\cdot \\frac{1}{6} = 1\n\\quad \\Longrightarrow \\quad c = 6.\n$$\n\n---\n\n2) Fonction de répartition $F(x)$\n\nPour $x < 0$ :\n$$\nF(x) = 0.\n$$\n\nPour $x \\in [0,1]$ :\n$$\nF(x) = \\int_0^x 6\\,t(1-t)\\,dt.\n$$\n\nCalculons :\n\n$$\n\\int_0^x 6(t - t^2)\\,dt\n= 6\\left[ \\frac{t^2}{2} - \\frac{t^3}{3} \\right]_0^x\n= 6\\left( \\frac{x^2}{2} - \\frac{x^3}{3} \\right).\n$$\n\nDonc :\n\n$$\nF(x) = 3x^2 - 2x^3.\n$$\n\n Pour $x > 1$ :\n$$\nF(x) = 1.\n$$\n\n---\n\nMédiane\n\nLa médiane $m$ vérifie :\n\n$$\nF(m) = 0.5.\n$$\n\nDonc :\n\n$$\n3m^2 - 2m^3 = \\frac12.\n$$\n\nSoit :\n\n$$\n2m^3 - 3m^2 + \\frac12 = 0.\n$$\n\nCeci est équivalent à :\n\n$$\n4m^3 - 6m^2 + 1 = 0.\n$$\n\nC'est une équation polynomiale du troisième degré.\nUne racine m de cette équation se trouve dans l'intervalle [0,1].\n\nEtant donné une telle équation, les racines s'écrivent comme le rapport des diviseurs du terme constant sur les diviseurs du coefficient dominant, qui est 4 dans ce cas.\nLes diviseurs de 1 sont $\\pm 1$, et les diviseurs de 4 sont $\\pm 1, \\pm 2, \\pm 4$.\nDonc les racines rationnelles possibles sont $\\pm 1, \\pm \\frac{1}{2}, \\pm \\frac{1}{4}$.\n\nOn doit choisir parmi ces valeurs celles qui sont dans l'intervalle [0,1], c'est-à-dire $1, \\frac{1}{2}, \\frac{1}{4}$.\n\nEn testant ces valeurs dans l'équation, on trouve que la racine dans l'intervalle [0,1] est :\n$$\nm = \\frac{1}{2}\n$$\n\nDonc la médiane est :\n$$\n\\boxed{m = \\frac{1}{2}}.\n$$\n\nUne autre méthode pour déterminer la médiane consiste à remarquer que la fonction de densité admet la droite d'équation $x = \\frac{1}{2}$ comme axe de symétrie.\n\nPour ce faire, on peut soit tracer la fonction de densité : \n\n::: {#7a01a2a5 .cell execution_count=2}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Define density on R, zero outside [0,1]\ndef f(x):\n    return np.where((x>=0)&(x<=1), 6*x*(1-x), 0)\n\n# Real line range\nx = np.linspace(-5, 5, 800)\ny = f(x)\n\nplt.figure(figsize=(7,4))\nplt.plot(x, y, linewidth=2)\nplt.axvline(0.5, linewidth=2, color='red', linestyle='--', label='x=0.5 (médiane)')\nplt.legend()\nplt.xlabel(\"x\")\nplt.ylabel(\"f(x)\")\nplt.title(\"Density f(x)=6x(1-x) on the whole real line (0 outside [0,1])\")\nplt.grid(True)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](ensai1A_files/figure-html/cell-3-output-1.png){width=589 height=376}\n:::\n:::\n\n\nLorsque que la droite d'équation $x = a$, est un axe de symétrie pour une fonction $f$, dérivable, alors $f'(a) = 0$.\n\nEt dans notre cas, on trouve bien $f'(\\frac{1}{2}) = 0$.\nEt $a = \\frac{1}{2}$ est la médiane recherchée.\n\n## Exercice 3\n\nSupposons que la fonction de répartition d’une variable aléatoire $X$, correspondant au temps en mois avant décès pour une personne atteinte de cancer, est donnée par\n\n$$\nF(x) =\n\\begin{cases}\n0, & x \\le 0, \\\\\n1 - e^{-0.03 x^{1.2}}, & x > 0.\n\\end{cases}\n$$\n\n1) Vérifier que $F$ est bien une fonction de répartition.  \n\n2) Calculer la probabilité de survivre au moins 12 mois.  \n\n3) Donner une densité de $X$.\n\n## Correction Exercice 3\n\n1) La fonction $F$ est croissante, continue sur $\\mathbb{R}$, avec  \n$\\displaystyle \\lim_{x \\to -\\infty} F(x) = 0$  \net  \n$\\displaystyle \\lim_{x \\to +\\infty} F(x) = 1$.  \n\nC’est donc bien une fonction de répartition.\n\n---\n\n2) Nous avons\n\n$$\n\\mathbb{P}(X \\ge 12)\n= 1 - F(12)\n= 55.3\\%.\n$$\n\n---\n\n3) Une densité, obtenue par dérivation, vaut\n\n$$\nf(x) =\n\\begin{cases}\n0, & x \\le 0, \\\\[6pt]\n0.03\\, x^{0.2} e^{-0.03 x^{1.2}}, & x > 0.\n\\end{cases}\n$$\n\n## Exercice 4\n\nSoit la fonction $F$ définie par $F(x) = 1 - \\exp(-x/2)$ pour $x > 0$, et $0$ sinon.\n\n1) Justifier que $F$ est une fonction de répartition.  \n2) Déterminer les quantiles d’ordres 0.25 et 0.75.  \n3) Soit $X$ une variable aléatoire suivant cette loi, calculer $\\mathbb{P}(1 < X \\le 2)$.\n\n---\n\n## Correction Exercice 4\n\n1)  \nUne fonction de répartition doit être croissante, cadlag (continue à droite, limites à gauche), avec\n\n$$\n\\lim_{x \\to -\\infty} F(x) = 0\n\\quad\\text{et}\\quad\n\\lim_{x \\to +\\infty} F(x) = 1.\n$$\n\nLa fonction $F(\\cdot)$ est nulle sur $]-\\infty, 0]$.  \nPour tout $x \\in ]0, +\\infty[$, $F(\\cdot)$ est dérivable en $x$ avec\n\n$$\nF'(x) = \\frac{1}{2} \\exp\\left(-\\frac{x}{2}\\right) > 0,\n$$\n\ndonc la fonction $F(\\cdot)$ est bien croissante.  \nElle est continue sur $\\mathbb{R}$, donc en particulier continue à droite et avec des limites à gauche.  \n\nNous obtenons également facilement que\n\n$$\n\\lim_{x \\to -\\infty} F(x) = 0\n\\quad\\text{et}\\quad\n\\lim_{x \\to +\\infty} F(x) = 1.\n$$\n\n---\n\n2) Quantiles\n\nOn cherche $x = F^{-1}(0.25)$ :\n\n$$\nF(x) = 1 - \\exp\\left(-\\frac{x}{2}\\right) = \\frac14\n\\;\\Longleftrightarrow\\;\n\\exp\\left(-\\frac{x}{2}\\right) = \\frac34\n\\;\\Longleftrightarrow\\;\nx = -2 \\ln\\left(\\frac34\\right).\n$$\n\nEnsuite, pour $x = F^{-1}(0.75)$ :\n\n$$\nF(x) = 1 - \\exp\\left(-\\frac{x}{2}\\right) = \\frac34\n\\;\\Longleftrightarrow\\;\n\\exp\\left(-\\frac{x}{2}\\right) = \\frac14\n\\;\\Longleftrightarrow\\;\nx = -2 \\ln\\left(\\frac14\\right).\n$$\n\n---\n\n3) Probabilité\n\nPar définition de la fonction de répartition,\n\n$$\n\\mathbb{P}(1 < X \\le 2)\n= F(2) - F(1)\n= \\exp\\left(-\\frac{1}{2}\\right) - \\exp(-1).\n= 0.238.\n$$\n\n## Exercice 5\n\nSoient $Y$ et $Z$ deux variables aléatoires réelles.  \nSoit $U$ une variable aléatoire indépendante de $Y$ et de $Z$, suivant une loi de Bernoulli de paramètre $\\alpha > 0$.\n\nSoit $X$ la variable aléatoire définie par\n\n$$\nX =\n\\begin{cases}\nY & \\text{si } U = 1,\\\\[4pt]\nZ & \\text{si } U = 0.\n\\end{cases}\n$$\n\nAutrement dit, $X = Y$ avec probabilité $\\alpha$ et $X = Z$ avec probabilité $1 - \\alpha$.  \nOn dit que la loi de $X$ est un *mélange de deux distributions*, ce que l’on note\n\n$$\n\\mathbb{P}_X = \\alpha \\mathbb{P}_Y + (1 - \\alpha)\\mathbb{P}_Z.\n$$\n\n---\n\n1) Montrer que\n\n$$\nF_X(x) = \\alpha F_Y(x) + (1 - \\alpha)F_Z(x).\n$$\n\n---\n\nSoit $X$ une variable aléatoire de fonction de répartition $F_X$ définie par\n\n$$\nF_X(x) =\n\\begin{cases}\n0 & \\text{si } x < 0,\\\\[4pt]\n\\dfrac{1}{8} & \\text{si } x \\in [0,1[,\\\\[8pt]\n\\dfrac{x + 1}{4} & \\text{si } x \\in [1,2[,\\\\[8pt]\n\\dfrac{3}{4} & \\text{si } x \\in [2,5[,\\\\[8pt]\n1 & \\text{si } x \\ge 5.\n\\end{cases}\n$$\n\n---\n\n2) Représenter graphiquement la fonction $F_X$.\n\n---\n\n3) Exprimer la loi de $X$ sous la forme\n\n$$\n\\mathbb{P}_X = \\alpha \\mathbb{P}_{\\text{dis}} + (1 - \\alpha)\\mathbb{P}_{\\text{cont}},\n$$\n\noù $\\alpha \\in [0,1]$,  \n$\\mathbb{P}_{\\text{dis}}$ est une loi de probabilité discrète,  \net $\\mathbb{P}_{\\text{cont}}$ est une loi de probabilité continue, qu’on déterminera.\n\n### Correction Exercice 5\n\n1) Montrons que $F_X(x) = \\alpha F_Y(x) + (1 - \\alpha)F_Z(x)$.\n\nNous savons que si $U = 1$, alors $X = Y$, et si $U = 0$, alors $X = Z$. De ce fait, pour exprimer la fonction de répartition de $X$, en fonction de celles de $Y$ et $Z$, nous devons parvenir à conditionner la variable aléatoire $X$ en fonction de la variable aléatoire $U$. \n\nPour cela, nous utilisons la formule des probabilités totales, suivi par la formule de Bayes.\n\n$$\nF_X(x) = \\mathbb{P}(X \\le x)\n= \\mathbb{P}(X \\le x \\cap \\Omega)\n= \\mathbb{P}(X \\le x \\cap \\{U = 1\\}) + \\mathbb{P}(X \\le x \\cap \\{U = 0\\}).\n$$\n\nNous utilisons ensuite la formule de Bayes pour chaque terme :\n\n$$\nF_X(x) = \\mathbb{P}(X \\le x \\mid U = 1) \\mathbb{P}(U = 1)\n+ \\mathbb{P}(X \\le x \\mid U = 0) \\mathbb{P}(U = 0).\n$$\n\nOr, par définition de $X$ :\n$$\n\\mathbb{P}(X \\le x \\mid U = 1) \n= \\mathbb{P}(Y \\le x \\mid U = 1)\n= \\mathbb{P}(Y \\le x) = F_Y(x),\n$$\n\net\n$$\n\\mathbb{P}(X \\le x \\mid U = 0) \n= \\mathbb{P}(Z \\le x \\mid U = 0)\n= \\mathbb{P}(Z \\le x) = F_Z(x).\n$$\n\nCar $U$ est indépendante de $Y$ et de $Z$.\n\nDonc,\n\n$$\nF_X(x) = F_Y(x) \\mathbb{P}(U = 1)\n+ F_Z(x) \\mathbb{P}(U = 0).\n$$\n\nComme $U$ suit une loi de Bernoulli de paramètre $\\alpha$, nous avons :\n\n$$\n\\mathbb{P}(U = 1) = \\alpha\n\\quad\\text{et}\\quad\n\\mathbb{P}(U = 0) = 1 - \\alpha.\n$$  \n\nDonc finalement :\n\n$$\nF_X(x) = \\alpha F_Y(x) + (1 - \\alpha) F_Z(x).\n$$\n\n2) Représentation graphique de la fonction $F_X$.\n\n::: {#2c668dba .cell execution_count=3}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef F(x):\n    x = np.asarray(x)\n    return np.where(\n        x < 0, 0,\n        np.where(\n            x < 1, 1/8,\n            np.where(\n                x < 2, (x + 1)/4,\n                np.where(\n                    x < 5, 3/4,\n                    1\n                )\n            )\n        )\n    )\n\n# Segments de la fonction de répartition\nsegments_horizontaux = [\n    (-10, 0, 0),     # de x=-10 à x=0 : F=0\n    (0, 1, 1/8),     # de x=0 à x=1 : F=1/8\n    (2, 5, 3/4),     # de x=2 à x=5 : F=3/4\n    (5, 10, 1)       # de x=5 à x=10 : F=1\n]\n\nplt.figure(figsize=(12, 5))\n\n# Tracé des segments horizontaux\nfor x_start, x_end, y_val in segments_horizontaux:\n    plt.hlines(y_val, x_start, x_end, colors=\"blue\", linewidth=2)\n\n# Tracé du segment linéaire entre x=1 et x=2 : F(x) = (x+1)/4\nx_linear = np.linspace(1, 2, 100)\ny_linear = (x_linear + 1) / 4\nplt.plot(x_linear, y_linear, 'b-', linewidth=2)\n\n# Points ouverts (limite à gauche non incluse)\nx_open = [0, 1, 5]\ny_open = [0, 1/8, 3/4]\nplt.scatter(x_open, y_open, facecolors=\"none\", edgecolors=\"black\", s=80, zorder=3, linewidths=2)\n\n# Points fermés (valeur incluse)\nx_closed = [0, 1, 2, 5]\ny_closed = [1/8, 2/4, 3/4, 1]\nplt.scatter(x_closed, y_closed, color=\"black\", s=80, zorder=3)\n\n# Format graphique\nplt.xlabel(\"x\", fontsize=12)\nplt.ylabel(\"$F_X(x)$\", fontsize=12)\nplt.title(\"Fonction de répartition $F_X(x)$\", fontsize=14)\nplt.grid(True, linestyle=\"--\", alpha=0.4)\nplt.xlim(-10, 10)\nplt.ylim(-0.05, 1.05)\n\n# Définir les graduations sur l'axe des x\nplt.xticks([0, 1, 2, 3, 4, 5])\n\n# Ajout de repères pour les valeurs importantes\nplt.axhline(y=1/8, color='gray', linestyle=':', alpha=0.3)\nplt.axhline(y=1/2, color='gray', linestyle=':', alpha=0.3)\nplt.axhline(y=3/4, color='gray', linestyle=':', alpha=0.3)\nplt.axvline(x=0, color='gray', linestyle=':', alpha=0.3)\nplt.axvline(x=1, color='gray', linestyle=':', alpha=0.3)\nplt.axvline(x=2, color='gray', linestyle=':', alpha=0.3)\nplt.axvline(x=5, color='gray', linestyle=':', alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](ensai1A_files/figure-html/cell-4-output-1.png){width=1141 height=469}\n:::\n:::\n\n\nLes points de discontinuité sont représentés par des cercles ouverts (limite à gauche non incluse) et des cercles pleins (valeur incluse) : 0, 1 et 5.\n\n3) Expression de la loi de $X$ sous la forme d’un mélange d'une loi discrète et d’une loi continue.\n\nNous savons que si $P_{cont}$ est une loi de probabilité continue, alors $P_{cont}(X = x) = 0$ pour tout $x \\in \\mathbb{R}$.\n\nDe ce fait, si nous devons écrire la loi de $X$ comme un mélange d'une loi discrète et d’une loi continue, ie \n$$\n\\mathbb{P}_X = \\alpha \\mathbb{P}_{dis} + (1 - \\alpha)\\mathbb{P}_{cont},\n$$\n\nalors aux points de discontinuité de la fonction de répartition $F_X$, la partie continue ne contribue pas à la probabilité. C'est-à-dire que pour tout $x \\in {0,1,5}$, nous avons :\n\n$$\n\\mathbb{P}_X(X = x) = \\alpha \\mathbb{P_{dis}(X = x)} + (1 - \\alpha) \\mathbb{P_{cont}(X = x)}\n= \\alpha \\mathbb{P_{dis}(X = x)} + 0\n= \\alpha \\mathbb{P_{dis}(X = x)}.\n$$\n\nDonc, nous avons les équations suivantes :\n\n$$\n\\mathbb{P_X(X = 0)} = \\alpha \\mathbb{P_{dis}(X = 0)}\n$$\n\n$$\n\\mathbb{P_X(X = 1)} = \\alpha \\mathbb{P_{dis}(X = 1)}\n$$\n\n$$\n\\mathbb{P_X(X = 5)} = \\alpha \\mathbb{P_{dis}(X = 5)}.\n$$\n\nCes trois équations ci-dessus nous permettent de déterminer la loi discrète $\\mathbb{P_{dis}}$ et le paramètre $\\alpha$.\n\nEn effet, en sommant les probabilités aux points de discontinuité, nous avons :\n\n$$\n\\mathbb{P_X(X = 0)} + \\mathbb{P_X(X = 1)} + \\mathbb{P_X(X = 5)}\n= \\alpha \\left( \\mathbb{P_{dis}(X = 0)} + \\mathbb{P_{dis}(X = 1)} + \\mathbb{P_{dis}(X = 5)} \\right).\n$$\n\nDonc si nous définissons la fonction discrète $\\mathbb{P_{dis}}$ comme ayant 0, 1 et 5 comme seuls points de masse, nous avons :\n$$\n\\mathbb{P_{dis}(X = 0)} + \\mathbb{P_{dis}(X = 1)} + \\mathbb{P_{dis}(X = 5)} = 1.\n$$\n\nDans ce cas, nous obtenons :\n\n$$\n\\alpha = \\mathbb{P_X(X = 0)} + \\mathbb{P_X(X = 1)} + \\mathbb{P_X(X = 5)}.\n$$\n\nCalculons les probabilités aux points de discontinuité à l'aide de la fonction de répartition $F_X$.\n\n$$\n\\mathbb{P_X(X = 0)} = F_X(0) - F_X(0^-) = \\frac{1}{8} - 0 = \\frac{1}{8}.\n$$\n\n$$\n\\mathbb{P_X(X = 1)} = F_X(1) - F_X(1^-) = \\frac{2}{4} - \\frac{1}{8} = \\frac{3}{8}.\n$$\n\n$$\n\\mathbb{P_X(X = 5)} = F_X(5) - F_X(5^-) = 1 - \\frac{3}{4} = \\frac{1}{4}.\n$$\n\nEt nous obtenons la loi de $P_{dis}$ :\n\n$$\n\\mathbb{P_{dis}(X = 0)} = \\frac{\\frac{1}{8}}{\\alpha} = \\frac{1}{6},\n\\quad\n\\mathbb{P_{dis}(X = 1)} = \\frac{\\frac{3}{8}}{\\alpha} = \\frac{1}{2},\n\\quad\n\\mathbb{P_{dis}(X = 5)} = \\frac{\\frac{1}{4}}{\\alpha} = \\frac{1}{3}.\n$$\n\nLa fonction de répartition correspondante est donc :\n\n$$F_{dis}(x) =\n\\begin{cases}\n0 & \\text{si } x < 0,\\\\[4pt]\n\\dfrac{1}{6} & \\text{si } x \\in [0,1[,\\\\[8pt]\n\\dfrac{2}{3} & \\text{si } x \\in [1,5[,\\\\[8pt]\n1 & \\text{si } x \\ge 5.\n\\end{cases}\n$$\n\nCeci nous permet de trouver facilement la fonction de répartition de la partie continue.\n\nEn effet, nous avons :\n\n$$\nF_X(x) = \\alpha F_{dis}(x) + (1 - \\alpha) F_{cont}(x).\n$$\n\nDonc,\n\n$$\nF_{cont}(x) = \\frac{F_X(x) - \\alpha F_{dis}(x)}{1 - \\alpha}.\n$$\n\nCe qui nous donne après calcul :\n\n$$\nF_{\\text{cont}}(x) =\n\\begin{cases}\n0 & \\text{si } x < 1,\\\\[4pt]\nx - 1 & \\text{si } x \\in [1,2[,\\\\[8pt]\n1 & \\text{si } x \\ge 2.\n\\end{cases}\n$$\n\nOn peut vérifier que cette fonction est bien une fonction de répartition (croissante, continue, avec des limites 0 et 1 aux extrémités). Elle est dérivable sur $\\mathbb{R}$ avec une densité associée donnée par :\n\n$$\nf_{\\text{cont}}(x) =\n\\begin{cases}\n0 & \\text{si } x < 1,\\\\[4pt]\n1 & \\text{si } x \\in [1,2[,\\\\[8pt]\n0 & \\text{si } x \\ge 2.\n\\end{cases}\n$$\n\nCe qui correspond à la densité d'une variable aléatoire uniformément distribuée sur l'intervalle $[1,2]$.\n\n---\n\n\n## Exercice 6\n\nSoit $X$ une variable aléatoire suivant une loi uniforme sur $[0,1]$, et soit\n\n$$\nY = -\\theta \\ln(1 - X)\n$$\n\navec $\\theta > 0$.  \nDéterminer la fonction de répartition et la densité de $Y$.\n\n## Correction Exercice 6\n\nL’objectif de l’exercice est que vous soyez capables de maîtriser les différentes techniques de calcul de la densité pour une fonction d’une v.a.  \n\n\nNotons tout d’abord que la fonction\n\n$$\ng : [0,1[ \\longrightarrow \\mathbb{R}, \\qquad x \\longmapsto -\\theta \\ln(1 - x)\n$$\n\nest dérivable sur $[0,1[$, à valeurs dans $\\mathbb{R}^+$, avec\n\n$$\ng'(x) = \\frac{\\theta}{1 - x} > 0,\n$$\n\ndonc la fonction $g$ est de plus croissante sur $[0,1[$.\n\nNous pouvons obtenir la fonction de répartition de $Y$ par calcul direct, puis en déduire la densité de $Y$.  \nNous pouvons également utiliser le résultat donnant la densité de la transformation d’une variable aléatoire par une fonction croissante.\n\nLe calcul direct donne, pour tout $x \\in \\mathbb{R}$ :\n\n$$\nF_Y(x) = \\mathbb{P}(Y \\le x)\n= \\mathbb{P}\\{\\ln(1-X) \\ge -x/\\theta\\}\n= \\mathbb{P}\\left\\{ X \\le 1 - \\exp\\!\\left(-\\frac{x}{\\theta}\\right) \\right\\}.\n$$\n\nAinsi,\n\n$$\nF_Y(x) =\n\\begin{cases}\n0, & \\text{si } x < 0,\\\\[6pt]\n1 - \\exp(-x/\\theta), & \\text{si } x \\ge 0,\n\\end{cases}\n$$\n\nen utilisant le fait que la fonction de répartition de la loi uniforme sur $[0,1]$ vaut :\n\n$$\nF_X(x) =\n\\begin{cases}\n0, & x < 0,\\\\\nx, & 0 \\le x < 1,\\\\\n1, & x \\ge 1.\n\\end{cases}\n$$\n\nNous reconnaissons la fonction de répartition d’une loi exponentielle de paramètre $1/\\theta$.\nPour tout $x>0$, la densité de $Y$ vaut :\n\n$$\nf_Y(x) = \\frac{1}{\\theta}\\exp\\!\\left(-\\frac{x}{\\theta}\\right).\n$$\n\n---\n\nPar la méthode de la transformation d’une variable aléatoire\n\nEn utilisant le résultat donnant la densité d’une variable transformée par une fonction croissante :\n\n$$\nf_Y(y) = \\left|\\frac{1}{g'(g^{-1}(y))}\\right| \\, f_X(g^{-1}(y)).\n$$\n\nLa fonction réciproque est :\n\n$$\ny = g(x) = -\\theta \\ln(1-x)\n\\quad\\Longleftrightarrow\\quad\nx = g^{-1}(y) = 1 - \\exp\\!\\left(-\\frac{y}{\\theta}\\right).\n$$\n\nOn obtient alors :\n\n$$\ng'(x) = \\frac{\\theta}{1-x},\n\\qquad\ng'(g^{-1}(y)) = \\frac{\\theta}{1 - (1-e^{-y/\\theta})}\n= \\theta e^{y/\\theta}.\n$$\n\nDonc :\n\n$$\nf_Y(y)\n= \\left|\\frac{1}{g'(g^{-1}(y))}\\right| f_X(g^{-1}(y))\n= \\frac{1}{\\theta e^{y/\\theta}} \\times 1\n= \\frac{1}{\\theta} e^{-y/\\theta},\n\\quad y \\ge 0.\n$$\n\n---\n\nPar la méthode de la fonction muette\n\nSoit $\\varphi$ une fonction continue bornée. Alors :\n\n$$\n\\mathbb{E}\\{\\varphi(Y)\\}\n= \\mathbb{E}\\{\\varphi(-\\theta\\ln(1-X))\\}\n= \\int_{\\mathbb{R}} \\varphi(-\\theta\\ln(1-x)) f_X(x)\\, dx.\n$$\n\nComme $X\\sim \\text{Unif}(0,1)$ :\n\n$$\n\\mathbb{E}\\{\\varphi(Y)\\}\n= \\int_0^1 \\varphi(-\\theta\\ln(1-x))\\, dx.\n$$\n\nAvec le changement de variable :\n\n$$\ny = -\\theta\\ln(1-x)\n\\quad\\Longleftrightarrow\\quad\nx = 1 - e^{-y/\\theta},\n\\qquad\ndx = \\frac{1}{\\theta} e^{-y/\\theta}\\, dy,\n$$\n\non obtient :\n\n$$\n\\mathbb{E}\\{\\varphi(Y)\\}\n= \\int_0^{+\\infty} \\varphi(y)\\, \\frac{1}{\\theta} e^{-y/\\theta}\\, dy\n= \\int_{\\mathbb{R}} \\varphi(y) f_Y(y)\\, dy,\n$$\n\navec\n\n$$\nf_Y(y) = \\frac{1}{\\theta} e^{-y/\\theta},\\qquad y\\ge 0.\n$$\n\n\n## Exercice 7\n\nSoit $X$ une variable aléatoire de densité $f_X(x) = 2x$ pour $x \\in [0,1]$ et $0$ sinon.\n\n1. Déterminer la fonction de répartition et la densité de $Y = 1/X$.\n\n2. Déterminer la fonction de répartition et la densité de $Z = \\ln(1/X)$.\n\n## Correction Exercice 7\n\n1. Fonction de répartition et densité de $Y = 1/X$\n\nAvant de déteminer la fonction de répartition de $Y$, prenons une minute pour déterminer l'ensemble des valeurs que peut prendre $Y$ ou le *support* de $Y$.\n\nComme le support de $X$ est $[0,1]$, et que $Y = 1/X$, on a :\n- lorsque $X$ tend vers $0$ par valeurs positives, $Y$ tend vers $+\\infty$,\n- lorsque $X = 1$, $Y = 1$.\nDonc le support de $Y$ est $[1, +\\infty[$.  \nCalculons la fonction de répartition de $Y$.\n\nLa fonction de répartition se calcule toujours sur l'ensemble $\\mathbb{R}$.\n\nsoit $y \\in \\mathbb{R}$,\n\n$$\nF_Y(y) = \\mathbb{P}(Y \\le y)\n= \\mathbb{P}\\left(\\frac{1}{X} \\le y\\right)\n= \\mathbb{P}\\left(X \\ge \\frac{1}{y}\\right).\n$$\n\n$$\nF_Y(y) = 1 - \\mathbb{P}\\left(X < \\frac{1}{y}\\right)\n= 1 - F_X\\left(\\frac{1}{y}\\right).\n$$\n\nDonc :\n\nsi $y < 1$, alors $\\frac{1}{y} > 1$ et donc $F_X\\left(\\frac{1}{y}\\right) = 1$, donc\n$$\nF_Y(y) = 0.\n$$\nsi $y \\ge 1$, alors $\\frac{1}{y} \\in [0,1]$ et donc\n$$\nF_X\\left(\\frac{1}{y}\\right) = \\left(\\frac{1}{y}\\right)^2 = \\frac{1}{y^2},\n$$\ndonc\n$$\nF_Y(y) = 1 - \\frac{1}{y^2}.\n$$\n\nDonc la fonction de répartition de $Y$ est donnée par :\n$$\nF_Y(y) =\n\\begin{cases}\n0, & y < 1, \\\\[6pt]\n1 - \\frac{1}{y^2}, & y \\ge 1.\n\\end{cases}\n$$\n\nCalculons la densité de $Y$.\nPour $y < 1$, $F_Y(y) = 0$, donc $f_Y(y) = 0$.\nPour $y \\ge 1$,\n$$\nf_Y(y) = \\frac{d}{dy} F_Y(y)\n= \\frac{d}{dy} \\left(1 - \\frac{1}{y^2}\\right)\n= \\frac{2}{y^3}.\n$$  \nDonc la densité de $Y$ est donnée par :\n$$\nf_Y(y) =\n\\begin{cases}\n0, & y < 1, \\\\[6pt]\n\\frac{2}{y^3}, & y \\ge 1.\n\\end{cases}\n$$\n\nNous pouvons vérifier que $f_Y$ est bien une densité, en appliquant la méthode de la fonction muette :\nSoit $\\varphi$ une fonction continue bornée. Alors :\n$$\n\\mathbb{E}\\{\\varphi(Y)\\}\n= \\mathbb{E}\\{\\varphi(1/X)\\}\n= \\int_{\\mathbb{R}} \\varphi(1/x) f_X(x)\\, dx.\n$$\nComme $X$ a pour densité $f_X(x) = 2x$ pour $x \\in [0,1]$ et $0$ sinon, on a :\n$$\n\\mathbb{E}\\{\\varphi(Y)\\}\n= \\int_0^1 \\varphi(1/x) 2x\\, dx.\n$$\nAvec le changement de variable :\n$$\ny = 1/x\n\\quad\\Longleftrightarrow\\quad\nx = 1/y,\n\\qquad\ndx = -\\frac{1}{y^2}\\, dy,\n$$\non obtient :\n$$\n\\mathbb{E}\\{\\varphi(Y)\\}\n= \\int_1^{+\\infty} \\varphi(y) 2 \\cdot \\frac{1}{y} \\cdot \\left(-\\frac{1}{y^2}\\right) dy\n= \\int_1^{+\\infty} \\varphi(y) \\frac{2}{y^3}\\, dy\n= \\int_{\\mathbb{R}} \\varphi(y) f_Y(y)\\, dy,\n$$\navec\n$$\nf_Y(y) =\n\\begin{cases}\n0, & y < 1, \\\\[6pt]\n\\frac{2}{y^3}, & y \\ge 1.\n\\end{cases}\n$$\n\n---\n\n2. Fonction de répartition et densité de $Z = \\ln(1/X)$\n\nAvant de déteminer la fonction de répartition de $Z$, prenons une autre minute pour déterminer l'ensemble des valeurs que peut prendre $Z$ ou le *support* de $Z$.\n\nComme le support de $X$ est $[0,1]$, et que $Z = \\ln(1/X)$, on a :\n- lorsque $X$ tend vers $0$ par valeurs positives, $1/X$ tend vers $+\\infty$, donc $Z$ tend vers $+\\infty$,\n- lorsque $X = 1$, $1/X = 1$, donc $Z = \\ln(1) = 0$.\nDonc le support de $Z$ est $[0, +\\infty[$.  \n\nCalculons la fonction de répartition de $Z$.\n\nLa fonction de répartition se calcule toujours sur l'ensemble $\\mathbb{R}$.\nsoit $z \\in \\mathbb{R}$,\n\n$$\nF_Z(z) = \\mathbb{P}(Z \\le z)\n= \\mathbb{P}(\\ln(1/X) \\le z)\n= \\mathbb{P}\\left(1/X \\le e^z\\right)\n= \\mathbb{P}\\left(X \\ge e^{-z}\\right).\n$$\n\n$$\nF_Z(z) = 1 - \\mathbb{P}\\left(X < e^{-z}\\right)\n= 1 - F_X\\left(e^{-z}\\right).\n$$\n\nDonc :\nsi $z < 0$, alors $e^{-z} > 1$ et donc $F_X\\left(e^{-z}\\right) = 1$, donc\n$$\nF_Z(z) = 0.\n$$\nsi $z \\ge 0$, alors $e^{-z} \\in [0,1]$ et donc\n$$\nF_X\\left(e^{-z}\\right) = \\left(e^{-z}\\right)^2 = e^{-2z},\n$$\ndonc\n$$\nF_Z(z) = 1 - e^{-2z}.\n$$\nDonc la fonction de répartition de $Z$ est donnée par :\n$$\nF_Z(z) =\n\\begin{cases}\n0, & z < 0, \\\\[6pt]\n1 - e^{-2z}, & z \\ge 0.\n\\end{cases}\n$$\n\nCalculons la densité de $Z$.\nPour $z < 0$, $F_Z(z) = 0$, donc $f_Z(z) = 0$.\nPour $z \\ge 0$,\n$$\nf_Z(z) = \\frac{d}{dz} F_Z(z)\n= \\frac{d}{dz} \\left(1 - e^{-2z}\\right)\n= 2 e^{-2z}.\n$$  \nDonc la densité de $Z$ est donnée par :\n$$\nf_Z(z) =\n\\begin{cases}\n0, & z < 0, \\\\[6pt]\n2 e^{-2z}, & z \\ge 0.\n\\end{cases}\n$$\n\nDonc $Z$ suit une loi exponentielle de paramètre $2$.\n\nNous pouvons confirmer que $f_Z$ est bien une densité, en appliquant la méthode de la fonction muette :\nSoit $\\varphi$ une fonction continue bornée. Alors :\n$$\n\\mathbb{E}\\{\\varphi(Z)\\}\n= \\mathbb{E}\\{\\varphi(\\ln(1/X))\\}\n= \\int_{\\mathbb{R}} \\varphi(\\ln(1/x)) f_X(x)\\, dx.\n$$\nComme $X$ a pour densité $f_X(x) = 2x$ pour $x \\in [0,1]$ et $0$ sinon, on a :\n$$\n\\mathbb{E}\\{\\varphi(Z)\\}\n= \\int_0^1 \\varphi(\\ln(1/x)) 2x\\, dx.\n$$\nAvec le changement de variable :\n$$\nz = \\ln(1/x)\n\\quad\\Longleftrightarrow\\quad\nx = e^{-z},\n\\qquad\ndx = -e^{-z}\\, dz,\n$$\non obtient :\n$$\n\\mathbb{E}\\{\\varphi(Z)\\}\n= \\int_0^{+\\infty} \\varphi(z) 2 \\cdot e^{-z} \\cdot \\left(-e^{-z}\\right) dz\n= \\int_0^{+\\infty} \\varphi(z) 2 e^{-2z}\\, dz\n= \\int_{\\mathbb{R}} \\varphi(z) f_Z(z)\\, dz,\n$$\navec\n$$\nf_Z(z) =\n\\begin{cases}\n0, & z < 0, \\\\[6pt]\n2 e^{-2z}, & z \\ge 0.\n\\end{cases}\n$$\n\n## Exercice 8\n\nSoit $X$ une variable aléatoire suivant une loi normale centrée réduite.  \nDéterminer la loi, l’espérance et la variance des variables aléatoires $|X|$, $X^2$ et $e^X$.\n\n## Correction Exercice 8\n\n1) Loi, espérance et variance de $|X|$\n\nNous savons que la densité de $X$ est donnée par :\n$$\nf_X(x) = \\frac{1}{\\sqrt{2\\pi}} e^{-x^2/2}, \\quad x \\in \\mathbb{R}.\n$$\n\nLa densité de $|X|$ caractérise la loi de $|X|$. Nous pouvons la déterminer en utilisant la méthode de la fonction muette.\n\nSoit $\\varphi$ une fonction continue bornée. Alors :\n\n$$\n\\mathbb{E}\\{\\varphi(|X|)\\}\n= \\int_{\\mathbb{R}} \\varphi(|x|) f_X(x)\\, dx.\n= \\int_{-\\infty}^0 \\varphi(-x) f_X(x)\\, dx + \\int_0^{+\\infty} \\varphi(x) f_X(x)\\, dx.\n$$\n\nNous effectuons le changement de variable $y = -x$ dans la première intégrale :\n$$\n\\mathbb{E}\\{\\varphi(|X|)\\}\n= \\int_0^{+\\infty} \\varphi(y) f_X(-y)\\, dy + \\int_0^{+\\infty} \\varphi(x) f_X(x)\\, dx.\n$$\n\nOr, la densité de $X$ est une fonction paire, ie $f_X(-y) = f_X(y)$ pour tout $y \\in \\mathbb{R}$. Donc,\n$$\n\\mathbb{E}\\{\\varphi(|X|)\\}\n= \\int_0^{+\\infty} \\varphi(y) f_X(y)\\, dy + \\int_0^{+\\infty} \\varphi(x) f_X(x)\\, dx\n= 2 \\int_0^{+\\infty} \\varphi(t) f_X(t)\\, dt.\n$$\n\nOn en déduit que la densité de $|X|$ est donnée par :\n$$\nf_{|X|}(t) =\n\\begin{cases}\n2 f_X(t) = \\frac{2}{\\sqrt{2\\pi}} e^{-t^2/2}, & t \\ge 0, \\\\[6pt]\n0, & t < 0.\n\\end{cases}\n$$\nCalculons l’espérance de $|X|$ :\n$$\n\\mathbb{E}\\{|X|\\}\n= \\int_{\\mathbb{R}} t f_{|X|}(t)\\, dt\n= \\int_0^{+\\infty} t \\cdot \\frac{2}{\\sqrt{2\\pi}} e^{-t^2/2}\\, dt.\n$$\n\nEn effectuant le changement de variable $u = t^2/2$, on obtient :\n$$\n\\mathbb{E}\\{|X|\\}\n= \\frac{2}{\\sqrt{2\\pi}} \\int_0^{+\\infty} e^{-u}\\, du\n= \\frac{2}{\\sqrt{2\\pi}} = \\sqrt{\\frac{2}{\\pi}}.\n$$  \n\nPour calculer la variance de $|X|$, nous avons besoin de calculer $\\mathbb{E}\\{|X|^2\\} = \\mathbb{E}\\{X^2\\}$ \n\nNous savons que pour une variable aléatoire suivant une loi normale centrée réduite, $\\mathbb{E}\\{X^2\\} = 1$.  \nDonc,\n$$\n\\text{Var}(|X|) = \\mathbb{E}\\{|X|^2\\} - (\\mathbb{E}\\{|X|\\})^2\n= 1 - \\frac{2}{\\pi} \n$$\n\n2) Loi, espérance et variance de $X^2$\n\nNous utilisons à nouveau la méthode de la fonction muette pour déterminer la densité de $X^2$.\n\nSoit $\\varphi$ une fonction continue bornée. Alors :\n\n$$\n\\mathbb{E}\\{\\varphi(X^2)\\}\n= \\int_{\\mathbb{R}} \\varphi(x^2) f_X(x)\\, dx\n$$\n\nLa fonction $x \\mapsto \\varphi(x^2) f_X(x)$ est paire, donc :\n$$\n\\mathbb{E}\\{\\varphi(X^2)\\}\n= 2 \\int_0^{+\\infty} \\varphi(t^2) f_X(t)\\, dt.\n$$\n\nEn effectuant le changement de variable $y = t^2$, on obtient :\n$$\n\\mathbb{E}\\{\\varphi(X^2)\\}\n= \\int_0^{+\\infty} \\varphi(y) \\frac{1}{\\sqrt{2\\pi y}} e^{-y/2}\\, dy.\n$$  \n\nOn en déduit que la densité de $X^2$ est donnée par :\n$$\nf_{X^2}(y) =\n\\begin{cases}\n\\frac{1}{\\sqrt{2\\pi y}} e^{-y/2}, & y \\ge 0, \\\\[6pt]\n0, & y < 0.\n\\end{cases}\n$$\n\nOn peut reconnaitre que la variable aléatoire $X^2$ suit une de $\\chi^2$ de paramètre $1$. Dans ce cas, nous savons que :\n$$\n\\mathbb{E}\\{X^2\\} = 1,\n\\quad\n\\text{Var}(X^2) = 2.\n$$\n\nSinon, nous pouvons calculer l’espérance et la variance de $X^2$ en utilisant en utilisant les propriétés de la loi normale centrée réduite.\n\nCalculons l’espérance de $X^2$ :\n$$\n\\mathbb{E}\\{X^2\\} = 1.\n$$\n\nCalculons la variance de $X^2$ :\n\nPour cela, nous avons besoin de calculer $\\mathbb{E}\\{X^4\\}$.\n\net\n$$\n\\mathbb{E}\\{X^4\\} = \\frac{1}{\\sqrt{2\\pi}} \\int_{\\mathbb{R}} x^4 e^{-x^2/2}\\, dx.  \n$$\n\n\nSi nous posons $u = -\\frac{x^2}{2}$,\nalors $du = -x\\, dx$,\n\nDonc par une intégration par parties, nous obtenons :\n$$\n\\mathbb{E}\\{X^4\\}\n= \\left[ -\\frac{1}{\\sqrt{2\\pi}} x^{3} e^{-x^{2}/2} \\right]_{-\\infty}^{+\\infty}\n\\;+\\;\n\\frac{3}{\\sqrt{2\\pi}} \\int_{-\\infty}^{+\\infty} x^{2} e^{-x^{2}/2} \\, dx\n$$\n\n$$\n= 3 \\mathbb{E}(X^{2}) = 3.\n$$\n\nDonc,\n$$\n\\text{Var}(X^2) = \\mathbb{E}\\{X^4\\} - (\\mathbb{E}\\{X^2\\})^2\n= 3 - 1 = 2.\n$$\n\n3) Loi, espérance et variance de $e^X$  \n\nNous utilisons encore une fois la méthode de la fonction muette pour déterminer la densité de $e^X$.\n\nSoit $\\varphi$ une fonction continue bornée. Alors :\n\n$$\n\\mathbb{E}\\{\\varphi(e^X)\\}\n= \\int_{\\mathbb{R}} \\varphi(e^x) f_X(x)\\, dx\n= \\int_{-\\infty}^{+\\infty} \\varphi(e^x) \\frac{1}{\\sqrt{2\\pi}} e^{-x^2/2}\\, dx.\n$$\n\nEn effectuant le changement de variable $y = e^x$, on obtient :\n$$\n\\mathbb{E}\\{\\varphi(e^X)\\}\n= \\int_0^{+\\infty} \\varphi(y) \\frac{1}{y \\sqrt{2\\pi}} e^{-(\\ln y)^2/2}\\, dy.\n$$\n\nOn en déduit que la densité de $e^X$ est donnée par :\n\n$$\nf_{e^X}(y) =\n\\begin{cases}\n\\frac{1}{y \\sqrt{2\\pi}} e^{-(\\ln y)^2/2}, & y > 0, \\\\[6pt]\n0, & y \\le 0.\n\\end{cases}\n$$\n\nNous disons que $e^X$ suit une loi log-normale de paramètres $\\mu = 0$ et $\\sigma^2 = 1$.\n\nCalculons l’espérance de $e^X$ et la variance de $e^X$.:\n\n$$\n\\mathbb{E}\\{e^X\\} = M_X(1),\n$$\n\n$$\n\\mathbb{E}\\{e^{2X}\\} = M_X(2),\n$$\n\noù $M_X(t)$ est la fonction génératrice des moments de $X$.\n\nDonc il nous suffit de calculer $M_X(t)$.\n\nNous avons :\n$$\nM_X(t) = \\mathbb{E}\\{e^{tX}\\}\n= \\int_{\\mathbb{R}} e^{tx} f_X(x)\\, dx\n= \\frac{1}{\\sqrt{2\\pi}} \\int_{-\\infty}^{+\\infty} e^{tx} e^{-x^2/2}\\, dx.  \n$$\n\n$$\n= \\frac{1}{\\sqrt{2\\pi}} \\int_{-\\infty}^{+\\infty} e^{-\\frac{1}{2}(x^2 - 2tx)}\\, dx\n= \\frac{1}{\\sqrt{2\\pi}} \\int_{-\\infty}^{+\\infty} e^{-\\frac{1}{2}(x - t)^2 + \\frac{t^2}{2}}\\, dx\n= e^{t^2/2} \\cdot \\frac{1}{\\sqrt{2\\pi}} \\int_{-\\infty}^{+\\infty} e^{-\\frac{1}{2}(x - t)^2}\\, dx.\n$$\n\nLa dernière intégrale est égale à $1$ car c’est l’intégrale de la densité d’une loi normale centrée réduite. Donc,\n$$\nM_X(t) = e^{t^2/2}.\n$$\nAinsi,\n$$\n\\mathbb{E}\\{e^X\\} = M_X(1) = e^{1/2},\n$$\n$$\n\\mathbb{E}\\{e^{2X}\\} = M_X(2) = e^{2}.\n$$\nDonc,\n$$\n\\text{Var}(e^X) = \\mathbb{E}\\{e^{2X}\\} - (\\mathbb{E}\\{e^X\\})^2    \n= e^{2} - e^{1} = e(e - 1).\n$$\n\n\n## Exercice 9\n\nLa loi Beta de paramètres $a, b > 0$ a pour densité\n\n$$\nf(x) = \n\\begin{cases}\n\\dfrac{x^{a-1}(1-x)^{\\,b-1}}{\\displaystyle \\int_{0}^{1} t^{a-1}(1-t)^{\\,b-1} \\, dt} & \\text{si } x \\in ]0,1[, \\\\[10pt]\n0 & \\text{sinon}.\n\\end{cases}\n$$\n\nNous noterons \n$$\nB(a,b) = \\int_{0}^{1} t^{a-1}(1-t)^{\\,b-1}\\, dt.\n$$\n\n1) Soit $U$ une variable aléatoire de loi uniforme sur $[0,1]$.  \nMontrer que $U$ et $U^{2}$ suivent chacune une loi Beta, dont on précisera les paramètres.\n\n2) Montrer que, pour tous $a, b > 0$, on a\n\n$$\nB(a,b) = B(a+1,b) + B(a,b+1)\n\\quad\\text{et}\\quad\na\\,B(a,b+1) = b\\,B(a+1,b).\n$$\n\nEn déduire que  \n$$\n(a+b)\\,B(a+1,b) = a\\,B(a,b).\n$$\n\n3) Soit $X$ une variable aléatoire suivant la loi Beta de paramètres $a,b > 0$.  \nMontrer que, pour tout entier $p \\ge 1$, $X$ admet un moment d’ordre $p$, et donner une expression simple (ne dépendant pas de l’intégrale définissant la fonction Beta) de $\\mathbb{E}(X^{p})$.\n\n4) En déduire l’espérance et la variance de $X$.\n\n## Correction Exercice 9\n\n 1) Loi de $U$ et de $U^2$\n\nLa variable aléatoire $U$ suit une loi uniforme sur $[0,1]$.  \nSa densité vaut donc :\n\n$$\nf_U(x) = 1_{[0,1]}(x),\n$$\n\nce qui correspond à une loi Beta de paramètres\n\n$$\na = 1, \\qquad b = 1.\n$$\n\n---\n\n\n**Loi de $Y = U^2$**\n\nNous utilisons la méthode de la fonction muette pour déterminer la densité de $Y = U^2$.\n\nPour toute fonction $\\varphi$ continue bornée, on a :\n\n$$\n\\mathbb{E}\\{\\varphi(U^2)\\}\n  = \\int_{0}^{1} \\varphi(x^2)\\, dx\n  = \\int_{0}^{1} \\varphi(y) \\cdot \\frac{1}{2\\sqrt{y}}\\, dy.\n$$\n\nAinsi, la densité de $Y = U^2$ est :\n\n$$\nf_Y(y) = \\frac{1}{2\\sqrt{y}} \\, 1_{]0,1]}(y).\n$$\n\nOn reconnaît une loi Beta de paramètres :\n\n$$\na = \\frac12, \\qquad b = 1.\n$$\n\n---\n\n2) Identités pour la fonction Beta\n\nOn veut montrer :\n\n$$\nB(a,b) = B(a+1,b) + B(a,b+1).\n$$\n\nOn écrit :\n\n$$\nB(a,b) - B(a+1,b) =\n\\int_0^1 t^{a-1}(1-t)^{b-1} dt \n- \\int_0^1 t^{a}(1-t)^{b-1} dt\n$$\n\nsoit :\n\n$$\n\\int_0^1 t^{a-1}(1-t)^{b-1}(1-t)\\, dt\n= \\int_0^1 t^{a-1}(1-t)^{b}\\, dt\n= B(a,b+1).\n$$\n\nD’où :\n\n$$\nB(a,b) = B(a+1,b) + B(a,b+1).\n$$\n\n---\n\n**Relation $aB(a,b+1) = b B(a+1,b)$**\n\nPar intégration par parties :\n\n$$\nB(a,b+1)\n= \\int_0^1 t^{a-1}(1-t)^b dt\n= \\left[ (1-t)^b \\frac{t^a}{a} \\right]_0^1\n+ \\frac{b}{a}\\int_0^1 t^a (1-t)^{b-1} dt.\n$$\n\nLe terme de bord s’annule, donc :\n\n$$\nB(a,b+1)\n= \\frac{b}{a} B(a+1,b).\n$$\n\nCe qui est équivalent à :\n\n$$\na B(a,b+1) = b B(a+1,b).\n$$\n\n---\n\n**Combinaison des identités**\n\nÀ partir de\n\n$$\nB(a,b) = B(a+1,b) + B(a,b+1)\n$$\n\net\n\n$$\nB(a,b+1) = \\frac{b}{a} B(a+1,b),\n$$\n\non obtient :\n\n$$\nB(a,b) = \\left(1 + \\frac{b}{a}\\right) B(a+1,b)\n= \\frac{a+b}{a} B(a+1,b).\n$$\n\nDonc :\n\n$$\n(a+b) B(a+1,b) = a B(a,b).\n$$\n\n---\n\n3) Moments de la loi Beta\n\nPuisque $0 \\le X \\le 1$, tous les moments existent.\n\nPour $p \\ge 1$, on a :\n\n$$\n\\mathbb{E}(X^p)\n= \\int_0^1 x^p f_X(x)\\, dx\n= \\frac{\\int_0^1 x^{a+p-1}(1-t)^{b-1} dt}{\\int_0^1 t^{a-1}(1-t)^{b-1} dt}\n$$\n\nsoit :\n\n$$\n\\mathbb{E}(X^p)\n= \\frac{B(a+p,b)}{B(a,b)}.\n$$\n\nEn utilisant l’identité montrée précédemment :\n\n$$\n\\frac{B(a+p,b)}{B(a,b)}\n= \\frac{(a+p-1)!}{(a-1)!} \\times \\frac{(a+b-1)!}{(a+b+p-1)!}.\n$$\n\n---\n\n### 4) Espérance et variance\n\n#### Espérance\n\n$$\n\\mathbb{E}(X)\n= \\frac{B(a+1,b)}{B(a,b)}\n= \\frac{a}{a+b}.\n$$\n\n#### Moment d’ordre 2\n\n$$\n\\mathbb{E}(X^2)\n= \\frac{B(a+2,b)}{B(a,b)}\n= \\frac{a(a+1)}{(a+b)(a+b+1)}.\n$$\n\n#### Variance\n\n$$  \n\\begin{aligned}\n\\mathrm{Var}(X)\n&= \\mathbb{E}(X^2) - \\{\\mathbb{E}(X)\\}^2 \\\\[6pt]\n&= \\frac{a(a+1)}{(a+b)(a+b+1)}\n  - \\left(\\frac{a}{a+b}\\right)^2.\n\\end{aligned}\n$$\n\nSimplification :\n\n$$\n\\mathrm{Var}(X)\n= \\frac{ab}{(a+b)^2 (a+b+1)}.\n$$\n\n\n## Exercice 10\n\nOn dit qu’une variable aléatoire $X$ suit une loi de Weibull de paramètres $\\alpha, \\beta > 0$ lorsqu’elle a pour densité\n\n$$\nf_X(x) =\n\\begin{cases}\n\\alpha \\beta x^{\\beta - 1} e^{-\\alpha x^\\beta}, & x > 0, \\\\[4pt]\n0, & \\text{sinon}.\n\\end{cases}\n$$\n\nLa loi de Weibull est utilisée en ingénierie pour l’analyse des défaillances (durée de vie d’un appareil) ou la variation de la contrainte à la rupture d’un matériau.\n\nDans tout l’exercice, on notera :\n\n$$\n\\Gamma(a) = \\int_0^{+\\infty} t^{a-1} e^{-t} \\, dt\n$$\n\nla fonction Gamma d’Euler.\n\n1. Calculer la fonction de répartition $F_X$ de $X$.\n\n2. On appelle taux ou fonction de défaillance (hazard rate) de $X$ la fonction\n\n$$\nh_X(x) = \\frac{f_X(x)}{1 - F_X(x)}\n\\quad \\text{pour } F_X(x) < 1.\n$$\n\nDéterminer l’expression de $h_X(x)$.\n\n3. Montrer que, pour tout entier $p \\ge 1$, $X$ admet un moment d’ordre $p$, et donner une expression de $\\mathbb{E}(X^p)$.\n\n4. En déduire l’espérance et la variance de $X$.\n\n## Correction Exercice 10\n\n1) Fonction de répartition\n\nPour $x < 0$, $F_X(x) = 0$.\n\nPour $x \\ge 0$ :\n\n$$\nF_X(x)\n= \\int_0^x f_X(t)\\, dt\n= \\int_0^x \\alpha \\beta t^{\\beta - 1} e^{-\\alpha t^\\beta} \\, dt.\n$$\n\nOn reconnaît la dérivée :\n\n$$\n\\frac{d}{dt}\\left( - e^{-\\alpha t^\\beta} \\right)\n= \\alpha \\beta t^{\\beta - 1} e^{-\\alpha t^\\beta}.\n$$\n\nDonc :\n\n$$\nF_X(x)\n= \\left[ - e^{-\\alpha t^\\beta} \\right]_{0}^{x}\n= 1 - e^{-\\alpha x^\\beta}.\n$$\n\n---\n\n2) Fonction de défaillance (hazard rate)\n\nPour $x < 0$, $h_X(x) = 0$.  \nPour $x \\ge 0$ :\n\n$$\nh_X(x)\n= \\frac{f_X(x)}{1 - F_X(x)}\n= \\frac{\\alpha \\beta x^{\\beta - 1} e^{-\\alpha x^\\beta}}{e^{-\\alpha x^\\beta}}\n= \\alpha \\beta x^{\\beta - 1}.\n$$\n\n---\n\n3) Moment d’ordre \\(p\\)\n\nOn veut montrer que :\n\n$$\n\\mathbb{E}(X^p)\n= \\int_0^{+\\infty} x^p f_X(x) \\, dx\n= \\alpha \\beta \\int_0^{+\\infty} x^{p + \\beta - 1} e^{-\\alpha x^\\beta} \\, dx.\n$$\n\nLa fonction est intégrable pour tout $p \\ge 1$ car :\n\n$$\ne^{-\\alpha x^\\beta} = o\\!\\left( \\frac{1}{x^{p+\\beta+1}} \\right),\n\\quad x \\to +\\infty.\n$$\n\n---\n\n**Changement de variable**\n\nPosons :\n\n$$\ny = \\alpha x^\\beta,\n\\qquad x = \\left( \\frac{y}{\\alpha} \\right)^{1/\\beta},\n\\qquad dx = \\frac{1}{\\beta} \\alpha^{-1/\\beta} y^{1/\\beta - 1} dy.\n$$\n\nAlors :\n\n$$\n\\mathbb{E}(X^p)\n= \\alpha \\beta \\int_0^{+\\infty}\n\\left( \\frac{y}{\\alpha} \\right)^{\\frac{p+\\beta}{\\beta}}\ne^{-y}\n\\cdot \\frac{1}{\\beta} \\alpha^{-1/\\beta} y^{1/\\beta - 1}\n\\, dy.\n$$\n\nSimplification :\n\n$$\n\\mathbb{E}(X^p)\n= \\alpha^{-p/\\beta}\n\\int_0^{+\\infty} y^{\\frac{p}{\\beta}} e^{-y} \\, dy\n= \\alpha^{-p/\\beta} \\Gamma\\!\\left( 1 + \\frac{p}{\\beta} \\right).\n$$\n\nDonc pour tout $p \\ge 1$ :\n\n$$\n\\boxed{\\mathbb{E}(X^p)\n= \\alpha^{-p/\\beta} \\Gamma\\!\\left( 1 + \\frac{p}{\\beta} \\right)}.\n$$\n\n---\n\n4) Espérance et variance\n\n- Pour $p = 1$ :\n\n$$\n\\mathbb{E}(X)\n= \\alpha^{-1/\\beta} \\Gamma\\!\\left( 1 + \\frac{1}{\\beta} \\right).\n$$\n\n- Pour $p = 2$ :\n\n$$\n\\mathbb{E}(X^2)\n= \\alpha^{-2/\\beta} \\Gamma\\!\\left( 1 + \\frac{2}{\\beta} \\right).\n$$\n\n- Variance :\n\n$$\n\\mathrm{Var}(X)\n= \\mathbb{E}(X^2) - \\mathbb{E}(X)^2\n= \\alpha^{-2/\\beta} \\Gamma\\!\\left( 1 + \\frac{2}{\\beta} \\right)\n- \\alpha^{-2/\\beta}\n\\Gamma^2\\!\\left( 1 + \\frac{1}{\\beta} \\right).\n$$\n\n## Exercice 11\n\nSoient $\\theta > 0$ et $X$ une variable aléatoire à densité $f_\\theta$ définie par\n\n$$\nf_\\theta(x) =\n\\begin{cases}\nc_\\theta \\, x, & \\text{si } x \\in [0,\\theta], \\\\[4pt]\n0, & \\text{sinon}.\n\\end{cases}\n$$\n\nSoit également $n \\ge 2$ et $X_1, \\ldots, X_n$ des variables aléatoires indépendantes de même loi que $X$.\n\n1. Calculer la constante $c_\\theta$ pour que $f_\\theta$ soit une densité de probabilité, puis la fonction de répartition de $X$, et enfin $\\mathbb{E}(X)$ et $V(X)$.\n\n2. Nous posons \n   $$\n   \\overline{X}_n = \\frac{1}{n} \\sum_{k=1}^n X_k.\n   $$\n   Calculer $\\mathbb{E}(\\overline{X}_n)$ et $V(\\overline{X}_n)$.\n\n3. Nous posons $M_n = \\max(X_1, \\ldots, X_n)$. Déterminer la fonction de répartition de $M_n$.\n\n4. Montrer que la loi de $M_n$ est à densité et calculer cette densité.\n\n5. Calculer $\\mathbb{E}(M_n)$ et $V(M_n)$.\n\n## Correction Exercice 11\n\n1) Constante de normalisation et loi de $X$\n\nLa constante $c_\\theta$ doit vérifier :\n\n$$\n\\int_0^\\theta c_\\theta x \\, dx = 1\n\\quad \\Longrightarrow \\quad\nc_\\theta \\left[ \\frac{x^2}{2} \\right]_0^\\theta\n= c_\\theta \\frac{\\theta^2}{2} = 1.\n$$\n\nDonc :\n\n$$\nc_\\theta = \\frac{2}{\\theta^2}.\n$$\n\nLa fonction de répartition de $X$ est :\n\n$$\nF_X(x) =\n\\begin{cases}\n0, & x < 0, \\\\[4pt]\n\\displaystyle \\int_0^x \\frac{2}{\\theta^2} t \\, dt\n= \\frac{x^2}{\\theta^2},\n& 0 \\le x \\le \\theta, \\\\[10pt]\n1, & x > \\theta.\n\\end{cases}\n$$\n\nMoments :\n\n$$\n\\mathbb{E}(X)\n= \\frac{2}{\\theta^2} \\int_0^\\theta x^2 \\, dx\n= \\frac{2}{\\theta^2} \\cdot \\frac{\\theta^3}{3}\n= \\frac{2\\theta}{3}.\n$$\n\n$$\n\\mathbb{E}(X^2)\n= \\frac{2}{\\theta^2} \\int_0^\\theta x^3 \\, dx\n= \\frac{2}{\\theta^2} \\cdot \\frac{\\theta^4}{4}\n= \\frac{\\theta^2}{2}.\n$$\n\nVariance :\n\n$$\nV(X)\n= \\mathbb{E}(X^2) - \\mathbb{E}(X)^2\n= \\frac{\\theta^2}{2} - \\left( \\frac{2\\theta}{3} \\right)^2\n= \\frac{\\theta^2}{18}.\n$$\n\n---\n\n2) Moments de la moyenne $\\overline{X}_n$ \n\nPar linéarité :\n\n$$\n\\mathbb{E}(\\overline{X}_n)\n= \\mathbb{E}(X)\n= \\frac{2\\theta}{3}.\n$$\n\nVariance (indépendance) :\n\n$$\nV(\\overline{X}_n)\n= \\frac{1}{n^2} \\sum_{k=1}^n V(X_k)\n= \\frac{1}{n} V(X)\n= \\frac{\\theta^2}{18n}.\n$$\n\n---\n\n3) Fonction de répartition de $M_n = \\max(X_1, \\ldots, X_n)$\n\nPour $x < 0$, $P(M_n \\le x) = 0$.  \nPour $x \\ge \\theta$, $P(M_n \\le x) = 1$.\n\nPour $x \\in [0,\\theta]$, on a :\n\n$$\nP(M_n \\le x)\n= P(X_1 \\le x, \\ldots, X_n \\le x)\n= \\prod_{i=1}^n F_X(x)\n= \\left( \\frac{x^2}{\\theta^2} \\right)^n\n= \\frac{x^{2n}}{\\theta^{2n}}.\n$$\n\nDonc :\n\n$$\nF_{M_n}(x)\n= \\frac{x^{2n}}{\\theta^{2n}}, \\qquad x \\in [0,\\theta].\n$$\n\n---\n\n4) Densité de $M_n$\n\nOn dérive la fonction de répartition :\n\n$$\nf_{M_n}(x)\n= \\frac{d}{dx} \\left( \\frac{x^{2n}}{\\theta^{2n}} \\right)\n= \\frac{2n \\, x^{2n-1}}{\\theta^{2n}}, \\qquad x \\in [0,\\theta].\n$$\n\nSinon, $f_{M_n}(x) = 0$.\n\n---\n\n5) Moments de $M_n$\n\nPour l’espérance :\n\n$$\n\\mathbb{E}(M_n)\n= \\int_0^\\theta x \\, f_{M_n}(x) \\, dx\n= \\int_0^\\theta \\frac{2n x^{2n}}{\\theta^{2n}} \\, dx.\n$$\n\nDonc :\n\n$$\n\\mathbb{E}(M_n)\n= \\frac{2n}{\\theta^{2n}}\n\\left[ \\frac{x^{2n+1}}{2n+1} \\right]_0^\\theta\n= \\frac{2n}{2n+1} \\theta.\n$$\n\nCalcul de \\(\\mathbb{E}(M_n^2)\\) :\n\n$$\n\\mathbb{E}(M_n^2)\n= \\int_0^\\theta \\frac{2n x^{2n+1}}{\\theta^{2n}} \\, dx\n= \\frac{2n}{\\theta^{2n}}\n\\left[ \\frac{x^{2n+2}}{2n+2} \\right]_0^\\theta\n= \\frac{2n}{2n+2} \\theta^2\n= \\frac{n}{n+1} \\theta^2.\n$$\n\nVariance :\n\n$$\nV(M_n)\n= \\mathbb{E}(M_n^2) - \\mathbb{E}(M_n)^2\n= \\frac{n}{n+1} \\theta^2\n- \\left( \\frac{2n}{2n+1} \\theta \\right)^2.\n$$\n\nOn obtient :\n\n$$\nV(M_n)\n= \\theta^2 \\left( \\frac{n}{n+1}\n- \\left( \\frac{2n}{2n+1} \\right)^2 \\right).\n$$\n\n\n## Exercice 15\n\nSoit $X$ une variable aléatoire suivant une loi normale centrée réduite.  \nOn pose\n\n$$\nY = \\min(X, 0).\n$$\n\n1. Déterminer la loi de $Y$.\n\n2. Montrer que $Y$ admet un moment d’ordre 2, et calculer l’espérance et la variance de $Y$.\n\n3. En déduire que $Z = \\max(X, 0)$ admet un moment d’ordre 2 et calculer son espérance et sa variance.\n\n## Correction Exercice 15\n\n1) Loi de $Y = \\min(X, 0)$\n\nNous utilisons la méthode de la fonction muette pour déterminer la densité de $Y$.\n\nSoit $\\varphi$ une fonction continue bornée. Alors :\n\n$$\n\\mathbb{E}\\{\\varphi(Y)\\}\n= \\int_{\\mathbb{R}} \\varphi(\\min(x, 0)) f_X(x)\\, dx.\n$$\nOn décompose l’intégrale en deux parties :\n\n$$\n\\mathbb{E}\\{\\varphi(Y)\\}\n= \\int_{-\\infty}^0 \\varphi(x) f_X(x)\\, dx\n+ \\int_0^{+\\infty} \\varphi(0) f\n_X(x)\\, dx.\n$$\n\nDonc,\n$$\n\\mathbb{E}\\{\\varphi(Y)\\}\n= \\int_{-\\infty}^0 \\varphi(y) f_X(y)\\, dy\n+ \\varphi(0) \\int_0^{+\\infty} f_X(x)\\, dx.\n$$\n\nNous savons que $X$ suit une loi normale centrée réduite, donc :\n$$\n\\int_0^{+\\infty} f_X(x)\\, dx = \\frac{1}{2}.\n$$\n\nAinsi,\n$$\n\\mathbb{E}\\{\\varphi(Y)\\}\n= \\int_{-\\infty}^0 \\varphi(y) f_X(y)\\, dy\n+ \\frac{1}{2} \\varphi(0).\n$$\nOn en déduit que la densité de $Y$ est donnée par :\n$$\nf_Y(y) =\n\\begin{cases}\nf_X(y), & y < 0, \\\\[6pt]\n\\frac{1}{2}, & y = 0, \\\\[6pt]\n0, & y > 0.\n\\end{cases}\n$$\n\nOn aurait aussi pu écrire la fonction de répartition de $Y$ :\n\nOn sait que le support de $Y$ est $(-\\infty, 0]$.\n\nPour $y < 0$,\n$$\nF_Y(y)\n= P(Y \\le y)\n= P(\\min(X, 0) \\le y)\n= P(X \\le y). \n= F_X(y).\n$$\n\nCar lorsque $y < 0$, l’événement $\\{\\min(X, 0) \\le y\\}$ est équivalent à l’événement $\\{X \\le y\\}$.\n\nOn peut aussi le voir ainsi :\n$$\n= P(\\min(X, 0) \\le y | X \\le 0) P(X \\le 0) + P(\\min(X, 0) \\le y | X > 0) P(X > 0)\n$$\n\n$$\n= P(X \\le y | X \\le 0) \\cdot \\frac{1}{2} + 0 \\cdot \\frac{1}{2}\n$$\n\n$$\n= \\frac{1}{2} P(X \\le y | X \\le 0)\n= \\frac{1}{2} \\cdot \\frac{P(X \\le y)}{P(X \\le 0)}\n= P(X \\le y).\n$$\n\nEt\n\npour $y >= 0$,\n$$\nF_Y(y)\n= P(Y \\le y)\n= P(\\min(X, 0) \\le y)\n= 1.\n$$\n\nDonc la fonction de répartition de $Y$ est donnée par :\n$$\nF_Y(y) =\n\\begin{cases}\nF_X(y), & y < 0, \\\\[6pt]\n1, & y \\ge 0.\n\\end{cases}\n$$\n\nEt la densité de $Y$ est la dérivée de $F_Y$ :\n$$\nf_Y(y) =\n\\begin{cases}\nf_X(y), & y < 0, \\\\[6pt]\n0, & y > 0.\n\\end{cases}\n$$\n\nPour $y = 0$, \non a $P(Y = 0) = P(X > 0) = \\frac{1}{2}$.\n\nDonc la densité de $Y$ est bien :\n\n$$\nf_Y(y) =\n\\begin{cases}\nf_X(y), & y < 0, \\\\[6pt]\n\\frac{1}{2}, & y = 0, \\\\[6pt]\n0, & y > 0.\n\\end{cases}\n$$\n\n2) Montrer que $Y$ admet un moment d’ordre 2, et calculer l’espérance et la variance de $Y$.\n\nNous savons que $|Y| = |\\min(X, 0)| \\le |X|$.\n\nComme $X$ suit une loi normale centrée réduite, $X$ admet un moment d’ordre 2. Par conséquent, $Y$ admet également un moment d’ordre 2.\nCalculons l’espérance de $Y$ :\nLe support de $Y$ est $(-\\infty, 0]$, donc :\n\n$$\n\\mathbb{E}(Y)\n= \\int_{-\\infty}^0 y f_X(y)\\, dy\n+ 0 \\cdot \\frac{1}{2}\n= \\int_{-\\infty}^0 y \\cdot \\frac{1}{\\sqrt{2\\pi}} e^{-y^2/2}\\, dy.\n$$\n\nEn effectuant le changement de variable $u = -\\frac{y^2}{2}$, on obtient :\n\n$$\n\\mathbb{E}(Y)\n= -\\frac{1}{\\sqrt{2\\pi}} \\int_{-\\infty}^0 e^{u}\\, du\n= -\\frac{1}{\\sqrt{2\\pi}}.\n$$\n\nCalculons $\\mathbb{E}(Y^2)$ :\n\n$$\n\\mathbb{E}(Y^2)\n= \\int_{-\\infty}^0 y^2 f_X(y)\\, dy\n+ 0^2 \\cdot \\frac{1}{2}\n= \\int_{-\\infty}^0 y^2 \\cdot \\frac{1}{\\sqrt{2\\pi}} e^{-y^2/2}\\, dy. \n$$\n\nPar une intégration par parties, on obtient :\n$$\n\\mathbb{E}(Y^2)\n= \\left[ -\\frac{1}{\\sqrt{2\\pi}} y e^{-y^2/2} \\right]_{-\\infty}^0\n\\;+\\;\n\\frac{1}{\\sqrt{2\\pi}} \\int_{-\\infty}^0 e^{-y^2/2}\\, dy\n= \\frac{1}{\\sqrt{2\\pi}} \\cdot \\frac{\\sqrt{2\\pi}}{2}\n= \\frac{1}{2}.\n$$\n\nDonc,\n$$\n\\text{Var}(Y) = \\mathbb{E}(Y^2) - (\\mathbb{E}(Y))^2\n= \\frac{1}{2} - \\left(-\\frac{1}{\\sqrt{2\\pi}}\\right)^2\n= \\frac{1}{2} - \\frac{1}{2\\pi}\n$$\n\n3) Loi, espérance et variance de $Z = \\max(X, 0)$\n\nNous savons que $X = Y + Z$.\n\nDonc, $Z = X - Y$.\n\nComme $X$ et $Y$ admettent un moment d’ordre 2, $Z$ admet également un moment d’ordre 2.\n\nEn effet, on a :\n$$\n|Z| = |X - Y| \\le |X| + |Y| \\le 2|X|.\n$$\n\nEt\n\n$$\n|Z|^2 \\le 4|X|^2.\n$$\n\n\nCalculons l’espérance de $Z$ :\n\n$$\n\\mathbb{E}(Z)\n= \\mathbb{E}(X) - \\mathbb{E}(Y)\n= 0 - \\left(-\\frac{1}{\\sqrt{2\\pi}}\\right)\n= \\frac{1}{\\sqrt{2\\pi}}.\n$$\n\nCalculons la  de $\\mathbb{E}(Z^2)$ :\n\n$$\n\\mathbb{E}(X^2)\n= \\mathbb{E}((Y + Z)^2)\n= \\mathbb{E}(Y^2) + 2\\mathbb{E}(YZ) + \\mathbb{E}(Z^2)\n= \\mathbb{E}(Y^2) + \\mathbb{E}(Z^2)\n$$\n\nCar $YZ = \\min(X, 0) \\cdot \\max(X, 0) = 0$.\n\nNous avons déjà calculé $\\mathbb{E}(X^2) = 1$ et $\\mathbb{E}(Y^2) = \\frac{1}{2}$.\n\n$$\n\\mathbb{E}(Z^2)\n= \\mathbb{E}(X^2) - \\mathbb{E}(Y^2)\n= 1 - \\frac{1}{2}\n= \\frac{1}{2}.\n$$\n\nAinsi,\n$$\n\\text{Var}(Z) = \\mathbb{E}(Z^2) - (\\mathbb{E}(Z))^2\n= \\frac{1}{2} - \\left(\\frac{1}{\\sqrt{2\\pi}}\\right)^2\n= \\frac{1}{2} - \\frac{1}{2\\pi}.\n$$\n\n</div>\n\n# Bonne chance !\n\n",
    "supporting": [
      "ensai1A_files"
    ],
    "filters": [],
    "includes": {}
  }
}