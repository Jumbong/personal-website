{
  "hash": "0e6b5ce541394837610ae734d7d09d3e",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \" Sampling Bias and Class Imbalance in Maximum-likelihood Logistic Regression\"\nsidebar: auto\nauthor:\n  - Jumbong Junior \ncategories: [article]\ntags: [Bias, Sampling, Class Imbalance, Logistic Regression, AUC, Precision, Recall, F1-score, Monte-Carlo Simulation, Maximum-likelihood Logistic Regression,python]\n\ntitle-block-banner: false\nformat: \n  html: \n    mainfont: Times New Roman\n    fontsize: 16pt\n\njupyter: python3\n        \n---\n\n\n\n\n\n# Introduction \n\nIn this article, the impact of sampling bias (sample dataset distribution different from the population distribution) \nand class imbalance on logistic regression models is explored. We hypothesize that the predictive performance of a logistic regression model is related to the sampling bias associated with the data and it has a performance advantage when the data is balanced. The hypothesis is testing with two simulated datasets : a balanced dataset (50:50) and an imbalanced dataset (80:20).  Each dataset will be sampled to produce samples with the following distribution : 50:50, 60:40, 70:30, 80:20, 90:10, 95:5, 99:1. \n\nThe performance of the logistic regression model will be evaluated using the Area Under the Curve (AUC), Area Under the Precision-Recall Curve (AU-PCR), Precision, Recall, and F1-score. \n\nMonte-Carlo simulations will be carried out to evaluate the distribution of the performance metrics for each of the samples and insure the robustness of the results.\n\nThis study gives three main results :\n\nA. The predicted probability using a maximum logistic regression (MLLR) model is closest to the true probability when the sample has the same class distribution as the original population. Therefore, in probabilistic modeling with MLLR, it is important to create a sample that matches the class distribution of the original population rather than ensuring equal class sampling, see @fig-plotsamplecasea and @fig-plotsamplecaseb.\n\nB. AUC measures how well probabilistic classifiers predict. It ranges from 0.5 (random) to 1 (perfect). AUC shows class separability regardless of class imbalance or sampling bias, see @fig-plot_metrics_a and @fig-plot_metrics_b.\n\nC. We recommend AUC to evaluate class separability in probabilistic models. To analyse sampling biais as well as the difference in the true and predicted probabilities, AUC-PR, Recall, precision and f1-score can be used as indicator.\n\nThe protocol of this paper is as follows. First, we describe how to simulate data. Next, we present the methodology. Finally, we present the results.\n\n\n# 1. Simulated Data Generation\n\n\nMany authors document that, for logistic regression , the probability distribution of the dependent variable is assumed to be Bernoulli and the mass function f is given by :\n\n$$\nf(y, x, \\alpha, \\beta) = p(x, \\alpha, \\beta)^y(1-p(x, \\alpha, \\beta))^{1-y}\n$$\n\nwhere \n$$\np(x, \\alpha, \\beta) = \\frac{\\exp(\\alpha + \\beta x)}{1 + \\exp(\\alpha + \\beta x)} \n$$\n\nand where y is the dependent variable, x is the independent variable, $\\alpha$ and $\\beta$ are the parameters to be estimated using the maximum likelihood method (MLE).\n\nFor generating the the bernouilli trial y using for a fixed parameter P, we use the following equation :\n\n$$\ny(p) = \n\\begin{cases}\n\\text{dummy} \\leftarrow \\mathrm{rnd}(1), \\\\\n0, & \\text{if dummy} < p, \\\\\n1, & \\text{otherwise}.\n\\end{cases}\n$$\n\nwhere rnd(1) is a random number generator that generates a random number between 0 and 1.\n\nThe conditional bernouilli trials y are then generated by substituting of $p(x, \\alpha, \\beta)$ :\n\n$$\ny(x, \\alpha, \\beta) =\n\\begin{cases}\n0, & \\text{if rnd(1)} < p(x, \\alpha, \\beta), \\\\\n1, & \\text{otherwise}.\n\\end{cases}\n$$\n\n\nIn order to generate the data, the following steps will be respected : \n- Generate x the predictor variable from a uniform distribution, which ranges from 0 to 10.\n- Choose the parameters $\\alpha$ and $\\beta$, which will help to genererate the distribution of the dependent variable y.\n- Generate the dependent variable y using the logistic function $p(x, \\alpha, \\beta)$.\n\n\n## 1.1 Numerical Approach to determine parameters $\\alpha$ and $\\beta$ knowing the proportion of y=1.\n\nThe numerical approach consists to determine, for a given value of $\\alpha =-10$, the value of $\\beta$ that will allow to have a proportion of y=1 equal to 0.5 in the case of a balanced dataset and 0.2 in the case of an imbalanced dataset.\n\nThe optimization problem can be formulated as follows :\n\n\n$$\n\\min_{\\beta} \\left( \\text{prop} - \\frac{1}{n}\\sum_{i=1}^{n} \\frac{\\exp(\\alpha + \\beta x_i)}{1 + \\exp(\\alpha + \\beta x_i)} \\right)^2 \n$$\n\nwhere $\\text{prop}$ is the proportion of y=1 in the dataset, $x_i$ is the predictor variable, and $n$ is the number of observations.\n\nThe optimization problem can be solved using the `scipy.optimize.minimize` function with the Nelder-Mead method.\n\n::: {#fd5b2aa7 .cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\nfrom scipy.optimize import minimize\n\n\n# Define the logistic function\ndef logistic_function(x, alpha, beta):\n    return 1 / (1 + np.exp(-(alpha + beta * x)))\n\n# Objective function: minimize the squared difference between mean(pi) and 0.2\n\ndef objective(alpha, prop, beta, n):\n    x = np.random.uniform(0, 10, n)  # Simulate x values\n    pi = logistic_function(x, alpha, beta)\n    return (np.mean(pi) - 0.2)**2  # Target mean(pi) = 0.2\n\n# Initial guesses for alpha and beta\n\ninitial_params = [0]\n\n# Optimize alpha and beta\nresult = minimize(lambda params: objective(-10, 0.2, params, 50000), initial_params, method='Nelder-Mead')\n\n# Get optimized alpha and beta\n\nbeta_opt = result.x\nprint(f\"Optimized alpha: {-10}, beta: {beta_opt}\")\n\n# Generate x and simulate y\nx = np.random.uniform(0, 10, 1000)\npi = logistic_function(x, -10, beta_opt)\ny = (np.random.uniform(0, 1, 1000) < pi).astype(int)\n\n# Verify proportions\ny_mean = np.mean(y)\nprint(f\"Proportion of y=1: {y_mean:.2f}, y=0: {1-y_mean:.2f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nOptimized alpha: -10, beta: [1.23775]\nProportion of y=1: 0.20, y=0: 0.80\n```\n:::\n:::\n\n\n## Simulated Data Generation with $\\alpha = -10$ \n\n\n\nLet's consider two cases :\n\n  - Case A : A balanced dataset with 50:50 distribution of y=0 and y=1.\n  - Case B : An imbalanced dataset with 80:20 distribution of y=0 and y=1.\n\nThe code below generates the data for the two cases and plots the proportion of y=1 as a function of beta.\n\nThe graph @fig-simulate_case_data, both the datasets have a total of 50,000 events, with the Case A dataset having a class distribution of about 50:50 and Case B dataset having a class distribution of about 80:20.\n\n::: {#cell-fig-simulate_case_data .cell execution_count=2}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Inputs : n_events, alpha, beta, random_state\n# Outputs : x, y, prop_y0, prop_y1\n# Objective : Simulate data from a logistic model with given alpha, beta.\n\ndef simulate_case_data(n_events, alpha, beta, random_state=42):\n    \"\"\"\n    Simulate data from a logistic model with given alpha, beta.\n    \n    x ~ Uniform(0, 10), y ~ Bernoulli(pi(x)), \n    where pi(x) = exp(alpha + beta*x) / (1 + exp(alpha + beta*x)).\n\n    Parameters\n    ----------\n    n_events : int\n        Number of observations (events) to generate.\n    alpha : float\n        Intercept (alpha) for the logistic function.\n    beta : float\n        Slope (beta) for the logistic function.\n    random_state : int\n        Seed for reproducibility.\n\n    Returns\n    -------\n    x : np.ndarray of shape (n_events,)\n        Predictor values sampled from Uniform(0,10).\n    y : np.ndarray of shape (n_events,)\n        Binary outcomes (0 or 1) from Bernoulli trials.\n    prop_y0 : float\n        Proportion of y==0 in the dataset.\n    prop_y1 : float\n        Proportion of y==1 in the dataset.\n    \"\"\"\n    np.random.seed(random_state)\n    \n    # 1) Draw x from Uniform(0,10)\n    x = np.random.uniform(0, 10, size=n_events)\n    \n    # 2) Compute pi(x, alpha, beta)\n    logit = alpha + beta*x\n    pi = np.exp(logit) / (1.0 + np.exp(logit))\n    \n    # 3) Generate y via Bernoulli(pi)\n    dummy = np.random.rand(n_events)\n    y = (dummy < pi).astype(int)\n    \n    # 4) Calculate proportions of 0 and 1\n    prop_y0 = np.mean(y == 0)\n    prop_y1 = np.mean(y == 1)\n    \n    return x, y, prop_y0, prop_y1\n\n# ---------------- Example usage ----------------\n\n\n# Case A: alpha=-10, beta=2 --> expected ~50:50 distribution\nxA, yA, p0_A, p1_A = simulate_case_data(\n    n_events=50000,\n    alpha=-10,\n    beta=2,\n    random_state=42\n)\n\n# Case B: alpha=-10, beta=3.85 --> expected ~80:20 distribution\nxB, yB, p0_B, p1_B = simulate_case_data(\n    n_events=50000,\n    alpha=-10,\n    beta=beta_opt,\n    random_state=42\n)\n\n# Verify proportions\n\n# Suppose p0_A, p1_A, p0_B, p1_B are already defined\n# e.g., p0_A = 0.50; p1_A = 0.50; p0_B = 0.80; p1_B = 0.20\n\nfig, axes = plt.subplots(1, 2, figsize=(5, 3))  # 1 ligne, 2 colonnes\n\ncolors = ['royalblue', 'darkorange']  # Couleurs distinctes pour y=0 et y=1\n\n# -------- LEFT SUBPLOT: Case A -----------\nax1 = axes[0]\nbar_container_A = ax1.bar(['y=0', 'y=1'], [p0_A, p1_A], color=colors)\nax1.set_title('Case A')\nax1.set_xlabel('Classe')\nax1.set_ylabel('Proportion')\nax1.set_ylim([0, 1])  # Echelle de 0 à 1\nax1.bar_label(bar_container_A, fmt='%.2f')\n\n# -------- RIGHT SUBPLOT: Case B -----------\nax2 = axes[1]\nbar_container_B = ax2.bar(['y=0', 'y=1'], [p0_B, p1_B], color=colors)\nax2.set_title('Case B')\nax2.set_xlabel('Classe')\nax2.set_ylabel('Proportion')\nax2.set_ylim([0, 1])\nax2.bar_label(bar_container_B, fmt='%.2f')\n\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Simulated data from logistic model with alpha=-10, beta=2 and alpha=-10, beta=beta_opt](bias_sample_files/figure-html/fig-simulate_case_data-output-1.png){#fig-simulate_case_data width=470 height=278}\n:::\n:::\n\n\nThe theoretical properties of the simulated datasets are presented in @fig-plot_logit_vs_x. The left subplot shows the probability of y=1 as a function of x for Case A and Case B. The right subplot shows the logit function as a function of x for Case A and Case B. The logit function is given by $\\alpha + \\beta x$.\n\n::: {#cell-fig-plot_logit_vs_x .cell execution_count=3}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef logistic(x, alpha, beta):\n    return np.exp(alpha + beta*x) / (1.0 + np.exp(alpha + beta*x))\n\nx_values = np.linspace(0, 10, 50000)\n\n# Case A\nalpha_A, beta_A = -10, 2\npi_A = logistic(x_values, alpha_A, beta_A)\nlogit_A = alpha_A + beta_A * x_values\n\n# Case B\nalpha_B, beta_B = -10, beta_opt\npi_B = logistic(x_values, alpha_B, beta_B)\nlogit_B = alpha_B + beta_B * x_values\n\nfig, axes = plt.subplots(2, 2, figsize=(5, 5))  # Taille un peu plus grande\n\n# (a) Probability vs. x for Case A\naxes[0, 0].plot(x_values, pi_A, color='b', label='Probability')\naxes[0, 0].set_title('Case A: Probability vs. x')\naxes[0, 0].set_xlabel('x')\naxes[0, 0].set_ylabel(r'$\\pi(x,\\alpha,\\beta)$')\naxes[0, 0].axhline(y=0.5, color='k', linestyle='--', label='y=0.5')\naxes[0, 0].axvline(x=5, color='gray', linestyle='--', label='x=5')\naxes[0, 0].set_ylim([0,1])  # Probabilité entre 0 et 1\naxes[0, 0].legend(loc='best')\n\n# (b) Logit vs. x for Case A\naxes[0, 1].plot(x_values, logit_A, color='b', label='Logit')\naxes[0, 1].set_title('Case A: Logit vs. x')\naxes[0, 1].set_xlabel('x')\naxes[0, 1].set_ylabel(r'$\\alpha + \\beta x$')\naxes[0, 1].legend(loc='best')\n\n# (c) Probability vs. x for Case B\naxes[1, 0].plot(x_values, pi_B, color='r', label='Probability')\naxes[1, 0].set_title('Case B: Probability vs. x')\naxes[1, 0].set_xlabel('x')\naxes[1, 0].set_ylabel(r'$\\pi(x,\\alpha,\\beta)$')\naxes[1, 0].axhline(y=0.2, color='k', linestyle='--', label='y=0.2')\naxes[1, 0].axvline(x=5, color='gray', linestyle='--', label='x=5')\naxes[1, 0].set_ylim([0,1])\naxes[1, 0].legend(loc='best')\n\n# (d) Logit vs. x for Case B\naxes[1, 1].plot(x_values, logit_B, color='r', label='Logit')\naxes[1, 1].set_title('Case B: Logit vs. x')\naxes[1, 1].set_xlabel('x')\naxes[1, 1].set_ylabel(r'$\\alpha + \\beta x$')\naxes[1, 1].legend(loc='best')\n\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Logit vs. x for Case A and Case B](bias_sample_files/figure-html/fig-plot_logit_vs_x-output-1.png){#fig-plot_logit_vs_x width=469 height=470}\n:::\n:::\n\n\n# Methodology\n\nTo test the hypothesis that sampling bias controls the optimal class balance required for the best predictive performance of maximum-likelihood logistic regression, samples will be drawn from the two datasets (Case A and Case B) with varying class distributions. The class distributions will be as follows: 50:50, 60:40, 70:30, 80:20, 90:10, 95:5, and 99:1, respectively.\n\nFor each each class sample distribution, 1000 monte-carlo simulations will be carried out and for each simulation the maximum-likelihood logistic regression model will be fitted. The predictive performance of the model will be evaluated using AUC(Area Under the Curve), AU-PCR(Area Under the Precision-Recall Curve), Precision, Recall, F1-score. Those metrics are computed from elements of the confusion matrix : True Positive (TP), False Positive (FP), True Negative (TN), False Negative (FN). The confusion matrix is the matrix with the observed in columns and the predicted in rows. \n\n# Results\n\n## Case A : Balanced Dataset\n\nThe Case A dataset has a balanced class distribution of 50:50. This will be the true distribution of the dataset. Eight random samples will be extracted from the Case A population with varying class distributions of 50:50, 60:40, 70:30, 80:20, 90:10, 95:5, and 99:1. \nA sample with a class distributiion of 60:40 from the Case A is referred as $A_{60:40}$.\nThe sample size for each of the eight samples is determined by fixing the lenght of the majority class (class 0) at 5000. In order word, for the $A_{60:40}$ sample, the number of observations in class 0 is 5000 and the number of observations in class 1 is :\n\n$$\n\\text{Number of observations in class 1} = \\frac{40}{60} \\times 5000 = 3333\n$$\n\n### Eight sub-samples generated from Case A.\n\nThe code below generates the eight sub-samples from the Case A dataset with varying class distributions. \n\n::: {#c48e9d4d .cell execution_count=4}\n``` {.python .cell-code}\ndef create_subsample_fixed_majority(\n    X, y, \n    fraction_class0=0.6,  # e.g., 0.6 => 60:40\n    majority_class0_size=5000,\n    random_state=42\n):\n    \"\"\"\n    Extract a subsample where the number of class-0 = majority_class0_size,\n    and overall fraction of class-0 is fraction_class0.\n    \n    Returns X_sub, y_sub.\n    \"\"\"\n    np.random.seed(random_state)\n    \n    # Indices of class 0 and 1 in the population\n    idx_0 = np.where(y == 0)[0]\n    idx_1 = np.where(y == 1)[0]\n    \n    # We fix #class0 = 5000\n    n0 = majority_class0_size\n    \n    # fraction_class0 = n0 / (n0 + n1) => n1 = n0 * (1 - p)/p\n    p = fraction_class0\n    n1 = int(round(n0 * (1 - p) / p))\n    \n    chosen_0 = np.random.choice(idx_0, size=n0, replace=False)\n    chosen_1 = np.random.choice(idx_1, size=n1, replace=False)\n    \n    chosen_indices = np.concatenate([chosen_0, chosen_1])\n    np.random.shuffle(chosen_indices)\n    \n    return X[chosen_indices], y[chosen_indices]\n```\n:::\n\n\nThe code below gives examples of generating the eight sub-samples from the Case A dataset with varying class distributions. @fig-plotsamplecasea shows the distribution of the dependent variable y for each of the eight sub-samples.\n\n::: {#cell-fig-plotsamplecasea .cell fig='true' execution_count=5}\n``` {.python .cell-code}\n# Module : Generation\n# Inputs : fraction_class0 = [0.5, 0.6, 0.7, 0.8, 0.9, 0.95, 0.99], create_subsample_fixed_majority.\n# Ourputs : A dictionnarie with keys in fraction_class0 and values a tuple (X_sub, y_sub).\n\nfractions_class0 = [0.50, 0.60, 0.70, 0.80, 0.90, 0.95, 0.98, 0.99]\n\nsamples_A = {}\nfor frac0 in fractions_class0:\n    X_sub, y_sub = create_subsample_fixed_majority(\n        xA, yA, \n        fraction_class0=frac0, \n        majority_class0_size=5000,\n        random_state=42\n    )\n    # Store or process each sample\n    samples_A[frac0] = (X_sub, y_sub)\n\n\nffig, axes = plt.subplots(4, 2, figsize=(6, 8))\naxes = axes.ravel()  # on a maintenant 8 sous-graphiques\n\n# 1) Déterminer la fréquence max pour fixer une échelle cohérente\nall_counts = [np.bincount(y_sub) for _, (_, y_sub) in samples_A.items()]\nglobal_max_count = max(counts.max() for counts in all_counts)\n\nfor i, (label, (X_sub, y_sub)) in enumerate(samples_A.items()):\n    ax = axes[i]\n    \n    # 2) Histogramme sur 2 bins => classes 0 et 1\n    ax.hist(y_sub, bins=[-0.5, 0.5, 1.5],  # histogramme \"catégoriel\"\n            color='steelblue', edgecolor='black', alpha=0.7)\n    \n    # 3) Titre + proportion de y=1\n    mean_y = np.mean(y_sub)\n    ax.set_title(f\"{label}:{mean_y:.2f}\", fontsize=9)\n    \n    # 4) Ajuster l’axe X pour forcer l’affichage (0,1)\n    ax.set_xticks([0, 1])\n    ax.set_xticklabels(['0', '1'], fontsize=8)\n    \n    # 5) Limiter l’axe Y pour comparer visuellement entre les sous-figures\n    ax.set_ylim(0, global_max_count)\n    \n    # 6) N’afficher “Frequency” que sur la première colonne\n    #    pour éviter la répétition\n    if i % 2 == 0:\n        ax.set_ylabel('Frequency', fontsize=9)\n    else:\n        ax.set_ylabel('')\n    \n    # 7) Ajouter un label X plus discret\n    ax.set_xlabel('y', fontsize=9)\n    \n    # 8) Afficher le count exact sur chacune des barres\n    counts = np.bincount(y_sub)\n    for j, c in enumerate(counts):\n        ax.text(j, c + 0.5, str(c), ha='center', va='bottom', fontsize=8)\n\nplt.tight_layout()\nplt.show()\n\n```\n\n::: {.cell-output .cell-output-display}\n![Distribution of y for Case A sub-samples](bias_sample_files/figure-html/fig-plotsamplecasea-output-1.png){#fig-plotsamplecasea width=565 height=757}\n:::\n:::\n\n\nA maximum-likelihood logistic regression model will be fitted to each of the eight sub-samples. A plot of the true $p(x)$ versus the estimated $p(x)$ is presented in @fig-testpa.\n\n::: {#cell-fig-testpa .cell execution_count=6}\n``` {.python .cell-code}\nimport numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm\n\n# Inputs : samples_A, fractions_class0\n# Outputs : fig\n\n\n\n\n# For illustration, let's create a 2 x 4 grid to show all eight ratio-samples\nfig, axes = plt.subplots(nrows=4, ncols=2, figsize=(6,8))\naxes = axes.ravel()  # flatten into 1D array [ax0, ax1, ..., ax7]\n\n# Hardcode alpha=-10, beta=2 for \"true\" logistic in Case A\nALPHA_TRUE = -10\nBETA_TRUE  = 2\n\nfor i, (label, (X_sub, y_sub)) in enumerate(samples_A.items()):\n    ax = axes[i]\n    \n    # 1) Put the data into a DataFrame for convenience\n    df_sub = pd.DataFrame({\n        'X': X_sub,       # predictor\n        'y': y_sub        # binary outcome\n    })\n    \n    # 2) Add a constant column for the intercept in statsmodels\n    df_sub = sm.add_constant(df_sub, has_constant='add')  \n    # Now df_sub has columns ['const', 'X', 'y']\n    \n    # 3) Fit the logistic model\n    model = sm.Logit(df_sub['y'], df_sub[['const', 'X']])\n    results = model.fit(disp=False)  # disp=False to suppress output\n    \n    # 4) Predict the fitted probability\n    df_sub['pi_pred'] = results.predict(df_sub[['const', 'X']])\n    \n    # 5) Compute the \"true\" pi for comparison\n    df_sub['pi_true'] = logistic(df_sub['X'].values, alpha=ALPHA_TRUE, beta=BETA_TRUE)\n    \n    # 6) Plot pi_true vs. pi_pred\n    ax.scatter(df_sub['pi_true'], df_sub['pi_pred'], \n               alpha=0.3, s=10, color='blue', edgecolors='none')\n    # Add a diagonal line for reference\n    ax.plot([0, 1], [0, 1], color='black', linestyle='--')\n    # 7) Decorate the subplot\n    ax.set_xlabel(\"True pi(x)\")\n    ax.set_ylabel(\"Predicted pi_hat(x)\")\n    ax.set_title(f\"{label}:{1-label:.2f}\")  # e.g., \"Case A60:40\"\n\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![True vs. estimated p(x) for Case A sub-samples](bias_sample_files/figure-html/fig-testpa-output-1.png){#fig-testpa width=566 height=758}\n:::\n:::\n\n\n@fig-testpa shows that the sample (Case $A_{50:50}$) with no class imbalance and sampling bias has the best fit between the true and estimated probabilities. As the class imbalance increases from 60:40 to 99:1, the fit between the true and estimated probabilities deteriorates. To have more confidence in the results, the distribution of the performance metrics for each of the eight sub-samples using monte-carlo simulations will be carried out.  \n\n### Distribution of the performance metrics for the eight sub-samples from Case A with monte-carlo simulations.\n\nWe perform 1000 monte-carlo simulations for each of the eight sub-samples from Case A. For each simulation, we fit a maximum-likelihood logistic regression model and compute the performance metrics : AUC, AU-PCR, Precision, Recall, F1-score. In order words, in the end of this exercise, we will have 1000 values for each of the performance metrics for each of the eight sub-samples. \n\nThe code below gives the distribution of the performance metrics for each of the eight sub-samples from Case A with monte-carlo simulations.\n\n::: {#ce390c01 .cell execution_count=7}\n``` {.python .cell-code}\nfrom sklearn.metrics import (\n    roc_auc_score,\n    precision_score,\n    recall_score,\n    f1_score,\n    average_precision_score\n)\n\ndef evaluate_model_performance(y_true, y_proba, threshold=0.5):\n    \"\"\"\n    Given true labels and predicted probabilities, compute AUC, AU-PRC,\n    Precision, Recall, and F1 at a chosen threshold.\n    \"\"\"\n    # 1) AUC (ROC)\n    auc = roc_auc_score(y_true, y_proba)\n    \n    # 2) AU-PRC (average precision)\n    auprc = average_precision_score(y_true, y_proba)\n    \n    # 3) Convert probas -> hard predictions\n    y_pred = (y_proba >= threshold).astype(int)\n    \n    # 4) Precision, Recall, F1\n    prec = precision_score(y_true, y_pred, zero_division=0)\n    rec  = recall_score(y_true, y_pred, zero_division=0)\n    f1   = f1_score(y_true, y_pred, zero_division=0)\n    \n    return auc, auprc, prec, rec, f1\n\nfrom sklearn.model_selection import train_test_split\n\nMC_RUNS = 1000\nSAMPLE_SIZE = 5000  # e.g., majority class size if using a fixed majority approach\nratios = [0.50, 0.60, 0.70, 0.80, 0.90, 0.95, 0.98, 0.99]\nresults_list = []\n\n#for r, (X_sub, y_sub) in samples_A.items():\nfor r in ratios:\n    for mc_i in range(MC_RUNS):\n        \n        \n        # 2) Split the subsample into train/test\n        #    stratify ensures class distribution is preserved\n        X_sub, y_sub = create_subsample_fixed_majority(\n          xA, yA,\n          fraction_class0=r,\n          majority_class0_size=SAMPLE_SIZE,\n          random_state=None\n      )\n\n        X_train, X_test, y_train, y_test = train_test_split(\n            X_sub,\n            y_sub,\n            test_size=0.3,\n            random_state=42,\n            stratify=y_sub\n        )\n        \n        # Prepare DataFrame for the training set\n        df_train = pd.DataFrame({'X': X_train, 'y': y_train})\n        df_train = sm.add_constant(df_train, prepend=True, has_constant='add')  \n        # => columns: ['const', 'X', 'y']\n        \n        # 3) Fit logistic regression on the TRAIN portion\n        logit_model = sm.Logit(df_train['y'], df_train[['const', 'X']])\n        result = logit_model.fit(disp=False)\n        \n        # 4) Predict probabilities on the TEST portion\n        df_test = pd.DataFrame({'X': X_test})\n        df_test = sm.add_constant(df_test, prepend=True, has_constant='add')\n        \n        y_proba_test = result.predict(df_test[['const', 'X']])\n        \n        # 5) Evaluate performance metrics on the TEST set\n        auc, auprc, prec, rec, f1 = evaluate_model_performance(y_test, y_proba_test, threshold=0.5)\n        \n        # 6) Store results\n        results_list.append({\n            'ratio_0': r,\n            'auc': auc,\n            'auprc': auprc,\n            'precision': prec,\n            'recall': rec,\n            'f1': f1\n        })\n\n# Convert collected results to a DataFrame\ndf_results_A = pd.DataFrame(results_list)\n# Compute the mean performance metrics for each ratio\ndf_results_A.groupby('ratio_0').mean()\n```\n\n::: {.cell-output .cell-output-display execution_count=7}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>auc</th>\n      <th>auprc</th>\n      <th>precision</th>\n      <th>recall</th>\n      <th>f1</th>\n    </tr>\n    <tr>\n      <th>ratio_0</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0.50</th>\n      <td>0.984082</td>\n      <td>0.984669</td>\n      <td>0.930641</td>\n      <td>0.930979</td>\n      <td>0.930783</td>\n    </tr>\n    <tr>\n      <th>0.60</th>\n      <td>0.984122</td>\n      <td>0.978174</td>\n      <td>0.924246</td>\n      <td>0.908146</td>\n      <td>0.916079</td>\n    </tr>\n    <tr>\n      <th>0.70</th>\n      <td>0.983972</td>\n      <td>0.968164</td>\n      <td>0.914901</td>\n      <td>0.882000</td>\n      <td>0.898063</td>\n    </tr>\n    <tr>\n      <th>0.80</th>\n      <td>0.984141</td>\n      <td>0.952157</td>\n      <td>0.903439</td>\n      <td>0.842323</td>\n      <td>0.871622</td>\n    </tr>\n    <tr>\n      <th>0.90</th>\n      <td>0.984098</td>\n      <td>0.916591</td>\n      <td>0.897012</td>\n      <td>0.777216</td>\n      <td>0.832299</td>\n    </tr>\n    <tr>\n      <th>0.95</th>\n      <td>0.984102</td>\n      <td>0.871136</td>\n      <td>0.885726</td>\n      <td>0.710380</td>\n      <td>0.787117</td>\n    </tr>\n    <tr>\n      <th>0.98</th>\n      <td>0.984290</td>\n      <td>0.799221</td>\n      <td>0.866131</td>\n      <td>0.618839</td>\n      <td>0.717647</td>\n    </tr>\n    <tr>\n      <th>0.99</th>\n      <td>0.983755</td>\n      <td>0.737946</td>\n      <td>0.863657</td>\n      <td>0.552467</td>\n      <td>0.663418</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nThe mean of the performance metrics for each of the eight sub-samples from Case A is presented in @fig-plot_metrics_a.\n\n::: {#cell-fig-plot_metrics_a .cell execution_count=8}\n``` {.python .cell-code}\n# Module : Plotting\n# Inputs : df_results_A\n# Outputs : fig showing the performance metrics vs the ratios\n# Objective : Group data by ratio_0 and plot the performance metrics.\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\ndf_grouped_A = df_results_A.groupby('ratio_0').mean().reset_index()\nmetrics = ['auc', 'auprc', 'precision', 'recall', 'f1']\ncolours = ['blue', 'orange', 'green', 'red', 'purple']\n\nplt.figure(figsize=(6, 5))\n\nfor metric, colour in zip(metrics, colours):\n    plt.plot(df_grouped_A['ratio_0'], df_grouped_A[metric], label=metric, color=colour, marker='o')\n\n# Improve readability with grid and styling\nplt.xlabel(\"Rate of Y = 0\", fontsize=12, weight='bold')\nplt.ylabel(\"Mean of Performance Metrics\", fontsize=12, weight='bold')\nplt.title(\"Mean of Performance Metrics vs Ratios\", fontsize=14, weight='bold')\nplt.xticks(fontsize=10)\nplt.yticks(fontsize=10)\nplt.grid(False)\nplt.legend(title=\"Metrics\", fontsize=10, title_fontsize=12, loc=\"best\")\nplt.tight_layout()\n\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Performance metrics Vs. Ratios for Case A](bias_sample_files/figure-html/fig-plot_metrics_a-output-1.png){#fig-plot_metrics_a width=567 height=470}\n:::\n:::\n\n\nAs the class imbalance and sampling bias increase, the performance metrics deteriorate except for the AUC metric. It seems that the AUC metric is not sensitive to class imbalance and sampling bias.\n\n## Case B : Imbalanced Dataset\n\nThe Case B dataset has an imbalanced class distribution of 80:20. This will be the true distribution of the dataset. Eight random samples will be extracted from the Case B population with varying class distributions of 50:50, 60:40, 70:30, 80:20, 90:10, 95:5, and 99:1.\n\n### Eight sub-samples generated from Case B.\n\nThe code below generates the eight sub-samples from the Case B dataset with varying class distributions. @fig-plotsamplecaseb shows the distribution of the dependent variable y for each of the eight sub-samples.\n\n::: {#cell-fig-plotsamplecaseb .cell execution_count=9}\n``` {.python .cell-code}\n# Module : Generation\n# Inputs : fraction_class0 = [0.5, 0.6, 0.7, 0.8, 0.9, 0.95, 0.99], create_subsample_fixed_majority.\n# Ourputs : A dictionnarie with keys in fraction_class0 and values a tuple (X_sub, y_sub).\n\nfractions_class0 = [0.50, 0.60, 0.70, 0.80, 0.90, 0.95, 0.98, 0.99]\n\nsamples_B = {}\nfor frac0 in fractions_class0:\n    X_sub, y_sub = create_subsample_fixed_majority(\n        xB, yB, \n        fraction_class0=frac0, \n        majority_class0_size=5000,\n        random_state=42\n    )\n    # Store or process each sample\n    samples_B[frac0] = (X_sub, y_sub)\n\n\nfig, axes = plt.subplots(4, 2, figsize=(6, 8))\naxes = axes.ravel()  # on a maintenant 8 sous-graphiques\n\n# 1) Déterminer la fréquence max pour fixer une échelle cohérente\nall_counts = [np.bincount(y_sub) for _, (_, y_sub) in samples_B.items()]\nglobal_max_count = max(counts.max() for counts in all_counts)\n\nfor i, (label, (X_sub, y_sub)) in enumerate(samples_B.items()):\n    ax = axes[i]\n    \n    # 2) Histogramme sur 2 bins => classes 0 et 1\n    ax.hist(y_sub, bins=[-0.5, 0.5, 1.5],  # histogramme \"catégoriel\"\n            color='steelblue', edgecolor='black', alpha=0.7)\n    \n    # 3) Titre + proportion de y=1\n    mean_y = np.mean(y_sub)\n    ax.set_title(f\"{label}:{mean_y:.2f}\", fontsize=9)\n    \n    # 4) Ajuster l’axe X pour forcer l’affichage (0,1)\n    ax.set_xticks([0, 1])\n    ax.set_xticklabels(['0', '1'], fontsize=8)\n    \n    # 5) Limiter l’axe Y pour comparer visuellement entre les sous-figures\n    ax.set_ylim(0, global_max_count)\n    \n    # 6) N’afficher “Frequency” que sur la première colonne\n    #    pour éviter la répétition\n    if i % 2 == 0:\n        ax.set_ylabel('Frequency', fontsize=9)\n    else:\n        ax.set_ylabel('')\n    \n    # 7) Ajouter un label X plus discret\n    ax.set_xlabel('y', fontsize=9)\n    \n    # 8) Afficher le count exact sur chacune des barres\n    counts = np.bincount(y_sub)\n    for j, c in enumerate(counts):\n        ax.text(j, c + 0.5, str(c), ha='center', va='bottom', fontsize=8)\n\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Distribution of y for Case B sub-samples](bias_sample_files/figure-html/fig-plotsamplecaseb-output-1.png){#fig-plotsamplecaseb width=565 height=757}\n:::\n:::\n\n\nA maximum-likelihood logistic regression model will be fitted to each of the eight sub-samples. A plot of the true $p(x)$ versus the estimated $p(x)$ is presented in @fig-plot_true_vs_estimated_p_b.\n\n::: {#cell-fig-plot_true_vs_estimated_p_b .cell execution_count=10}\n``` {.python .cell-code}\nimport numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm\n\n# Inputs : samples_B, fractions_class0\n# Outputs : fig\n\n\n\n\n# For illustration, let's create a 2 x 4 grid to show all eight ratio-samples\nfig, axes = plt.subplots(nrows=4, ncols=2, figsize=(6,8))\naxes = axes.ravel()  # flatten into 1D array [ax0, ax1, ..., ax7]\n\n# Hardcode alpha=-10, beta=2 for \"true\" logistic in Case A\nALPHA_TRUE = -10\nBETA_TRUE  = beta_opt\nprint(BETA_TRUE)\n\nfor i, (label, (X_sub, y_sub)) in enumerate(samples_B.items()):\n    ax = axes[i]\n    \n    # 1) Put the data into a DataFrame for convenience\n    df_sub = pd.DataFrame({\n        'X': X_sub,       # predictor\n        'y': y_sub        # binary outcome\n    })\n    \n    # 2) Add a constant column for the intercept in statsmodels\n    df_sub = sm.add_constant(df_sub, has_constant='add')  \n    # Now df_sub has columns ['const', 'X', 'y']\n    \n    # 3) Fit the logistic model\n    model = sm.Logit(df_sub['y'], df_sub[['const', 'X']])\n    results = model.fit(disp=False)  # disp=False to suppress output\n    \n    # 4) Predict the fitted probability\n    df_sub['pi_pred'] = results.predict(df_sub[['const', 'X']])\n    \n    # 5) Compute the \"true\" pi for comparison\n    df_sub['pi_true'] = logistic(df_sub['X'].values, alpha=ALPHA_TRUE, beta=BETA_TRUE)\n    \n    \n    # 6) Plot pi_true vs. pi_pred\n    ax.scatter(df_sub['pi_true'], df_sub['pi_pred'], \n               alpha=0.3, s=10, color='blue', edgecolors='none')\n    # Add a diagonal line for reference\n    ax.plot([0, 1], [0, 1], color='black', linestyle='--')\n    # 7) Decorate the subplot\n    ax.set_xlabel(\"True pi(x)\")\n    ax.set_ylabel(\"Predicted pi_hat(x)\")\n    ax.set_title(f\"{label}:{1-label:.2f}\")  # e.g., \"Case A60:40\"\n\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1.23775]\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![True vs. estimated p(x) for Case B sub-samples](bias_sample_files/figure-html/fig-plot_true_vs_estimated_p_b-output-2.png){#fig-plot_true_vs_estimated_p_b width=566 height=758}\n:::\n:::\n\n\nHere, it is evident from @fig-plot_true_vs_estimated_p_b that the sample (Case $B_{50:50}$) with balance class no longer has the best fit between the true and estimated probabilities.\n\nThe sample (Case $B_{80:20}$) that performs the best does not have the sampling bias because it that case, the class distribution of the sample (80:20) is equal to the class distribution of the population (80:20). Furthermore, as the sample bias increases, the maximum-likelihood logistic regression model's highly under- or overpredicts the probability.\n\nWhen the distribution of the minority in the sample is less than the distribution of the minority in the population, the model underpredicts the probability. Conversely, when the distribution of the minority in the sample is greater than the distribution of the minority in the population, the model overpredicts the probability.\n\n### Distribution of the performance metrics for the eight sub-samples from Case B with monte-carlo simulations.\n\nWe perform 1000 monte-carlo simulations for each of the eight sub-samples from Case B. For each simulation, we fit a maximum-likelihood logistic regression model and compute the performance metrics : AUC, AU-PCR, Precision, Recall, F1-score. In order words, in the end of this exercise, we will have 1000 values for each of the performance metrics for each of the eight sub-samples.\n\nThe code below gives the distribution of the performance metrics for each of the eight sub-samples from Case B with monte-carlo simulations.\n\n::: {#79eb4f4d .cell execution_count=11}\n``` {.python .cell-code}\nC_RUNS = 1000\nSAMPLE_SIZE = 5000  # e.g., majority class size if using a fixed majority approach\nratios = [0.50, 0.60, 0.70, 0.80, 0.90, 0.95, 0.98, 0.99]\nresults_list = []\n\n#for r, (X_sub, y_sub) in samples_B.items():\nfor r in ratios:\n    # 1) Create a random subsample\n    #    Use None or vary random_state so each iteration is unique\n    for mc_i in range(MC_RUNS):\n      X_sub, y_sub = create_subsample_fixed_majority(\n          xB, yB,\n          fraction_class0=r,\n          majority_class0_size=SAMPLE_SIZE,\n          random_state=None\n      )\n      \n      # 2) Split the subsample into train/test\n      #    stratify ensures class distribution is preserved\n      X_train, X_test, y_train, y_test = train_test_split(\n          X_sub,\n          y_sub,\n          test_size=0.3,\n          random_state=42,\n          stratify=y_sub\n      )\n      \n      # Prepare DataFrame for the training set\n      df_train = pd.DataFrame({'X': X_train, 'y': y_train})\n      df_train = sm.add_constant(df_train, prepend=True, has_constant='add')  \n      # => columns: ['const', 'X', 'y']\n      \n      # 3) Fit logistic regression on the TRAIN portion\n      logit_model = sm.Logit(df_train['y'], df_train[['const', 'X']])\n      result = logit_model.fit(disp=False)\n      \n      # 4) Predict probabilities on the TEST portion\n      df_test = pd.DataFrame({'X': X_test})\n      df_test = sm.add_constant(df_test, prepend=True, has_constant='add')\n      \n      y_proba_test = result.predict(df_test[['const', 'X']])\n      \n      # 5) Evaluate performance metrics on the TEST set\n      auc, auprc, prec, rec, f1 = evaluate_model_performance(y_test, y_proba_test, threshold=0.5)\n      \n      # 6) Store results\n      results_list.append({\n          'ratio_0': r,\n          'auc': auc,\n          'auprc': auprc,\n          'precision': prec,\n          'recall': rec,\n          'f1': f1\n      })\n\n# Convert collected results to a DataFrame\ndf_results_B = pd.DataFrame(results_list)\n# Compute the mean performance metrics for each ratio\ndf_results_B.groupby('ratio_0').mean()\n\n```\n\n::: {.cell-output .cell-output-display execution_count=11}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>auc</th>\n      <th>auprc</th>\n      <th>precision</th>\n      <th>recall</th>\n      <th>f1</th>\n    </tr>\n    <tr>\n      <th>ratio_0</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0.50</th>\n      <td>0.946381</td>\n      <td>0.933513</td>\n      <td>0.857843</td>\n      <td>0.911224</td>\n      <td>0.883693</td>\n    </tr>\n    <tr>\n      <th>0.60</th>\n      <td>0.945966</td>\n      <td>0.904535</td>\n      <td>0.826579</td>\n      <td>0.870215</td>\n      <td>0.847776</td>\n    </tr>\n    <tr>\n      <th>0.70</th>\n      <td>0.946137</td>\n      <td>0.863102</td>\n      <td>0.791813</td>\n      <td>0.812588</td>\n      <td>0.801942</td>\n    </tr>\n    <tr>\n      <th>0.80</th>\n      <td>0.946245</td>\n      <td>0.794082</td>\n      <td>0.751109</td>\n      <td>0.724216</td>\n      <td>0.737141</td>\n    </tr>\n    <tr>\n      <th>0.90</th>\n      <td>0.945992</td>\n      <td>0.652381</td>\n      <td>0.672349</td>\n      <td>0.527701</td>\n      <td>0.590304</td>\n    </tr>\n    <tr>\n      <th>0.95</th>\n      <td>0.946195</td>\n      <td>0.499444</td>\n      <td>0.613992</td>\n      <td>0.301671</td>\n      <td>0.400875</td>\n    </tr>\n    <tr>\n      <th>0.98</th>\n      <td>0.946513</td>\n      <td>0.319746</td>\n      <td>0.114888</td>\n      <td>0.014935</td>\n      <td>0.025306</td>\n    </tr>\n    <tr>\n      <th>0.99</th>\n      <td>0.946483</td>\n      <td>0.218801</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nThe mean of the performance metrics for each of the eight sub-samples from Case B is presented in @fig-plot_metrics_b.\n\n::: {#cell-fig-plot_metrics_b .cell execution_count=12}\n``` {.python .cell-code}\n# Module : Plotting\n# Inputs : df_results_B\n# Objective : Group data by ratio_0 and plot the performance metrics.\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndf_grouped_B = df_results_B.groupby('ratio_0').mean().reset_index()\n\nplt.figure(figsize=(6, 6))\n\nfor metric, colour in zip(metrics, colours):\n    plt.plot(df_grouped_B['ratio_0'], df_grouped_B[metric], label=metric, color=colour, marker='o')\n\n# Improve readability with grid and styling\n\n# Add vertical lines at 0.8.\nplt.axvline(x=0.8, color='black', linestyle='--', linewidth=1.0)\nplt.xlabel(\"Ratio of Y=0\", fontsize=12, weight='bold')\nplt.ylabel(\"Mean of Performance Metrics\", fontsize=12, weight='bold')\nplt.title(\"Mean of Performance Metrics vs Ratios\", fontsize=14, weight='bold')\nplt.xticks(fontsize=10)\nplt.yticks(fontsize=10)\n#plt.grid(False, which='both', linestyle='--', linewidth=0.5, alpha=0.7)\nplt.legend(title=\"Metrics\", fontsize=10, title_fontsize=12, loc=\"best\")\nplt.tight_layout()\n\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Performance metrics Vs. Ratios for Case B](bias_sample_files/figure-html/fig-plot_metrics_b-output-1.png){#fig-plot_metrics_b width=567 height=566}\n:::\n:::\n\n\nWhen the distribution of the minority in the sample is less than the distribution of the minority in the population, the performance metrics deteriorate considerably; when the distribution of the minority in the sample is greater than the distribution of the minority in the population, the performance metrics improve. Similar to Case A, there is not a significant change in the AUC metric values due to class imbalance and sampling bias.\n\nNext, it can be interesting to compare the performance measures from the samples that have the best performance in Case A (Case $A_{50:50}$) and Case B (Case $B_{80:20}$). In case, the sample $A_{50:50}$ that performs the best has no sampling bias and class imbalance, while the sample $B_{80:20}$ that performs the best has no sampling bias but has class imbalance. From these comparisons, it can be concluded that the performance of maximum-likelihood logistic regression is more sensitive to sampling bias than class imbalance. \n\n",
    "supporting": [
      "bias_sample_files/figure-html"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}