{
  "hash": "c075f850c22e27bf922852ab1d7267d1",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \" Sampling Bias and Class Imbalance (Oversampling and Undersampling) in Maximum-likelihood Logistic Regression\"\nsidebar: auto\nauthor:\n  - Jumbong Junior \ncategories: []\ntags: []\n\ntitle-block-banner: false\nformat: \n  html: \n    mainfont: Times New Roman\n    fontsize: 16pt\n\njupyter: python3\n        \n---\n\n\n# Introduction \n\nIn this article, the impact of sampling bias and class imbalance on logistic regression models is explored. We hypothesize that the predictive performance of a logistic regression model is related to the sampling bias associated with the data and it has a performance advantage when the data is balanced. The hypothesis is testing with two simulated datasets : a balanced dataset (50:50) and an imbalanced dataset (80:20).  Each dataset will be sampled to produce samples with the following distribution : 50:50, 60:40, 70:30, 80:20, 90:10, 95:5, 99:1. \n\nThe performance of the logistic regression model will be evaluated using the Area Under the Curve (AUC), Area Under the Precision-Recall Curve (AU-PCR), Precision, Recall, and F1-score. \n\nMonte-Carlo simulations will be carried out to evaluate the distribution of the performance metrics for each of the samples and insure the robustness of the results.\n\nThis study gives some main results : \n- When the distribution of the minority in the sample is less than the distribution of the minority in the population, the performance metrics deteriorate considerably except the AUC metric.; \n- when the distribution of the minority in the sample is greater than the distribution of the minority in the population, the performance metrics improve except the AUC metric; \n- The performance metrics are excellent to evaluate the sample bias in the logistic regression model when the true distribution of the population is balanced.\n- The performance metrics except the AUC metric are sensitive to the class imbalance. So the AUC metric must be avoided to evaluate the performance of the logistic regression model when the class imbalance is present in the sample.\n- In case of imbalance dataset, AUC is not sensitive to class imbalance and sampling bias. However, other metrics such as AU-PCR, Precision, Recall, and F1-score are sensitive to class imbalance and sampling bias. When using these metrics, the performance of the model in the sample deteriorates considerably when the distribution of the minority in the sample is less than the distribution of the minority in the population, and improves when the distribution of the minority in the sample is greater than the distribution of the minority in the population.\n\n\n# 1. Simulated Data Generation\n\nMany authors document that, for logistic regressionthe , the probability distribution of the dependent variable is assumed to be Bernoulli and the mass function f is given by :\n\n$$\nf(y, x, \\alpha, \\beta) = p(x, \\alpha, \\beta)^y(1-p(x, \\alpha, \\beta))^{1-y}\n$$\n\nwhere \n$$\np(x, \\alpha, \\beta) = \\frac{\\exp(\\alpha + \\beta x)}{1 + \\exp(\\alpha + \\beta x)} \n$$\n\nand where y is the dependent variable, x is the independent variable, $\\alpha$ and $\\beta$ are the parameters to be estimated using the maximum likelihood method (MLE).\n\nFor generating the the bernouilli trial y using for a fixed parameter P, we use the following equation :\n\n$$\ny(p) = \n\\begin{cases}\n\\text{dummy} \\leftarrow \\mathrm{rnd}(1), \\\\\n0, & \\text{if dummy} < p, \\\\\n1, & \\text{otherwise}.\n\\end{cases}\n$$\n\nwhere rnd(1) is a random number generator that generates a random number between 0 and 1.\n\nThe conditional bernouilli trials y are then generated by substituting of $p(x, \\alpha, \\beta)$ :\n\n$$\ny(x, \\alpha, \\beta) =\n\\begin{cases}\n0, & \\text{if rnd(1)} < p(x, \\alpha, \\beta), \\\\\n1, & \\text{otherwise}.\n\\end{cases}\n$$\n\n\nIn order to generate the data, the following steps will be respected : \n- Generate x the predictor variable from a uniform distribution, which ranges from 0 to 10.\n- Choose the parameters $\\alpha$ and $\\beta$, which will help to genererate the distribution of the dependent variable y.\n- Generate the dependent variable y using the logistic function $p(x, \\alpha, \\beta)$.\n\n\n## 1.1 Numerical Approach to determine the value of $\\beta$\n\nThe numerical approach consists to determine, for a given value of $\\alpha =-10$, the value of $\\beta$ that will allow to have a proportion of y=1 equal to 0.5 in the case of a balanced dataset and 0.2 in the case of an imbalanced dataset.\n\nThe optimization problem can be formulated as follows :\n\n\n$$\n\\min_{\\beta} \\left( \\text{prop} - \\frac{1}{n}\\sum_{i=1}^{n} \\frac{\\exp(\\alpha + \\beta x_i)}{1 + \\exp(\\alpha + \\beta x_i)} \\right)^2 \n$$\n\nwhere $\\text{prop}$ is the proportion of y=1 in the dataset, $x_i$ is the predictor variable, and $n$ is the number of observations.\n\nThe optimization problem can be solved using the `scipy.optimize.minimize` function with the Nelder-Mead method.\n\n::: {#d9638a9e .cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\nfrom scipy.optimize import minimize\n\n# Define the logistic function\ndef logistic_function(x, alpha, beta):\n    return 1 / (1 + np.exp(-(alpha + beta * x)))\n\n# Objective function: minimize the squared difference between mean(pi) and 0.2\ndef objective(alpha, prop, beta, n):\n    x = np.random.uniform(0, 10, n)  # Simulate x values\n    pi = logistic_function(x, alpha, beta)\n    return (np.mean(pi) - 0.2)**2  # Target mean(pi) = 0.2\n\n# Initial guesses for alpha and beta\ninitial_params = [0]\n\n# Optimize alpha and beta\nresult = minimize(lambda params: objective(-10, 0.2, params, 50000), initial_params, method='Nelder-Mead')\n\n# Get optimized alpha and beta\nbeta_opt = result.x\nprint(f\"Optimized alpha: {-10}, beta: {beta_opt}\")\n\n# Generate x and simulate y\nx = np.random.uniform(0, 10, 1000)\npi = logistic_function(x, -10, beta_opt)\ny = (np.random.uniform(0, 1, 1000) < pi).astype(int)\n\n# Verify proportions\ny_mean = np.mean(y)\nprint(f\"Proportion of y=1: {y_mean:.2f}, y=0: {1-y_mean:.2f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nOptimized alpha: -10, beta: [1.236]\nProportion of y=1: 0.18, y=0: 0.82\n```\n:::\n:::\n\n\n## Simulated Data Generation with $\\alpha = -10$ \n\nThe graphical approach consists to fix the value of $\\alpha$ for example $\\alpha = -10$ and to vary the value of $\\beta$ in order to see the suitable distribution of the dependent variable y.\n\nLet's consider two cases :\n\n  - Case A : A balanced dataset with 50:50 distribution of y=0 and y=1.\n  - Case B : An imbalanced dataset with 80:20 distribution of y=0 and y=1.\n\nThe code below generates the data for the two cases and plots the proportion of y=1 as a function of beta.\n\nThe graph @fig-simulate_case_data, both the datasets have a total of 50,000 events, with the Case A dataset having a class distribution of about 50:50 and Case B dataset having a class distribution of about 80:20.\n\n::: {#cell-fig-simulate_case_data .cell execution_count=2}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Inputs : n_events, alpha, beta, random_state\n# Outputs : x, y, prop_y0, prop_y1\n# Objective : Simulate data from a logistic model with given alpha, beta.\n\ndef simulate_case_data(n_events, alpha, beta, random_state=42):\n    \"\"\"\n    Simulate data from a logistic model with given alpha, beta.\n    \n    x ~ Uniform(0, 10), y ~ Bernoulli(pi(x)), \n    where pi(x) = exp(alpha + beta*x) / (1 + exp(alpha + beta*x)).\n\n    Parameters\n    ----------\n    n_events : int\n        Number of observations (events) to generate.\n    alpha : float\n        Intercept (alpha) for the logistic function.\n    beta : float\n        Slope (beta) for the logistic function.\n    random_state : int\n        Seed for reproducibility.\n\n    Returns\n    -------\n    x : np.ndarray of shape (n_events,)\n        Predictor values sampled from Uniform(0,10).\n    y : np.ndarray of shape (n_events,)\n        Binary outcomes (0 or 1) from Bernoulli trials.\n    prop_y0 : float\n        Proportion of y==0 in the dataset.\n    prop_y1 : float\n        Proportion of y==1 in the dataset.\n    \"\"\"\n    np.random.seed(random_state)\n    \n    # 1) Draw x from Uniform(0,10)\n    x = np.random.uniform(0, 10, size=n_events)\n    \n    # 2) Compute pi(x, alpha, beta)\n    logit = alpha + beta*x\n    pi = np.exp(logit) / (1.0 + np.exp(logit))\n    \n    # 3) Generate y via Bernoulli(pi)\n    dummy = np.random.rand(n_events)\n    y = (dummy < pi).astype(int)\n    \n    # 4) Calculate proportions of 0 and 1\n    prop_y0 = np.mean(y == 0)\n    prop_y1 = np.mean(y == 1)\n    \n    return x, y, prop_y0, prop_y1\n\n# ---------------- Example usage ----------------\n\n\n# Case A: alpha=-10, beta=2 --> expected ~50:50 distribution\nxA, yA, p0_A, p1_A = simulate_case_data(\n    n_events=50000,\n    alpha=-10,\n    beta=2,\n    random_state=42\n)\n\n# Case B: alpha=-10, beta=3.85 --> expected ~80:20 distribution\nxB, yB, p0_B, p1_B = simulate_case_data(\n    n_events=50000,\n    alpha=-10,\n    beta=beta_opt,\n    random_state=42\n)\n\n# Verify proportions\n\n# Suppose p0_A, p1_A, p0_B, p1_B are already defined\n# e.g., p0_A = 0.50; p1_A = 0.50; p0_B = 0.80; p1_B = 0.20\n\nfig, axes = plt.subplots(1, 2, figsize=(5, 3))  # 1 row, 2 columns\n\n# -------- LEFT SUBPLOT: Case A -----------\nax1 = axes[0]\nbar_container_A = ax1.bar(['y=0', 'y=1'], [p0_A, p1_A], color=['steelblue', 'steelblue'])\nax1.set_title('Case A')\nax1.set_xlabel('y')\nax1.set_ylabel('Proportion')\nax1.bar_label(bar_container_A, fmt='%.2f')\n\n# -------- RIGHT SUBPLOT: Case B -----------\nax2 = axes[1]\nbar_container_B = ax2.bar(['y=0', 'y=1'], [p0_B, p1_B], color=['steelblue', 'steelblue'])\nax2.set_title('Case B')\nax2.set_xlabel('y')\nax2.set_ylabel('Proportion')\nax2.bar_label(bar_container_B, fmt='%.2f')\n\nplt.tight_layout()  # improves spacing\nplt.show()\n\n```\n\n::: {.cell-output .cell-output-display}\n![Simulated data from logistic model with alpha=-10, beta=2 and alpha=-10, beta=beta_opt](bias_sample_files/figure-html/fig-simulate_case_data-output-1.png){#fig-simulate_case_data width=470 height=278}\n:::\n:::\n\n\nThe theoretical properties of the simulated datasets are presented in @fig-plot_logit_vs_x. The left subplot shows the probability of y=1 as a function of x for Case A and Case B. The right subplot shows the logit function as a function of x for Case A and Case B. The logit function is given by $\\alpha + \\beta x$.\n\n::: {#cell-fig-plot_logit_vs_x .cell execution_count=3}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef logistic(x, alpha, beta):\n    return np.exp(alpha + beta*x) / (1.0 + np.exp(alpha + beta*x))\n\nx_values = np.linspace(0, 10, 50000)\n\n# Case A\nalpha_A, beta_A = -10, 2\npi_A = logistic(x_values, alpha_A, beta_A)\nlogit_A = alpha_A + beta_A * x_values\n\n# Case B\nalpha_B, beta_B = -10, beta_opt\npi_B = logistic(x_values, alpha_B, beta_B)\nlogit_B = alpha_B + beta_B * x_values\n\nfig, axes = plt.subplots(2, 2, figsize=(5, 5))\n\n# (a) Probability vs. x for Case A\naxes[0, 0].plot(x_values, pi_A, 'r')\naxes[0, 0].set_title('Case A: Probability vs. x')\naxes[0, 0].set_xlabel('x')\naxes[0, 0].set_ylabel(r'$\\pi(x,\\alpha,\\beta)$')  # <-- raw string\n# Add horizontal line at y=0.5 and vertical line at x= 5\naxes[0, 0].axhline(y=0.5, color='k', linestyle='--')\naxes[0, 0].axvline(x=5, color='k', linestyle='--')\n# (b) Logit vs. x for Case A\naxes[0, 1].plot(x_values, logit_A, 'r')\naxes[0, 1].set_title('Case A: Logit vs. x')\naxes[0, 1].set_xlabel('x')\naxes[0, 1].set_ylabel(r'$\\alpha + \\beta x$')      # <-- raw string\n\n# (c) Probability vs. x for Case B\naxes[1, 0].plot(x_values, pi_B, 'r')\naxes[1, 0].set_title('Case B: Probability vs. x')\naxes[1, 0].set_xlabel('x')\naxes[1, 0].set_ylabel(r'$\\pi(x,\\alpha,\\beta)$')  # <-- raw string\n# Add horizontal line at y=0.5 and vertical line at x= 5\naxes[1, 0].axhline(y=0.2, color='k', linestyle='--')\naxes[1, 0].axvline(x=5, color='k', linestyle='--')\n\n# (d) Logit vs. x for Case B\naxes[1, 1].plot(x_values, logit_B, 'r')\naxes[1, 1].set_title('Case B: Logit vs. x')\naxes[1, 1].set_xlabel('x')\naxes[1, 1].set_ylabel(r'$\\alpha + \\beta x$')      # <-- raw string\n\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Logit vs. x for Case A and Case B](bias_sample_files/figure-html/fig-plot_logit_vs_x-output-1.png){#fig-plot_logit_vs_x width=477 height=470}\n:::\n:::\n\n\n# Methodology\n\nTo test the hypothesis that sampling bias controls the optimal class balance required for the best predictive performance of maximum-likelihood logistic regression, samples will be drawn from the two datasets (Case A and Case B) with varying class distributions. The class distributions will be as follows: 50:50, 60:40, 70:30, 80:20, 90:10, 95:5, and 99:1, respectively.\n\nFor each each class sample distribution, 1000 monte-carlo simulations will be carried out and for each simulation the maximum-likelihood logistic regression model will be fitted. The predictive performance of the model will be evaluated using AUC(Area Under the Curve), AU-PCR(Area Under the Precision-Recall Curve), Precision, Recall, F1-score. Those metrics are computed from elements of the confusion matrix : True Positive (TP), False Positive (FP), True Negative (TN), False Negative (FN). The confusion matrix is the matrix with the observed in columns and the predicted in rows. \n\n# Results\n\n## Case A : Balanced Dataset\n\nThe Case A dataset has a balanced class distribution of 50:50. This will be the true distribution of the dataset. Eight random samples will be extracted from the Case A population with varying class distributions of 50:50, 60:40, 70:30, 80:20, 90:10, 95:5, and 99:1. \nA sample with a class distributiion of 60:40 from the Case A is referred as $A_{60:40}$.\nThe sample size for each of the eight samples is determinded by fixing the lenght of the majority class (class 0) at 5000. In order word, for the $A_{60:40}$ sample, the number of observations in class 0 is 5000 and the number of observations in class 1 is :\n\n$$\n\\text{Number of observations in class 1} = \\frac{40}{60} \\times 5000 = 3333\n$$\n\n### Eight sub-samples generated from Case A.\n\nThe code below generates the eight sub-samples from the Case A dataset with varying class distributions. \n\n::: {#1dacfd81 .cell execution_count=4}\n``` {.python .cell-code}\n# Case A: Balanced dataset\n# Inputs : dependent variable y_A, independent variable x_A, majority_class_size0 = 5000, fraction_class0 = 0.5, random_state = 42\n# Outputs : X_sub, y_sub\n\ndef create_subsample_fixed_majority(\n    X, y, \n    fraction_class0=0.6,  # e.g., 0.6 => 60:40\n    majority_class0_size=5000,\n    random_state=42\n):\n    \"\"\"\n    Extract a subsample where the number of class-0 = majority_class0_size,\n    and overall fraction of class-0 is fraction_class0.\n    \n    Returns X_sub, y_sub.\n    \"\"\"\n    np.random.seed(random_state)\n    \n    # Indices of class 0 and 1 in the population\n    idx_0 = np.where(y == 0)[0]\n    idx_1 = np.where(y == 1)[0]\n    \n    # We fix #class0 = 5000\n    n0 = majority_class0_size\n    \n    # fraction_class0 = n0 / (n0 + n1) => n1 = n0 * (1 - p)/p\n    p = fraction_class0\n    n1 = int(round(n0 * (1 - p) / p))\n    \n    chosen_0 = np.random.choice(idx_0, size=n0, replace=False)\n    chosen_1 = np.random.choice(idx_1, size=n1, replace=False)\n    \n    chosen_indices = np.concatenate([chosen_0, chosen_1])\n    np.random.shuffle(chosen_indices)\n    \n    return X[chosen_indices], y[chosen_indices]\n```\n:::\n\n\nThe code below generates the eight sub-samples from the Case A dataset with varying class distributions.\n\n::: {#863c1230 .cell execution_count=5}\n``` {.python .cell-code}\n# Module : Generation\n# Inputs : fraction_class0 = [0.5, 0.6, 0.7, 0.8, 0.9, 0.95, 0.99], create_subsample_fixed_majority.\n# Ourputs : A dictionnarie with keys in fraction_class0 and values a tuple (X_sub, y_sub).\n\nfractions_class0 = [0.50, 0.60, 0.70, 0.80, 0.90, 0.95, 0.98, 0.99]\n\nsamples_A = {}\nfor frac0 in fractions_class0:\n    X_sub, y_sub = create_subsample_fixed_majority(\n        xA, yA, \n        fraction_class0=frac0, \n        majority_class0_size=5000,\n        random_state=42\n    )\n    # Store or process each sample\n    samples_A[frac0] = (X_sub, y_sub)\n\n\nfig, axes = plt.subplots(4, 2, figsize=(6, 8))\naxes = axes.ravel()  # flatten into 1D array [ax0, ax1, ..., ax7]\n\nfor i, (label, (X_sub, y_sub)) in enumerate(samples_A.items()):\n    ax = axes[i]\n    ax.hist(y_sub, bins=2, color='steelblue', edgecolor='black', alpha=0.7)\n    ax.set_title(f\"{label}: {np.mean(y_sub):.2f}\")\n    ax.set_xticks([0, 1])\n    ax.set_xticklabels(['0', '1'])\n    ax.set_xlabel('y')\n    ax.set_ylabel('Frequency')\n\n```\n\n::: {.cell-output .cell-output-display}\n![](bias_sample_files/figure-html/cell-6-output-1.png){width=527 height=671}\n:::\n:::\n\n\nA maximum-likelihood logistic regression model will be fitted to each of the eight sub-samples. A plot of the true $p(x)$ versus the estimated $p(x)$ is presented in @fig-plot_true_vs_estimated_p_A.\n\n::: {#cell-fig-plot_true_vs_estimated_p_A .cell execution_count=6}\n``` {.python .cell-code}\nimport numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm\n\n# Inputs : samples_A, fractions_class0\n# Outputs : fig\n\n\n\n\n# For illustration, let's create a 2 x 4 grid to show all eight ratio-samples\nfig, axes = plt.subplots(nrows=4, ncols=2, figsize=(6,8))\naxes = axes.ravel()  # flatten into 1D array [ax0, ax1, ..., ax7]\n\n# Hardcode alpha=-10, beta=2 for \"true\" logistic in Case A\nALPHA_TRUE = -10\nBETA_TRUE  = 2\n\nfor i, (label, (X_sub, y_sub)) in enumerate(samples_A.items()):\n    ax = axes[i]\n    \n    # 1) Put the data into a DataFrame for convenience\n    df_sub = pd.DataFrame({\n        'X': X_sub,       # predictor\n        'y': y_sub        # binary outcome\n    })\n    \n    # 2) Add a constant column for the intercept in statsmodels\n    df_sub = sm.add_constant(df_sub, has_constant='add')  \n    # Now df_sub has columns ['const', 'X', 'y']\n    \n    # 3) Fit the logistic model\n    model = sm.Logit(df_sub['y'], df_sub[['const', 'X']])\n    results = model.fit(disp=False)  # disp=False to suppress output\n    \n    # 4) Predict the fitted probability\n    df_sub['pi_pred'] = results.predict(df_sub[['const', 'X']])\n    \n    # 5) Compute the \"true\" pi for comparison\n    df_sub['pi_true'] = logistic(df_sub['X'].values, alpha=ALPHA_TRUE, beta=BETA_TRUE)\n    \n    # 6) Plot pi_true vs. pi_pred\n    ax.scatter(df_sub['pi_true'], df_sub['pi_pred'], \n               alpha=0.3, s=10, color='blue', edgecolors='none')\n    # Add a diagonal line for reference\n    ax.plot([0, 1], [0, 1], color='black', linestyle='--')\n    # 7) Decorate the subplot\n    ax.set_xlabel(\"True pi(x)\")\n    ax.set_ylabel(\"Predicted pi_hat(x)\")\n    ax.set_title(f\"{label}\")  # e.g., \"Case A60:40\"\n\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![True vs. estimated p(x) for Case A sub-samples](bias_sample_files/figure-html/fig-plot_true_vs_estimated_p_a-output-1.png){#fig-plot_true_vs_estimated_p_a width=566 height=758}\n:::\n:::\n\n\n@fig-plot_true_vs_estimated_p_A shows that the sample (Case $A_{50:50}$) with no class imbalance and sampling bias has the best fit between the true and estimated probabilities. As the class imbalance increases from 60:40 to 99:1, the fit between the true and estimated probabilities deteriorates. To have more confidence in the results, the distribution of the performance metrics for each of the eight sub-samples using monte-carlo simulations will be carried out.  \n\n### Distribution of the performance metrics for the eight sub-samples from Case A with monte-carlo simulations.\n\nWe perform 1000 monte-carlo simulations for each of the eight sub-samples from Case A. For each simulation, we fit a maximum-likelihood logistic regression model and compute the performance metrics : AUC, AU-PCR, Precision, Recall, F1-score. In order words, in the end of this exercise, we will have 1000 values for each of the performance metrics for each of the eight sub-samples. It can be necessary to plot the boxplot of the performance metrics for each of the eight sub-samples in order to see the distribution of the performance metrics.\n\n::: {#078e4802 .cell execution_count=7}\n``` {.python .cell-code}\nfrom sklearn.metrics import (\n    roc_auc_score,\n    precision_score,\n    recall_score,\n    f1_score,\n    average_precision_score\n)\n\ndef evaluate_model_performance(y_true, y_proba, threshold=0.5):\n    \"\"\"\n    Given true labels and predicted probabilities, compute AUC, AU-PRC,\n    Precision, Recall, and F1 at a chosen threshold.\n    \"\"\"\n    # 1) AUC (ROC)\n    auc = roc_auc_score(y_true, y_proba)\n    \n    # 2) AU-PRC (average precision)\n    auprc = average_precision_score(y_true, y_proba)\n    \n    # 3) Convert probas -> hard predictions\n    y_pred = (y_proba >= threshold).astype(int)\n    \n    # 4) Precision, Recall, F1\n    prec = precision_score(y_true, y_pred, zero_division=0)\n    rec  = recall_score(y_true, y_pred, zero_division=0)\n    f1   = f1_score(y_true, y_pred, zero_division=0)\n    \n    return auc, auprc, prec, rec, f1\n\nfrom sklearn.model_selection import train_test_split\n\nMC_RUNS = 1000\nSAMPLE_SIZE = 5000  # e.g., majority class size if using a fixed majority approach\nratios = [0.50, 0.60, 0.70, 0.80, 0.90, 0.95, 0.98, 0.99]\nresults_list = []\n\nfor r, (X_sub, y_sub) in samples_A.items():\n    for mc_i in range(MC_RUNS):\n        \n        \n        # 2) Split the subsample into train/test\n        #    stratify ensures class distribution is preserved\n        X_train, X_test, y_train, y_test = train_test_split(\n            X_sub,\n            y_sub,\n            test_size=0.3,\n            random_state=None,\n            stratify=y_sub\n        )\n        \n        # Prepare DataFrame for the training set\n        df_train = pd.DataFrame({'X': X_train, 'y': y_train})\n        df_train = sm.add_constant(df_train, prepend=True, has_constant='add')  \n        # => columns: ['const', 'X', 'y']\n        \n        # 3) Fit logistic regression on the TRAIN portion\n        logit_model = sm.Logit(df_train['y'], df_train[['const', 'X']])\n        result = logit_model.fit(disp=False)\n        \n        # 4) Predict probabilities on the TEST portion\n        df_test = pd.DataFrame({'X': X_test})\n        df_test = sm.add_constant(df_test, prepend=True, has_constant='add')\n        \n        y_proba_test = result.predict(df_test[['const', 'X']])\n        \n        # 5) Evaluate performance metrics on the TEST set\n        auc, auprc, prec, rec, f1 = evaluate_model_performance(y_test, y_proba_test, threshold=0.5)\n        \n        # 6) Store results\n        results_list.append({\n            'ratio_0': r,\n            'auc': auc,\n            'auprc': auprc,\n            'precision': prec,\n            'recall': rec,\n            'f1': f1\n        })\n\n# Convert collected results to a DataFrame\ndf_results = pd.DataFrame(results_list)\ndf_results.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=7}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ratio_0</th>\n      <th>auc</th>\n      <th>auprc</th>\n      <th>precision</th>\n      <th>recall</th>\n      <th>f1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.5</td>\n      <td>0.985545</td>\n      <td>0.986193</td>\n      <td>0.936955</td>\n      <td>0.931333</td>\n      <td>0.934136</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.5</td>\n      <td>0.985192</td>\n      <td>0.986305</td>\n      <td>0.929280</td>\n      <td>0.937333</td>\n      <td>0.933289</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.5</td>\n      <td>0.985872</td>\n      <td>0.986666</td>\n      <td>0.926587</td>\n      <td>0.934000</td>\n      <td>0.930279</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.5</td>\n      <td>0.986354</td>\n      <td>0.987258</td>\n      <td>0.943729</td>\n      <td>0.928000</td>\n      <td>0.935798</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.5</td>\n      <td>0.985046</td>\n      <td>0.985356</td>\n      <td>0.928191</td>\n      <td>0.930667</td>\n      <td>0.929427</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nThe boxplot of the performance metrics for each of the eight sub-samples from Case A is presented in @fig-plot_boxplot_metrics_A.\n\n::: {#cell-fig-plot_boxplot_metrics_A .cell execution_count=8}\n``` {.python .cell-code}\n# Module : Plotting\n# Inputs : df_results, ratios\n# Outputs : a boxplot 4x2 of the performances metrics for each ratio.\n# Objective : Filter the results for each ratio and plot the boxplot of the performance metrics. the results must be fig, axes = plt.subplots(4,2, figsize=(10,12)).\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Suppose 'df_results' contains columns: ratio_0, auc, auprc, precision, recall, f1\n# Suppose 'ratios' = [0.50, 0.60, 0.70, 0.80, 0.90, 0.95, 0.98, 0.99]\n\nfig, axes = plt.subplots(nrows=4, ncols=2, figsize=(6,8))\naxes = axes.ravel()  # Flatten (4x2) -> (8,)\n\nmetrics = ['auc', 'auprc', 'precision', 'recall', 'f1']  # which metrics to show\n\nfor i, ratio in enumerate(ratios):\n    ax = axes[i]\n    \n    # Filter df_results for the current ratio\n    df_ratio = df_results[df_results['ratio_0'] == ratio]\n    \n    # We can make a new df that includes only the metrics we want\n    # For instance, to get boxplot of [auc, auprc, precision, recall, f1]:\n    # one approach: melt the dataframe so we have columns: [metric, value]\n    df_melt = df_ratio[metrics].melt(var_name='Metric', value_name='Value')\n    \n    # Create a boxplot on this subplot\n    sns.boxplot(data=df_melt, x='Metric', y='Value', ax=ax)\n    \n    ax.set_title(f\"Ratio_0 = {ratio:.2f}\")\n    ax.set_xlabel(\"\")\n    ax.set_ylabel(\"Performance Value\")\n    # Optionally rotate x-labels if you have many metrics\n    ax.tick_params(axis='x', rotation=20)\n\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Boxplot of performance metrics for Case A sub-samples](bias_sample_files/figure-html/fig-plot_boxplot_metrics_a-output-1.png){#fig-plot_boxplot_metrics_a width=566 height=758}\n:::\n:::\n\n\nAs the class imbalance and sampling bias increase, the performance metrics deteriorate except for the AUC metric. It seems that the AUC metric is not sensitive to class imbalance and sampling bias.\n\n## Case B : Imbalanced Dataset\n\nThe Case B dataset has an imbalanced class distribution of 80:20. This will be the true distribution of the dataset. Eight random samples will be extracted from the Case B population with varying class distributions of 50:50, 60:40, 70:30, 80:20, 90:10, 95:5, and 99:1.\n\n### Eight sub-samples generated from Case B.\n\nThe code below generates the eight sub-samples from the Case B dataset with varying class distributions.\n\n::: {#5fdd92dd .cell execution_count=9}\n``` {.python .cell-code}\n# Module : Generation\n# Inputs : fraction_class0 = [0.5, 0.6, 0.7, 0.8, 0.9, 0.95, 0.99], create_subsample_fixed_majority.\n# Ourputs : A dictionnarie with keys in fraction_class0 and values a tuple (X_sub, y_sub).\n\nfractions_class0 = [0.50, 0.60, 0.70, 0.80, 0.90, 0.95, 0.98, 0.99]\n\nsamples_B = {}\nfor frac0 in fractions_class0:\n    X_sub, y_sub = create_subsample_fixed_majority(\n        xB, yB, \n        fraction_class0=frac0, \n        majority_class0_size=5000,\n        random_state=42\n    )\n    # Store or process each sample\n    samples_B[frac0] = (X_sub, y_sub)\n\n\nfig, axes = plt.subplots(4, 2, figsize=(6, 8))\naxes = axes.ravel()  # flatten into 1D array [ax0, ax1, ..., ax7]\n\nfor i, (label, (X_sub, y_sub)) in enumerate(samples_B.items()):\n    ax = axes[i]\n    ax.hist(y_sub, bins=2, color='steelblue', edgecolor='black', alpha=0.7)\n    ax.set_title(f\"{label}: {np.mean(y_sub):.2f}\")\n    ax.set_xticks([0, 1])\n    ax.set_xticklabels(['0', '1'])\n    ax.set_xlabel('y')\n    ax.set_ylabel('Frequency')\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](bias_sample_files/figure-html/cell-10-output-1.png){width=566 height=758}\n:::\n:::\n\n\nA maximum-likelihood logistic regression model will be fitted to each of the eight sub-samples. A plot of the true $p(x)$ versus the estimated $p(x)$ is presented in @fig-plot_true_vs_estimated_p_B.\n\n::: {#cell-fig-plot_true_vs_estimated_p_B .cell execution_count=10}\n``` {.python .cell-code}\nimport numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm\n\n# Inputs : samples_B, fractions_class0\n# Outputs : fig\n\n\n\n\n# For illustration, let's create a 2 x 4 grid to show all eight ratio-samples\nfig, axes = plt.subplots(nrows=4, ncols=2, figsize=(6,8))\naxes = axes.ravel()  # flatten into 1D array [ax0, ax1, ..., ax7]\n\n# Hardcode alpha=-10, beta=2 for \"true\" logistic in Case A\nALPHA_TRUE = -10\nBETA_TRUE  = beta_opt\n\nfor i, (label, (X_sub, y_sub)) in enumerate(samples_B.items()):\n    ax = axes[i]\n    \n    # 1) Put the data into a DataFrame for convenience\n    df_sub = pd.DataFrame({\n        'X': X_sub,       # predictor\n        'y': y_sub        # binary outcome\n    })\n    \n    # 2) Add a constant column for the intercept in statsmodels\n    df_sub = sm.add_constant(df_sub, has_constant='add')  \n    # Now df_sub has columns ['const', 'X', 'y']\n    \n    # 3) Fit the logistic model\n    model = sm.Logit(df_sub['y'], df_sub[['const', 'X']])\n    results = model.fit(disp=False)  # disp=False to suppress output\n    \n    # 4) Predict the fitted probability\n    df_sub['pi_pred'] = results.predict(df_sub[['const', 'X']])\n    \n    # 5) Compute the \"true\" pi for comparison\n    df_sub['pi_true'] = logistic(df_sub['X'].values, alpha=ALPHA_TRUE, beta=BETA_TRUE)\n    \n    \n    # 6) Plot pi_true vs. pi_pred\n    ax.scatter(df_sub['pi_true'], df_sub['pi_pred'], \n               alpha=0.3, s=10, color='blue', edgecolors='none')\n    # Add a diagonal line for reference\n    ax.plot([0, 1], [0, 1], color='black', linestyle='--')\n    # 7) Decorate the subplot\n    ax.set_xlabel(\"True pi(x)\")\n    ax.set_ylabel(\"Predicted pi_hat(x)\")\n    ax.set_title(f\"{label}\")  # e.g., \"Case A60:40\"\n\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![True vs. estimated p(x) for Case B sub-samples](bias_sample_files/figure-html/fig-plot_true_vs_estimated_p_b-output-1.png){#fig-plot_true_vs_estimated_p_b width=566 height=758}\n:::\n:::\n\n\nHere, it is evident from @fig-plot_true_vs_estimated_p_B that the sample (Case $B_{50:50}$) with balance class no longer perform the best.\n\nThe sample (Case $B_{80:20}$) that performs the best does not have the sampling bias because it that case, the class distribution of the sample (80:20) is equal to the class distribution of the population (80:20). Furthermore, as the sample bias increases, the maximum-likelihood logistic regression model's highly under- or overpredicts the probability.\n\nWhen the distribution of the minority in the sample is less than the distribution of the minority in the population, the model underpredicts the probability. Conversely, when the distribution of the minority in the sample is greater than the distribution of the minority in the population, the model overpredicts the probability.\n\n### Distribution of the performance metrics for the eight sub-samples from Case B with monte-carlo simulations.\n\nWe perform 1000 monte-carlo simulations for each of the eight sub-samples from Case B. For each simulation, we fit a maximum-likelihood logistic regression model and compute the performance metrics : AUC, AU-PCR, Precision, Recall, F1-score. In order words, in the end of this exercise, we will have 1000 values for each of the performance metrics for each of the eight sub-samples. It can be necessary to plot the boxplot of the performance metrics for each of the eight sub-samples in order to see the distribution of the performance metrics.\n\n::: {#6d59bc24 .cell execution_count=11}\n``` {.python .cell-code}\nC_RUNS = 1000\nSAMPLE_SIZE = 5000  # e.g., majority class size if using a fixed majority approach\nratios = [0.50, 0.60, 0.70, 0.80, 0.90, 0.95, 0.98, 0.99]\nresults_list = []\n\nfor r, (X_sub, y_sub) in samples_B.items():\n    for mc_i in range(MC_RUNS):\n        # 1) Create a random subsample\n        #    Use None or vary random_state so each iteration is unique\n        #X_sub, y_sub = create_subsample_fixed_majority(\n       #     xB, yB,\n       #     fraction_class0=r,\n       #     majority_class0_size=SAMPLE_SIZE,\n       #     random_state=None\n       # )\n        \n        # 2) Split the subsample into train/test\n        #    stratify ensures class distribution is preserved\n        X_train, X_test, y_train, y_test = train_test_split(\n            X_sub,\n            y_sub,\n            test_size=0.3,\n            random_state=None,\n            stratify=y_sub\n        )\n        \n        # Prepare DataFrame for the training set\n        df_train = pd.DataFrame({'X': X_train, 'y': y_train})\n        df_train = sm.add_constant(df_train, prepend=True, has_constant='add')  \n        # => columns: ['const', 'X', 'y']\n        \n        # 3) Fit logistic regression on the TRAIN portion\n        logit_model = sm.Logit(df_train['y'], df_train[['const', 'X']])\n        result = logit_model.fit(disp=False)\n        \n        # 4) Predict probabilities on the TEST portion\n        df_test = pd.DataFrame({'X': X_test})\n        df_test = sm.add_constant(df_test, prepend=True, has_constant='add')\n        \n        y_proba_test = result.predict(df_test[['const', 'X']])\n        \n        # 5) Evaluate performance metrics on the TEST set\n        auc, auprc, prec, rec, f1 = evaluate_model_performance(y_test, y_proba_test, threshold=0.5)\n        \n        # 6) Store results\n        results_list.append({\n            'ratio_0': r,\n            'auc': auc,\n            'auprc': auprc,\n            'precision': prec,\n            'recall': rec,\n            'f1': f1\n        })\n\n# Convert collected results to a DataFrame\ndf_results = pd.DataFrame(results_list)\ndf_results.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=11}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ratio_0</th>\n      <th>auc</th>\n      <th>auprc</th>\n      <th>precision</th>\n      <th>recall</th>\n      <th>f1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.5</td>\n      <td>0.944489</td>\n      <td>0.931251</td>\n      <td>0.858868</td>\n      <td>0.900667</td>\n      <td>0.879271</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.5</td>\n      <td>0.946816</td>\n      <td>0.929791</td>\n      <td>0.856604</td>\n      <td>0.908000</td>\n      <td>0.881553</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.5</td>\n      <td>0.950224</td>\n      <td>0.935502</td>\n      <td>0.856173</td>\n      <td>0.924667</td>\n      <td>0.889103</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.5</td>\n      <td>0.947915</td>\n      <td>0.935371</td>\n      <td>0.852651</td>\n      <td>0.922000</td>\n      <td>0.885971</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.5</td>\n      <td>0.948604</td>\n      <td>0.933922</td>\n      <td>0.864385</td>\n      <td>0.909333</td>\n      <td>0.886290</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nThe boxplot of the performance metrics for each of the eight sub-samples from Case B is presented in @fig-plot_boxplot_metrics_B.\n\n::: {#cell-fig-plot_boxplot_metrics_B .cell execution_count=12}\n``` {.python .cell-code}\n# Module : Plotting\n# Inputs : df_results, ratios\n# Outputs : a boxplot 4x2 of the performances metrics for each ratio.\n# Objective : Filter the results for each ratio and plot the boxplot of the performance metrics. the results must be fig, axes = plt.subplots(4,2, figsize=(10,12)).\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Suppose 'df_results' contains columns: ratio_0, auc, auprc, precision, recall, f1\n# Suppose 'ratios' = [0.50, 0.60, 0.70, 0.80, 0.90, 0.95, 0.98, 0.99]\n\nfig, axes = plt.subplots(nrows=4, ncols=2, figsize=(6,8))\naxes = axes.ravel()  # Flatten (4x2) -> (8,)\nmetrics = ['auc', 'auprc', 'precision', 'recall', 'f1']  # which metrics to show\n\nfor i, ratio in enumerate(ratios):\n    ax = axes[i]\n    \n    # Filter df_results for the current ratio\n    df_ratio = df_results[df_results['ratio_0'] == ratio]\n    \n    # We can make a new df that includes only the metrics we want\n    # For instance, to get boxplot of [auc, auprc, precision, recall, f1]:\n    # one approach: melt the dataframe so we have columns: [metric, value]\n    df_melt = df_ratio[metrics].melt(var_name='Metric', value_name='Value')\n    \n    # Create a boxplot on this subplot\n    sns.boxplot(data=df_melt, x='Metric', y='Value', ax=ax)\n    \n    ax.set_title(f\"Ratio_0 = {ratio:.2f}\")\n    ax.set_xlabel(\"\")\n    ax.set_ylabel(\"Performance Value\")\n    # Optionally rotate x-labels if you have many metrics\n    ax.tick_params(axis='x', rotation=20)\n\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Boxplot of performance metrics for Case B sub-samples](bias_sample_files/figure-html/fig-plot_boxplot_metrics_b-output-1.png){#fig-plot_boxplot_metrics_b width=566 height=758}\n:::\n:::\n\n\nWhen the distribution of the minority in the sample is less than the distribution of the minority in the population, the performance metrics deteriorate considerably; when the distribution of the minority in the sample is greater than the distribution of the minority in the population, the performance metrics improve. Similar to Case A, there is not a significant change in the AUC metric values due to class imbalance and sampling bias.\n\nNext, it can be interesting to compare the performance measures from the samples that have the best performance in Case A (Case $A_{50:50}$) and Case B (Case $B_{80:20}$). In case, the sample $A_{50:50}$ that performs the best has no sampling bias and class imbalance, while the sample $B_{80:20}$ that performs the best has no sampling bias but has class imbalance. From these comparisons, it can be concluded that the performance of maximum-likelihood logistic regression is more sensitive to sampling bias than class imbalance. \n\n",
    "supporting": [
      "bias_sample_files/figure-html"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}