{
  "hash": "0c8b482856d94e6f6269fe15eef76921",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Missing Data Assessment\"\nsubtitle: \"\"\ndate: last-modified\nsidebar: auto\nnumber-sections: false\ntoc: true\nauthor:\n  - Jumbong Junior \n\ncategories: []\ntags: [\"MCAR\", \"MAR\", \"MNAR\", \"missing data\"]\ntitle-block-banner: false\nformat: \n  html: \n    mainfont: Times New Roman\n    fontsize: 1.1em\n\njupyter: python3    \n---\n\n::: {#f546bc14 .cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\nimport pandas as pd\n#random seed\nnp.random.seed(991)\n#simulate the variable x\nx = np.random.normal(loc= 50,\nscale= 3,\nsize= 1000)\n#equidistant binning\nx_edb= pd.cut(x= x,\nbins= 4)\n#check bins\nx_edb.value_counts().sort_index()\n```\n\n::: {.cell-output .cell-output-display execution_count=1}\n```\n(41.515, 46.471]    124\n(46.471, 51.407]    559\n(51.407, 56.343]    297\n(56.343, 61.278]     20\nName: count, dtype: int64\n```\n:::\n:::\n\n\n::: {#b611f370 .cell execution_count=2}\n``` {.python .cell-code}\n#quantile binning\nx_qub= pd.qcut(x= x,\nq= 4)\n#check bins\nx_qub.value_counts().sort_index()\n```\n\n::: {.cell-output .cell-output-display execution_count=2}\n```\n(41.534, 47.897]    250\n(47.897, 49.991]    250\n(49.991, 52.048]    250\n(52.048, 61.278]    250\nName: count, dtype: int64\n```\n:::\n:::\n\n\n::: {#d092a496 .cell execution_count=3}\n``` {.python .cell-code}\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Random seed for reproducibility\nnp.random.seed(991)\n\n# Simulate the variable x\nx = np.random.normal(loc=50, scale=3, size=1000)\n\n# Equidistant binning\nx_edb = pd.cut(x, bins=4)\n\n# Quantile binning\nx_qb = pd.qcut(x, q=4)\n\n# Plot\nfig, axes = plt.subplots(2, 2, figsize=(12, 5),\n                         gridspec_kw={'height_ratios':[1,1], 'width_ratios':[1,1]})\nplt.subplots_adjust(wspace=0.3, hspace=0.5)\n\n# Histogram of original values\naxes[0,0].hist(x, bins=30, color='lightgray', edgecolor='none')\naxes[0,0].set_title(\"Original values\", fontsize=13, weight='bold')\naxes[0,0].set_xlabel(\"x\")\naxes[0,0].set_ylabel(\"count\")\n\n# Remove empty cell (bottom-left)\nfig.delaxes(axes[1,0])\n\n# Equidistant binning counts\nx_edb.value_counts().sort_index().plot(kind='bar', color='lightcoral', ax=axes[0,1])\naxes[0,1].set_title(\"Equidistant binning\", fontsize=12, weight='bold')\naxes[0,1].set_xlabel(\"\")\naxes[0,1].set_ylabel(\"\")\n\n# Quantile binning counts\nx_qb.value_counts().sort_index().plot(kind='bar', color='lightcoral', ax=axes[1,1])\naxes[1,1].set_title(\"Quantile binning\", fontsize=12, weight='bold')\naxes[1,1].set_xlabel(\"\")\naxes[1,1].set_ylabel(\"\")\n\n# Clean style\nfor ax in axes.flat:\n    if ax:\n        ax.spines['top'].set_visible(False)\n        ax.spines['right'].set_visible(False)\n\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](1_variable_engineering_files/figure-html/cell-4-output-1.png){width=957 height=532}\n:::\n:::\n\n\n$$\nx_{\\text{normalized}} = \\frac{x - x_{\\min}}{x_{\\max} - x_{\\min}} \\, ( \\text{new}_{\\max} - \\text{new}_{\\min} ) + \\text{new}_{\\min}\n$$\n\n::: {#506f401e .cell execution_count=4}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# -------------------------------\n# Data simulation\n# -------------------------------\nnp.random.seed(123)\nx = np.random.normal(loc=50, scale=3, size=1000)\n\n# Normalization function (min-max scaling)\ndef normalize(x, new_min, new_max):\n    x_r = np.ptp(x)\n    x_t = (x - np.min(x)) / x_r * (new_max - new_min) + new_min\n    return x_t\n\nx_n = normalize(x=x, new_min=0, new_max=10)\n\n# -------------------------------\n# Visualization setup\n# -------------------------------\nsns.set_theme(style=\"whitegrid\", font_scale=1.1)\n\nfig, axes = plt.subplots(1, 2, figsize=(10, 5))\nplt.subplots_adjust(wspace=0.3)\n\n# Histogram of original values\naxes[0].hist(x, bins=30, color='lightgray', edgecolor='black', alpha=0.7)\naxes[0].set_title(\"Original values\", fontsize=13, weight='bold')\naxes[0].set_xlabel(\"x\")\naxes[0].set_ylabel(\"count\")\n\n# Histogram of transformed values\naxes[1].hist(x_n, bins=30, color='lightcoral', edgecolor='black', alpha=0.8)\naxes[1].set_title(\"Transformed values\", fontsize=13, weight='bold')\naxes[1].set_xlabel(\"x normalized\")\naxes[1].set_ylabel(\"count\")\n\n# -------------------------------\n# Global title & annotation\n# -------------------------------\nfig.suptitle(\"Comparison Before and After Normalization\", fontsize=14, weight='bold', y=1.03)\nfig.text(0.5, 0.01,\n         \"Normalization rescales the data to a defined range (here 0–10) without changing its distribution shape.\",\n         ha='center', fontsize=10, color='gray')\n\n# -------------------------------\n# Style adjustments\n# -------------------------------\nfor ax in axes:\n    ax.spines['top'].set_visible(False)\n    ax.spines['right'].set_visible(False)\n    ax.grid(False)\n\nplt.tight_layout()\n\nplt.show()\n\n```\n\n::: {.cell-output .cell-output-display}\n![](1_variable_engineering_files/figure-html/cell-5-output-1.png){width=940 height=511}\n:::\n:::\n\n\n$$\nx_{\\text{standardized}} = \\frac{x - \\mu}{\\sigma}\n$$\n\n::: {#c422ff93 .cell execution_count=5}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# -------------------------------\n# Random seed\n# -------------------------------\nnp.random.seed(123)\n\n# Simulate the variable x\nx = np.random.normal(loc=70, scale=5, size=1000)\n\n# Check mean and standard deviation\nprint([np.mean(x), np.std(x)])\n\n# -------------------------------\n# Standardization function\n# -------------------------------\ndef standardize(x):\n    \"\"\"Standardize the variable x.\"\"\"\n    x_s = (x - np.mean(x)) / np.std(x)\n    return x_s\n\n# Run the function\nx_s = standardize(x)\n\n# Check the standardized mean and std\nprint([np.mean(x_s), np.std(x_s)])\n\n# -------------------------------\n# Visualization setup\n# -------------------------------\nsns.set_theme(style=\"whitegrid\", font_scale=1.1)\n\nfig, axes = plt.subplots(1, 2, figsize=(10, 5))\nplt.subplots_adjust(wspace=0.3)\n\n# Histogram: Original values\naxes[0].hist(x, bins=30, color='lightgray', edgecolor='black', alpha=0.7)\naxes[0].set_title(\"Original values\", fontsize=13, weight='bold')\naxes[0].set_xlabel(\"x\")\naxes[0].set_ylabel(\"count\")\n\n# Histogram: Standardized values\naxes[1].hist(x_s, bins=30, color='lightcoral', edgecolor='black', alpha=0.8)\naxes[1].set_title(\"Transformed values\", fontsize=13, weight='bold')\naxes[1].set_xlabel(\"x standardized\")\naxes[1].set_ylabel(\"count\")\n\n# -------------------------------\n# Global title and note\n# -------------------------------\nfig.suptitle(\"Comparison Before and After Standardization\", fontsize=14, weight='bold', y=1.03)\nfig.text(0.5, 0.01,\n         \"Standardization centers the data (μ = 0) and rescales it so that σ = 1, without altering its shape.\",\n         ha='center', fontsize=10, color='gray')\n\n# -------------------------------\n# Clean style adjustments\n# -------------------------------\nfor ax in axes:\n    ax.grid(False)  # no gridlines\n    ax.spines['top'].set_visible(False)\n    ax.spines['right'].set_visible(False)\n\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[np.float64(69.80217931959604), np.float64(5.003937687581167)]\n[np.float64(1.0222933610748442e-15), np.float64(1.0)]\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](1_variable_engineering_files/figure-html/cell-6-output-2.png){width=940 height=511}\n:::\n:::\n\n\n$$\nx_{\\text{transformed}} = \\Phi^{-1} \\left( \\frac{r - k}{n - 2k + 1} \\right)\n$$\n\n::: {#7579dbf8 .cell execution_count=6}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import rankdata, norm\n\n# -------------------------------\n# Random seed\n# -------------------------------\nnp.random.seed(984)\n\n# Simulate a bimodal variable\nx1 = np.random.normal(loc=10, scale=2, size=500)\nx2 = np.random.normal(loc=20, scale=3, size=500)\nx = np.concatenate((x1, x2))\n\n# -------------------------------\n# Rank-based inverse normal transformation\n# -------------------------------\ndef inverse_normal_transform(x, k=3/8, method=\"average\"):\n    \"\"\"\n    Apply rank-based inverse normal transformation (Blom method by default).\n    x : array-like\n        Input variable\n    k : float, optional\n        Constant (default=3/8)\n    method : str, optional\n        Method for dealing with ties ('average' recommended)\n    \"\"\"\n    n = len(x)\n    r = rankdata(a=x, method=method)\n    xt = norm.ppf((r - k) / (n - 2 * k + 1))\n    return xt\n\n# Apply the transformation\nx_trans = inverse_normal_transform(x)\n\n# -------------------------------\n# Visualization setup\n# -------------------------------\nsns.set_theme(style=\"white\", font_scale=1.1)\nfig, axes = plt.subplots(1, 2, figsize=(10, 5))\nplt.subplots_adjust(wspace=0.3)\n\n# -------------------------------\n# Original variable histogram\n# -------------------------------\naxes[0].hist(x, bins=30, color='lightgray', edgecolor='black', alpha=0.7)\naxes[0].set_title(\"Original values\", fontsize=13, weight='bold')\naxes[0].set_xlabel(\"x\")\naxes[0].set_ylabel(\"count\")\n\n# -------------------------------\n# Transformed variable histogram\n# -------------------------------\naxes[1].hist(x_trans, bins=30, color='lightcoral', edgecolor='black', alpha=0.8)\naxes[1].set_title(\"Rank–based inverse normal transformation\", fontsize=13, weight='bold')\naxes[1].set_xlabel(\"x transformed\")\naxes[1].set_ylabel(\"count\")\n\n# -------------------------------\n# Global title and caption\n# -------------------------------\nfig.suptitle(\"Comparison Before and After Rank–Based Inverse Normal Transformation\",\n             fontsize=14, weight='bold', y=1.03)\nfig.text(0.5, 0.01,\n         \"The transformation converts a non-normal distribution into a shape approximating a standard normal distribution.\",\n         ha='center', fontsize=10, color='gray')\n\n# -------------------------------\n# Style adjustments\n# -------------------------------\nfor ax in axes:\n    ax.grid(False)  # remove gridlines\n    ax.spines['top'].set_visible(False)\n    ax.spines['right'].set_visible(False)\n\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](1_variable_engineering_files/figure-html/cell-7-output-1.png){width=940 height=511}\n:::\n:::\n\n\n$$\nWoE = \\ln \\left( \\frac{dist.ne}{dist.e} \\right)\n$$\n\n::: {#5f239bcd .cell execution_count=7}\n``` {.python .cell-code}\nimport pandas as pd\n\n# -------------------------------\n# Simulate data\n# -------------------------------\ndata = pd.DataFrame({\n    \"category\": [\"A\", \"B\", \"A\", \"C\", \"B\"],\n    \"value\": [1, 2, 3, 4, 5]\n})\n\n# -------------------------------\n# Dummy encoding (equivalent to model.matrix(~ 0 + category, data))\n# -------------------------------\ndummy = pd.get_dummies(data[\"category\"], prefix=\"category\", drop_first=True).astype(int)\n\n# Display the dummy-encoded DataFrame\nprint(dummy)\n\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   category_B  category_C\n0           0           0\n1           1           0\n2           0           0\n3           0           1\n4           1           0\n```\n:::\n:::\n\n\n::: {#968f91e2 .cell execution_count=8}\n``` {.python .cell-code}\nimport pandas as pd\nimport statsmodels.api as sm\ny = data[\"value\"]\nX_with_const = sm.add_constant(dummy)   # like adding intercept in R\nmodel = sm.OLS(y, X_with_const).fit()\n\n# -------------------------------\n# Summary of the regression\n# -------------------------------\nprint(model.summary())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                  value   R-squared:                       0.350\nModel:                            OLS   Adj. R-squared:                 -0.300\nMethod:                 Least Squares   F-statistic:                    0.5385\nDate:                Fri, 17 Oct 2025   Prob (F-statistic):              0.650\nTime:                        21:33:08   Log-Likelihood:                -7.7506\nNo. Observations:                   5   AIC:                             21.50\nDf Residuals:                       2   BIC:                             20.33\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          2.0000      1.275      1.569      0.257      -3.485       7.485\ncategory_B     1.5000      1.803      0.832      0.493      -6.257       9.257\ncategory_C     2.0000      2.208      0.906      0.461      -7.500      11.500\n==============================================================================\nOmnibus:                          nan   Durbin-Watson:                   1.500\nProb(Omnibus):                    nan   Jarque-Bera (JB):                0.510\nSkew:                           0.000   Prob(JB):                        0.775\nKurtosis:                       1.435   Cond. No.                         3.60\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n/Users/juniorjumbong/Desktop/personal-website/.venv/lib/python3.13/site-packages/statsmodels/stats/stattools.py:74: ValueWarning:\n\nomni_normtest is not valid with less than 8 observations; 5 samples were given.\n\n```\n:::\n:::\n\n\n",
    "supporting": [
      "1_variable_engineering_files/figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}