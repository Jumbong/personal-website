{
  "hash": "10b95817241796605ad284d803237eeb",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \" Evaluation of the Logistic Regression : The Hosmer-Lemeshow test\"\nsidebar: auto\nauthor:\n  - Jumbong Junior \ncategories: []\ntags: [Hosmer-Lemeshow, Logistic Regression, Goodness of fit, Python]\ntitle-block-banner: false\nhtml:\n    code-fold : true\njupyter: python3\n\n---\n\n\n\n\n\n\n# Hosmer-Lemeshow Test\n\nModel performance is commonly evaluated based on two main criteria. Discrimination and calibration. The discrimination describes the ability of the model to assign higher probabilities of the outcomes to those observations that actually experience the outcome. A recognized metric for assessing a model’s discrimination is the area under the receiver operating characteristic (ROC) curve. Calibration or fit, on the other hand, captures how accurately the predicted probabilities is close to the actual occurence of the outcome. Several tests and graphical methods have been proposed to assess the calibration of a model, which is often referred to as “goodness of fit.” Among the goodness of fit tests, the Hosmer-Lemeshow (HL) test is the most widely applied approach (Nattino, 2020).\n\n\n**Central idea of the test** : The main idea of the test is to divide the observations into groups and compute a chi2 statistic that reflects the overall mismatch between the observed number of events and the expected number of events in each group-outcome category.\n\nAs with most goodness of fit tests, the HL test is designed to decide between a null hypothesis of perfect fit, where the probabilities assumed by the model are hypothesized to coincide with the real probabilities, and a general alternative hypothesis of nonperfect fit. Let’s present the framework of the test.\n\n\nThe value of the test statistic is :\n$$\nHL  = \\sum_{g=1}^{G} \\frac{(O_{D,k} - E_{D,k})^2}{E_{D,k}} + \\frac{(O_{ND,k} - E_{ND,k})^2}{E_{ND,k}}\n$$\n\nWhere :\n\n- $O_{D,k}$ and $E_{D,k}$ are respectively the number of observed events (default for example) and the number of expected events in the group k.\n- $O_{ND,k}$ and $E_{ND,k}$ are respectively the number of observed non-events (non-default for example) and the number of expected non-events in the group k.\n- G is the number of groups (typically 10).\n\n\n# Background : The Hosmer-Lemeshow goodness of fit test.\n\nTo understand the Hosmer-Lemeshow test, it is important to identify key concepts and definitions:\n\n1. **Observed Proportions of Events**: The proportion of observed successes or failures in the data.\n2. **Expected Event Rates**: The predicted probabilities of success or failure derived from the logistic regression model.\n3. **Logistic Regression**: Logistic regression analyzes the relationship between a dependent categorical variable and a set of independent explanatory variables.\n4. **Dependent Variable**: The dependent variable has two categories:\n   - Failure or Success\n   - Default or Non-default\n5. **Goal of Logistic Regression**:\n   - Predict the probability of an event (\"success\" or \"default\") for any given value of the predictor(s).\n\n## Logistic Regression Model\n\n- Let  Y be the dependent variable:\n  $Y = \\begin{cases} \n  1 & \\text{if default} \\\\\n  0 & \\text{if not default}\n  \\end{cases}$\n\n- Let $( X_1, \\ldots, X_p )$ represent the set of  p \\explanatory variables.\n\n- The logistic regression equation can be written as:\n  $$\n  \\text{logit}(P(Y = 1 \\mid X)) = \\beta_0 + \\beta_1 X_1 + \\ldots + \\beta_p X_p\n  $$\n\n- Solving for  P:\n  $$\n  P(Y = 1 \\mid X) = \\frac{1}{1 + \\exp{-(\\beta_0 + \\beta_1 X_1 + \\ldots + \\beta_p X_p)}}\n  $$\n\n- Here,  $\\beta_0, \\beta_1, \\ldots, \\beta_p $ are parameters to be estimated.\n\n## Computing Probabilities\n\nFor a dataset with  N individuals:\n\n1. For each individual  i, compute the probability of success:\n   $$\n   P_i = P(Y_i = 1 \\mid X_1^i, \\ldots, X_p^i) = \\frac{1}{1 + \\exp{-(\\beta_0 + \\beta_1 X_1^i + \\ldots + \\beta_p X_p^i)}}\n   $$\n\n2. $P_i$ represents the expected probability for individual i .\n\n3. Create a table of individuals with their observed outcomes  Y and predicted probabilities $P_i$.\n\n\n\n#### Example Table\nAfter computing $P_i$ for all individuals, the results can be summarized in a table:\n\n| Individual | Event ($Y$) | $P_i$ |\n|------------|----------------|-----------|\n| 1          | 1              | 0.8       |\n| 2          | 0              | 0.2       |\n| 3          | 1              | 0.9       |\n| 4          | 0              | 0.1       |\n| ...        | ...            | ...       |\n| N    | 1              | 0.95      |\n\nIf the logistic regression fits well, the predicted probability $P_i$ for each individual should align closely with the observed outcome Y Specifically:\n\n- When Y=1 (the event occurred), \n$P_i$ should be close to 1, reflecting high confidence in predicting the event.\n\n- When Y = 0 (the event did not occur), $P_i$ should be close to 0, reflecting high confidence in predicting non-occurrence.\n\n### Performing the Hosmer-Lemeshow Test\n\nAfter this stage, it is not difficult to carry out the Hosmer-Lemeshow test. What is necessary is ordering and grouping individuals. The Hosmer-Lemeshow test can be performed by dividing the predicted probabilities (Pi) into deciles (10 groups based on percentile ranks) and then computing the Chi-square statistic that compares the predicted to the observed frequencies (Hyeoun-AE, 2013).\n\nThe value of the Hosmer-Lemeshow statistic is given by:\n\n$$\nH = \\sum_{g=1}^{G} \\frac{(O_g - E_g)^2}{E_g} + \\frac{(n_g - O_g - (n_g - E_g))^2}{n_g - E_g}\n$$\n\nWhere:\n\n- G : Number of groups (typically 10).\n- $O_g$ : Observed number of events in group g.\n- $E_g$ : Expected number of events in group g or the sum of predicted probabilities for the group g ($\\sum_{i =1}^{n_g}P_i$). \n- $n_g$ : Total number of individuals in group g.\n\n### Example of Computing Each Element in One Group\n\nTo illustrate how the statistic is computed for a single group  g :\n\n- Suppose the group contains  $n_g = 100$ individuals.\n- Out of these,  $O_g = 20$ individuals experienced the event (e.g., default).\n- The sum of predicted probabilities for the group is  $E_g = 18.5$.\n\nUsing the formula:\n\n$$\nH_g = \\frac{(O_g - E_g)^2}{E_g} + \\frac{(n_g - O_g - (n_g - E_g))^2}{n_g - E_g}\n$$\n\nSubstitute the values:\n\n- First term:  $\\frac{(20 - 18.5)^2}{18.5}$\n- Second term:  $\\frac{(100 - 20 - (100 - 18.5))^2}{100 - 18.5}$\n\nCalculate each term and sum them to obtain the contribution of group  g to the overall Hosmer-Lemeshow statistic.\n\n### Interpreting the Hosmer-Lemeshow Test\n\nUnder the null hypothesis (the observed default numbers correspond to the expected default numbers), the test statistic asymptotically follows the  $\\chi^2 $ distribution with  G - 2  degrees of freedom, where  G is the number of groups.\n\n- If the p-value is higher than 5%, it indicates a small statistic and thus a limited gap between observed defaults and expected defaults, suggesting a good model fit.\n- If the p-value is lower than 5%, it indicates a significant discrepancy between observed and expected values, suggesting a poor model fit.\n\n**Caution**: Hosmer and Lemeshow recommend avoiding the use of this test when there is a small number of observations (less than 400), as the test may yield unreliable results.\n\n# Application in python\n\nTo implement the Hosmer-Lemeshow test in python, a dataset is essential. The dependent variable or the event Y is generated using the bernouilli distribution. To simplify, only two independent continuous variables $X_1$ and $X_2$ are drawn from the normal distribution. In this example, the dataset have 1000 individuals.\n\n::: {#7149ad58 .cell execution_count=1}\n``` {.python .cell-code}\n#Entry : Python package, the number of indivuals and the parameter of the normal law : 0.5\n#Output : A dataset of 1000 individuals.\n# Objective : Generate a dataset of 1000 individuals. Generate the Y variable using the bernouill law and X1 and X2 using the normal law.\nimport jupyter_cache\nimport pandas as pd\nimport numpy as np \n\n\n# Set random seed for reproducibility\nnp.random.seed(42)\n\n# Generate data for 1000 individuals\nn_individuals = 1000\n\n# Generate X1 and X2 from a normal distribution\nX1 = np.random.normal(loc=0, scale=1, size=n_individuals)\nX2 = np.random.normal(loc=0, scale=1, size=n_individuals)\n\n# Generate Y using a Bernoulli distribution with a fixed probability of 0.5\nY = np.random.binomial(1, 0.5, size=n_individuals)\n\n# Create a DataFrame\ndata = pd.DataFrame({\n    'X1': X1,\n    'X2': X2,\n    'Y': Y\n})\n\n# Display the first few rows of the dataset\ndata.head(4)\n```\n\n::: {.cell-output .cell-output-display execution_count=1}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>X1</th>\n      <th>X2</th>\n      <th>Y</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.496714</td>\n      <td>1.399355</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-0.138264</td>\n      <td>0.924634</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.647689</td>\n      <td>0.059630</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1.523030</td>\n      <td>-0.646937</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nThe parameters of logistic regression is estimated and the probability of the event $P_(Y =1 | X_1, X_2)$.\n\n::: {#3fd71821 .cell execution_count=2}\n``` {.python .cell-code}\n# Entry : the data dataset data_2.\n# Output : A dataset with the event variable Y and the Pi expected probability computed for each individuals.\n# Objective : Estimating of parameters and compute pi using sm.Logit.\n\nimport statsmodels.api as sm\n\n# Prepare the data for logistic regression\nX = data[['X1', 'X2']]\nX = sm.add_constant(X)  # Add an intercept term\ny = data['Y']\n\n# Fit the logistic regression model\nlogit_model = sm.Logit(y, X)\nresult = logit_model.fit()\n\n# Predict the probability of Y = 1 for each individual\ndata['P(Y=1 | X1, X2)'] = result.predict(X)\n\n# Add an Individual ID column\ndata['Individual'] = range(1, len(data) + 1)\n\ndata_2 = data = data[['Individual', 'Y', 'P(Y=1 | X1, X2)']]\n\ndata_2.head(5)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nOptimization terminated successfully.\n         Current function value: 0.691401\n         Iterations 4\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=2}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Individual</th>\n      <th>Y</th>\n      <th>P(Y=1 | X1, X2)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0.531302</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>0</td>\n      <td>0.508046</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>0</td>\n      <td>0.520704</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>0</td>\n      <td>0.537612</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>1</td>\n      <td>0.502806</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nTo compute the Hosmer-test, individuals are divided into deciles (10 g groups) based on their predicted probabilities $P_i$. For each group g,  compute the Observed number of the event in each group g ($O_g$) and the expected number of event in each group g ($E_g$). \n\n::: {#02b0d86f .cell execution_count=3}\n``` {.python .cell-code}\n# Entry : the dataset data_2.\n# Output : the group of deciles with the O_g observed number of event by group and the E_g expected number of event by group.\n\n\n# Create decile groups based on the predicted probabilities\ndata_2['Decile'] = pd.qcut(data_2['P(Y=1 | X1, X2)'], q=10, labels=False) + 1\n\n# Calculate Observed (Og) and Expected (Eg) numbers for each decile group\ngrouped = data_2.groupby('Decile').agg(\n    n_g=('Y', 'size'),  # Number of individuals per group\n    O_g=('Y', 'sum'),  # Observed number of events\n    E_g=('P(Y=1 | X1, X2)', 'sum')  # Expected number of events\n).reset_index()\n\ngrouped.head(5)\n```\n\n::: {.cell-output .cell-output-display execution_count=3}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Decile</th>\n      <th>n_g</th>\n      <th>O_g</th>\n      <th>E_g</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>100</td>\n      <td>44</td>\n      <td>45.189216</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>100</td>\n      <td>44</td>\n      <td>47.344473</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>100</td>\n      <td>47</td>\n      <td>48.343571</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>100</td>\n      <td>52</td>\n      <td>49.132066</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>100</td>\n      <td>48</td>\n      <td>49.877603</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nFinally, it is easier to compute the value of the Hosmer-Lemeshow statistic. \n\n::: {#0c8dbc55 .cell execution_count=4}\n``` {.python .cell-code}\n# Entry : the grouped dataset.\n# Output : the Hosmer-Lemeshow test.\n# Objective : Using the grouped data in order to calculate the Hosmer-Lemeshow statistic HL.\n\n# Compute the Hosmer-Lemeshow test statistic\n\n# Calculate the HL term for each group\ngrouped['HL_term'] = ((grouped['O_g'] - grouped['E_g']) ** 2) / grouped['E_g'] + \\\n                     ((grouped['n_g'] - grouped['O_g'] - (grouped['n_g'] - grouped['E_g'])) ** 2) / (grouped['n_g'] - grouped['E_g'])\n\n# Calculate the total HL statistic\nHL_statistic = grouped['HL_term'].sum()\n\n# Degrees of freedom: Number of groups - 2\ndegrees_of_freedom = len(grouped) - 2\n\n# Display the Hosmer-Lemeshow test results\nHL_statistic, degrees_of_freedom\n```\n\n::: {.cell-output .cell-output-display execution_count=4}\n```\n(np.float64(5.49669887032658), 8)\n```\n:::\n:::\n\n\nUnder the null hypothesis (the observed default numbers correspond to the expected default numbers) the test statistic asymptotically follows a $\\chi_2$ distribution with 8 degrees of freedom. The p value is given by the code below.\n\n::: {#7a550bd7 .cell execution_count=5}\n``` {.python .cell-code}\n# Entry : the HL_statistic and the degrees_of_freedom.\n# Output : the P value of the test.\n# Objective : Knowing that the HL follows the chi2 distribution with 8 degrees of freedom, we compute the p value.\n\nfrom scipy.stats import chi2\n\n# Calculate the p-value for the HL test\np_value = 1 - chi2.cdf(HL_statistic, degrees_of_freedom)\n\n# Display the p-value\np_value\n```\n\n::: {.cell-output .cell-output-display execution_count=5}\n```\nnp.float64(0.7034057045508038)\n```\n:::\n:::\n\n\nIn this example, the p value is 0.7 higher than 5%, it indicates as small statistic and so a limited gap between observed default and expected ones and so a good model fit. Put in nutshell, two funcions can be used to compute the Hosmer-Lemeshow test. The first one is the homser_lemeshow_test and the second one is the hosmer_lemeshow_test2.\n\n::: {#a1448668 .cell execution_count=6}\n``` {.python .cell-code}\n# Entry : A dataset which contains The dependent variable Y, the predicted probabilities P, or the discretized probabilities P or rating classes if possible and the number of groups if we don't have the rating classes.\n# Output : The Hosmer-Lemeshow test.\n# Objective : Compute the Hosmer-Lemeshow test.\n\ndef homser_lemeshow_test(data, depend_variable, pred_prb, rating_class = None, n_groups = 10):\n    \"\"\"\n    Compute the Hosmer-Lemeshow test.\n    \n    Parameters:\n    data (DataFrame): The dataset containing the dependent variable Y and the predicted probabilities P.\n    depend_variable (str): The name of the dependent variable Y.\n    pred_prb (str): The name of the predicted probabilities P.\n    rating_class (str): The name of the rating classes if available.\n    n_groups (int): The number of groups for the test.\n    \n    Returns:\n    HL_statistic (float): The Hosmer-Lemeshow test statistic.\n    degrees_of_freedom (int): The degrees of freedom for the test.\n    p_value (float): The p-value for the test.\n    \"\"\"\n    # Create decile groups based on the predicted probabilities\n    if rating_class is None:\n        data['Decile'] = pd.qcut(data[pred_prb], q=n_groups, labels=False) + 1\n    else:\n        data['Decile'] = data[rating_class]\n    \n    # Calculate Observed (Og) and Expected (Eg) numbers for each decile group\n    grouped = data.groupby('Decile').agg(\n        n_g=(depend_variable, 'size'),  # Number of individuals per group\n        O_g=(depend_variable, 'sum'),  # Observed number of events\n        E_g=(pred_prb, 'sum')  # Expected number of events\n    ).reset_index()\n    \n    # Compute the HL term for each group\n    grouped['HL_term'] = ((grouped['O_g'] - grouped['E_g']) ** 2) / grouped['E_g'] + \\\n                         ((grouped['n_g'] - grouped['O_g'] - (grouped['n_g'] - grouped['E_g'])) ** 2) / (grouped['n_g'] - grouped['E_g'])\n    \n    # Calculate the total HL statistic\n    HL_statistic = grouped['HL_term'].sum()\n    \n    # Degrees of freedom: Number of groups - 2\n    degrees_of_freedom = len(grouped) - 2\n    \n    # Calculate the p-value for the HL test\n    p_value = 1 - chi2.cdf(HL_statistic, degrees_of_freedom)\n    \n    return HL_statistic, degrees_of_freedom, p_value\n\n# Test the function with the example data\nHL_statistic, degrees_of_freedom, p_value = homser_lemeshow_test(data_2, 'Y', 'P(Y=1 | X1, X2)')\nHL_statistic, degrees_of_freedom, p_value\n```\n\n::: {.cell-output .cell-output-display execution_count=6}\n```\n(np.float64(5.49669887032658), 8, np.float64(0.7034057045508038))\n```\n:::\n:::\n\n\n::: {#48bf7b63 .cell execution_count=7}\n``` {.python .cell-code}\ndef hosmer_lemeshow_test2(data, depend_variable, pred_prb, rating_class = None, n_groups = 10):\n    \"\"\"\n    Compute the Hosmer-Lemeshow test.\n    \n    Parameters:\n    data (DataFrame): The dataset containing the dependent variable Y and the predicted probabilities P.\n    depend_variable (str): The name of the dependent variable Y.\n    pred_prb (str): The name of the predicted probabilities P.\n    rating_class (str): The name of the rating classes if available.\n    n_groups (int): The number of groups for the test.\n    \n    Returns:\n    HL_statistic (float): The Hosmer-Lemeshow test statistic.\n    degrees_of_freedom (int): The degrees of freedom for the test.\n    p_value (float): The p-value for the test.\n    \"\"\"\n    # Create decile groups based on the predicted probabilities\n    if rating_class is None:\n        data['Decile'] = pd.qcut(data[pred_prb], q=n_groups, labels=False) + 1\n    else:\n        data['Decile'] = data[rating_class]\n    \n    # Compute the observed and expected number of events for each decile group\n    obsevents_1 = data[depend_variable].groupby(data['Decile']).sum()\n    obsevents_0 = data[depend_variable].groupby(data['Decile']).count() - obsevents_1\n    expevents_1 = data[pred_prb].groupby(data['Decile']).sum()\n    expevents_0 = data[pred_prb].groupby(data['Decile']).count() - expevents_1\n    hosmer = ((obsevents_1 - expevents_1) ** 2 / expevents_1).sum() + ((obsevents_0 - expevents_0) ** 2 / expevents_0).sum()\n\n    # Degrees of freedom: Number of groups - 2\n    degrees_of_freedom = len(obsevents_1) - 2\n\n    # Calculate the p-value for the HL test\n    p_value = 1 - chi2.cdf(hosmer, degrees_of_freedom)\n\n    return hosmer, degrees_of_freedom, p_value\n\n# Test the function with the example data\nhosmer, degrees_of_freedom, p_value = hosmer_lemeshow_test2(data_2, 'Y', 'P(Y=1 | X1, X2)')\nhosmer, degrees_of_freedom, p_value\n```\n\n::: {.cell-output .cell-output-display execution_count=7}\n```\n(np.float64(5.496698870326581), 8, np.float64(0.7034057045508036))\n```\n:::\n:::\n\n\n# Conclusion\n\nThe Hosmer-Lemeshow test is a valuable tool for evaluating the goodness-of-fit of a logistic regression model. By comparing the observed and expected event rates in decile groups, the test provides insights into the model's predictive performance. A high p-value suggests a good model fit, while a low p-value indicates a significant discrepancy between observed and expected values. However, caution should be exercised when using the test with small sample sizes, as it may yield unreliable results.\n\n",
    "supporting": [
      "HosmerL_files/figure-html"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}