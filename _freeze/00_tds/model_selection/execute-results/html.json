{
  "hash": "768d2312565c035f44dfc3d7562262ea",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Model Selection\"\nsubtitle: \"\"\ndate: last-modified\nsidebar: auto\nnumber-sections: false\ntoc: true\nauthor:\n  - Jumbong Junior \n\ncategories: []\ntags: [\"Model Selection\"]\ntitle-block-banner: false\nbibliography: references.bib\nformat: \n  html: \n    mainfont: Times New Roman\n    fontsize: 1.1em\n\njupyter: python3\nnotice: |\n    @wasserman2004all \n---\n\n\n\n# Introduction \nWe will discuss the problem that may arise in multiple regression in practice. We have data with many variables but we may not want to include all of them in the model. We may not want to include all of them because some variables may be irrelevat, or because a smaller model with fewer variables has several advantages: It might give better predictions than a larger model and it is more parimonious [simple] then easier to interpret.In fact, as you add more variables, the bias of the model decreases, but the variance increases. This is known as the bias-variance trade-off. Too few variables yield high bias [this called underfitting].Too many covariates yields high variance [this called overfitting]. Good prediction requires a balance between bias and variance.\n\nMême après avoir utilisé une approche experte, [ou éliminer les rédondances entre les variables fortement corrélés, ou les variables qui ont un vif élévé dans le cadre de certains modèles de régressions] pour select relevant variables, il est possible que nous ayons encore trop de variables. A smaller model with fewer variables has several advantages: It might give better predictions than a larger model and it is more parimonious [simple] then easier to interpret. If we take the example of regression, as you add more variables, the bias of the model decreases, but the variance increases. This is known as the bias-variance trade-off. Too few variables yield high bias [this called underfitting].Too many covariates yields high variance [this called overfitting]. Good prediction requires a balance between bias and variance.\n\nC'est dans ce contexte que les mèthodes de sélection de variables sont utiles. There exists two main approaches to variable selection that we will explore : the first method consists to minimize the risk prediction and the second method assumes some subset of the $\\beta$ coefficients are exactly equal to 0 and tries to find the true model, that is, the smallest sub-model consisting of nonzero $\\beta$ terms.\n\n",
    "supporting": [
      "model_selection_files/figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}