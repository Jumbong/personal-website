{
  "hash": "24635b7c0ee971f517a38bca9ff902ac",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Proportional Odds Model for Ordinal Logistic Regression\"\nsubtitle: \"Assessing Proportionality in the Proportional Odds Model for Ordinal Logistic Regression using python\"\ndate: last-modified\nsidebar: auto\nnumber-sections: false\ntoc: true\nauthor:\n  - Jumbong Junior \n\ncategories: []\ntags: [Time Series, Spurious Regression]\ntitle-block-banner: false\nbibliography: references.bib\nformat: \n  html: \n    mainfont: Times New Roman\n    fontsize: 1.1em\n\njupyter: python3\nnotice: |\n    @mccullagh1980regression @brant1990assessing @wine_quality_186\n---\n\n\n\n\n\nThe proportional odds model for ordinal logistic regression was first introduced by @mccullagh1980regression. This model extends binary logistic regression to situations where the dependent variable is ordinalâ€”that is, it consists of ordered categorical values. The proportional odds model is built on several assumptions, including independence of observations, linearity of the log-odds, absence of multicollinearity among predictors, and, most importantly, the proportional odds assumption. This last assumption states that the regression coefficients are constant across all thresholds of the ordinal dependent variable. Ensuring the proportional odds assumption holds is crucial for the validity and interpretability of the model.\n\nA variety of methods have been proposed in the literature to assess model fit and, in particular, to test the proportional odds assumption. In this paper, we focus on two approaches developed by Brant in his article @brant1990assessing, \"Assessing Proportionality in the Proportional Odds Model for Ordinal Logistic Regression.\" We also demonstrate how to implement these techniques in Python, applying them to real-world data. Whether you come from a background in data science, machine learning, or statistics, this article aims to help your understand how to evaluate model fit in ordinal logistic regression.\n\nThis paper is organized into four main sections:\n\n1. The first section introduces the proportional odds model and its assumptions.\n2. The second section discusses how to assess the proportional odds assumption using the likelihood ratio test.\n3. The third section covers the assessment of the proportional odds assumption using the separate fits approach.\n4. The final section provides examples, illustrating the implementation of these assessment methods in Python with data.\n\n\n\n## Introduction to the Proportional Odds Model\n\nBefore presenting the model, we introduce the data structure. We assume we have $N$ independent observations. Each observation is represented by a vector of $p$ explanatory variables $X_i = (X_{i1}, X_{i2}, \\ldots, X_{ip})$, along with a dependent or response variable $Y$ that takes ordinal values from $1$ to $K$. The proportional odds model specifically models the cumulative distribution probabilities of the response variable $Y$, defined as $\\gamma_j = P(Y \\leq j \\mid X_i)$ for $j = 1, 2, \\dots, K-1$, as functions of the explanatory variables $X_i$. The model is formulated as follows:\n\n$$\n\\text{logit}(\\gamma_j) = \\log\\left(\\frac{\\gamma_j}{1 - \\gamma_j}\\right) = \\theta_j - \\beta^\\top \\mathbf{X}\n$$ {#eq-proportional_odds_model}\n\nWhere $\\theta_j$ are the intercepts for each category j and respect the condition $\\theta_1 < \\theta_2 < ... < \\theta_{K-1}$, and $\\beta$ is the vector of regression coefficients which are the same for all categories. \nWe observe a monotonic trend in the coefficients $\\theta_j$ across the categories of the response variable Y.\n\n\nThis model is also known as the grouped continuous model, as it can be derived by assuming the existence of a continuous latent variable $Y^*$. This latent variable follows a linear regression model with conditional mean $\\eta = \\boldsymbol{\\beta}^{\\top} \\mathbf{X}$, and it relates to the observed ordinal variable $Y$ through thresholds $\\theta_j$ defined as follows:\n$$\ny^* = {\\beta}^{T}\\mathbf{X} + \\epsilon\n$$ {#eq-latent_variable_model}\n\n\nwhere $\\epsilon$ is an error term (random noise), generally assumed to follow a standard logistic distribution in the proportional odds model.\n\nThe latent variable $Y^*$ is unobserved and partitioned into intervals defined by thresholds $\\theta_1, \\theta_2, \\dots, \\theta_{K-1}$, generating the observed ordinal variable $Y$ as follows:\n\n$$\nY = \\begin{cases}\n1 & \\text{if } Y^* \\leq \\theta_1 \\\\\n2 & \\text{if } \\theta_1 < Y^* \\leq \\theta_2 \\\\\n\\vdots & \\\\\nK & \\text{if } Y^* > \\theta_{K-1}\n\\end{cases}\n$$ {#eq-observed_variable_model}\n\n\n\nIn the next section, we introduce the various approaches proposed by @brant1990assessing for assessing the proportional odds assumption. These methods evaluate whether the regression coefficients remain constant across the categories defined by the ordinal response variable.\n\n## Assessing the Proportional Odds Assumption: The Likelihood Ratio Test\n\nTo assess the proportional odds assumption in an ordinal logistic regression model, @brant1990assessing proposes the use of the likelihood ratio test. This approach begins by fitting a less restrictive model in which the regression coefficients are allowed to vary across categories. This model is expressed as:\n$$\n\\text{logit}(\\gamma_j) = \\theta_j - \\beta_j^\\top \\mathbf{X}\n$$ {#eq-likelihood_ratio_model}\n\nwhere $\\beta_j$ is the vector of regression coefficients for each category j. Here the coefficients $\\beta_j$ are allowed to vary across categories, which means that the proportional odds assumption is not satisfied. We then use the conventionnel likelihood ratio test to assess the hypothesis :\n$$\nH_0: \\beta_j = \\beta \\quad \\text{for all } j = 1, 2, \\ldots, K-1\n$$ {#eq-likelihood_ratio_test}\n\nTo perform this test, we conduct a likelihood ratio test comparing the unconstrained (non-proportional or satured) model with the constrained (proportional odds or reduced) model. \n\nBefore proceeding further, we briefly recall how to use the likelihood ratio test in hypothesis testing. Suppose we want to evaluate the null hypothesis $H_0 : \\theta \\in \\Theta_0$ against the alternative $H_1 : \\theta \\in \\Theta_1$, \n\nThe likelihood ratio statistic is defined as:\n$$\n\\lambda = 2 \\log\\left(\\frac{\\displaystyle\\sup_{\\theta \\in \\Theta}\\mathcal{L}(\\theta)}{\\displaystyle\\sup_{\\theta \\in \\Theta_0}\\mathcal{L}(\\theta)}\\right) \n= 2\\log\\left(\\frac{\\mathcal{L}(\\hat{\\theta})}{\\mathcal{L}(\\hat{\\theta}_0)}\\right),\n$${#eq-likelihood_ratio_statistic}\n\nwhere $\\mathcal{L}(\\theta)$ is the likelihood function, $\\hat{\\theta}$ is the maximum likelihood estimate (MLE) under the full model, and $\\hat{\\theta}_0$ is the MLE under the constrained model. The test statistic $\\lambda$ follows a chi-square distribution with degrees of freedom equal to the difference in the number of parameters between the full and constrained models.\n\n\n\nHere, $\\hat{\\theta}$ is the **maximum likelihood estimate (MLE)** under the full (unconstrained) model, and $\\hat{\\theta}_0$ is the MLE under the constrained model where the proportional odds assumption holds. The test statistic $\\lambda$ follows a chi-square distribution under the null hypothesis.\n\n\nIn a general setting, suppose the full parameter space is denoted by\n\n$$\n\\Theta = (\\theta_1, \\theta_2, \\ldots, \\theta_q, \\ldots, \\theta_p),\n$$\n\nand the restricted parameter space under the null hypothesis is\n\n$$\n\\Theta_0 = (\\theta_1, \\theta_2, \\ldots, \\theta_q).\n$$\n\n**(Note: These parameters are generic and should not be confused with the $K - 1$ thresholds or intercepts in the proportional odds model.)**, the likelihood ratio test statistic $\\lambda$ follows a chi-square distribution with $p - q$ degrees of freedom. Where $p$ represents the total number of parameters in the full (unconstrained or \"saturated\") model, while $K - 1$ corresponds to the number of parameters in the reduced (restricted) model.\n\n\nNow, let us apply this approach to the ordinal logistic regression model with the proportional odds assumption.\nAssume that our response variable has $K$ ordered categories and that we have $p$ predictor variables.\nTo use the likelihood ratio test to evaluate the proportional odds assumption, we need to compare two models:\n\n\n#### 1. **Unconstrained Model (non-proportional odds):**\n\n\nThis model allows each outcome threshold to have its own set of regression coefficients, meaning that we do not assume the regression coefficients are equal across all thresholds. The model is defined as:\n\n$$\n\\text{logit}(\\mathbb{P}(Y \\leq j \\mid \\mathbf{X})) = \\theta_j - \\boldsymbol{\\beta}_j^\\top \\mathbf{X}\n$$\n\n* There are $K - 1$ threshold (intercept) parameters: $\\theta_1, \\theta_2, \\ldots, \\theta_{K-1}$\n* Each threshold has its own vector of slope coefficients ${\\beta}_j$ of dimension $p$\n\nThus, the total number of parameters in the unconstrained model is:\n\n$$\n(K - 1) \\text{ thresholds} + (K - 1) \\times p \\text{ slopes} = (K - 1)(p + 1)\n$$\n\n#### 2. **Proportional Odds Model:**\n\nThis model assumes a single set of regression coefficients for all thresholds:\n\n$$\n\\text{logit}(\\mathbb{P}(Y \\leq j \\mid \\mathbf{X})) = \\theta_j - {\\beta}^\\top \\mathbf{X}\n$$\n\n* There are $K - 1$ threshold parameters\n* There is one common slope vector ${\\beta}$ for all $j$\n\nThus, the total number of parameters in the proportional odds model is:\n\n$$\n(K - 1) \\text{ thresholds} + p \\text{ slopes} = (K - 1) + p\n$$\n\n\n\nThus, the likelihood ratio test statistic follows a chi-square distribution with degrees of freedom:\n\n$$\n\\text{df} = [(K - 1) \\times (p+1)] - [(K - 1) + p] = (K - 2) \\times p\n$${#eq-likelihood_ratio_df}\n\n\nThis test provides a formal way to assess whether the proportional odds assumption holds for the given data. At a significance level of 1%, 5%, or any other conventional threshold, the proportional odds assumption is rejected if the test statistic $\\lambda$ exceeds the critical value from the chi-square distribution with $(K - 2) \\times p$ degrees of freedom. \n\nIn other words, we reject the null hypothesis\n\n$$\nH_0 : {\\beta}_1 = {\\beta}_2 = \\cdots = {\\beta}_{K-1} = {\\beta},\n$$\n\nwhich states that the regression coefficients are equal across all cumulative logits. This test has the advantage of being straightforward to implement and provides an overall assessment of the proportional odds assumption.\n\nIn the next section, we introduce the proportional odds test based on separate fits.\n\n2. **Assessing the Proportional Odds Assumption: The Separate Fits Approach**\n\nTo understand this part, you must understand the Mahalanobis distance and its properties. The Mahalanobis distance can be used to measure the dissimilarity between two vectors $x=(x_1, x_2, \\ldots, x_p)^\\top$ and $y=(y_1, y_2, \\ldots, y_p)^\\top$ in a multivariate space with the same distribution. It is defined as:\n$$\nD_M(x, y) = \\sqrt{(x - y)^\\top \\Sigma^{-1} (x - y)}\n$${#eq-mahalanobis_distance}\n\nwhere $\\Sigma$ is the covariance matrix of the distribution. The Mahalanobis distance is linked with the $\\chi^2$ distribution, specifically, if $X \\sim N(\\mu, \\Sigma)$ is a p-dimensional normal random vector, with the mean $\\mu$ and covariance matrix $\\Sigma$, then the Mahalanobis distance $D_M(X, \\mu)$ follows a $\\chi^2$ distribution with $p$ degrees of freedom. This step is essential for understanding how to assess proportionality using separate fits. You will see why shortly.\n\nIn fact, the author notes that the natural approach to evaluating the proportional odds assumption is to fit a set of $K-1$ binary logistic regression models (where $K$ is the number of categories of the response variable), and then use the statistical properties of the estimated parameters to construct a test statistic for the proportional odds hypothesis.\n\nThe procedure is as follows:\n\nFirst, we construct separate binary logistic regression models for each threshold $j = 1, 2, \\ldots, K-1$ of the ordinal response variable $Y$. For each threshold $j$, we define a binary variable $Z_j$, which takes the value 1 if the observation exceeds threshold $j$, and 0 otherwise. Specifically, we have:\n$$\nZ_j = \\begin{cases}\n0 & \\text{if } Y > j \\\\\n0 & \\text{if } Y \\leq j\n\\end{cases}\n$${#eq-binary_response}\n\nWith the probaility, $\\pi_j = P(Z_j = 1 \\mid \\mathbf{X}) = 1 - \\gamma_j$ satisfying the logistic regression model:\n$$\n\\text{logit}(\\pi_j) = \\theta_j - \\beta_j^\\top \\mathbf{X}.\n$${#eq-binary_logit_model}\n\n\nThen, assessing the proportional odds assumption in this context involves testing the hypothesis that the regression coefficients $\\beta_j$ are equal across all $K-1$ models. This is equivalent to testing the hypothesis:\n\n$$\nH_0 : \\beta_1 = \\beta_2 = \\cdots = \\beta_{K-1} = \\beta\n$${#eq-proportional_odds_hypothesis}\n\n\n\nLet $\\hat{\\beta}_j$ denote the maximum likelihood estimators of the regression coefficients for each binary model, and let $\\hat{\\beta} = (\\hat{\\beta}_1^\\top, \\hat{\\beta}_2^\\top, \\ldots, \\hat{\\beta}_{K-1}^\\top)^\\top$ represent the global vector of estimators. This vector is asymptotically normally distributed, such that $\\mathbb{E}(\\hat{\\beta}_j) \\approx \\beta$, with variance-covariance matrix $\\mathbb{V}(\\hat{\\beta}_j)$. The general term of this matrix, $\\text{cov}(\\hat{\\beta}_j, \\hat{\\beta}_k)$, needs to be determined and is given by:\n\n\n$$\n\\widehat{V}(\\hat{\\boldsymbol{\\beta}}) =\n\\begin{bmatrix}\n\\text{Cov}(\\hat{\\boldsymbol{\\beta}}_1, \\hat{\\boldsymbol{\\beta}}_1) & \\text{Cov}(\\hat{\\boldsymbol{\\beta}}_1, \\hat{\\boldsymbol{\\beta}}_2) & \\cdots & \\text{Cov}(\\hat{\\boldsymbol{\\beta}}_1, \\hat{\\boldsymbol{\\beta}}_{K-1}) \\\\\n\\text{Cov}(\\hat{\\boldsymbol{\\beta}}_2, \\hat{\\boldsymbol{\\beta}}_1) & \\text{Cov}(\\hat{\\boldsymbol{\\beta}}_2, \\hat{\\boldsymbol{\\beta}}_2) & \\cdots & \\text{Cov}(\\hat{\\boldsymbol{\\beta}}_2, \\hat{\\boldsymbol{\\beta}}_{K-1}) \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\text{Cov}(\\hat{\\boldsymbol{\\beta}}_{K-1}, \\hat{\\boldsymbol{\\beta}}_1) & \\text{Cov}(\\hat{\\boldsymbol{\\beta}}_{K-1}, \\hat{\\boldsymbol{\\beta}}_2) & \\cdots & \\text{Cov}(\\hat{\\boldsymbol{\\beta}}_{K-1}, \\hat{\\boldsymbol{\\beta}}_{K-1})\n\\end{bmatrix}\n\\in \\mathbb{R}^{(K-1)p \\times (K-1)p}\n$${#eq-variance_covariance_matrix}\n\nwhere $\\text{Cov}(\\hat{\\boldsymbol{\\beta}}_j, \\hat{\\boldsymbol{\\beta}}_k)$ is the covariance between the estimated coefficients of the $j$-th and $k$-th binary models.\nTo evaluate the proportional odds assumption, Brant constructs a matrix $\\mathbf{D}$ that captures the differences between the coefficients $\\hat{\\beta}_j$. Recall that each vector $\\hat{\\beta}_j$ has dimension $p$. The matrix $\\mathbf{D}$ is defined as follows:\n\n\n$$\n\\mathbf{D} = \n\\begin{bmatrix}\nI & -I & 0 & \\cdots & 0 \\\\\nI & 0 & -I & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\nI & 0 & 0 & \\cdots & -I \\\\\n\\end{bmatrix}\n\\in \\mathbb{R}^{(K-2)p \\times (K-1)p}\n$${#eq-difference_matrix}\n\nwhere $I$ is the identity matrix of size $p \\times p$.  The first row of the matrix D corresponds to the difference between the first and second coefficients, the second row corresponds to the difference between the second and third coefficients, and so on, until the last row which corresponds to the difference between the $(K-2)$-th and $(K-1)$-th coefficients. We can notice that the product $\\mathbf{D} \\hat{{\\beta}}$ will yield a vector of differences between the coefficients $\\hat{\\beta_j}$. \n\nOnce the matrix $\\mathbf{D}$ is constructed, Brant defines the Wald statistic $X^2$ to test the proportional odds assumption. This statistic can be interpreted as the Mahalanobis distance between the vector $\\mathbf{D} \\hat{\\boldsymbol{\\beta}}$ and the zero vector. The Wald statistic is defined as follows:\n\n$$\nX^2 = (\\mathbf{D} \\hat{{\\beta}})^\\top \\left[ \\mathbf{D} \\widehat{V}(\\hat{{\\beta}}) \\mathbf{D}^\\top \\right]^{-1} (\\mathbf{D} \\hat{{\\beta}})\n$${#eq-wald_statistic}\n\nwhich will be asymptotically $\\chi^2$ distributed with $(K - 2)p$ degrees of freedom under the null hypothesis. The challenging part here is to determine the variance-covariance matrix $\\widehat{V}(\\hat{\\beta})$. In his article, Brant provides an explicit estimator for this variance-covariance matrix, which is based on the maximum likelihood estimators $\\hat{\\beta}_j$ from each binary model.\n\nIn the following sections, we implement these approaches in Python, using the `statsmodels` package for the regressions and statistical tests.\n\n## Example\n\n\n\nThe data for this example comes from the \"Wine Quality\" dataset, which contains information about red wine samples and their quality ratings. The dataset includes 1,599 observations and 12 variables. The target variable, \"quality,\" is ordinal and originally ranges from 3 to 8. To ensure enough observations in each group, we combine categories 3 and 4 into a single category (labeled 4), and categories 7 and 8 into a single category (labeled 7), so the response variable has four levels. We then handle outliers in the explanatory variables using the Interquartile Range (IQR) method. Finally, we select three predictorsâ€”volatile acidity, free sulfur dioxide, and total sulfur dioxideâ€”to use in our ordinal logistic regression model, and we standardize these variables to have a mean of 0 and a standard deviation of 1.\n\nTables 1 and 2 present the results of the three binary logistic regression models and the proportional odds model, respectively. Several discrepancies can be seen in these tables, particularly in the \"volatile acidity\" coefficients. For instance, the difference in the \"volatile acidity\" coefficient between the first and second binary models is -0.280, while the difference between the second and third models is 0.361. These differencesâ€”especially when compared alongside the standard errorsâ€”suggest that the proportional odds assumption may not hold.\n\n\n\nTo assess the overall significance of the proportional odds assumption, we perform the likelihood ratio test, which yields a test statistic of $\\mathrm{LR} = 53.207$ and a p-value of $1.066 \\times 10^{-9}$ when compared to the chi-square distribution with 6 degrees of freedom. This result indicates that the proportional odds assumption is violated at the 5% significance level, suggesting that the model may not be appropriate for the data. We also use the separate fits approach to further investigate this assumption. The Wald test statistic is computed as $X^2 = 41.880$, with a p-value of $1.232 \\times 10^{-7}$, also based on the chi-square distribution with 6 degrees of freedom. This further confirms that the proportional odds assumption is violated at the 5% significance level.\n\n\n## Conclusion\n\nThis paper had two main goals: first, to illustrate how to test the **proportional odds assumption** in the context of **ordinal logistic regression**, and second, to encourage readers to explore @brant1990assessingâ€™s article for a deeper understanding of the topic.\n\nBrantâ€™s work extends beyond assessing the proportional odds assumptionâ€”it also provides methods for evaluating the overall adequacy of the ordinal logistic regression model. For instance, he discusses how to test whether the latent variable $Y^*$ truly follows a logistic distribution or whether an alternative link function might be more appropriate.\n\nIn this article, we focused on a global assessment of the proportional odds assumption, without investigating which specific coefficients may be responsible for any violations. Brant also addresses this finer-grained analysis, which is why we **strongly encourage** you to read his 1990 article in full.\n\n\n\nWe welcome any comments or suggestions. Happy reading!\n\n::: {#4117ad42 .cell execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd\n\ndata = pd.read_csv(\"winequality-red.csv\", sep=\";\")\ndata.head()\n\n# Repartition de la variable cible quality \n\ndata['quality'].value_counts(normalize=False).sort_index()\n\n# I want to regroup modalities 3, 4 and the modalities 7 and 8\ndata['quality'] = data['quality'].replace({3: 4, 8: 7})\ndata['quality'].value_counts(normalize=False).sort_index()\nprint(\"Number of observations:\", data.shape[0])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nNumber of observations: 1599\n```\n:::\n:::\n\n\n::: {#6390bf48 .cell execution_count=2}\n``` {.python .cell-code}\n# Traitons les outliers des variables privÃ©es de la variable cible quality par IQR.\n\ndef remove_outliers_iqr(df, column):\n    Q1 = df[column].quantile(0.25)\n    Q3 = df[column].quantile(0.75)\n    IQR = Q3 - Q1\n    lower_bound = Q1 - 1.5 * IQR\n    upper_bound = Q3 + 1.5 * IQR\n    return df[(df[column] >= lower_bound) & (df[column] <= upper_bound)]\nfor col in data.columns:\n    if col != 'quality':\n        data = remove_outliers_iqr(data, col)\n```\n:::\n\n\n::: {#b28e5626 .cell execution_count=3}\n``` {.python .cell-code}\nvar_names_without_quality = [col for col in data.columns if col != 'quality']\n\n##  Create the boxplot of each variable per group of quality\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.figure(figsize=(15, 10))\nfor i, var in enumerate(var_names_without_quality):\n    plt.subplot(3, 4, i + 1)\n    sns.boxplot(x='quality', y=var, data=data)\n    plt.title(f'Boxplot of {var} by quality')\n    plt.xlabel('Quality')\n    plt.ylabel(var)\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](proportional_ordinal_regression_files/figure-html/cell-4-output-1.png){width=1433 height=950}\n:::\n:::\n\n\n::: {#51b7a205 .cell execution_count=4}\n``` {.python .cell-code}\n# Implement the ordered logistic regression to variables 'volatile acidity', 'free sulfur dioxide', and 'total sulfur dioxide'\nfrom statsmodels.miscmodels.ordinal_model import OrderedModel\nfrom sklearn.preprocessing import StandardScaler\nexplanatory_vars = ['volatile acidity', 'free sulfur dioxide', 'total sulfur dioxide']\n# Standardize the explanatory variables\ndata[explanatory_vars] = StandardScaler().fit_transform(data[explanatory_vars])\n\ndef fit_ordered_logistic_regression(data, response_var, explanatory_vars):\n    model = OrderedModel(\n        data[response_var],\n        data[explanatory_vars],\n        distr='logit'\n    )\n    result = model.fit(method='bfgs', disp=False)\n    return result\nresponse_var = 'quality'\n\nresult = fit_ordered_logistic_regression(data, response_var, explanatory_vars)\nprint(result.summary())\n# Compute the log-likelihood of the model\nlog_reduced = result.llf\nprint(f\"Log-likelihood of the reduced model: {log_reduced}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                             OrderedModel Results                             \n==============================================================================\nDep. Variable:                quality   Log-Likelihood:                -1130.1\nModel:                   OrderedModel   AIC:                             2272.\nMethod:            Maximum Likelihood   BIC:                             2302.\nDate:                Tue, 10 Jun 2025                                         \nTime:                        23:03:11                                         \nNo. Observations:                1135                                         \nDf Residuals:                    1129                                         \nDf Model:                           3                                         \n========================================================================================\n                           coef    std err          z      P>|z|      [0.025      0.975]\n----------------------------------------------------------------------------------------\nvolatile acidity        -0.7180      0.064    -11.302      0.000      -0.842      -0.593\nfree sulfur dioxide      0.3627      0.076      4.770      0.000       0.214       0.512\ntotal sulfur dioxide    -0.5903      0.080     -7.406      0.000      -0.747      -0.434\n4/5                     -3.8601      0.182    -21.153      0.000      -4.218      -3.502\n5/6                      1.3002      0.050     25.863      0.000       1.202       1.399\n6/7                      0.8830      0.042     20.948      0.000       0.800       0.966\n========================================================================================\nLog-likelihood of the reduced model: -1130.0713953351503\n```\n:::\n:::\n\n\n::: {#1803504d .cell execution_count=5}\n``` {.python .cell-code}\nnum_of_thresholds = len(result.params) - len(explanatory_vars)  # Number of thresholds is total params minus explanatory vars\nOrderedModel(\n        data[response_var],\n        data[explanatory_vars],\n        distr='logit'\n    ).transform_threshold_params(result.params[-num_of_thresholds:])\n```\n\n::: {.cell-output .cell-output-display execution_count=5}\n```\narray([       -inf, -3.86010874, -0.19012621,  2.2279648 ,         inf])\n```\n:::\n:::\n\n\n::: {#378df161 .cell execution_count=6}\n``` {.python .cell-code}\n# The likelihood ratio test\n# Compute the full multinomial model\nimport statsmodels.api as sm\n\ndata_sm = sm.add_constant(data[explanatory_vars])\nmodel_full = sm.MNLogit(data[response_var], data_sm)\nresult_full = model_full.fit(method='bfgs', disp=False)\n#summary\nprint(result_full.summary())\n# Commpute the log-likelihood of the full model\nlog_full = result_full.llf\nprint(f\"Log-likelihood of the full model: {log_full}\")\n\n# Compute the likelihood ratio statistic\n\nLR_statistic = 2 * (log_full - log_reduced)\nprint(f\"Likelihood Ratio Statistic: {LR_statistic}\")\n\n# Compute the degrees of freedom\ndf1 = (num_of_thresholds - 1) * len(explanatory_vars)\ndf2 = result_full.df_model - OrderedModel(\n        data[response_var],\n        data[explanatory_vars],\n        distr='logit'\n    ).fit().df_model\nprint(f\"Degrees of Freedom: {df1}\")\nprint(f\"Degrees of Freedom for the full model: {df2}\")\n\n# Compute the p-value\nfrom scipy.stats import chi2\nprint(\"The LR statistic :\", LR_statistic)\np_value = chi2.sf(LR_statistic, df1)\nprint(f\"P-value: {p_value}\")\nif p_value < 0.05:\n    print(\"Reject the null hypothesis: The proportional odds assumption is violated.\")\nelse:\n    print(\"Fail to reject the null hypothesis: The proportional odds assumption holds.\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                          MNLogit Regression Results                          \n==============================================================================\nDep. Variable:                quality   No. Observations:                 1135\nModel:                        MNLogit   Df Residuals:                     1123\nMethod:                           MLE   Df Model:                            9\nDate:                Tue, 10 Jun 2025   Pseudo R-squ.:                  0.1079\nTime:                        23:03:15   Log-Likelihood:                -1103.5\nconverged:                      False   LL-Null:                       -1236.9\nCovariance Type:            nonrobust   LLR p-value:                 2.753e-52\n========================================================================================\n           quality=5       coef    std err          z      P>|z|      [0.025      0.975]\n----------------------------------------------------------------------------------------\nconst                    3.2418      0.269     12.034      0.000       2.714       3.770\nvolatile acidity        -0.6541      0.180     -3.624      0.000      -1.008      -0.300\nfree sulfur dioxide      0.2494      0.323      0.772      0.440      -0.384       0.882\ntotal sulfur dioxide     0.6314      0.310      2.037      0.042       0.024       1.239\n----------------------------------------------------------------------------------------\n           quality=6       coef    std err          z      P>|z|      [0.025      0.975]\n----------------------------------------------------------------------------------------\nconst                    3.2549      0.269     12.089      0.000       2.727       3.783\nvolatile acidity        -1.0838      0.184     -5.880      0.000      -1.445      -0.723\nfree sulfur dioxide      0.6269      0.325      1.930      0.054      -0.010       1.264\ntotal sulfur dioxide     0.0723      0.315      0.230      0.818      -0.544       0.689\n----------------------------------------------------------------------------------------\n           quality=7       coef    std err          z      P>|z|      [0.025      0.975]\n----------------------------------------------------------------------------------------\nconst                    1.4139      0.302      4.684      0.000       0.822       2.006\nvolatile acidity        -1.8364      0.214     -8.596      0.000      -2.255      -1.418\nfree sulfur dioxide      1.0125      0.358      2.830      0.005       0.311       1.714\ntotal sulfur dioxide    -0.9086      0.389     -2.337      0.019      -1.671      -0.147\n========================================================================================\nLog-likelihood of the full model: -1103.467809036406\nLikelihood Ratio Statistic: 53.20717259748881\nDegrees of Freedom: 6\nDegrees of Freedom for the full model: 6.0\nThe LR statistic : 53.20717259748881\nP-value: 1.0658102529671109e-09\nReject the null hypothesis: The proportional odds assumption is violated.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n/Users/juniorjumbong/Desktop/personal-website/.venv/lib/python3.13/site-packages/statsmodels/base/model.py:607: ConvergenceWarning:\n\nMaximum Likelihood optimization failed to converge. Check mle_retvals\n\n/Users/juniorjumbong/Desktop/personal-website/.venv/lib/python3.13/site-packages/statsmodels/base/optimizer.py:737: RuntimeWarning:\n\nMaximum number of iterations has been exceeded.\n\n/Users/juniorjumbong/Desktop/personal-website/.venv/lib/python3.13/site-packages/statsmodels/base/model.py:607: ConvergenceWarning:\n\nMaximum Likelihood optimization failed to converge. Check mle_retvals\n\n```\n:::\n:::\n\n\n::: {#b38f0d0d .cell execution_count=7}\n``` {.python .cell-code}\nimport numpy as np\nimport statsmodels.api as sm\nimport pandas as pd\n\ndef fit_binary_models(data, explanatory_vars, y):\n    \"\"\"\n    - data : DataFrame pandas original (doit contenir toutes les variables)\n    - explanatory_vars : liste des variables explicatives\n    - y : array-like, cible ordinale (n,) (ex: 4, 5, 6, 7)\n\n    Retourne :\n      - binary_models : liste d'objets Logit results (statsmodels)\n      - beta_hat : array (K-1, p+1) (coeffs incluant l'intercept)\n      - var_hat : liste de matrices (p+1, p+1) (variance-covariance complÃ¨te)\n      - z_mat : DataFrame des variables binaires z_j (pour debug/inspection)\n      - thresholds : liste des seuils utilisÃ©s\n    \"\"\"\n    qualities = np.sort(np.unique(y))   # toutes les modalitÃ©s, triÃ©es\n    thresholds = qualities[:-1]         # seuils pour les modÃ¨les binaires (K-1)\n    p = len(explanatory_vars)\n    n = len(y)\n    K_1 = len(thresholds)\n\n    binary_models = []\n    beta_hat = np.full((K_1, p+1), np.nan)\n    p_values_beta_hat = np.full((K_1, p+1), np.nan)  # pour les p-values\n    var_hat = []\n    z_mat = pd.DataFrame(index=np.arange(n))\n    X_with_const = sm.add_constant(data[explanatory_vars])\n\n    # Construction et estimation des modÃ¨les binaires pour chaque seuil\n    for j, t in enumerate(thresholds):\n        z_j = (y > t).astype(int)\n        z_mat[f'z>{t}'] = z_j\n        model = sm.Logit(z_j, X_with_const)\n        res = model.fit(disp=0)\n        binary_models.append(res)\n        beta_hat[j, :] = res.params.values           # Incluant intercept\n        p_values_beta_hat[j, :] = res.pvalues.values  # P-values des coefficients\n        var_hat.append(res.cov_params().values)      # Covariance complÃ¨te (p+1, p+1)\n\n    return binary_models, beta_hat, X_with_const, var_hat, z_mat, thresholds\nbinary_models, beta_hat,X_with_const, var_hat, z_mat, thresholds = fit_binary_models(data, explanatory_vars, data[response_var])\n# Afficher les coefficients estimÃ©s\nprint(\"Estimated coefficients (beta_hat):\")\nprint(beta_hat)\n# Afficher les p-values des coefficients\nprint(\"P-values of coefficients (p_values_beta_hat):\")\nprint(X_with_const)\n# Afficher les seuils\nprint(\"Thresholds:\")\nprint(thresholds)   \nprint(\"z_mat (variables binaires crÃ©Ã©es) :\\n\", z_mat.head())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nEstimated coefficients (beta_hat):\n[[ 4.09606917 -0.88743434  0.63477387  0.20921617]\n [ 0.15729349 -0.60735704  0.4339553  -0.65663161]\n [-2.60302245 -0.9677302   0.60691768 -1.30246297]]\nP-values of coefficients (p_values_beta_hat):\n      const  volatile acidity  free sulfur dioxide  total sulfur dioxide\n0       1.0          1.080055            -0.441353             -0.282198\n1       1.0          2.173545             1.189601              1.058458\n2       1.0          1.444552             0.024634              0.530321\n3       1.0         -1.471421             0.257627              0.774077\n4       1.0          1.080055            -0.441353             -0.282198\n...     ...               ...                  ...                   ...\n1594    1.0          0.472561             2.005078              0.124061\n1595    1.0          0.168814             2.820555              0.408443\n1596    1.0         -0.074184             1.655588             -0.038443\n1597    1.0          0.745933             2.005078              0.124061\n1598    1.0         -1.289172             0.374124              0.042809\n\n[1135 rows x 4 columns]\nThresholds:\n[4 5 6]\nz_mat (variables binaires crÃ©Ã©es) :\n    z>4  z>5  z>6\n0  1.0  0.0  0.0\n1  1.0  0.0  0.0\n2  1.0  0.0  0.0\n3  1.0  1.0  0.0\n4  1.0  0.0  0.0\n```\n:::\n:::\n\n\n::: {#ed19efe6 .cell execution_count=8}\n``` {.python .cell-code}\ndef compute_pi_hat(binary_models, X_with_const):\n    \"\"\"\n    - binary_models : liste d'objets Logit results (statsmodels)\n    - X_with_const  : matrice (n, p+1) des variables explicatives AVEC constante\n\n    Retourne :\n      - pi_hat : array (n, K-1) des fitted values pour chaque modÃ¨le binaire\n    \"\"\"\n    n = X_with_const.shape[0]\n    K_1 = len(binary_models)\n    pi_hat = np.full((n, K_1), np.nan)\n    for m, model in enumerate(binary_models):\n        pi_hat[:, m] = model.predict(X_with_const)\n    return pi_hat\n\n# Supposons que tu as :\n# - binary_models (liste)\n# - X_with_const (matrice numpy (n, p+1) crÃ©Ã©e dans la fonction prÃ©cÃ©dente)\n\npi_hat = compute_pi_hat(binary_models, X_with_const)\nprint(\"Shape de pi_hat :\", pi_hat.shape)  # (n, K-1)\nprint(\"AperÃ§u de pi_hat :\\n\", pi_hat[:5, :])\n\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nShape de pi_hat : (1135, 3)\nAperÃ§u de pi_hat :\n [[0.94258882 0.37638681 0.02796232]\n [0.95866233 0.20724576 0.00466477]\n [0.94982271 0.25776823 0.00922353]\n [0.99675485 0.65802083 0.11599334]\n [0.94258882 0.37638681 0.02796232]]\n```\n:::\n:::\n\n\n::: {#7c403d74 .cell execution_count=9}\n``` {.python .cell-code}\nimport numpy as np\n\ndef assemble_varBeta(pi_hat, X_with_const):\n    \"\"\"\n    Construit la matrice de variance-covariance globale varBeta pour les estimateurs des modÃ¨les binaires.\n    - pi_hat : array (n, K-1), chaque colonne = fitted proba du modÃ¨le binaire j\n    - X_with_const : array (n, p+1), matrice de design AVEC constante\n\n    Retourne :\n      - varBeta : array ((K-1)*p, (K-1)*p) [sans l'intercept]\n    \"\"\"\n    # Assure que tout est en numpy\n    if hasattr(X_with_const, 'values'):\n        X = X_with_const.values\n    else:\n        X = np.asarray(X_with_const)\n    n, p1 = X.shape  # p1 = p + 1 (avec intercept)\n    p = p1 - 1\n    K_1 = pi_hat.shape[1]\n\n    # Initialisation de la matrice globale\n    varBeta = np.zeros(((K_1)*p, (K_1)*p))\n\n    # Pour chaque bloc (j, l)\n    for j in range(K_1):\n        pi_j = pi_hat[:, j]\n        Wj = np.diag(pi_j * (1 - pi_j))\n        X_j = X\n        Xt = X_j.T\n\n        # Diagonale principale (variance de beta_j)\n        inv_XtWjX = np.linalg.pinv(Xt @ Wj @ X_j)\n        # On enlÃ¨ve la premiÃ¨re ligne/colonne (intercept)\n        inv_XtWjX_no_const = inv_XtWjX[1:, 1:]\n\n        row_start = j * p\n        row_end = (j + 1) * p\n        varBeta[row_start:row_end, row_start:row_end] = inv_XtWjX_no_const\n\n        # Blocs hors diagonale (covariances entre beta_j et beta_l)\n        for l in range(j + 1, K_1):\n            pi_l = pi_hat[:, l]\n            Wml = np.diag(pi_l - pi_j * pi_l)\n            Wl = np.diag(pi_l * (1 - pi_l))\n            # Termes croisÃ©s\n            inv_XtWlX = np.linalg.pinv(Xt @ Wl @ X_j)\n            block_vars = (\n                inv_XtWjX @ (Xt @ Wml @ X_j) @ inv_XtWlX\n            )[1:, 1:]  # Retirer intercept\n            # Place les blocs (symÃ©triques)\n            col_start = l * p\n            col_end = (l + 1) * p\n            varBeta[row_start:row_end, col_start:col_end] = block_vars\n            varBeta[col_start:col_end, row_start:row_end] = block_vars.T  # symÃ©trie\n\n    return varBeta\n\nvarBeta = assemble_varBeta(pi_hat, X_with_const)\nprint(\"Shape de varBeta :\", varBeta.shape)  # ((K-1)*p, (K-1)*p)\nprint(\"AperÃ§u de varBeta :\\n\", varBeta[:5, :5])  # Afficher un aperÃ§u\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nShape de varBeta : (9, 9)\nAperÃ§u de varBeta :\n [[ 2.87696584e-02  8.96840197e-04 -9.43834509e-04  1.80729535e-03\n   2.32011452e-04]\n [ 8.96840197e-04  1.09112963e-01 -5.78619412e-02  3.21424832e-04\n   3.10295243e-03]\n [-9.43834509e-04 -5.78619412e-02  7.73627725e-02 -4.69694598e-04\n  -1.48837502e-03]\n [ 1.80729535e-03  3.21424832e-04 -4.69694598e-04  4.61543407e-03\n   1.04759944e-04]\n [ 2.32011452e-04  3.10295243e-03 -1.48837502e-03  1.04759944e-04\n   7.48753786e-03]]\n```\n:::\n:::\n\n\n::: {#fbb18a2e .cell execution_count=10}\n``` {.python .cell-code}\ndef fill_varBeta_diagonal(varBeta, var_hat):\n    K_1 = len(var_hat)\n    p = var_hat[0].shape[0] - 1  # -1 car on enlÃ¨ve l'intercept\n    for m in range(K_1):\n        block = var_hat[m][1:, 1:]  # enlÃ¨ve intercept\n        row_start = m * p\n        row_end = (m + 1) * p\n        varBeta[row_start:row_end, row_start:row_end] = block\n    return varBeta\n\n# betaStar : concatÃ©nation des coefficients sans intercept\nbetaStar = beta_hat[:, 1:].flatten()\n\n# ComplÃ©ter les blocs diagonaux de varBeta\nvarBeta = fill_varBeta_diagonal(varBeta, var_hat)\nprint(\"Shape de varBeta aprÃ¨s remplissage diagonal :\", varBeta.shape)  # ((K-1)*p, (K-1)*p)\nprint(\"AperÃ§u de varBeta aprÃ¨s remplissage diagonal :\\n\", varBeta[:5, :5])  # Afficher un aperÃ§u    \n```\n\n::: {.cell-output .cell-output-stdout}\n```\nShape de varBeta aprÃ¨s remplissage diagonal : (9, 9)\nAperÃ§u de varBeta aprÃ¨s remplissage diagonal :\n [[ 2.87696584e-02  8.96840197e-04 -9.43834509e-04  1.80729535e-03\n   2.32011452e-04]\n [ 8.96840197e-04  1.09112963e-01 -5.78619412e-02  3.21424832e-04\n   3.10295243e-03]\n [-9.43834509e-04 -5.78619412e-02  7.73627725e-02 -4.69694598e-04\n  -1.48837502e-03]\n [ 1.80729535e-03  3.21424832e-04 -4.69694598e-04  4.61543407e-03\n   1.04759944e-04]\n [ 2.32011452e-04  3.10295243e-03 -1.48837502e-03  1.04759944e-04\n   7.48753786e-03]]\n```\n:::\n:::\n\n\n::: {#4091ee74 .cell execution_count=11}\n``` {.python .cell-code}\nimport numpy as np\n\ndef construct_D(K_1, p):\n    \"\"\"\n    Construit la matrice D de taille ((K-2)*p, (K-1)*p) pour le test de Wald.\n    K_1 : nombre de seuils (K-1)\n    p   : nombre de variables explicatives (hors intercept)\n    \"\"\"\n    D = np.zeros(((K_1-1)*p, K_1*p))\n    I = np.eye(p)\n    for i in range(K_1-1):  # i = 0 Ã  K-2\n        for j in range(K_1):\n            if j == 0:\n                temp = I\n            elif j == i+1:\n                temp = -I\n            else:\n                temp = np.zeros((p, p))\n            col_start = j*p\n            col_end = (j+1)*p\n            row_start = i*p\n            row_end = (i+1)*p\n            D[row_start:row_end, col_start:col_end] += temp\n    return D\nD = construct_D(len(thresholds), len(explanatory_vars))\nprint(\"Shape de D :\", D.shape)  # ((K-2)*p, (K-1)*p)\nprint(\"AperÃ§u de D :\\n\", D[:5, :5])  # Afficher un aperÃ§u\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nShape de D : (6, 9)\nAperÃ§u de D :\n [[ 1.  0.  0. -1.  0.]\n [ 0.  1.  0.  0. -1.]\n [ 0.  0.  1.  0.  0.]\n [ 1.  0.  0.  0.  0.]\n [ 0.  1.  0.  0.  0.]]\n```\n:::\n:::\n\n\n::: {#d0b5c87e .cell execution_count=12}\n``` {.python .cell-code}\ndef wald_statistic(D, betaStar, varBeta):\n    \"\"\"\n    Calcule la statistique de Wald X^2 pour le test de proportionnalitÃ©.\n    \"\"\"\n    Db = D @ betaStar\n    V = D @ varBeta @ D.T\n    # SymÃ©triser V pour stabilitÃ©\n    #V = 0.5 * (V + V.T)\n    # Utilise le pseudo-inverse par sÃ©curitÃ© numÃ©rique\n    inv_V = np.linalg.inv(V)\n    X2 = float(Db.T @ inv_V @ Db)\n    return X2\n```\n:::\n\n\n::: {#5b7095dd .cell execution_count=13}\n``` {.python .cell-code}\n# Supposons que tu as K_1, p, betaStar, varBeta\nK_1 = len(binary_models)\np = len(explanatory_vars)  # Nombre de variables explicatives (hors intercept)\nD = construct_D(K_1, p)\nX2 = wald_statistic(D, betaStar, varBeta)\nddl = (K_1-1)*p\n\nfrom scipy.stats import chi2\npval = 1 - chi2.cdf(X2, ddl)\n\nprint(f\"Statistique XÂ² = {X2:.4f}\")\nprint(f\"DegrÃ©s de libertÃ© = {ddl}\")\nprint(f\"p-value = {pval:.4g}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nStatistique XÂ² = 42.8803\nDegrÃ©s de libertÃ© = 6\np-value = 1.232e-07\n```\n:::\n:::\n\n\n",
    "supporting": [
      "proportional_ordinal_regression_files"
    ],
    "filters": [],
    "includes": {}
  }
}