{
  "hash": "d7a1fae49bfc1625c8630ffa04894164",
  "result": {
    "engine": "jupyter",
    "markdown": "---\nformat: \n  html: \n    fontsize: 1.3em\n---\n\n\n\n\n<div class=\"box\">\nVous trouverez ici certains exercices de la fiche de TD, qui vous permettront de mieux comprendre les concepts abordés par le professeur pendant les cours. N'hésitez pas à me contacter si vous avez des questions ou des suggestions. Je mettrai l'accent sur la rédaction des corrections.\n</div>\n\n# Espaces probabilisés\n\nJe reviendrai ici sur l'exercice 5 que nous n'avons pas eu la possibilité de mettre oeuvre la simulation monte Carlo.\n\n## Correction de l'exercice 5\n\nNous sélectionnons un entier au hasard parmi les N = 5,000 entiers qui sont dans l'intervalle [0, 4999]. \nQuelle est la probabilité qu’il soit divisible par 4,7 ou 10 ?\n\nModifier le code R vu en cours pour estimer cette probabilité par une simulation\nde Monte-Carlo pour vérifier ce résultat. Vous utiliserez au moins 50,000\nsimulations.\n\nPour résoudre cette exercice, il faut d'abord définir l'espace probabilisé. \nL'espace probabilisé est défini par le triplet (Ω, F, P) où:\n\n\n- Ω est l'ensemble des résultats possibles. Dans notre cas, Ω = {0, 1, 2, ..., 4999}.\n\n- F est la σ-algèbre des événements. Ici, nous pouvons considérer les événements comme les sous-ensembles de Ω.\n\n- P est la mesure de probabilité. Dans notre cas, chaque entier a une probabilité égale d'être sélectionné. On dit encore que la loi de probabilité est uniforme sur Ω.\n\nLe cardinal de l'ensemble Ω est |Ω| = 5000.\nLa probabilité d'un élément spécifique dans Ω est donc $P({x}) = \\frac{1}{5000}$ pour tout x dans Ω.\n\nAprès avoir défini l'espace probabilisé, nous pouvons maintenant définir l'événement A que nous voulons étudier. \nDe ce fait, définissons les événements suivants:\n\n- $A_4$ : l'événement que l'entier sélectionné est divisible par 4.\n- $A_7$ : l'événement que l'entier sélectionné est divisible par 7.\n- $A_{10}$ : l'événement que l'entier sélectionné est divisible par 10.\nL'événement A que nous voulons étudier est l'union de ces trois événements:\n\n$$\nA = A_4 \\cup A_7 \\cup A_{10}\n$$\n\nNous cherchons à calculer la probabilité de l'événement A, c'est-à-dire $P(A)$. Pour cela, nous allons utiliser la formule inclusion-exclusion:\n$$\n\\begin{aligned}\n\\mathbb{P}(A_4 \\cup A_7 \\cup A_{10})\n&= \\mathbb{P}(A_4) + \\mathbb{P}(A_7) + \\mathbb{P}(A_{10}) \\\\\n&\\quad - \\big\\{\\, \\mathbb{P}(A_4 \\cap A_7) + \\mathbb{P}(A_4 \\cap A_{10}) + \\mathbb{P}(A_7 \\cap A_{10}) \\,\\big\\} \\\\\n&\\quad + \\mathbb{P}(A_4 \\cap A_7 \\cap A_{10}).\n\\end{aligned}\n$$\n\nCalculons maintenant chaque terme de cette formule:\nPour calculer chaque probabilité, nous devons compter le nombre d'entiers dans Ω qui satisfont chaque condition $A_i$.\nEt la probabilité de chaque événement est donnée par le rapport du nombre d'entiers satisfaisant la condition sur le cardinal de l'ensemble Ω : \n\n$$\nP(A_i) = \\frac{\\text{nombre d'entiers satisfaisant } A_i}{5000}.\n$$\n\n\n- Pour $A_4$ : Les entiers divisibles par 4 sont 0, 4, 8, ..., 4996. Ainsi le nombre d'entiers divisibles par 4 est de la forme $4k$ et qui vérifient $0 \\leq 4(k+1) \\leq 5000 < 4(k+2)$, donc de cette inégalité, on déduit que $k$ vérifie l'inégalité sous dessous :\n\n$$\nk \\leq \\frac{4999}{4} < k+1.\n$$\n\nDonc le nombre d'entiers divisibles par 4 correspond à la partie entière de $\\frac{4999}{4}$ plus 1 (pour inclure le 0), soit:\n$$\n\\text{nombre d'entiers divisibles par 4} = \\left\\lfloor\\frac{4999}{4}\\right\\rfloor + 1 = 1249 + 1 = 1250.\n$$\n\nDonc, la probabilité que l'entier sélectionné soit divisible par 4 est:\n$$\nP(A_4) = \\frac{1250}{5000} = 0.25.\n$$\n\n- Pour $A_7$ : Les entiers divisibles par 7 sont 0, 7, 14, ..., 4996. En suivant le même raisonnement que pour $A_4$, nous trouvons:\n\n$$\n\\text{nombre d'entiers divisibles par 7} = \\left\\lfloor\\frac{4999}{7}\\right\\rfloor + 1 = 714 + 1 = 715.\n$$\n\nDonc, la probabilité que l'entier sélectionné soit divisible par 7 est:\n$$\nP(A_7) = \\frac{715}{5000} = 0.143.\n$$\n\n- Pour $A_{10}$ : Les entiers divisibles par 10 sont 0, 10, 20, ..., 4990. En suivant le même raisonnement que pour $A_4$, nous trouvons:\n$$\n\\text{nombre d'entiers divisibles par 10} = \\left\\lfloor\\frac{4999}{10}\\right\\rfloor + 1 = 499 + 1 = 500.\n$$\n\nDonc, la probabilité que l'entier sélectionné soit divisible par 10 est:\n$$\nP(A_{10}) = \\frac{500}{5000} = 0.1.\n$$  \n\n- Pour $A_4 \\cap A_7$ : Les entiers qui sont divisibles par 4 et 7 sont ceux qui sont divisibles par 28 qui est le PPCM de 4 et 7. En suivant le même raisonnement que pour $A_4$, nous trouvons:\n$$\n\\text{nombre d'entiers divisibles par 28} = \\left\\lfloor\\frac{4999}{28}\\right\\rfloor + 1 = 178 + 1 = 179.\n$$\n\nDonc, la probabilité que l'entier sélectionné soit divisible par 4 et 7 est:\n\n$$\nP(A_4 \\cap A_7) = \\frac{179}{5000} = 0.0358.\n$$\n\n- Pour $A_4 \\cap A_{10}$ : Les entiers qui sont divisibles par 4 et 10 sont ceux qui sont divisibles par 20 qui est le PPCM de 4 et 10. En suivant le même raisonnement que pour $A_4, nous trouvons:\n$$\n\\text{nombre d'entiers divisibles par 20} = \\left\\lfloor\\frac{4999}{20}\\right\\rfloor + 1 = 249 + 1 = 250.\n$$\n\nDonc, la probabilité que l'entier sélectionné soit divisible par 4 et 10 est:\n$$\nP(A_4 \\cap A_{10}) = \\frac{250}{5000} = 0.05.\n$$\n\n- Pour $A_7 \\cap A_{10}$ : Les entiers qui sont divisibles par 7 et 10 sont ceux qui sont divisibles par 70 qui est le PPCM de 7 et 10. En suivant le même raisonnement que pour $A_4, nous trouvons:\n\n$$\n\\text{nombre d'entiers divisibles par 70} = \\left\\lfloor\\frac{4999}{70}\\right\\rfloor + 1 = 71 + 1 = 72.\n$$\n\nDonc, la probabilité que l'entier sélectionné soit divisible par 7 et 10 est:\n$$\nP(A_7 \\cap A_{10}) = \\frac{72}{5000} = 0.0144.\n$$\n\n- Pour $A_4 \\cap A_7 \\cap A_{10}$ : Les entiers qui sont divisibles par 4, 7 et 10 sont ceux qui sont divisibles par 140 qui est le PPCM de 4, 7 et 10. En suivant le même raisonnement que pour $A_4$, nous trouvons:\n\n$$\n\\text{nombre d'entiers divisibles par 140} = \\left\\lfloor\\frac{4999}{140}\\right\\rfloor + 1 = 35 + 1 = 36.\n$$\n\nDonc, la probabilité que l'entier sélectionné soit divisible par 4, 7 et 10 est:\n$$\nP(A_4 \\cap A_7 \\cap A_{10}) = \\frac{36}{5000} = 0.0072.\n$$  \n\nEn substituant ces valeurs dans la formule d'inclusion-exclusion, nous obtenons:\n$$\n\\begin{aligned}\nP(A)\n&= P(A_4) + P(A_7) + P(A_{10}) \\\\\n&\\quad - \\big\\{\\, P(A_4 \\cap A_7) + P(A_4 \\cap A_{10}) + P(A_7 \\cap A_{10}) \\,\\big\\} \\\\\n&\\quad + P(A_4 \\cap A_7 \\cap A_{10}) \\\\\n&= 0.25 + 0.143 + 0.1 \\\\\n&\\quad - \\big\\{\\, 0.0358 + 0.05 + 0.0144 \\,\\big\\} \\\\\n&\\quad + 0.0072 \\\\\n&= 0.493 - 0.1002 + 0.0072 \\\\\n&= 0.4.\n\\end{aligned}\n$$  \n\nDonc, la probabilité qu'un entier sélectionné au hasard parmi les 5000 entiers soit divisible par 4, 7 ou 10 est de 0.4 ou 40%.\n\nMaintenant, nous allons vérifier ce résultat par une simulation de Monte-Carlo en R avec au moins 50,000 simulations.\nUne simulation de Monte-Carlo est une méthode statistique qui utilise des échantillons aléatoires pour estimer des propriétés mathématiques ou physiques comme des espérances, des intégrales ou des probabilités. Cette méthode fonctionne en générant un grand nombre de scénarios aléatoires et en observant les résultats pour obtenir une estimation statistique :\n\n- On fixe le nombre de simulations, disons n = 50000.\n- On initialise un compteur pour le nombre de succès (entiers divisibles par 4, 7 ou 10).\n- Pour chaque simulation, on génère un entier aléatoire entre 0 et 4999.\n- On vérifie si cet entier est divisible par 4, 7 ou 10. Si c'est le cas, on incrémente le compteur de succès.\n- Après avoir effectué toutes les simulations, on calcule la probabilité estimée comme le ratio du nombre de succès sur le nombre total de simulations.\n\nVoici un exemple de code R pour effectuer cette simulation de Monte-Carlo :\n\n::: {#6884e959 .cell execution_count=1}\n``` {.python .cell-code}\n# Monte Carlo in Python with running estimate and plot (matplotlib only)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# --- Parameters (mirror your R snippet) ---\nseed = 123\nN = 4999               # sample from 0..N (inclusive)\nM = 50_000             # number of simulations\n\nrng = np.random.default_rng(seed)\nx = rng.integers(low=0, high=N+1, size=M)  # uniform integers in [0, N]\n\nis_div = (x % 4 == 0) | (x % 7 == 0) | (x % 10 == 0)\n\n# Final Monte Carlo estimate (same as R's mean(is_div))\np_hat = is_div.mean()\n\n# Running estimates vs number of simulations\nrunning_est = np.cumsum(is_div) / np.arange(1, M + 1)\n\n# Mean of the running estimates (to draw an horizontal reference line)\nmean_running = running_est.mean()\n\n# --- Plot ---\nplt.figure(figsize=(7.24, 4.07), dpi=100)  # ~724x407 px\n\nplt.plot(np.arange(1, M + 1), running_est, linewidth=2)\nplt.axhline(mean_running, linestyle=\"--\", linewidth=2)\nplt.xlabel(\"Nombre de simulations\")\nplt.ylabel(\"Estimation de la probabilité\")\nplt.title(\"Convergence de l’estimation Monte-Carlo\", fontsize=14, weight=\"bold\")\nplt.suptitle(\n    \"Courbe de l’estimation cumulée; ligne horizontale = moyenne des estimations\",\n    fontsize=10, color=\"gray\"\n)\n\n# Style cues similar to the provided seaborn example\nax = plt.gca()\nfor spine in [\"top\", \"right\"]:\n    ax.spines[spine].set_visible(False)\nplt.grid(False)\n\nplt.tight_layout()\nplt.show()\n\np_hat\n\n```\n\n::: {.cell-output .cell-output-display}\n![](index_gdr_files/figure-html/cell-2-output-1.png){width=714 height=403}\n:::\n\n::: {.cell-output .cell-output-display execution_count=2}\n```\nnp.float64(0.3965)\n```\n:::\n:::\n\n\n::: {.byline}\n<span class=\"date\">May 28, 2025</span>\n:::\n\n# Rappels de la séance précédente\n\nL'objectif de la prémière séance de TD était de consolider vos connaissances sur les espaces probabilisés et sur les probabilités conditionnelles.\n\nVous devez actuellement être capable de :\n\n- Définir un espace probabilisé (Ω, F, P).\n\n- Utiliser les propriétés des probabilités pour calculer des probabilités d'événements simples et composés.\n\n- Vous devez maitriser les lois de Morgan :\n\n  - La loi de Morgan pour l'union : $\\overline{A \\cup B} = \\overline{A} \\cap \\overline{B}$\n  - La loi de Morgan pour l'intersection : $\\overline{A \\cap B} = \\overline{A} \\cup \\overline{B}$\n\n- Appliquer la formule d'inclusion-exclusion pour calculer la probabilité de l'union de deux événements :\n\n  $$\n  P(A \\cup B) = P(A) + P(B) - P(A \\cap B)\n  $$\n\n- Appliquer la formule d'inclusion-exclusion pour calculer la probabilité de l'union de plusieurs événements :\n\n  $$\n  \\begin{aligned}\n  P\\left(\\bigcup_{i=1}^{n} A_i\\right) &= \\sum_{i=1}^{n} P(A_i) - \\sum_{1 \\leq i < j \\leq n} P(A_i \\cap A_j) \\\\\n  &\\quad + \\sum_{1 \\leq i < j < k \\leq n} P(A_i \\cap A_j \\cap A_k) - \\ldots + (-1)^{n+1} P(A_1 \\cap A_2 \\cap \\ldots \\cap A_n)\n  \\end{aligned}\n  $$\n- Calculer des probabilités conditionnelles en utilisant la formule de Bayes :\n\n$$\n  P(A|B) = \\frac{P(A \\cap B)}{P(B)}.\n$$\n\nou bien encore :\n\n$$\n  P(A\\cap B) = P(A|B) \\cdot P(B).\n$$\n\n- Appliquer la loi des probabilités totales pour décomposer des probabilités complexes en utilisant des événements disjoints et exhaustifs :\n\nPar exemple, si $C_1, C_2$ sont deux événements disjoints et exhaustifs, c'est-à-dire que $C_1 \\cap C_2 = \\emptyset$ et $C_1 \\cup C_2 = \\Omega$, alors pour tout événement A, on a :\n\n$$\nP(A) = P(A|C_1) \\cdot P(C_1) + P(A|C_2) \\cdot P(C_2).\n$$\n\nEt finalement, cela nous permet de calculer la probabilité de $C_1$ sachant A en utilisant la formule de Bayes :\n\n$$\nP(C_1|A) = \\frac{P(A|C_1) \\cdot P(C_1)}{P(A|C_1) \\cdot P(C_1) + P(A|C_2) \\cdot P(C_2)}.\n$$\n\n\n\n\n\n```{mermaid}\nflowchart LR\n  A[Espaces probabilisés] --> B(Variable aléatoire X)\n  B --> C{type}\n  C --> D[Discrète]\n  C --> E[Continue]\n  D --> F[Caractérisation de la loi de X]\n  E --> G[Caractérisation de la loi de X]\n```\n\n\n\n\n# Variables aléatoires générales\n\n\n## Exercice 1\n\nNous considérons l’expérience aléatoire consistant à lancer 3 fois une pièce de monnaie. Soit $X$ la variable aléatoire donnant le nombre de « Pile » obtenu.\n\n1) Donner la loi de la variable aléatoire $X$.\n2) Donner la fonction de répartition de $X$, et la représenter graphiquement.\n3) Calculer l’espérance, la variance et le coefficient de variation de $X$.\n\n## Correction de l'exercice 1\n\n1) Nous considérons l'espace probabilisé $(\\Omega, \\mathcal{F}, P)$ associé à l'expérience aléatoire de lancer 3 fois une pièce de monnaie. L'ensemble des résultats possibles $\\Omega$ est donné par :\n\n$$\n\\Omega = \\{PPP, PPF, PFP, PFF, FPP, FPF, FFP, FFF\\} = \\{P, F\\}^3\n$$\n\nNous munissons cet espace de la tribu $\\mathcal{F}$ des parties de $\\Omega$ et de la probabilité uniforme $P$.\n\nLa variable aléatoire $X$ est définie comme le nombre de « Pile » obtenus lors des 3 lancers. Ainsi, $X$ peut prendre les valeurs suivantes :\n- $X = 0$ : lorsque le résultat est FFF\n- $X = 1$ : lorsque le résultat est PFF, FPF, ou FFP\n- $X = 2$ : lorsque le résultat est PPF, PFP, ou FPP\n- $X = 3$ : lorsque le résultat est PPP\n\nCette variable est donc définie comme suit :\n\n\n$$\nX : \\Omega^3 \\longrightarrow \\Omega' = \\{0,1,2,3\\}\n$$\n\n$$\n(\\omega_1, \\omega_2, \\omega_3) \\longmapsto\n\\begin{cases}\n0 & \\text{si } (\\omega_1, \\omega_2, \\omega_3) = (FFF),\\\\[4pt]\n1 & \\text{si } (\\omega_1, \\omega_2, \\omega_3) \\in \\{(PFF),(FPF),(FFP)\\},\\\\[4pt]\n2 & \\text{si } (\\omega_1, \\omega_2, \\omega_3) \\in \\{(PPF),(PFP),(FPP)\\},\\\\[4pt]\n3 & \\text{si } (\\omega_1, \\omega_2, \\omega_3) = (PPP).\n\\end{cases}\n$$\n\nComme la variable aléatoire X est **discrète**, il suffit de déterminer la probabilité $\\mathbb{P}_X$ sur les singletons.\n\n$$\n\\begin{aligned}\n\\mathbb{P}_X(0) &= \\frac{1}{8},\\\n\\mathbb{P}_X(1) &= \\frac{3}{8},\\\n\\mathbb{P}_X(2) &= \\frac{3}{8},\\\n\\mathbb{P}_X(3) &= \\frac{1}{8}.\n\\end{aligned}\n$$\n\nAinsi, ( X ) suit une **loi binomiale** :\n$$\nX \\sim \\mathcal{B}(n=3,, p=\\tfrac{1}{2}).\n$$\n\n\n\n2. Fonction de répartition\n\nÀ partir de la définition de la fonction de répartition :\n$$\nF_X(x) = \\mathbb{P}(X \\le x),\n$$\non obtient :\n\n$$\nF_X(x) =\n\\begin{cases}\n0 & \\text{si } x < 0,\\\\[4pt]\n\\frac{1}{8} & \\text{si } 0 \\le x < 1,\\\\[4pt]\n\\frac{4}{8} = \\frac{1}{2} & \\text{si } 1 \\le x < 2,\\\\[4pt]\n\\frac{7}{8} & \\text{si } 2 \\le x < 3,\\\\[4pt]\n1 & \\text{si } 3 \\le x.\n\\end{cases}\n$$\n\n3. Espérance, variance et coefficient de variation\n\nPour calculer l'espérance, la variance et le coefficient de variation de la variable aléatoire $X$, nous allons utiliser la formule de transfert en temps discret :\n\n$$\n\\mathbb{E}[g(X)] = \\sum_{i} g(x_i) \\cdot \\mathbb{P}_X(x_i),\n$$\noù $g$ est une fonction mésurable.\n\nAinsi, pour l'espérance, nous avons :\n$$\n\\mathbb{E}[X] = \\sum_{x=0}^{3} x \\cdot \\mathbb{P}_X(x) = 0 \\cdot \\frac{1}{8} + 1 \\cdot \\frac{3}{8} + 2 \\cdot \\frac{3}{8} + 3 \\cdot \\frac{1}{8} = \\frac{12}{8} = np =1.5.\n$$\n\nPour la variance, nous utilisons la formule :\n\n$$\n\\text{Var}(X) = \\sum_{x=0}^{3} (x - \\mathbb{E}[X])^2 \\cdot \\mathbb{P}_X(x) = (0 - 1.5)^2 \\cdot \\frac{1}{8} + (1 - 1.5)^2 \\cdot \\frac{3}{8} + (2 - 1.5)^2 \\cdot \\frac{3}{8} + (3 - 1.5)^2 \\cdot \\frac{1}{8} = \\frac{6}{8} = np(1-p) = 0.75.\n$$\n\nEnfin, le coefficient de variation est donné par la formule :\n\n$$\nCV(X) = \\frac{\\sigma_X}{\\mathbb{E}[X]} = \\frac{\\sqrt{\\text{Var}(X)}}{\\mathbb{E}[X]} = \\frac{\\sqrt{0.75}}{1.5} = \\frac{\\sqrt{3}}{3} \\approx 0.577.\n$$\n\n# Exercice 2\n\nNous considérons l’expérience aléatoire consistant à jeter un dé jusqu’à obtenir un **« 6 »**. Soit $X$ la variable aléatoire donnant le nombre de jets.\n\n1. Donner la loi de la variable aléatoire $X$.\n2. Donner la fonction de répartition de $X$, et la représenter graphiquement.\n3. Montrer que la fonction génératrice des moments de $X$ vaut :\n\n   $$\n   M_X(t) = \\frac{pe^t}{1 - e^t(1 - p)},\n   $$\n\n   avec $p = \\frac{1}{6}$.\n    \n4. En déduire l’espérance et la variance de $X$, puis son coefficient de variation.\n\n## Correction de l'exercice 2\n\n1) Loi de la variable aléatoire X\n\nNous considérons l'espace probabilisé $(\\Omega^{\\mathbb{N}}, \\mathcal{P}(\\Omega^{\\mathbb{N}}), \\mathbb{P})$ avec $\\Omega = \\{1,2,3,4,5,6\\}$ muni de la tribu produit et de la probabilité uniforme $\\mathbb{P}$. La variable aléatoire étudiée est\n\n$$\nX : \\Omega^{\\mathbb{N}} \\longrightarrow \\Omega' = \\mathbb{N}^*\n$$\n\n$$\n(\\omega_1, \\omega_2, \\ldots) \\longmapsto\n\\begin{cases}\n1 & \\text{si } \\omega_1 = \\{6\\},\\\\[4pt]\n2 & \\text{si } \\omega_1 \\neq \\{6\\} \\text{ et } \\omega_2 = \\{6\\},\\\\[4pt]\n& \\cdots\\\\[4pt]\nk & \\text{si } \\omega_1, \\ldots, \\omega_{k-1} \\neq \\{6\\} \\text{ et } \\omega_k = \\{6\\},\\\\[4pt]\n& \\cdots\n\\end{cases}\n$$\n\nPour tout $k \\in \\mathbb{N}^*$, on a :\n\n$$\n\\mathbb{P}(X = k) = \\mathbb{P}(\\omega_1, \\ldots, \\omega_{k-1} \\neq \\{6\\} \\text{ et } \\omega_k = \\{6\\})\n$$\n\n$$\n= \\prod_{i=1}^{k-1} \\mathbb{P}(\\omega_i \\neq \\{6\\}) \\times \\mathbb{P}(\\omega_k = \\{6\\})\n$$\n\n$$\n= (1 - p)^{k-1}p,\n$$\n\navec $p = \\frac{1}{6}$ la probabilité d'obtenir un \"6\" lors d'un lancer. Il s'agit d'une distribution géométrique de paramètre $p$. \n\n2) Fonction de répartition\n\nSoit $k \\geq 1$. Nous avons :\n\n$$\n\\mathbb{P}(X \\leq k) = \\sum_{j=1}^{k} (1-p)^{j-1}p\n$$\n\n$$\n= p\\sum_{j=0}^{k-1} (1-p)^{j} = p\\frac{1-(1-p)^k}{1-(1-p)} = 1-(1-p)^k.\n$$\n\nA partir de la définition de la fonction de répartition, nous obtenons\n\n$$\nF_X(x) = \n\\begin{cases}\n0 & \\text{si } x < 1,\\\\[4pt]\np & \\text{si } 1 \\leq x < 2,\\\\[4pt]\n& \\cdots,\\\\[4pt]\n1-(1-p)^k & \\text{si } k \\leq x < k+1,\\\\[4pt]\n& \\cdots.\n\\end{cases}\n$$\n\n3) Fonction génératrice des moments\n\nEn utilisant la formule de transfert en temps discret, nous avons :\n$$\nM_X(t) = \\int e^{tX} d\\mathbb{P} = \\sum_{x=1}^{\\infty} e^{tx}\\mathbb{P}_X(x)\n$$\n\n$$\n= p\\sum_{j=1}^{\\infty} e^{tj}(1-p)^{j-1}\n$$\n\n$$\n= pe^t\\sum_{j=0}^{\\infty} e^{tj}(1-p)^j\n$$\n\n$$\n= pe^t\\sum_{j=0}^{\\infty} q^j \\text{ avec } q = e^t(1-p)\n$$\n\n$$\n= \\frac{pe^t}{1-q} = \\frac{pe^t}{1-e^t(1-p)}.\n$$\n\nLa série converge si et seulement si $|q| < 1$, c'est-à-dire si $|e^t(1-p)| < 1$, soit $e^t < \\frac{1}{1-p}$, ce qui donne $t < \\ln\\left(\\frac{1}{1-p}\\right) = -\\ln(1-p)$.\n\n4) Espérance, variance et coefficient de variation\n\nNous savons que la fonction $M_X$ est indéfiniment dérivable dans un voisinage de 0, et que $M'_X(0) = E(X)$ et $M''_X(0) = E(X^2)$. Nous avons après calcul\n\n$$\nM'_x(t) = \\frac{pe^t}{\\{1-e^t(1-p)\\}^2},\n$$\n\n$$\nM''_x(t) = \\frac{pe^t\\{1-(1-p)^2e^{2t}\\}}{\\{1-e^t(1-p)\\}^4},\n$$\n\ndont nous déduisons que\n\n$$\nM'_x(0) = \\frac{1}{p} \\equiv E(X),\n$$\n\n$$\nM''_x(0) = \\frac{p\\{1-(1-p)^2\\}}{\\{1-(1-p)\\}^4} = \\frac{2-p}{p^2} \\equiv E(X^2),\n$$\n\npuis\n\n$$\nV(X) = \\frac{1-p}{p^2},\n$$\n\n$$\nCV(X) = \\sqrt{1-p}.\n$$\n\nNumériquement, nous obtenons\n\n$$\nE(X) = 6, \\quad V(X) = 30, \\quad CV(X) = 0.91.\n$$\n\n\n## Exercice 3\n\nSoit $X$ une variable aléatoire admettant un moment d'ordre 2. Nous notons $m = E(X)$ et $\\sigma^2 = V(X)$. Soient $X_1, \\ldots, X_n$ des variables aléatoires indépendantes et de même loi que $X$. Nous notons\n\n$$\n\\overline{X}_n = \\frac{1}{n}\\sum_{i=1}^{n} X_i\n$$\n\nla moyenne empirique des $X_i$, $i = 1, \\ldots, n$, et\n\n$$\ns_X^2 = \\frac{1}{n-1}\\sum_{i=1}^{n} (X_i - \\overline{X}_n)^2\n$$\n\nla dispersion des $X_i$, $i = 1, \\ldots, n$.\n\n1) Montrer que $E(\\overline{X}_n) = m$.\n\n2) Montrer que $E(s_X^2) = \\sigma^2$. Vous pourrez utiliser l'identité\n\n$$\ns_X^2 = \\frac{1}{2n(n-1)} \\sum_{i=1}^{n} \\sum_{\\substack{j=1\\\\j \\neq i}}^{n} (X_i - X_j)^2.\n$$\n\n3) Utiliser ces résultats pour obtenir une approximation de Monte-Carlo de l'espérance, la variance et le coefficient de variation de la variable aléatoire $X$ de l'exercice 2.\n\n- Démonstration de l'équivalence des formules de variance\n\nNous voulons démontrer que :\n\n$$\\frac{1}{n-1}\\sum_{i=1}^{n}(X_i - \\bar{X}_n)^2 = \\frac{1}{2n(n-1)}\\sum_{i=1}^{n}\\sum_{\\substack{j=1\\\\j\\neq i}}^{n}(X_i - X_j)^2$$\n\n- Étape 1 : Développement de la somme double\n\nCommençons par développer le membre de droite :\n\n$$\\sum_{i=1}^{n}\\sum_{\\substack{j=1\\\\j\\neq i}}^{n}(X_i - X_j)^2$$\n\nDéveloppons $(X_i - X_j)^2$ :\n\n$$= \\sum_{i=1}^{n}\\sum_{\\substack{j=1\\\\j\\neq i}}^{n}(X_i^2 - 2X_iX_j + X_j^2)$$\n\n$$= \\sum_{i=1}^{n}\\sum_{\\substack{j=1\\\\j\\neq i}}^{n}X_i^2 - 2\\sum_{i=1}^{n}\\sum_{\\substack{j=1\\\\j\\neq i}}^{n}X_iX_j + \\sum_{i=1}^{n}\\sum_{\\substack{j=1\\\\j\\neq i}}^{n}X_j^2$$\n\n- Étape 2 : Simplification de chaque terme\n\nPremier terme :\n$$\\sum_{i=1}^{n}\\sum_{\\substack{j=1\\\\j\\neq i}}^{n}X_i^2 = \\sum_{i=1}^{n}X_i^2 \\cdot (n-1) = (n-1)\\sum_{i=1}^{n}X_i^2$$\n\nCar pour chaque $i$ fixé, $X_i^2$ est sommé $(n-1)$ fois (pour tous les $j \\neq i$).\n\nTroisième terme :\nPar symétrie, le troisième terme donne le même résultat :\n$$\\sum_{i=1}^{n}\\sum_{\\substack{j=1\\\\j\\neq i}}^{n}X_j^2 = (n-1)\\sum_{j=1}^{n}X_j^2 = (n-1)\\sum_{i=1}^{n}X_i^2$$\n\nDeuxième terme :\n$$\\sum_{i=1}^{n}\\sum_{\\substack{j=1\\\\j\\neq i}}^{n}X_iX_j$$\n\nPour chaque paire $(i,j)$ avec $i \\neq j$, le produit $X_iX_j$ apparaît dans cette double somme. On peut réécrire :\n\n$$= \\sum_{i=1}^{n}X_i\\sum_{\\substack{j=1\\\\j\\neq i}}^{n}X_j = \\sum_{i=1}^{n}X_i\\left(\\sum_{j=1}^{n}X_j - X_i\\right)$$\n\n$$= \\sum_{i=1}^{n}X_i \\cdot n\\bar{X}_n - \\sum_{i=1}^{n}X_i^2$$\n\n$$= n\\bar{X}_n \\cdot n\\bar{X}_n - \\sum_{i=1}^{n}X_i^2 = n^2\\bar{X}_n^2 - \\sum_{i=1}^{n}X_i^2$$\n\n- Étape 3 : Assemblage\n\nEn rassemblant les trois termes :\n\n$$\\sum_{i=1}^{n}\\sum_{\\substack{j=1\\\\j\\neq i}}^{n}(X_i - X_j)^2 = (n-1)\\sum_{i=1}^{n}X_i^2 - 2\\left(n^2\\bar{X}_n^2 - \\sum_{i=1}^{n}X_i^2\\right) + (n-1)\\sum_{i=1}^{n}X_i^2$$\n\n$$= (n-1)\\sum_{i=1}^{n}X_i^2 - 2n^2\\bar{X}_n^2 + 2\\sum_{i=1}^{n}X_i^2 + (n-1)\\sum_{i=1}^{n}X_i^2$$\n\n$$= 2(n-1)\\sum_{i=1}^{n}X_i^2 + 2\\sum_{i=1}^{n}X_i^2 - 2n^2\\bar{X}_n^2$$\n\n$$= 2n\\sum_{i=1}^{n}X_i^2 - 2n^2\\bar{X}_n^2$$\n\n$$= 2n\\left(\\sum_{i=1}^{n}X_i^2 - n\\bar{X}_n^2\\right)$$\n\n- Étape 4 : Lien avec la variance classique\n\nRappelons la formule classique de la variance :\n\n$$\\sum_{i=1}^{n}(X_i - \\bar{X}_n)^2 = \\sum_{i=1}^{n}(X_i^2 - 2X_i\\bar{X}_n + \\bar{X}_n^2)$$\n\n$$= \\sum_{i=1}^{n}X_i^2 - 2\\bar{X}_n\\sum_{i=1}^{n}X_i + n\\bar{X}_n^2$$\n\n$$= \\sum_{i=1}^{n}X_i^2 - 2\\bar{X}_n \\cdot n\\bar{X}_n + n\\bar{X}_n^2$$\n\n$$= \\sum_{i=1}^{n}X_i^2 - n\\bar{X}_n^2$$\n\n- Étape 5 : Conclusion\n\nD'après l'étape 3 :\n$$\n\\sum_{i=1}^{n}\\sum_{\\substack{j=1\\\\j\\neq i}}^{n}(X_i - X_j)^2 = 2n\\left(\\sum_{i=1}^{n}X_i^2 - n\\bar{X}_n^2\\right)\n$$\n\n\nD'après l'étape 4 :\n$$\n\\sum_{i=1}^{n}(X_i - \\bar{X}_n)^2 = \\sum_{i=1}^{n}X_i^2 - n\\bar{X}_n^2\n$$\n\nDonc :\n$$\n\\sum_{i=1}^{n}\\sum_{\\substack{j=1\\\\j\\neq i}}^{n}(X_i - X_j)^2 = 2n\\sum_{i=1}^{n}(X_i - \\bar{X}_n)^2\n$$\n\nEn divisant par $2n(n-1)$ :\n\n$$\n\\frac{1}{2n(n-1)}\\sum_{i=1}^{n}\\sum_{\\substack{j=1\\\\j\\neq i}}^{n}(X_i - X_j)^2 = \\frac{1}{n-1}\\sum_{i=1}^{n}(X_i - \\bar{X}_n)^2\n$$\n\n## Correction de l'exercice 3\n\n1) Calcul de $E(\\overline{X}_n)$\n\nPar linéarité de l'espérance, nous avons :\n\n$$\nE(\\overline{X}_n) = E\\left(\\frac{1}{n}\\sum_{i=1}^{n} X_i\\right) = \\frac{1}{n}\\sum_{i=1}^{n} E(X_i) = \\frac{1}{n} \\cdot n \\cdot m = m.\n$$\n\n\n\n2) Calcul de $E(s_X^2)$\n\nEn utilisant l'identité donnée, nous avons en utilisant la linéarité de l'espérance :\n\n$$\nE(s_X^2) = E\\left(\\frac{1}{2n(n-1)} \\sum_{i=1}^{n} \\sum_{\\substack{j=1\\\\j \\neq i}}^{n} (X_i - X_j)^2\\right)\n$$\n\n$$\n= \\frac{1}{2n(n-1)} \\sum_{i=1}^{n} \\sum_{\\substack{j=1\\\\j \\neq i}}^{n} E\\left((X_i - X_j)^2\\right).\n$$\n\nComme les $X_i$ sont indépendantes et de même loi, nous avons :\n\n$$\n= \\frac{1}{2} E\\left((X_1 - X_2)^2\\right).\n$$\n\nD'autre part, nous avons :\n\n$$\nE\\left((X_1 - X_2)^2\\right) = E[{(X_1 - m) - (X_2 - m)}^2]\n$$\n\n$$\n= E\\left((X_1 - m)^2\\right) + E\\left((X_2 - m)^2\\right) - 2E\\left((X_1 - m)(X_2 - m)\\right)\n$$\n\n$$\n= \\sigma^2 + \\sigma^2 - 2 \\cdot 0 = 2\\sigma^2.\n$$\n\nDonc, nous obtenons :\n$$\nE(s_X^2) = \\frac{1}{2} \\cdot 2\\sigma^2 = \\sigma^2.\n$$\n\n3) Approximation de Monte-Carlo\n\nCe que nous avons montré aux points précédents, est que nous pouvons estimer l'espérance et la variance de la variable aléatoire $X$ en utilisant les statistiques d'échantillon $\\overline{X}_n$ et $s_X^2$. C'est-à-dire en générant un grand nombre de réalisations indépendantes de $X$ et en calculant la moyenne empirique et la dispersion de ces réalisations.\n\nVoici un exemple de code R pour effectuer cette approximation de Monte-Carlo pour la variable aléatoire $X$ de l'exercice 2 :\n\n\n```default\n# Monte Carlo approximation in Python\n\n# Répétition de l'expérience aléatoire\n# Simulation du nombre de lancers nécessaires pour obtenir un 6\n\nset.seed(360)\nn <- 50000\nctr <- 0\nsimlist <- numeric(n)\n\nwhile (ctr < n) {\n  ctr <- ctr + 1\n  \n  # Valeur du lancer (initialisation)\n  trial <- 0\n  \n  # Nombre de tentatives (initialisation)\n  nb_tent <- 0\n  \n  while (trial != 6) {\n    nb_tent <- nb_tent + 1\n    trial <- sample(1:6, 1)\n  }\n  \n  simlist[ctr] <- nb_tent\n}\n\n# Affichage des résultats\nmean(simlist)\nvar(simlist)\n\n# Statistiques supplémentaires\ncat(\"\\n=== Résultats de la simulation ===\\n\")\ncat(\"Nombre de simulations:\", n, \"\\n\")\ncat(\"Moyenne du nombre de lancers:\", mean(simlist), \"\\n\")\ncat(\"Variance:\", var(simlist), \"\\n\")\ncat(\"Écart-type:\", sd(simlist), \"\\n\")\ncat(\"Minimum:\", min(simlist), \"\\n\")\ncat(\"Maximum:\", max(simlist), \"\\n\")\ncat(\"Médiane:\", median(simlist), \"\\n\")\n\n# Histogramme\nhist(simlist, \n     breaks = 30, \n     col = \"lightblue\", \n     border = \"white\",\n     main = \"Distribution du nombre de lancers pour obtenir un 6\",\n     xlab = \"Nombre de lancers\",\n     ylab = \"Fréquence\",\n     xlim = c(0, max(simlist)))\n\n# Ajout de la moyenne théorique\nabline(v = 6, col = \"red\", lwd = 2, lty = 2)\nlegend(\"topright\", \n       legend = c(\"Moyenne observée\", \"Moyenne théorique = 6\"),\n       col = c(\"blue\", \"red\"), \n       lty = c(1, 2), \n       lwd = 2)\n```\n\n",
    "supporting": [
      "index_gdr_files"
    ],
    "filters": [],
    "includes": {}
  }
}