{
  "hash": "e6d6d0dd658d23ac0d957597efeeec12",
  "result": {
    "engine": "jupyter",
    "markdown": "---\nformat: \n  html: \n    fontsize: 1.3em\n---\n\n\n\n\n<div class=\"box\">\nVous trouverez ici certains exercices de la fiche de TD, qui vous permettront de mieux comprendre les concepts abordés par le professeur pendant les cours. N'hésitez pas à me contacter si vous avez des questions ou des suggestions. \n</div>\n\n# 1. Espaces probabilisés\n\n## Exercice 2\n\nSupposons que $\\mathbb{P}(A \\cup B) = 0.6$ et $\\mathbb{P}(A \\cup B^c) = 0.8$.  \nDéterminer $\\mathbb{P}(A)$.\n\n---\n\n## Correction Exercice 2\n\nOn se place sur un espace probabilisé $(\\Omega, \\mathcal{F}, \\mathbb{P})$.  \nSoient $A, B \\in \\mathcal{F}$ deux événements.\n\nOn utilise d’abord les propriétés des complémentaires :\n$$\n(A \\cup B)^c = A^c \\cap B^c,\n\\qquad\n(A \\cup B^c)^c = A^c \\cap B.\n$$\n\nLes deux événements $(A \\cup B)^c$ et $(A \\cup B^c)^c$ sont disjoints, car :\n$$\n(A \\cup B)^c \\cup (A \\cup B^c)^c\n= (A^c \\cap B^c) \\cup (A^c \\cap B)\n= A^c.\n$$\n\nDe plus, pour deux événements disjoints $E$ et $F$,\n$$\n\\mathbb{P}(E \\cup F) = \\mathbb{P}(E) + \\mathbb{P}(F).\n$$\n\nPar conséquent,\n$$\n\\mathbb{P}(A^c)\n= \\mathbb{P}\\big( (A \\cup B)^c \\big)\n+ \\mathbb{P}\\big( (A \\cup B^c)^c \\big).\n$$\n\nOr\n$$\n\\mathbb{P}\\big( (A \\cup B)^c \\big)\n= 1 - \\mathbb{P}(A \\cup B)\n= 1 - 0.6\n= 0.4,\n$$\n$$\n\\mathbb{P}\\big( (A \\cup B^c)^c \\big)\n= 1 - \\mathbb{P}(A \\cup B^c)\n= 1 - 0.8\n= 0.2.\n$$\n\nAinsi,\n$$\n\\mathbb{P}(A^c) = 0.4 + 0.2 = 0.6,\n$$\net donc\n$$\n\\mathbb{P}(A) = 1 - \\mathbb{P}(A^c) = 1 - 0.6 = 0.4.\n$$\n\n\n\n\n---\n\n$$\n\\boxed{\\mathbb{P}(A) = 0.4}\n$$\n\n\n## Exercice 3\n\nDans une ville, 75% de la population a les cheveux bruns, 50% a les yeux marron, et 35% possède à la fois des cheveux bruns et des yeux marron.  \nOn sélectionne une personne au hasard dans la ville. Quelle est la probabilité :\n\n- qu’elle ait les yeux marron **ou** des cheveux bruns ?\n- qu’elle n’ait ni les yeux marron, ni des cheveux bruns ?\n\n---\n\n\n\n\n## Correction Exercice 3\n\nOn se place sur un espace probabilisé $(\\Omega, \\mathcal{F}, \\mathbb{P})$ où $\\Omega = \\{\\text{personnes de la ville}\\}$.\n\nOn définit les événements :\n- $B = \\{\\text{la personne a les cheveux bruns}\\}$,\n- $M = \\{\\text{la personne a les yeux marron}\\}$.\n\nD’après l’énoncé :\n$$\n\\mathbb{P}(B) = 0.75, \\qquad \\mathbb{P}(M) = 0.50, \\qquad \\mathbb{P}(B \\cap M) = 0.35.\n$$\n\n---\n\n### 1 Probabilité que la personne ait les yeux marron **ou** des cheveux bruns\n\nOn utilise la formule d’addition :\n$$\n\\mathbb{P}(B \\cup M) = \\mathbb{P}(B) + \\mathbb{P}(M) - \\mathbb{P}(B \\cap M).\n$$\n\nDonc\n$$\n\\mathbb{P}(B \\cup M) = 0.75 + 0.50 - 0.35 = 0.90.\n$$\n\n---\n\n### 2 Probabilité qu’elle n’ait **ni** les yeux marron **ni** des cheveux bruns\n\nOn veut $\\mathbb{P}(B^c \\cap M^c)$.  \nOr\n$$\nB^c \\cap M^c = (B \\cup M)^c.\n$$\n\nDonc\n$$\n\\mathbb{P}(B^c \\cap M^c) = 1 - \\mathbb{P}(B \\cup M) = 1 - 0.90 = 0.10.\n$$\n\n---\n\n### Résultats\n\n$$\n\\boxed{\\mathbb{P}(\\text{yeux marron ou cheveux bruns}) = 0.90}\n$$\n\n$$\n\\boxed{\\mathbb{P}(\\text{ni yeux marron ni cheveux bruns}) = 0.10}\n$$\n\n## Exercice 4\n\nNous lançons 3 dés équilibrés et indépendants.  \nCalculer la probabilité d’obtenir **au moins un 6** :\n\n- en utilisant la formule d’inclusion–exclusion,\n- en calculant la probabilité de **n’obtenir aucun 6**.\n\n---\n\n## Correction Exercice 4\n\nOn se place sur un espace probabilisé $(\\Omega, \\mathcal{F}, \\mathbb{P})$ où la probabilité $\\mathbb{P}$ est uniforme sur $\\Omega$.\n\nL’univers $\\Omega$ peut s’écrire de deux façons équivalentes :\n\n- sous forme cartésienne :\n$$\n\\Omega = \\{1,2,3,4,5,6\\}^3\n$$\n\n- sous forme explicite :\n$$\n\\Omega = \\{(w_1, w_2, w_3) \\mid w_i \\in \\{1,2,3,4,5,6\\},\\ i = 1,2,3\\}.\n$$\n\nOn définit les événements suivants :\n\n- $A_1 = \\{\\text{le premier dé donne un } 6\\}$,\n- $A_2 = \\{\\text{le deuxième dé donne un } 6\\}$,\n- $A_3 = \\{\\text{le troisième dé donne un } 6\\}$.\n\nNous cherchons\n$$\n\\mathbb{P}(A_1 \\cup A_2 \\cup A_3),\n$$\nla probabilité d’obtenir au moins un 6.\n\n---\n\n### 1 Méthode 1 : formule d’inclusion–exclusion\n\nPour trois événements $A_1, A_2, A_3$, on a\n$$\n\\mathbb{P}(A_1 \\cup A_2 \\cup A_3)\n= \\mathbb{P}(A_1) + \\mathbb{P}(A_2) + \\mathbb{P}(A_3)\n- \\mathbb{P}(A_1 \\cap A_2) - \\mathbb{P}(A_1 \\cap A_3) - \\mathbb{P}(A_2 \\cap A_3)\n+ \\mathbb{P}(A_1 \\cap A_2 \\cap A_3).\n$$\n\nComme les dés sont équilibrés et indépendants :\n$$\n\\mathbb{P}(A_1) = \\mathbb{P}(A_2) = \\mathbb{P}(A_3) = \\frac{1}{6},\n$$\n$$\n\\mathbb{P}(A_i \\cap A_j) = \\frac{1}{6} \\times \\frac{1}{6} = \\frac{1}{36}, \\quad \\text{pour } i \\ne j,\n$$\n$$\n\\mathbb{P}(A_1 \\cap A_2 \\cap A_3) = \\left(\\frac{1}{6}\\right)^3 = \\frac{1}{216}.\n$$\n\nDonc :\n$$\n\\mathbb{P}(A_1 \\cup A_2 \\cup A_3)\n= 3 \\times \\frac{1}{6}\n- 3 \\times \\frac{1}{36}\n+ \\frac{1}{216}\n= \\frac{1}{2} - \\frac{1}{12} + \\frac{1}{216}\n= \\frac{91}{216}\n\\simeq 0.421.\n$$\n\n---\n\n### 2 Méthode 2 : en passant par l’événement complémentaire\n\nL’événement “obtenir au moins un 6” est le complément de “n’obtenir aucun 6”.\n\nL’événement “aucun 6” s’écrit :\n$$\n\\{A_1 \\cup A_2 \\cup A_3\\}^c\n= A_1^c \\cap A_2^c \\cap A_3^c,\n$$\noù $A_i^c$ signifie “le $i$-ème dé ne donne pas 6”.\n\nPour un dé, la probabilité de **ne pas** obtenir 6 est\n$$\n\\mathbb{P}(A_i^c) = \\frac{5}{6}.\n$$\n\nPar indépendance des dés :\n$$\n\\mathbb{P}(A_1^c \\cap A_2^c \\cap A_3^c)\n= \\left(\\frac{5}{6}\\right)^3 = \\frac{125}{216}.\n$$\n\nAinsi,\n$$\n\\mathbb{P}(A_1 \\cup A_2 \\cup A_3)\n= 1 - \\mathbb{P}(A_1^c \\cap A_2^c \\cap A_3^c)\n= 1 - \\frac{125}{216}\n= \\frac{91}{216}\n\\simeq 0.421.\n$$\n\n---\n\n### Résultat\n\nDans les deux cas, on obtient :\n$$\n\\boxed{\\mathbb{P}(\\text{obtenir au moins un 6 en 3 lancers}) = \\frac{91}{216} \\approx 0.421.}\n$$\n\n\n\nJe reviendrai ici sur l'exercice 5 que nous n'avons pas eu la possibilité de mettre oeuvre la simulation monte Carlo.\n\n## Correction de l'exercice 5\n\nNous sélectionnons un entier au hasard parmi les N = 5,000 entiers qui sont dans l'intervalle [0, 4999]. \nQuelle est la probabilité qu’il soit divisible par 4,7 ou 10 ?\n\nModifier le code R vu en cours pour estimer cette probabilité par une simulation\nde Monte-Carlo pour vérifier ce résultat. Vous utiliserez au moins 50,000\nsimulations.\n\nPour résoudre cette exercice, il faut d'abord définir l'espace probabilisé. \nL'espace probabilisé est défini par le triplet (Ω, F, P) où:\n\n\n- Ω est l'ensemble des résultats possibles. Dans notre cas, Ω = {0, 1, 2, ..., 4999}.\n\n- F est la σ-algèbre des événements. Ici, nous pouvons considérer les événements comme les sous-ensembles de Ω.\n\n- P est la mesure de probabilité. Dans notre cas, chaque entier a une probabilité égale d'être sélectionné. On dit encore que la loi de probabilité est uniforme sur Ω.\n\nLe cardinal de l'ensemble Ω est |Ω| = 5000.\nLa probabilité d'un élément spécifique dans Ω est donc $P({x}) = \\frac{1}{5000}$ pour tout x dans Ω.\n\nAprès avoir défini l'espace probabilisé, nous pouvons maintenant définir l'événement A que nous voulons étudier. \nDe ce fait, définissons les événements suivants:\n\n- $A_4$ : l'événement que l'entier sélectionné est divisible par 4.\n- $A_7$ : l'événement que l'entier sélectionné est divisible par 7.\n- $A_{10}$ : l'événement que l'entier sélectionné est divisible par 10.\nL'événement A que nous voulons étudier est l'union de ces trois événements:\n\n$$\nA = A_4 \\cup A_7 \\cup A_{10}\n$$\n\nNous cherchons à calculer la probabilité de l'événement A, c'est-à-dire $P(A)$. Pour cela, nous allons utiliser la formule inclusion-exclusion:\n$$\n\\begin{aligned}\n\\mathbb{P}(A_4 \\cup A_7 \\cup A_{10})\n&= \\mathbb{P}(A_4) + \\mathbb{P}(A_7) + \\mathbb{P}(A_{10}) \\\\\n&\\quad - \\big\\{\\, \\mathbb{P}(A_4 \\cap A_7) + \\mathbb{P}(A_4 \\cap A_{10}) + \\mathbb{P}(A_7 \\cap A_{10}) \\,\\big\\} \\\\\n&\\quad + \\mathbb{P}(A_4 \\cap A_7 \\cap A_{10}).\n\\end{aligned}\n$$\n\nCalculons maintenant chaque terme de cette formule:\nPour calculer chaque probabilité, nous devons compter le nombre d'entiers dans Ω qui satisfont chaque condition $A_i$.\nEt la probabilité de chaque événement est donnée par le rapport du nombre d'entiers satisfaisant la condition sur le cardinal de l'ensemble Ω : \n\n$$\nP(A_i) = \\frac{\\text{nombre d'entiers satisfaisant } A_i}{5000}.\n$$\n\n\n- Pour $A_4$ : Les entiers divisibles par 4 sont 0, 4, 8, ..., 4996. Ainsi le nombre d'entiers divisibles par 4 est de la forme $4k$ et qui vérifient $0 \\leq 4(k+1) \\leq 5000 < 4(k+2)$, donc de cette inégalité, on déduit que $k$ vérifie l'inégalité sous dessous :\n\n$$\nk \\leq \\frac{4999}{4} < k+1.\n$$\n\nDonc le nombre d'entiers divisibles par 4 correspond à la partie entière de $\\frac{4999}{4}$ plus 1 (pour inclure le 0), soit:\n$$\n\\text{nombre d'entiers divisibles par 4} = \\left\\lfloor\\frac{4999}{4}\\right\\rfloor + 1 = 1249 + 1 = 1250.\n$$\n\nDonc, la probabilité que l'entier sélectionné soit divisible par 4 est:\n$$\nP(A_4) = \\frac{1250}{5000} = 0.25.\n$$\n\n- Pour $A_7$ : Les entiers divisibles par 7 sont 0, 7, 14, ..., 4996. En suivant le même raisonnement que pour $A_4$, nous trouvons:\n\n$$\n\\text{nombre d'entiers divisibles par 7} = \\left\\lfloor\\frac{4999}{7}\\right\\rfloor + 1 = 714 + 1 = 715.\n$$\n\nDonc, la probabilité que l'entier sélectionné soit divisible par 7 est:\n$$\nP(A_7) = \\frac{715}{5000} = 0.143.\n$$\n\n- Pour $A_{10}$ : Les entiers divisibles par 10 sont 0, 10, 20, ..., 4990. En suivant le même raisonnement que pour $A_4$, nous trouvons:\n$$\n\\text{nombre d'entiers divisibles par 10} = \\left\\lfloor\\frac{4999}{10}\\right\\rfloor + 1 = 499 + 1 = 500.\n$$\n\nDonc, la probabilité que l'entier sélectionné soit divisible par 10 est:\n$$\nP(A_{10}) = \\frac{500}{5000} = 0.1.\n$$  \n\n- Pour $A_4 \\cap A_7$ : Les entiers qui sont divisibles par 4 et 7 sont ceux qui sont divisibles par 28 qui est le PPCM de 4 et 7. En suivant le même raisonnement que pour $A_4$, nous trouvons:\n$$\n\\text{nombre d'entiers divisibles par 28} = \\left\\lfloor\\frac{4999}{28}\\right\\rfloor + 1 = 178 + 1 = 179.\n$$\n\nDonc, la probabilité que l'entier sélectionné soit divisible par 4 et 7 est:\n\n$$\nP(A_4 \\cap A_7) = \\frac{179}{5000} = 0.0358.\n$$\n\n- Pour $A_4 \\cap A_{10}$ : Les entiers qui sont divisibles par 4 et 10 sont ceux qui sont divisibles par 20 qui est le PPCM de 4 et 10. En suivant le même raisonnement que pour $A_4, nous trouvons:\n$$\n\\text{nombre d'entiers divisibles par 20} = \\left\\lfloor\\frac{4999}{20}\\right\\rfloor + 1 = 249 + 1 = 250.\n$$\n\nDonc, la probabilité que l'entier sélectionné soit divisible par 4 et 10 est:\n$$\nP(A_4 \\cap A_{10}) = \\frac{250}{5000} = 0.05.\n$$\n\n- Pour $A_7 \\cap A_{10}$ : Les entiers qui sont divisibles par 7 et 10 sont ceux qui sont divisibles par 70 qui est le PPCM de 7 et 10. En suivant le même raisonnement que pour $A_4, nous trouvons:\n\n$$\n\\text{nombre d'entiers divisibles par 70} = \\left\\lfloor\\frac{4999}{70}\\right\\rfloor + 1 = 71 + 1 = 72.\n$$\n\nDonc, la probabilité que l'entier sélectionné soit divisible par 7 et 10 est:\n$$\nP(A_7 \\cap A_{10}) = \\frac{72}{5000} = 0.0144.\n$$\n\n- Pour $A_4 \\cap A_7 \\cap A_{10}$ : Les entiers qui sont divisibles par 4, 7 et 10 sont ceux qui sont divisibles par 140 qui est le PPCM de 4, 7 et 10. En suivant le même raisonnement que pour $A_4$, nous trouvons:\n\n$$\n\\text{nombre d'entiers divisibles par 140} = \\left\\lfloor\\frac{4999}{140}\\right\\rfloor + 1 = 35 + 1 = 36.\n$$\n\nDonc, la probabilité que l'entier sélectionné soit divisible par 4, 7 et 10 est:\n$$\nP(A_4 \\cap A_7 \\cap A_{10}) = \\frac{36}{5000} = 0.0072.\n$$  \n\nEn substituant ces valeurs dans la formule d'inclusion-exclusion, nous obtenons:\n$$\n\\begin{aligned}\nP(A)\n&= P(A_4) + P(A_7) + P(A_{10}) \\\\\n&\\quad - \\big\\{\\, P(A_4 \\cap A_7) + P(A_4 \\cap A_{10}) + P(A_7 \\cap A_{10}) \\,\\big\\} \\\\\n&\\quad + P(A_4 \\cap A_7 \\cap A_{10}) \\\\\n&= 0.25 + 0.143 + 0.1 \\\\\n&\\quad - \\big\\{\\, 0.0358 + 0.05 + 0.0144 \\,\\big\\} \\\\\n&\\quad + 0.0072 \\\\\n&= 0.493 - 0.1002 + 0.0072 \\\\\n&= 0.4.\n\\end{aligned}\n$$  \n\nDonc, la probabilité qu'un entier sélectionné au hasard parmi les 5000 entiers soit divisible par 4, 7 ou 10 est de 0.4 ou 40%.\n\nMaintenant, nous allons vérifier ce résultat par une simulation de Monte-Carlo en R avec au moins 50,000 simulations.\nUne simulation de Monte-Carlo est une méthode statistique qui utilise des échantillons aléatoires pour estimer des propriétés mathématiques ou physiques comme des espérances, des intégrales ou des probabilités. Cette méthode fonctionne en générant un grand nombre de scénarios aléatoires et en observant les résultats pour obtenir une estimation statistique :\n\n- On fixe le nombre de simulations, disons n = 50000.\n- On initialise un compteur pour le nombre de succès (entiers divisibles par 4, 7 ou 10).\n- Pour chaque simulation, on génère un entier aléatoire entre 0 et 4999.\n- On vérifie si cet entier est divisible par 4, 7 ou 10. Si c'est le cas, on incrémente le compteur de succès.\n- Après avoir effectué toutes les simulations, on calcule la probabilité estimée comme le ratio du nombre de succès sur le nombre total de simulations.\n\nVoici un exemple de code R pour effectuer cette simulation de Monte-Carlo :\n\n::: {#8e9bc60a .cell execution_count=1}\n``` {.python .cell-code}\n# Monte Carlo in Python with running estimate and plot (matplotlib only)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# --- Parameters (mirror your R snippet) ---\nseed = 123\nN = 4999               # sample from 0..N (inclusive)\nM = 50_000             # number of simulations\n\nrng = np.random.default_rng(seed)\nx = rng.integers(low=0, high=N+1, size=M)  # uniform integers in [0, N]\n\nis_div = (x % 4 == 0) | (x % 7 == 0) | (x % 10 == 0)\n\n# Final Monte Carlo estimate (same as R's mean(is_div))\np_hat = is_div.mean()\n\n# Running estimates vs number of simulations\nrunning_est = np.cumsum(is_div) / np.arange(1, M + 1)\n\n# Mean of the running estimates (to draw an horizontal reference line)\nmean_running = running_est.mean()\n\n# --- Plot ---\nplt.figure(figsize=(7.24, 4.07), dpi=100)  # ~724x407 px\n\nplt.plot(np.arange(1, M + 1), running_est, linewidth=2)\n# Add the colored horizontal line at mean_running\nplt.axhline(mean_running, linestyle=\"--\", linewidth=2, color=\"red\")\nplt.xlabel(\"Nombre de simulations\")\nplt.ylabel(\"Estimation de la probabilité\")\nplt.title(\"Convergence de l’estimation Monte-Carlo\", fontsize=14, weight=\"bold\")\nplt.suptitle(\n    \"Courbe de l’estimation cumulée; ligne horizontale = moyenne des estimations\",\n    fontsize=10, color=\"gray\"\n)\n\n# Style cues similar to the provided seaborn example\nax = plt.gca()\nfor spine in [\"top\", \"right\"]:\n    ax.spines[spine].set_visible(False)\nplt.grid(False)\n\nplt.tight_layout()\nplt.show()\n\np_hat\n\n```\n\n::: {.cell-output .cell-output-display}\n![](index_gdr_files/figure-html/cell-2-output-1.png){width=714 height=403}\n:::\n\n::: {.cell-output .cell-output-display execution_count=1}\n```\nnp.float64(0.3965)\n```\n:::\n:::\n\n\n# 2. Probabilité conditionnelle et indépendance\n\n## Exercice 2\n\nUne classe contient un total de 108 étudiants. Parmi ceux-ci, 36 indiquent qu’ils pratiquent un sport de haut niveau, et 21 étudiants parmi ces 36 indiquent qu’ils préfèrent s’entraîner le matin plutôt que l’après-midi.  \nParmi les étudiants qui ne pratiquent pas un sport de haut niveau, 24 indiquent qu’ils préfèrent pratiquer le sport le matin plutôt que l’après-midi.\n\nPour un étudiant pris au hasard dans la classe, calculer les probabilités suivantes :\n\n- probabilité d’être un sportif de haut niveau sachant que l’on préfère s’entraîner le matin ;\n- probabilité de préférer s’entraîner le matin sachant qu’on ne pratique pas un sport de haut niveau ;\n- probabilité de ne pas pratiquer un sport de haut niveau sachant que l’on préfère s’entraîner l’après-midi.\n\n---\n\n## Correction Exercice 2\n\nOn se place sur un espace probabilisé $(\\Omega, \\mathcal{F}, \\mathbb{P})$, où  \n$\\Omega = \\{\\text{ensemble des étudiants de la classe}\\}$,  \n$\\mathcal{F} = \\mathcal{P}(\\Omega)$ (toutes les parties de $\\Omega$)  \net $\\mathbb{P}$ est la probabilité uniforme sur $\\Omega$ (tous les étudiants sont équiprobables).\n\nNous notons :\n\n- $H = \\{\\text{l’étudiant pratique un sport de haut niveau}\\}$,\n- $M = \\{\\text{l’étudiant préfère s’entraîner le matin}\\}$.\n\nD’après l’énoncé, nous avons\n$$\n\\mathbb{P}(H) = \\frac{36}{108} = \\frac{1}{3},\n\\qquad\n\\mathbb{P}(M \\mid H) = \\frac{21}{36} = \\frac{7}{12},\n\\qquad\n\\mathbb{P}(M \\mid H^c) = \\frac{24}{72} = \\frac{1}{3}.\n$$\n\n---\n\n### 1 Probabilité d’être un sportif de haut niveau sachant que l’on préfère s’entraîner le matin\n\nOn cherche $\\mathbb{P}(H \\mid M)$.\n\nEn appliquant la formule de Bayes :\n$$\n\\mathbb{P}(H \\mid M) = \\frac{\\mathbb{P}(M \\mid H)\\mathbb{P}(H)}{\\mathbb{P}(M \\mid H)\\mathbb{P}(H) + \\mathbb{P}(M \\mid H^c)\\mathbb{P}(H^c)}.\n$$\n\nEn remplaçant par les valeurs numériques :\n$$\n\\mathbb{P}(H \\mid M)\n= \\frac{\\frac{7}{12} \\times \\frac{1}{3}}\n{\\frac{7}{12} \\times \\frac{1}{3} + \\frac{1}{3} \\times \\frac{2}{3}}\n= \\frac{\\frac{7}{36}}{\\frac{7}{36} + \\frac{2}{9}}\n= \\frac{7}{15}.\n$$\n\n---\n\n### 2 Probabilité de préférer s’entraîner le matin sachant qu’on ne pratique pas un sport de haut niveau\n\nCette probabilité est directement donnée par l’énoncé :\n$$\n\\mathbb{P}(M \\mid H^c) = \\frac{1}{3}.\n$$\n\n---\n\n### 3 Probabilité de ne pas pratiquer un sport de haut niveau sachant que l’on préfère s’entraîner l’après-midi\n\nOn cherche $\\mathbb{P}(H^c \\mid M^c)$.\n\nOn applique encore une fois la formule de Bayes :\n$$\n\\mathbb{P}(H^c \\mid M^c)\n= \\frac{\\mathbb{P}(M^c \\mid H^c)\\mathbb{P}(H^c)}\n{\\mathbb{P}(M^c \\mid H^c)\\mathbb{P}(H^c) + \\mathbb{P}(M^c \\mid H)\\mathbb{P}(H)}.\n$$\n\nSachant que\n$$\n\\mathbb{P}(M^c \\mid H^c) = \\frac{2}{3},\n\\qquad\n\\mathbb{P}(M^c \\mid H) = \\frac{5}{12},\n\\qquad\n\\mathbb{P}(H^c) = \\frac{2}{3},\n\\qquad\n\\mathbb{P}(H) = \\frac{1}{3},\n$$\n\nnous obtenons\n$$\n\\mathbb{P}(H^c \\mid M^c)\n= \\frac{\\frac{2}{3} \\times \\frac{2}{3}}\n{\\frac{2}{3} \\times \\frac{2}{3} + \\frac{5}{12} \\times \\frac{1}{3}}\n= \\frac{\\frac{4}{9}}{\\frac{4}{9} + \\frac{5}{36}}\n= \\frac{16}{21}.\n$$\n\n---\n\n###  Résumé des résultats\n\n$$\n\\boxed{\\mathbb{P}(H \\mid M) = \\frac{7}{15}}\n$$\n$$\n\\boxed{\\mathbb{P}(M \\mid H^c) = \\frac{1}{3}}\n$$\n$$\n\\boxed{\\mathbb{P}(H^c \\mid M^c) = \\frac{16}{21}}\n$$\n\n## Exercice 3\n\nPour chacune des deux affirmations suivantes, montrer qu’elle est vraie pour tous les événements $A$ et $B$ pour lesquels les probabilités conditionnelles sont calculables, ou donner un contre-exemple.\n\n$\\mathbb{P}(A \\mid B) + \\mathbb{P}(A \\mid B^c) = 1.$\n\n$\\mathbb{P}(A \\mid B) + \\mathbb{P}(A^c \\mid B) = 1.$\n\n## Correction Exercice 3\n\n1. Pour la première affirmation, nous avons en utilisant la formule de Bayes et en supposant que $\\mathbb{P}(B) > 0$ et $\\mathbb{P}(B^c) > 0$ :\n\n$$\n\\mathbb{P}(A \\mid B)  = \\frac{\\mathbb{P}(A \\cap B)}{\\mathbb{P}(B)},\n\\qquad \\text{et} \\qquad\n\\mathbb{P}(A \\mid B^c)  = \\frac{\\mathbb{P}(A \\cap B^c)}{\\mathbb{P}(B^c)}.\n$$\n\nDe ce fait, si nous additionnons les deux probabilités conditionnelles, nous obtenons :\n$$\n\\mathbb{P}(A \\mid B) + \\mathbb{P}(A \\mid B^c)\n= \\frac{\\mathbb{P}(A \\cap B)}{\\mathbb{P}(B)} + \\frac{\\mathbb{P}(A \\cap B^c)}{\\mathbb{P}(B^c)}.\n$$\n\nSi on se met dans une situation où $\\mathbb{P}(B) = \\mathbb{P}(B^c) = 0.5$, et en utilisant le fait que $\\mathbb{P}(A \\cap B) + \\mathbb{P}(A \\cap B^c) = \\mathbb{P}(A)$, nous obtenons :\n$$\n\\mathbb{P}(A \\mid B) + \\mathbb{P}(A \\mid B^c) = 2 \\mathbb{P}(A).\n$$\n\nDonc, cette affirmation est fausse dès que $\\mathbb{P}(A) \\neq 0.5$. Et il est facile de trouver un contre-exemple, c'est-à-dire une expérience aléatoire où cette condition n'est pas satisfaite.\n\nEn effet, si nous considérons une expérience aléatoire consistant à lancer un dé équilibré à six faces, et nous définissons les événements suivants :\n- $A$ : l'événement que le résultat est un 6,\n- $B$ : l'événement que le résultat est un nombre pair.\n\nNous avons :\n$$\n\\mathbb{P}(B) = \\frac{3}{6} = 0.5 = \\mathbb{P}(B^c),\n$$\net \n$$\n\\mathbb{P}(A) = \\frac{1}{6} \\neq 0.5.\n$$\n\nEt dans ce cas l'affirmation ne tient pas.\n\n2. La seconde affirmation est toujours vraie. On peut le démontrer en utilisant la formule de Bayes :\n$$\n\\mathbb{P}(A \\mid B) = \\frac{\\mathbb{P}(A \\cap B)}{\\mathbb{P}(B)},\n\\qquad \\text{et} \\qquad\n\\mathbb{P}(A^c \\mid B) = \\frac{\\mathbb{P}(A^c \\cap B)}{\\mathbb{P}(B)}.\n$$\n\nEn additionnant les deux probabilités conditionnelles, nous obtenons :\n$$\n\\mathbb{P}(A \\mid B) + \\mathbb{P}(A^c \\mid B)\n= \\frac{\\mathbb{P}(A \\cap B)}{\\mathbb{P}(B)} + \\frac{\\mathbb{P}(A^c \\cap B)}{\\mathbb{P}(B)}\n= \\frac{\\mathbb{P}((A \\cap B) \\cup (A^c \\cap B))}{\\mathbb{P}(B)}\n= \\frac{\\mathbb{P}(B)}{\\mathbb{P}(B)} = 1.\n$$\n\nOu tout simplement en remarquant que $P(.|B)$ définit une probabilité sur l'espace probabilisé restreint à l'événement B, et donc la somme des probabilités d'un événement et de son complément est égale à 1.\n\n## Exercice 4\n\nUn jeu de 32 cartes contient une carte manquante. Nous sélectionnons au hasard une carte parmi les 31 cartes restantes.  \nCalculer la probabilité que la carte tirée soit un cœur, en utilisant un conditionnement sur la carte manquante.\n\n---\n\n## Correction Exercice 4\n\nOn se place sur un espace probabilisé $(\\Omega,\\mathcal{F},\\mathbb{P})$ défini ainsi :\n\n- $\\Omega$ est l’ensemble des couples\n  $$\n  \\Omega = \\{(c_{\\text{manq}}, c_{\\text{tirée}})\\mid c_{\\text{manq}}, c_{\\text{tirée}} \\text{ sont des cartes du jeu de 32, } c_{\\text{tirée}}\\neq c_{\\text{manq}}\\}.\n  $$\n  Autrement dit, on choisit d’abord la carte manquante, puis on tire une carte parmi les 31 restantes.\n- Chaque couple possible est supposé équiprobable, donc\n  $$\n  \\mathbb{P}(\\{(c_{\\text{manq}}, c_{\\text{tirée}})\\}) = \\frac{1}{32\\times 31}.\n  $$\n\nOn introduit les événements suivants :\n\n- $M = \\{\\text{la carte manquante est un cœur}\\}$,\n- $T = \\{\\text{la carte tirée est un cœur}\\}$.\n\nDans un jeu de 32 cartes, il y a 8 cœurs au total.\n\nSi vous avez des difficultés pour définir l’espace probabilisé, je vous conseille de définir uniquement les événements $M$ et $T$ et de raisonner en termes de probabilités conditionnelles.\n\n---\n\n### 1 Probabilités conditionnelles\n\n- Si la carte manquante est un cœur (événement $M$), il reste $7$ cœurs parmi les $31$ cartes encore présentes, donc\n  $$\n  \\mathbb{P}(T \\mid M) = \\frac{7}{31}.\n  $$\n\n- Si la carte manquante n’est **pas** un cœur (événement $M^c$), il reste toujours les $8$ cœurs dans les $31$ cartes présentes, donc\n  $$\n  \\mathbb{P}(T \\mid M^c) = \\frac{8}{31}.\n  $$\n\nPar ailleurs, la carte manquante est choisie au hasard parmi les 32 cartes :\n\n- il y a 8 cœurs possibles pour la carte manquante, donc\n  $$\n  \\mathbb{P}(M) = \\frac{8}{32} = \\frac{1}{4},\n  $$\n- et donc\n  $$\n  \\mathbb{P}(M^c) = 1 - \\mathbb{P}(M) = \\frac{3}{4}.\n  $$\n\n---\n\n\n### 2 Application de la formule des probabilités totales\n\nOn veut calculer $\\mathbb{P}(T)$, la probabilité que la carte tirée soit un cœur.  \nOn conditionne par l’événement $M$ (la carte manquante est un cœur ou non) :\n\n$$\n\\mathbb{P}(T)\n= \\mathbb{P}(T \\mid M)\\mathbb{P}(M)\n+ \\mathbb{P}(T \\mid M^c)\\mathbb{P}(M^c).\n$$\n\nEn remplaçant par les valeurs trouvées :\n\n$$\n\\mathbb{P}(T)\n= \\frac{7}{31} \\times \\frac{1}{4}\n+ \\frac{8}{31} \\times \\frac{3}{4}\n= \\frac{7}{124} + \\frac{24}{124}\n= \\frac{31}{124}\n= \\frac{1}{4}.\n$$\n\n---\n\n### Résultat\n\nLa probabilité que la carte tirée soit un cœur vaut\n$$\n\\boxed{\\mathbb{P}(T) = \\frac{1}{4}}.\n$$\n\nOn retrouve d’ailleurs un résultat intuitif : en moyenne, retirer une carte au hasard puis tirer parmi les restantes ne modifie pas la probabilité d’obtenir un cœur, qui reste $8/32 = 1/4$.\n\n\n::: {.byline}\n<span class=\"date\">May 28, 2025</span>\n:::\n\n# Rappels de la séance précédente\n\nL'objectif de la prémière séance de TD était de consolider vos connaissances sur les espaces probabilisés et sur les probabilités conditionnelles.\n\nVous devez actuellement être capable de :\n\n- Définir un espace probabilisé (Ω, F, P).\n\n- Utiliser les propriétés des probabilités pour calculer des probabilités d'événements simples et composés.\n\n- Vous devez maitriser les lois de Morgan :\n\n  - La loi de Morgan pour l'union : $\\overline{A \\cup B} = \\overline{A} \\cap \\overline{B}$\n  - La loi de Morgan pour l'intersection : $\\overline{A \\cap B} = \\overline{A} \\cup \\overline{B}$\n\n- Appliquer la formule d'inclusion-exclusion pour calculer la probabilité de l'union de deux événements :\n\n  $$\n  P(A \\cup B) = P(A) + P(B) - P(A \\cap B)\n  $$\n\n- Appliquer la formule d'inclusion-exclusion pour calculer la probabilité de l'union de plusieurs événements :\n\n  $$\n  \\begin{aligned}\n  P\\left(\\bigcup_{i=1}^{n} A_i\\right) &= \\sum_{i=1}^{n} P(A_i) - \\sum_{1 \\leq i < j \\leq n} P(A_i \\cap A_j) \\\\\n  &\\quad + \\sum_{1 \\leq i < j < k \\leq n} P(A_i \\cap A_j \\cap A_k) - \\ldots + (-1)^{n+1} P(A_1 \\cap A_2 \\cap \\ldots \\cap A_n)\n  \\end{aligned}\n  $$\n- Calculer des probabilités conditionnelles en utilisant la formule de Bayes :\n\n$$\n  P(A|B) = \\frac{P(A \\cap B)}{P(B)}.\n$$\n\nou bien encore :\n\n$$\n  P(A\\cap B) = P(A|B) \\cdot P(B).\n$$\n\n- Appliquer la loi des probabilités totales pour décomposer des probabilités complexes en utilisant des événements disjoints et exhaustifs :\n\nPar exemple, si $C_1, C_2$ sont deux événements disjoints et exhaustifs, c'est-à-dire que $C_1 \\cap C_2 = \\emptyset$ et $C_1 \\cup C_2 = \\Omega$, alors pour tout événement A, on a :\n\n$$\nP(A) = P(A|C_1) \\cdot P(C_1) + P(A|C_2) \\cdot P(C_2).\n$$\n\nEt finalement, cela nous permet de calculer la probabilité de $C_1$ sachant A en utilisant la formule de Bayes :\n\n$$\nP(C_1|A) = \\frac{P(A|C_1) \\cdot P(C_1)}{P(A|C_1) \\cdot P(C_1) + P(A|C_2) \\cdot P(C_2)}.\n$$\n\n\n\n\n\n```{mermaid}\nflowchart LR\n  A[Espaces probabilisés] --> B(Variable aléatoire X)\n  B --> C{type}\n  C --> D[Discrète]\n  C --> E[Continue]\n  D --> F[Caractérisation de la loi de X]\n  E --> G[Caractérisation de la loi de X]\n```\n\n\n\n\n# 3 Variables aléatoires générales\n\n\n## Exercice 1\n\nNous considérons l’expérience aléatoire consistant à lancer 3 fois une pièce de monnaie. Soit $X$ la variable aléatoire donnant le nombre de « Pile » obtenu.\n\n1) Donner la loi de la variable aléatoire $X$.\n2) Donner la fonction de répartition de $X$, et la représenter graphiquement.\n3) Calculer l’espérance, la variance et le coefficient de variation de $X$.\n\n## Correction de l'exercice 1\n\n1) Nous considérons l'espace probabilisé $(\\Omega, \\mathcal{F}, P)$ associé à l'expérience aléatoire de lancer 3 fois une pièce de monnaie. L'ensemble des résultats possibles $\\Omega$ est donné par :\n\n$$\n\\Omega = \\{PPP, PPF, PFP, PFF, FPP, FPF, FFP, FFF\\} = \\{P, F\\}^3\n$$\n\nNous munissons cet espace de la tribu $\\mathcal{F}$ des parties de $\\Omega$ et de la probabilité uniforme $P$.\n\nLa variable aléatoire $X$ est définie comme le nombre de « Pile » obtenus lors des 3 lancers. Ainsi, $X$ peut prendre les valeurs suivantes :\n- $X = 0$ : lorsque le résultat est FFF\n- $X = 1$ : lorsque le résultat est PFF, FPF, ou FFP\n- $X = 2$ : lorsque le résultat est PPF, PFP, ou FPP\n- $X = 3$ : lorsque le résultat est PPP\n\nCette variable est donc définie comme suit :\n\n\n$$\nX : \\Omega^3 \\longrightarrow \\Omega' = \\{0,1,2,3\\}\n$$\n\n$$\n(\\omega_1, \\omega_2, \\omega_3) \\longmapsto\n\\begin{cases}\n0 & \\text{si } (\\omega_1, \\omega_2, \\omega_3) = (FFF),\\\\[4pt]\n1 & \\text{si } (\\omega_1, \\omega_2, \\omega_3) \\in \\{(PFF),(FPF),(FFP)\\},\\\\[4pt]\n2 & \\text{si } (\\omega_1, \\omega_2, \\omega_3) \\in \\{(PPF),(PFP),(FPP)\\},\\\\[4pt]\n3 & \\text{si } (\\omega_1, \\omega_2, \\omega_3) = (PPP).\n\\end{cases}\n$$\n\nComme la variable aléatoire X est **discrète**, il suffit de déterminer la probabilité $\\mathbb{P}_X$ sur les singletons.\n\n$$\n\\begin{aligned}\n\\mathbb{P}_X(0) &= \\frac{1}{8},\\\n\\mathbb{P}_X(1) &= \\frac{3}{8},\\\n\\mathbb{P}_X(2) &= \\frac{3}{8},\\\n\\mathbb{P}_X(3) &= \\frac{1}{8}.\n\\end{aligned}\n$$\n\nAinsi, ( X ) suit une **loi binomiale** :\n$$\nX \\sim \\mathcal{B}(n=3,, p=\\tfrac{1}{2}).\n$$\n\n\n\n2. Fonction de répartition\n\nÀ partir de la définition de la fonction de répartition :\n$$\nF_X(x) = \\mathbb{P}(X \\le x),\n$$\non obtient :\n\n$$\nF_X(x) =\n\\begin{cases}\n0 & \\text{si } x < 0,\\\\[4pt]\n\\frac{1}{8} & \\text{si } 0 \\le x < 1,\\\\[4pt]\n\\frac{4}{8} = \\frac{1}{2} & \\text{si } 1 \\le x < 2,\\\\[4pt]\n\\frac{7}{8} & \\text{si } 2 \\le x < 3,\\\\[4pt]\n1 & \\text{si } 3 \\le x.\n\\end{cases}\n$$\n\nRéprésentation graphique de la fonction de répartition :\n\n::: {#8e3f8aa5 .cell execution_count=2}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef F(x):\n    x = np.asarray(x)\n    return np.where(\n        x < 0, 0,\n        np.where(\n            x < 1, 1/8,\n            np.where(\n                x < 2, 4/8,\n                np.where(\n                    x < 3, 7/8,\n                    1\n                )\n            )\n        )\n    )\n\n# Segments horizontaux de la fonction de répartition\nsegments = [\n    (-1, 0, 0),   # de x=-1 à x=0 : F=0\n    (0, 1, 1/8),\n    (1, 2, 4/8),\n    (2, 3, 7/8),\n    (3, 4, 1)\n]\n\nplt.figure(figsize=(6,4))\n\n# Tracé des segments horizontaux uniquement\nfor x_start, x_end, y_val in segments:\n    plt.hlines(y_val, x_start, x_end, colors=\"blue\")\n\n# Points ouverts (limite à gauche non incluse)\nx_open  = [0,   1,   2,   3]\ny_open  = [0, 1/8, 4/8, 7/8]\nplt.scatter(x_open, y_open, facecolors=\"none\", edgecolors=\"black\", s=60, zorder=3)\n\n# Points fermés (valeur incluse)\nx_closed = [0,   1,   2,   3]\ny_closed = [1/8, 4/8, 7/8, 1]\nplt.scatter(x_closed, y_closed, color=\"black\", s=60, zorder=3)\n\n# Format graphique\nplt.xlabel(\"x\")\nplt.ylabel(\"F(x)\")\nplt.title(\"Fonction de répartition \")\nplt.grid(True, linestyle=\"--\", alpha=0.4)\nplt.xlim(-1, 4)\nplt.ylim(-0.05, 1.05)\n\nplt.show()\n\n```\n\n::: {.cell-output .cell-output-display}\n![](index_gdr_files/figure-html/cell-3-output-1.png){width=519 height=377}\n:::\n:::\n\n\n3. Espérance, variance et coefficient de variation\n\nPour calculer l'espérance, la variance et le coefficient de variation de la variable aléatoire $X$, nous allons utiliser la formule de transfert en temps discret :\n\n$$\n\\mathbb{E}[g(X)] = \\sum_{i} g(x_i) \\cdot \\mathbb{P}_X(x_i),\n$$\noù $g$ est une fonction mésurable.\n\nAinsi, pour l'espérance, nous avons :\n$$\n\\mathbb{E}[X] = \\sum_{x=0}^{3} x \\cdot \\mathbb{P}_X(x) = 0 \\cdot \\frac{1}{8} + 1 \\cdot \\frac{3}{8} + 2 \\cdot \\frac{3}{8} + 3 \\cdot \\frac{1}{8} = \\frac{12}{8} = np =1.5.\n$$\n\nPour la variance, nous utilisons la formule :\n\n$$\n\\text{Var}(X) = \\sum_{x=0}^{3} (x - \\mathbb{E}[X])^2 \\cdot \\mathbb{P}_X(x) = (0 - 1.5)^2 \\cdot \\frac{1}{8} + (1 - 1.5)^2 \\cdot \\frac{3}{8} + (2 - 1.5)^2 \\cdot \\frac{3}{8} + (3 - 1.5)^2 \\cdot \\frac{1}{8} = \\frac{6}{8} = np(1-p) = 0.75.\n$$\n\nEnfin, le coefficient de variation est donné par la formule :\n\n$$\nCV(X) = \\frac{\\sigma_X}{\\mathbb{E}[X]} = \\frac{\\sqrt{\\text{Var}(X)}}{\\mathbb{E}[X]} = \\frac{\\sqrt{0.75}}{1.5} = \\frac{\\sqrt{3}}{3} \\approx 0.577.\n$$\n\n## Exercice 2\n\nNous considérons l’expérience aléatoire consistant à jeter un dé jusqu’à obtenir un **« 6 »**. Soit $X$ la variable aléatoire donnant le nombre de jets.\n\n1. Donner la loi de la variable aléatoire $X$.\n2. Donner la fonction de répartition de $X$, et la représenter graphiquement.\n3. Montrer que la fonction génératrice des moments de $X$ vaut :\n\n   $$\n   M_X(t) = \\frac{pe^t}{1 - e^t(1 - p)},\n   $$\n\n   avec $p = \\frac{1}{6}$.\n    \n4. En déduire l’espérance et la variance de $X$, puis son coefficient de variation.\n\n## Correction de l'exercice 2\n\n1) Loi de la variable aléatoire X\n\nNous considérons l'espace probabilisé $(\\Omega^{\\mathbb{N}}, \\mathcal{P}(\\Omega^{\\mathbb{N}}), \\mathbb{P})$ avec $\\Omega = \\{1,2,3,4,5,6\\}$ muni de la tribu produit et de la probabilité uniforme $\\mathbb{P}$.\n\nL'univers $\\Omega^{\\mathbb{N}}$ correspond à l'ensemble des suites infinies de résultats de lancers de dé. Autrement dit on peut l'écrire comme l'ensemble des suites infinies :\n$$\n\\Omega^{\\mathbb{N}} = \\{(\\omega_1, \\omega_2, \\ldots) \\mid \\omega_i \\in \\Omega = \\{1,2,3,4,5,6\\} \\text{ pour tout } i \\in \\mathbb{N}\\}.\n$$\n\n La variable aléatoire étudiée est\n\n$$\nX : \\Omega^{\\mathbb{N}} \\longrightarrow \\Omega' = \\mathbb{N}^*\n$$\n\n$$\n(\\omega_1, \\omega_2, \\ldots) \\longmapsto\n\\begin{cases}\n1 & \\text{si } \\omega_1 = \\{6\\},\\\\[4pt]\n2 & \\text{si } \\omega_1 \\neq \\{6\\} \\text{ et } \\omega_2 = \\{6\\},\\\\[4pt]\n& \\cdots\\\\[4pt]\nk & \\text{si } \\omega_1, \\ldots, \\omega_{k-1} \\neq \\{6\\} \\text{ et } \\omega_k = \\{6\\},\\\\[4pt]\n& \\cdots\n\\end{cases}\n$$\n\nPour tout $k \\in \\mathbb{N}^*$, on a :\n\n$$\n\\mathbb{P}(X = k) = \\mathbb{P}(\\omega_1, \\ldots, \\omega_{k-1} \\neq \\{6\\} \\text{ et } \\omega_k = \\{6\\})\n$$\n\n$$\n= \\prod_{i=1}^{k-1} \\mathbb{P}(\\omega_i \\neq \\{6\\}) \\times \\mathbb{P}(\\omega_k = \\{6\\})\n$$\n\n$$\n= (1 - p)^{k-1}p,\n$$\n\navec $p = \\frac{1}{6}$ la probabilité d'obtenir un \"6\" lors d'un lancer. Il s'agit d'une distribution géométrique de paramètre $p$. \n\n2) Fonction de répartition\n\nSoit $k \\geq 1$. Nous avons :\n\n$$\n\\mathbb{P}(X \\leq k) = \\sum_{j=1}^{k} (1-p)^{j-1}p\n$$\n\n$$\n= p\\sum_{j=0}^{k-1} (1-p)^{j} = p\\frac{1-(1-p)^k}{1-(1-p)} = 1-(1-p)^k.\n$$\n\nA partir de la définition de la fonction de répartition, nous obtenons\n\n$$\nF_X(x) = \n\\begin{cases}\n0 & \\text{si } x < 1,\\\\[4pt]\np & \\text{si } 1 \\leq x < 2,\\\\[4pt]\n& \\cdots,\\\\[4pt]\n1-(1-p)^k & \\text{si } k \\leq x < k+1,\\\\[4pt]\n& \\cdots.\n\\end{cases}\n$$\n\nReprésentation graphique de la fonction de répartition :\n\n::: {#a197d8fe .cell execution_count=3}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef plot_cdf_geometrique(p=0.3, k_max=8):\n    \"\"\"\n    Trace la fonction de répartition\n        F_X(x) = 0                si x < 1\n               = 1 - (1-p)^k      si k <= x < k+1   pour k = 1,2,...\n    avec une troncature à k_max.\n    \"\"\"\n    if not (0 < p < 1):\n        raise ValueError(\"p doit être dans l'intervalle (0, 1).\")\n\n    plt.figure(figsize=(6, 4))\n\n    # Segment avant 1 : F(x) = 0 pour 0 <= x < 1\n    plt.hlines(0, 0, 1, linewidth=2)\n\n    # Segments horizontaux pour k = 1,...,k_max\n    for k in range(1, k_max + 1):\n        level = 1 - (1 - p)**k\n        plt.hlines(level, k, k + 1, linewidth=2)\n\n    # Points ouverts (limite à gauche, non incluse)\n    x_open, y_open = [], []\n    # Points fermés (valeur de la FDR)\n    x_closed, y_closed = [], []\n\n    for k in range(1, k_max + 1):\n        # Limite à gauche en k :\n        if k == 1:\n            left_limit = 0\n        else:\n            left_limit = 1 - (1 - p)**(k - 1)\n\n        value_at_k = 1 - (1 - p)**k\n\n        x_open.append(k)\n        y_open.append(left_limit)\n\n        x_closed.append(k)\n        y_closed.append(value_at_k)\n\n    # Cercles ouverts (non remplis) : limite à gauche\n    plt.scatter(\n        x_open, y_open,\n        facecolors=\"none\",\n        edgecolors=\"black\",\n        s=60,\n        zorder=3\n    )\n\n    # Cercles fermés (remplis) : valeur de F_X en k\n    plt.scatter(\n        x_closed, y_closed,\n        color=\"black\",\n        s=60,\n        zorder=3\n    )\n\n    # Mise en forme\n    plt.xlabel(\"x\")\n    plt.ylabel(\"F(x)\")\n    plt.title(f\"Fonction de répartition géométrique (p = {p})\")\n    plt.xlim(0, k_max + 1.5)\n    plt.ylim(-0.05, 1.05)\n    plt.grid(True, linestyle=\"--\", alpha=0.4)\n\n    plt.show()\n\n# Exemple d'appel\nplot_cdf_geometrique(p=0.25, k_max=8)\n```\n\n::: {.cell-output .cell-output-display}\n![](index_gdr_files/figure-html/cell-4-output-1.png){width=514 height=377}\n:::\n:::\n\n\n3) Fonction génératrice des moments\n\nEn utilisant la formule de transfert en temps discret, nous avons :\n$$\nM_X(t) = \\int e^{tX} d\\mathbb{P} = \\sum_{x=1}^{\\infty} e^{tx}\\mathbb{P}_X(x)\n$$\n\n$$\n= p\\sum_{j=1}^{\\infty} e^{tj}(1-p)^{j-1}\n$$\n\n$$\n= pe^t\\sum_{j=0}^{\\infty} e^{tj}(1-p)^j\n$$\n\n$$\n= pe^t\\sum_{j=0}^{\\infty} q^j \\text{ avec } q = e^t(1-p)\n$$\n\n$$\n= \\frac{pe^t}{1-q} = \\frac{pe^t}{1-e^t(1-p)}.\n$$\n\nLa série converge si et seulement si $|q| < 1$, c'est-à-dire si $|e^t(1-p)| < 1$, soit $e^t < \\frac{1}{1-p}$, ce qui donne $t < \\ln\\left(\\frac{1}{1-p}\\right) = -\\ln(1-p)$.\n\n4) Espérance, variance et coefficient de variation\n\nNous savons que la fonction $M_X$ est indéfiniment dérivable dans un voisinage de 0, et que $M'_X(0) = E(X)$ et $M''_X(0) = E(X^2)$. Nous avons après calcul\n\n$$\nM'_x(t) = \\frac{pe^t}{\\{1-e^t(1-p)\\}^2},\n$$\n\n$$\nM''_x(t) = \\frac{pe^t\\{1-(1-p)^2e^{2t}\\}}{\\{1-e^t(1-p)\\}^4},\n$$\n\ndont nous déduisons que\n\n$$\nM'_x(0) = \\frac{1}{p} \\equiv E(X),\n$$\n\n$$\nM''_x(0) = \\frac{p\\{1-(1-p)^2\\}}{\\{1-(1-p)\\}^4} = \\frac{2-p}{p^2} \\equiv E(X^2),\n$$\n\npuis\n\n$$\nV(X) = \\frac{1-p}{p^2},\n$$\n\n$$\nCV(X) = \\sqrt{1-p}.\n$$\n\nNumériquement, nous obtenons\n\n$$\nE(X) = 6, \\quad V(X) = 30, \\quad CV(X) = 0.91.\n$$\n\n\n## Exercice 3\n\nSoit $X$ une variable aléatoire admettant un moment d'ordre 2. Nous notons $m = E(X)$ et $\\sigma^2 = V(X)$. Soient $X_1, \\ldots, X_n$ des variables aléatoires indépendantes et de même loi que $X$. Nous notons\n\n$$\n\\overline{X}_n = \\frac{1}{n}\\sum_{i=1}^{n} X_i\n$$\n\nla moyenne empirique des $X_i$, $i = 1, \\ldots, n$, et\n\n$$\ns_X^2 = \\frac{1}{n-1}\\sum_{i=1}^{n} (X_i - \\overline{X}_n)^2\n$$\n\nla dispersion des $X_i$, $i = 1, \\ldots, n$.\n\n1) Montrer que $E(\\overline{X}_n) = m$.\n\n2) Montrer que $E(s_X^2) = \\sigma^2$. Vous pourrez utiliser l'identité\n\n$$\ns_X^2 = \\frac{1}{2n(n-1)} \\sum_{i=1}^{n} \\sum_{\\substack{j=1\\\\j \\neq i}}^{n} (X_i - X_j)^2.\n$$\n\n3) Utiliser ces résultats pour obtenir une approximation de Monte-Carlo de l'espérance, la variance et le coefficient de variation de la variable aléatoire $X$ de l'exercice 2.\n\n- Démonstration de l'équivalence des formules de variance\n\nNous voulons démontrer que :\n\n$$\\frac{1}{n-1}\\sum_{i=1}^{n}(X_i - \\bar{X}_n)^2 = \\frac{1}{2n(n-1)}\\sum_{i=1}^{n}\\sum_{\\substack{j=1\\\\j\\neq i}}^{n}(X_i - X_j)^2$$\n\n- Étape 1 : Développement de la somme double\n\nCommençons par développer le membre de droite :\n\n$$\\sum_{i=1}^{n}\\sum_{\\substack{j=1\\\\j\\neq i}}^{n}(X_i - X_j)^2$$\n\nDéveloppons $(X_i - X_j)^2$ :\n\n$$= \\sum_{i=1}^{n}\\sum_{\\substack{j=1\\\\j\\neq i}}^{n}(X_i^2 - 2X_iX_j + X_j^2)$$\n\n$$= \\sum_{i=1}^{n}\\sum_{\\substack{j=1\\\\j\\neq i}}^{n}X_i^2 - 2\\sum_{i=1}^{n}\\sum_{\\substack{j=1\\\\j\\neq i}}^{n}X_iX_j + \\sum_{i=1}^{n}\\sum_{\\substack{j=1\\\\j\\neq i}}^{n}X_j^2$$\n\n- Étape 2 : Simplification de chaque terme\n\nPremier terme :\n$$\\sum_{i=1}^{n}\\sum_{\\substack{j=1\\\\j\\neq i}}^{n}X_i^2 = \\sum_{i=1}^{n}X_i^2 \\cdot (n-1) = (n-1)\\sum_{i=1}^{n}X_i^2$$\n\nCar pour chaque $i$ fixé, $X_i^2$ est sommé $(n-1)$ fois (pour tous les $j \\neq i$).\n\nTroisième terme :\nPar symétrie, le troisième terme donne le même résultat :\n$$\\sum_{i=1}^{n}\\sum_{\\substack{j=1\\\\j\\neq i}}^{n}X_j^2 = (n-1)\\sum_{j=1}^{n}X_j^2 = (n-1)\\sum_{i=1}^{n}X_i^2$$\n\nDeuxième terme :\n$$\\sum_{i=1}^{n}\\sum_{\\substack{j=1\\\\j\\neq i}}^{n}X_iX_j$$\n\nPour chaque paire $(i,j)$ avec $i \\neq j$, le produit $X_iX_j$ apparaît dans cette double somme. On peut réécrire :\n\n$$= \\sum_{i=1}^{n}X_i\\sum_{\\substack{j=1\\\\j\\neq i}}^{n}X_j = \\sum_{i=1}^{n}X_i\\left(\\sum_{j=1}^{n}X_j - X_i\\right)$$\n\n$$= \\sum_{i=1}^{n}X_i \\cdot n\\bar{X}_n - \\sum_{i=1}^{n}X_i^2$$\n\n$$= n\\bar{X}_n \\cdot n\\bar{X}_n - \\sum_{i=1}^{n}X_i^2 = n^2\\bar{X}_n^2 - \\sum_{i=1}^{n}X_i^2$$\n\n- Étape 3 : Assemblage\n\nEn rassemblant les trois termes :\n\n$$\\sum_{i=1}^{n}\\sum_{\\substack{j=1\\\\j\\neq i}}^{n}(X_i - X_j)^2 = (n-1)\\sum_{i=1}^{n}X_i^2 - 2\\left(n^2\\bar{X}_n^2 - \\sum_{i=1}^{n}X_i^2\\right) + (n-1)\\sum_{i=1}^{n}X_i^2$$\n\n$$= (n-1)\\sum_{i=1}^{n}X_i^2 - 2n^2\\bar{X}_n^2 + 2\\sum_{i=1}^{n}X_i^2 + (n-1)\\sum_{i=1}^{n}X_i^2$$\n\n$$= 2(n-1)\\sum_{i=1}^{n}X_i^2 + 2\\sum_{i=1}^{n}X_i^2 - 2n^2\\bar{X}_n^2$$\n\n$$= 2n\\sum_{i=1}^{n}X_i^2 - 2n^2\\bar{X}_n^2$$\n\n$$= 2n\\left(\\sum_{i=1}^{n}X_i^2 - n\\bar{X}_n^2\\right)$$\n\n- Étape 4 : Lien avec la variance classique\n\nRappelons la formule classique de la variance :\n\n$$\\sum_{i=1}^{n}(X_i - \\bar{X}_n)^2 = \\sum_{i=1}^{n}(X_i^2 - 2X_i\\bar{X}_n + \\bar{X}_n^2)$$\n\n$$= \\sum_{i=1}^{n}X_i^2 - 2\\bar{X}_n\\sum_{i=1}^{n}X_i + n\\bar{X}_n^2$$\n\n$$= \\sum_{i=1}^{n}X_i^2 - 2\\bar{X}_n \\cdot n\\bar{X}_n + n\\bar{X}_n^2$$\n\n$$= \\sum_{i=1}^{n}X_i^2 - n\\bar{X}_n^2$$\n\n- Étape 5 : Conclusion\n\nD'après l'étape 3 :\n$$\n\\sum_{i=1}^{n}\\sum_{\\substack{j=1\\\\j\\neq i}}^{n}(X_i - X_j)^2 = 2n\\left(\\sum_{i=1}^{n}X_i^2 - n\\bar{X}_n^2\\right)\n$$\n\n\nD'après l'étape 4 :\n$$\n\\sum_{i=1}^{n}(X_i - \\bar{X}_n)^2 = \\sum_{i=1}^{n}X_i^2 - n\\bar{X}_n^2\n$$\n\nDonc :\n$$\n\\sum_{i=1}^{n}\\sum_{\\substack{j=1\\\\j\\neq i}}^{n}(X_i - X_j)^2 = 2n\\sum_{i=1}^{n}(X_i - \\bar{X}_n)^2\n$$\n\nEn divisant par $2n(n-1)$ :\n\n$$\n\\frac{1}{2n(n-1)}\\sum_{i=1}^{n}\\sum_{\\substack{j=1\\\\j\\neq i}}^{n}(X_i - X_j)^2 = \\frac{1}{n-1}\\sum_{i=1}^{n}(X_i - \\bar{X}_n)^2\n$$\n\n## Correction de l'exercice 3\n\n1) Calcul de $E(\\overline{X}_n)$\n\nPar linéarité de l'espérance, nous avons :\n\n$$\nE(\\overline{X}_n) = E\\left(\\frac{1}{n}\\sum_{i=1}^{n} X_i\\right) = \\frac{1}{n}\\sum_{i=1}^{n} E(X_i) = \\frac{1}{n} \\cdot n \\cdot m = m.\n$$\n\n\n\n2) Calcul de $E(s_X^2)$\n\nEn utilisant l'identité donnée, nous avons en utilisant la linéarité de l'espérance :\n\n$$\nE(s_X^2) = E\\left(\\frac{1}{2n(n-1)} \\sum_{i=1}^{n} \\sum_{\\substack{j=1\\\\j \\neq i}}^{n} (X_i - X_j)^2\\right)\n$$\n\n$$\n= \\frac{1}{2n(n-1)} \\sum_{i=1}^{n} \\sum_{\\substack{j=1\\\\j \\neq i}}^{n} E\\left((X_i - X_j)^2\\right).\n$$\n\nIci on peut utiliser deux méthodes de calcul qui sont équivalentes.\n\nLa première consiste à remarquer que pour $i \\neq j$ :\n\n$$\nE\\left((X_i - X_j)^2\\right) = E\\left(X_i^2\\right) - 2E(X_i X_j) + E\\left(X_j^2\\right)\n$$\n\nComme les $X_i$ sont indépendantes et de même loi, nous avons :\n\n$$\nE\\left(X_i^2\\right) = E\\left(X_1^2\\right) \\quad \\text{et} \\quad E(X_i X_j) = E(X_1)E(X_2) \\quad \\text{pour } i \\neq j.\n$$\n\nDe ce fait, nous obtenons :\n\n$$\nE\\left((X_i - X_j)^2\\right) = E\\left(X_1^2\\right) - 2E(X_1)E(X_2) + E\\left(X_2^2\\right)\n$$\n\n$$\n= E\\left(X_1^2\\right) - 2E(X_1X_2) + E\\left(X_2^2\\right)\n$$\n\nCar $X_1$ et $X_2$ sont indépendantes et de même loi.\n\nDonc finalement :\n\n$$\nE\\left((X_i - X_j)^2\\right) = E\\left((X_1 - X_2)^2\\right).\n$$\n\nUne autre méthode consiste à utiliser la formule de Koenig-Huygens pour la variance. Nous avons :\n\n$$\nE\\left((X_i - X_j)^2\\right) = E\\left(X_1^2\\right) - 2E(X_1)E(X_2) + E\\left(X_2^2\\right)\n$$\n\n$$\n= 2\\left(E\\left(X_1^2\\right) - E(X_1)^2\\right) = 2V(X) = 2\\sigma^2.\n$$\n\nLorsque nous utilisons la première méthode, nous devons calculer $E\\left((X_1 - X_2)^2\\right)$ en développant :\n\n$$\nE\\left((X_1 - X_2)^2\\right) = E[{(X_1 - m) - (X_2 - m)}^2]\n$$\n\n$$\n= E\\left((X_1 - m)^2\\right) + E\\left((X_2 - m)^2\\right) - 2E\\left((X_1 - m)(X_2 - m)\\right)\n$$\n\n$$\n= E\\left((X_1 - m)^2\\right) + E\\left((X_2 - m)^2\\right) - 2E(X_1 - m)\\times E(X_2 - m)\n$$\nCar $X_1$ et $X_2$ sont indépendantes.\n\n$$\n= \\sigma^2 + \\sigma^2 - 2 \\cdot 0 \\cdot 0 = 2\\sigma^2.\n$$\n\nDonc, nous obtenons finalement :\n$$\nE(s_X^2) = \\frac{1}{2} \\cdot 2\\sigma^2 = \\sigma^2.\n$$\n\n3) Approximation de Monte-Carlo\n\nCe que nous avons montré aux points précédents, est que nous pouvons estimer l'espérance et la variance de la variable aléatoire $X$ en utilisant les statistiques d'échantillon $\\overline{X}_n$ et $s_X^2$. C'est-à-dire en générant un grand nombre de réalisations indépendantes de $X$ et en calculant la moyenne empirique et la dispersion de ces réalisations.\n\nVoici un exemple de code R pour effectuer cette approximation de Monte-Carlo pour la variable aléatoire $X$ de l'exercice 2 :\n\n\n```default\n# Monte Carlo approximation in R\n\n# Répétition de l'expérience aléatoire\n# Simulation du nombre de lancers nécessaires pour obtenir un 6\n\nset.seed(360)\nn <- 50000\nctr <- 0\nsimlist <- numeric(n)\n\nwhile (ctr < n) {\n  ctr <- ctr + 1\n  \n  # Valeur du lancer (initialisation)\n  trial <- 0\n  \n  # Nombre de tentatives (initialisation)\n  nb_tent <- 0\n  \n  while (trial != 6) {\n    nb_tent <- nb_tent + 1\n    trial <- sample(1:6, 1)\n  }\n  \n  simlist[ctr] <- nb_tent\n}\n\n# Affichage des résultats\nmean(simlist)\nvar(simlist)\n\n# Statistiques supplémentaires\ncat(\"\\n=== Résultats de la simulation ===\\n\")\ncat(\"Nombre de simulations:\", n, \"\\n\")\ncat(\"Moyenne du nombre de lancers:\", mean(simlist), \"\\n\")\ncat(\"Variance:\", var(simlist), \"\\n\")\ncat(\"Écart-type:\", sd(simlist), \"\\n\")\ncat(\"Minimum:\", min(simlist), \"\\n\")\ncat(\"Maximum:\", max(simlist), \"\\n\")\ncat(\"Médiane:\", median(simlist), \"\\n\")\n\n# Histogramme\nhist(simlist, \n     breaks = 30, \n     col = \"lightblue\", \n     border = \"white\",\n     main = \"Distribution du nombre de lancers pour obtenir un 6\",\n     xlab = \"Nombre de lancers\",\n     ylab = \"Fréquence\",\n     xlim = c(0, max(simlist)))\n\n# Ajout de la moyenne théorique\nabline(v = 6, col = \"red\", lwd = 2, lty = 2)\nlegend(\"topright\", \n       legend = c(\"Moyenne observée\", \"Moyenne théorique = 6\"),\n       col = c(\"blue\", \"red\"), \n       lty = c(1, 2), \n       lwd = 2)\n```\n\n::: {#c2f02b28 .cell execution_count=4}\n``` {.python .cell-code}\nimport random\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Répétition de l'expérience\nrandom.seed(360)\nn = 50000\nsimlist = []\n\nfor _ in range(n):\n    trial = 0        # valeur du lancer\n    nb_tent = 0      # nombre de tentatives\n    \n    while trial != 6:\n        nb_tent += 1\n        trial = random.randint(1, 6)  # équivalent de sample(1:6,1)\n    \n    simlist.append(nb_tent)\n\nsimlist = np.array(simlist)\n\n# Résultats\nprint(\"\\n=== Résultats de la simulation ===\")\nprint(\"Nombre de simulations:\", n)\nprint(\"Moyenne du nombre de lancers:\", simlist.mean())\nprint(\"Variance:\", simlist.var(ddof=1))\nprint(\"Écart-type:\", simlist.std(ddof=1))\nprint(\"Minimum:\", simlist.min())\nprint(\"Maximum:\", simlist.max())\nprint(\"Médiane:\", np.median(simlist))\n\n# Calcul des moyennes cumulées\nmean_values = np.cumsum(simlist) / np.arange(1, n + 1)\n\n# Tracé\nplt.figure(figsize=(7, 4))\nplt.plot(mean_values, label=\"Moyenne empirique\")\nplt.axhline(6, color=\"red\", linestyle=\"--\", label=\"Espérance théorique = 6\")\n\nplt.xlabel(\"n (nombre d'expériences)\")\nplt.ylabel(\"Moyenne empirique\")\nplt.title(\"Convergence de la moyenne empirique vers l'espérance théorique\")\nplt.legend()\nplt.grid(alpha=0.4)\nplt.show() \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n=== Résultats de la simulation ===\nNombre de simulations: 50000\nMoyenne du nombre de lancers: 6.00114\nVariance: 30.1522217448349\nÉcart-type: 5.4911038730691395\nMinimum: 1\nMaximum: 65\nMédiane: 4.0\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_gdr_files/figure-html/cell-5-output-2.png){width=583 height=378}\n:::\n:::\n\n\n```default\n# Utilisation de la distribution géométrique\n# Alternative plus efficace à la simulation par boucle\n\nset.seed(360)\nn <- 50000\np <- 1/6\nctr <- 0\nsimlist <- numeric(n)\n\nwhile (ctr < n) {\n  ctr <- ctr + 1\n  \n  # Génération d'une réalisation d'une loi géométrique\n  # Attention, rgeom(n,p) donne le nombre d'échecs avant\n  # le premier succès\n  simlist[ctr] <- rgeom(1, p) + 1\n}\n\n# Affichage des résultats\nmean(simlist)\nvar(simlist)\n\n# Statistiques détaillées\ncat(\"\\n=== Résultats de la simulation (méthode géométrique) ===\\n\")\ncat(\"Nombre de simulations:\", n, \"\\n\")\ncat(\"Probabilité de succès (p):\", p, \"\\n\")\ncat(\"Moyenne observée:\", mean(simlist), \"\\n\")\ncat(\"Moyenne théorique:\", 1/p, \"\\n\")\ncat(\"Variance observée:\", var(simlist), \"\\n\")\ncat(\"Variance théorique:\", (1-p)/p^2, \"\\n\")\ncat(\"Écart-type observé:\", sd(simlist), \"\\n\")\ncat(\"Médiane:\", median(simlist), \"\\n\")\n\n# Comparaison graphique\npar(mfrow = c(1, 2))\n\n# Histogramme\nhist(simlist, \n     breaks = 50, \n     col = \"lightgreen\", \n     border = \"white\",\n     main = \"Distribution du nombre de lancers\\n(méthode rgeom)\",\n     xlab = \"Nombre de lancers\",\n     ylab = \"Fréquence\",\n     probability = TRUE)\n\n# Ajout de la moyenne\nabline(v = mean(simlist), col = \"blue\", lwd = 2)\nabline(v = 1/p, col = \"red\", lwd = 2, lty = 2)\nlegend(\"topright\", \n       legend = c(\"Moyenne obs.\", \"Moyenne théo.\"),\n       col = c(\"blue\", \"red\"), \n       lty = c(1, 2), \n       lwd = 2,\n       cex = 0.8)\n\n# QQ-plot pour vérifier la distribution\nqqplot(qgeom(ppoints(n), p) + 1, simlist,\n       main = \"QQ-Plot : Théorique vs Observé\",\n       xlab = \"Quantiles théoriques (Géométrique)\",\n       ylab = \"Quantiles observés\",\n       col = \"darkgreen\",\n       pch = 20,\n       cex = 0.5)\nabline(0, 1, col = \"red\", lwd = 2)\n\npar(mfrow = c(1, 1))\n\n# Méthode encore plus efficace (vectorisée)\ncat(\"\\n=== Méthode vectorisée (plus rapide) ===\\n\")\nset.seed(360)\nsimlist_vec <- rgeom(n, p) + 1\ncat(\"Moyenne:\", mean(simlist_vec), \"\\n\")\ncat(\"Variance:\", var(simlist_vec), \"\\n\")\n```\n\n::: {#b2ea409e .cell execution_count=5}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport scipy.stats as st\n\n# Paramètres\nnp.random.seed(360)\nn = 50000\np = 1/6\n\n# La loi géométrique de R donne \"nombre d'échecs avant succès\"\n# => équivalent python : st.geom(p).rvs() renvoie déjà nb_tentatives\n# MAIS Python compte à partir de 1, donc c'est ce qu’on veut !\nsimlist = st.geom(p).rvs(size=n)\n\n# Affichage des résultats\nprint(\"\\n=== Résultats de la simulation (méthode géométrique) ===\")\nprint(\"Nombre de simulations:\", n)\nprint(\"Probabilité de succès p:\", p)\nprint(\"Moyenne observée:\", simlist.mean())\nprint(\"Moyenne théorique:\", 1/p)\nprint(\"Variance observée:\", simlist.var(ddof=1))\nprint(\"Variance théorique:\", (1 - p) / p**2)\nprint(\"Écart-type observé:\", simlist.std(ddof=1))\nprint(\"Médiane:\", np.median(simlist))\n\n\n# Moyennes cumulées\nmean_values = np.cumsum(simlist) / np.arange(1, n + 1)\n\n# Tracé\nplt.figure(figsize=(7, 4))\nplt.plot(mean_values, label=\"Moyenne empirique\")\nplt.axhline(1/p, color=\"red\", linestyle=\"--\", label=f\"Espérance théorique = {1/p:.0f}\")\n\nplt.xlabel(\"n (nombre d'expériences)\")\nplt.ylabel(\"Moyenne empirique\")\nplt.title(\"Convergence de la moyenne empirique de la loi géométrique\")\nplt.legend()\nplt.grid(alpha=0.4)\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n=== Résultats de la simulation (méthode géométrique) ===\nNombre de simulations: 50000\nProbabilité de succès p: 0.16666666666666666\nMoyenne observée: 6.04552\nMoyenne théorique: 6.0\nVariance observée: 29.974807425748512\nVariance théorique: 30.000000000000004\nÉcart-type observé: 5.474925335175678\nMédiane: 4.0\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_gdr_files/figure-html/cell-6-output-2.png){width=585 height=378}\n:::\n:::\n\n\n## Exercice 4\n\nSoient X et Y deux variables aléatoires de carré $\\mathbb{P}$-intégrable.\n\n1. Soit $a \\in \\mathbb{R}$.  \n   Écrire  \n   $$\n   E\\big((|X| - a|Y|)^2\\big)\n   $$\n   sous la forme d’un polynôme en a, et calculer son discriminant $\\Delta$.\n\n2. Expliquer pourquoi $\\Delta \\le 0$, et en déduire l’inégalité de Hölder :\n   $$\n   E(|XY|) \\le \\sqrt{E(X^2)} \\, \\sqrt{E(Y^2)}.\n   $$\n\n3. En appliquant l’inégalité précédente à des variables bien choisies, en déduire que :\n   $$\n   |\\mathrm{Cov}(X,Y)| \\le \\sqrt{V(X)} \\, \\sqrt{V(Y)}.\n   $$\n\nL’inégalité de Hölder se généralise sous la forme suivante (admise dans la suite).  \nSoient deux nombres réels $p, q \\ge 1$ conjugués, c’est-à-dire tels que :\n$$\n\\frac{1}{p} + \\frac{1}{q} = 1.\n$$\n\nSoient X et Y deux variables aléatoires telles que  \n$\\int |X|^p \\, d\\mathbb{P} < \\infty$ et $\\int |Y|^q \\, d\\mathbb{P} < \\infty$.  \nAlors :\n$$\nE(|XY|) \\le \\big(E(|X|^p)\\big)^{1/p} \\, \\big(E(|Y|^q)\\big)^{1/q}.\n$$\n\nSoient maintenant deux réels $r$ et $s$ tels que $1 < r < s < \\infty$.  \nSoit $Z$ une variable aléatoire telle que $\\int |Z|^s \\, d\\mathbb{P} < \\infty$.\n\n4. Vérifier que les réels $\\frac{s}{r}$ et $\\frac{s}{s-r}$ sont conjugués.  \n   En appliquant l’inégalité de Hölder avec des variables aléatoires $X$ et $Y$ bien choisies, montrer que :\n   $$\n   E(|Z|^r) \\le \\big(E(|Z|^s)\\big)^{r/s}.\n   $$\n\n5. En déduire que si une variable aléatoire $Z$ admet un moment d’ordre $s > 1$, alors elle admet un moment d’ordre $r$ pour tous les réels $1 < r < s$.\n\n## Correction de l'exercice 4\n\n1) Calcul de $E\\big((|X| - a|Y|)^2\\big)$\n\nNous avons en utilisant la linéarité de l'espérance :\n\n$$\nE\\big((|X| - a|Y|)^2\\big) = E(|X|^2) - 2aE(|X||Y|) + a^2E(|Y|^2).\n$$\n\nest sa forme de polynôme en $a$.\n\nSon discriminant est donc donné par :\n\n$$\n\\Delta = (-2E(|X||Y|))^2 - 4E(|X|^2)E(|Y|^2) = 4\\big(E(|X||Y|)^2 - E(|X|^2)E(|Y|^2)\\big).\n$$\n\nComme $E\\big((|X| - a|Y|)^2\\big) \\geq 0$ pour tout $a \\in \\mathbb{R}$, le polynôme ne peut pas avoir deux racines réelles distinctes, donc $\\Delta \\leq 0$.\n\n3) En appliquant l’inégalité précédente en remplaçant $X$ par $X - E(X)$, et\n$Y$ par $Y - E(Y)$, nous obtenons :\n\n$$\nE \\big( \\{ X - E(X) \\} \\{ Y - E(Y) \\} \\big)\n\\le \\sqrt{E \\big( \\{ X - E(X) \\}^2 \\big)} \\, \\sqrt{E \\big( \\{ X - E(X) \\}^2 \\big)}.\n$$\n\nNous avons ensuite\n\n$$\n|\\mathrm{Cov}(X,Y)|\n  = \\big| E \\big( \\{ X - E(X) \\} \\{ Y - E(Y) \\} \\big) \\big|\n  \\le E \\big( \\{ X - E(X) \\} \\{ Y - E(Y) \\} \\big),\n$$\n\nce qui donne le résultat voulu.\n\n---\n\n4) Nous avons bien\n\n$$\n\\frac{r}{s} + \\frac{s - r}{s} = 1.\n$$\n\nEn appliquant l’inégalité de Hölder générale aux variables aléatoires $X = Z^r$\net $Y = 1$, et avec les nombres conjugués $\\frac{s}{r}$ et $\\frac{s}{s-r}$,\nnous obtenons\n\n$$\nE \\big( |Z^r \\times 1| \\big)\n\\le \\Big( E \\big( |Z^r|^{\\frac{s}{r}} \\big) \\Big)^{\\frac{r}{s}}\n     \\Big( E \\big( |1|^{\\frac{s}{s-r}} \\big) \\Big)^{\\frac{s-r}{s}}\n     = \\Big( E(|Z|^s) \\Big)^{\\frac{r}{s}}.\n$$\n\n$$\n\\Rightarrow \\quad\nE(|Z|^r) \\le \\big( E(|Z|^s) \\big)^{\\frac{r}{s}}.\n$$\n\n\n\n5) Cela découle de l’inégalité précédente.\n\n# 4. Variables aléatoires discrètes\n\n## Exercice 1\n\nDans chacune des situations, identifier si $X$ suit une loi binomiale. Si oui, donner les paramètres $n$ et $p$ correspondants. Sinon, expliquer pourquoi et proposer une modélisation alternative :\n\n- Chaque jour, Dean va déjeuner et il y a 25% de chances qu’il prenne une pizza. Soit $X$ le nombre de fois où il a pris une pizza la semaine dernière.\n\n- Jessica joue au basketball, et elle a 60% de chances de réussir un lancer franc. Soit $X$ le nombre de lancers francs réussis pendant le dernier match.\n\n- Une boîte contient 300 bonbons, dont 100 carambars et 200 chocolats. Sam prend un échantillon de 10 bonbons dans la boîte. Soit $X$ le nombre de carambars dans son échantillon.\n\n- Marie lit un livre de 600 pages. Sur les pages paires, il y a 1% de chances d’avoir une faute d’orthographe. Sur les pages impaires, il y a 2% de chances d’avoir une faute d’orthographe. Soit $X$ le nombre total de fautes d’orthographe dans le livre.\n\n- John lit un livre de 600 pages. Sur chaque page, le nombre de fautes d’orthographe est distribué selon une loi de Bernouilli de paramètre $0.01$. Soit $X$ le nombre total de fautes d’orthographe dans le livre.\n\n\n## Correction Exercice 1\n\nDans chaque situation, on indique si $X$ suit une loi binomiale. Si oui, on précise les paramètres $n$ et $p$. Sinon, on propose une modélisation alternative.\n\n---\n\n1. **Dean et les pizzas**\n\n- Chaque jour, Dean prend une pizza avec probabilité $p = 0{,}25$, pendant une semaine ($n = 7$ jours). \n- Ensuite, il faut se poser la question : la décision de prendre une pizza un jour est-elle indépendante des autres jours ? Si oui, alors :\n- Le nombre de jours $X$ où il prend une pizza suit donc une loi binomiale :\n\n$$X \\sim \\mathcal{B}(n = 7,\\; p = 0{,}25).$$\n\n---\n\n2. **Jessica et les lancers francs**\n\n- Jessica réussit un lancer franc avec probabilité $p = 0{,}6$, mais le nombre de lancers tentés pendant le match **n’est pas fixé à l’avance**.\n- Il ne s’agit donc pas d’une loi binomiale (condition nécessaire : nombre d’essais fixé). Dans une modélisation binomiale, le nombre d’essais $n$ doit être fixé et connu à l’avance. \n\nUne modélisation possible est :\n\n- On note $N$ le nombre de lancers francs tentés pendant le dernier match. Il y a plusieurs possibilités : Soit $N$ est une constante connue, soit $N$ est une variable aléatoire (par exemple, on peut modéliser $N$ par une loi de Poisson.). Ensuite, on considère le nombre de lancers réussis $X$ conditionnellement à $N$.\n\n- Conditionnellement à $N$, le nombre de lancers réussis suit une loi binomiale :\n\n$$X \\mid N \\sim \\mathcal{B}(N,\\; 0{,}6).$$\n\nsi les lancers sont indépendants.\n\n---\n\n3. **Boîte de bonbons (carambars/chocolats)**\n\n- La boîte contient $300$ bonbons dont $100$ carambars et $200$ chocolats.\n- Sam tire un échantillon de $n = 10$ bonbons **sans remise**.\n- Les tirages ne sont pas indépendants (sans remise), donc $X$ **ne suit pas** une loi binomiale.\n\nLa loi adaptée est l’hypergéométrique :\n\n- Taille de la population : $N = 300$.\n- Nombre de “succès” (carambars) : $K = 100$.\n- Taille de l’échantillon : $n = 10$.\n\nOn a alors :\n\n$$X \\sim \\mathcal{H}(N = 300,\\; K = 100,\\; n = 10).$$\n\n---\n\n4. **Marie et les fautes d’orthographe (1 % / 2 %)**\n\n- Le livre a $600$ pages.\n- Sur les pages paires (300 pages), probabilité de faute $p_1 = 0{,}01$.\n- Sur les pages impaires (300 pages), probabilité de faute $p_2 = 0{,}02$.\n- La probabilité de “succès” (faute) **n’est pas la même** sur tous les essais (pages), donc $X$ **ne suit pas** une loi binomiale.\n\nUne modélisation naturelle est de décomposer :\n\n- On suppose l'indépendance entre les pages.\n- $X_{\\text{paires}}$ : nombre de fautes sur les pages paires,\n- $X_{\\text{impaires}}$ : nombre de fautes sur les pages impaires.\n\nOn a :\n\n$$\nX_{\\text{paires}} \\sim \\mathcal{B}(300,\\; 0{,}01), \\qquad\nX_{\\text{impaires}} \\sim \\mathcal{B}(300,\\; 0{,}02),\n$$\n\net\n\n$$\nX = X_{\\text{paires}} + X_{\\text{impaires}}.\n$$\n\n---\n\n5. **John et les fautes d’orthographe (Bernoulli 0.01)**\n\n- Le livre a $600$ pages.\n- Sur chaque page, le nombre de fautes suit une loi de Bernoulli de paramètre $p = 0{,}01$ (on suppose indépendance entre pages).\n- Cette fois, la probabilité de faute est la même pour toutes les pages, et le nombre total de pages $n = 600$ est fixé.\n\nAinsi, $X$ suit une loi binomiale :\n\n$$X \\sim \\mathcal{B}(n = 600,\\; p = 0{,}01).$$\n\n\n## Exercice 2\n\nSoit $X_1, \\dots, X_n$ une suite de variables aléatoires indépendantes telles que, pour tout $k = 1, \\dots, n$,\n\n$$\\mathbb{P}(X = \\pm 1) = \\frac{1}{2}.$$\n\nSoit\n$$S = \\sum_{i=1}^n X_i.$$\nOn parle de **marche aléatoire symétrique**, où le point de départ est $0$ avec un déplacement aléatoire à gauche ou à droite à chaque temps.\n\n1. Calculer $E(S)$ et $V(S)$.\n\nNous supposons maintenant que\n\n$$\\mathbb{P}(X = 1) = p \\quad \\text{et} \\quad \\mathbb{P}(X = -1) = 1 - p.$$\n\nSi $p > \\dfrac{1}{2}$, on parle de **marche aléatoire à dérive positive**.\n\n2. Montrer qu’on peut réécrire $X = aY + b$, avec $Y \\sim B(p)$, et $a$ et $b$ deux constantes à déterminer.  \n3. En déduire $E(S)$ et $V(S)$.\n\n\n## Correction — Exercice 2\n\nOn considère une suite de variables aléatoires indépendantes $X_1, \\dots, X_n$ telles que\n$$\\mathbb{P}(X = \\pm 1) = \\frac{1}{2}.$$\n\nOn pose :\n$$S = \\sum_{i=1}^n X_i.$$\n\n---\n\n#### 1) Calculer $E(S)$ et $V(S)$ dans le cas symétrique\n\nPour chaque $X_i$ :\n\n- $\\mathbb{E}(X_i) = 1 \\cdot \\frac{1}{2} + (-1) \\cdot \\frac{1}{2} = 0$,\n\n- $\\mathbb{E}(X_i^2) = 1^2 = 1$ d’où $\\operatorname{Var}(X_i) = \\mathbb{E}(X_i^2) - \\mathbb{E}(X_i)^2 = 1$.\n\n\nPar linéarité de l’espérance :\n$$\\mathbb{E}(S) = \\sum_{i=1}^n \\mathbb{E}(X_i) = 0,$$\n\nComme les $X_i$ sont indépendantes :\n$$\\operatorname{Var}(S) = \\sum_{i=1}^n \\operatorname{Var}(X_i) = n.$$\n\n---\n\nNous supposons maintenant que\n$$\\mathbb{P}(X = 1) = p \\quad \\text{et} \\quad \\mathbb{P}(X = -1) = 1 - p,$$\navec $p > \\frac{1}{2}$ (marche aléatoire à dérive positive).\n\n---\n\n#### 2) Montrer que l’on peut réécrire $X = aY + b$, avec $Y \\sim B(p)$\n\nIl faut jouer ici avec le support des variables aléatoires. L'idée est de partir d'une variable de Bernoulli $Y$ qui prend les valeurs $0$ et $1$, et de la transformer linéairement pour obtenir une variable $X$ qui prend les valeurs $-1$ et $1$. Comme $P(X = 1) = p$ et $P(X = -1) = 1 - p$, on peut définir $Y$ comme une variable de Bernoulli telle que $P(Y = 1) = p$ et $P(Y = 0) = 1 - p$.\n\nOn cherche $a$ et $b$ tels que :\n- si $Y = 1$, alors $X = 1$,\n- si $Y = 0$, alors $X = -1$.\n\nOn résout :\n$$\n\\begin{cases}\na \\cdot 1 + b = 1,\\\\\na \\cdot 0 + b = -1.\n\\end{cases}\n$$\n\nD'où $b = -1$ et $a = 2$.\n\nAinsi :\n$$X = 2Y - 1 \\quad \\text{avec} \\quad Y \\sim \\text{Bernoulli}(p).$$\n\n---\n\n####  3) En déduire $E(S)$ et $V(S)$ dans le cas général\n\nComme $S = \\sum_{i=1}^n X_i$ et $X_i = 2Y_i - 1$ avec $Y_i \\sim B(p)$ indépendantes :\n\nPar linéarité de l’espérance :\n\n$$\n\\mathbb{E}(X_i) = \\mathbb{E}{(2Y_i - 1)} =\n2\\,\\mathbb{E}(Y_i) - 1 = 2p - 1.\n$$\n\nCalculons la variance :\n\nOn sait que pour toute variable aléatoire $X = aY + b$, on a $\\operatorname{Var}(X) = a^2\\,\\operatorname{Var}(Y)$. Donc :\n$$\n\\operatorname{Var}(X_i) = \\operatorname{Var}(2Y_i - 1) = 2^2\\,\\operatorname{Var}(Y_i) =\n   4\\,\\operatorname{Var}(Y_i) = 4\\,p(1 - p).\n$$\nDonc :\n\n$$\n\\mathbb{E}(S) = \\sum_{i=1}^n \\mathbb{E}(X_i) = n(2p - 1),\n$$\n\n$$\n\\operatorname{Var}(S) = \\sum_{i=1}^n \\operatorname{Var}(X_i) = 4n\\,p(1 - p).\n$$\n\n---\n\n####  Résumé final\n\n| Type de marche | $\\mathbb{E}(S)$ | $\\operatorname{Var}(S)$ |\n|---------------|----------------|-------------------------|\n| Symétrique ($p = \\tfrac12$) | $0$ | $n$ |\n| Dérive positive ($p > \\tfrac12$) | $n(2p - 1)$ | $4n\\,p(1 - p)$ |\n\n## Exercice 3\n\nIdentifier les lois des variables aléatoires suivantes, en se basant sur leur fonction génératrice des moments :\n\n1. $M_X(t) = 0.8 e^t + 0.2$\n\n2. $M_Y(t) = \\dfrac{0.1 e^t}{1 - 0.9 e^t}$\n\n3. $M_Z(t) = (0.3 e^t + 0.7)^{14}$\n\n## Correction — Exercice 3\n### Rappel : fonctions génératrices des moments\n\n1. **Loi de Bernoulli de paramètre $p$**\n\nOn a\n$$\n\\mathbb{P}(X = 1) = p, \\qquad \\mathbb{P}(X = 0) = 1 - p.\n$$\n\nAlors la fonction génératrice des moments est :\n$$\nM_X(t) = \\mathbb{E}(e^{tX})\n= (1-p) e^{t \\cdot 0} + p e^{t \\cdot 1}\n= (1 - p) + p e^t.\n$$\n\n2. **Loi géométrique de paramètre $p$ sur $\\{1,2,\\dots\\}$**\n\nOn prend la convention\n$$\n\\mathbb{P}(Y = k) = (1 - p)^{k-1} p, \\quad k \\ge 1.\n$$\n\nAlors\n$$\nM_Y(t) = \\mathbb{E}(e^{tY})\n= \\sum_{k=1}^{\\infty} e^{tk} (1-p)^{k-1} p\n= p e^t \\sum_{k=0}^{\\infty} \\big((1-p)e^t\\big)^k.\n$$\n\nPour $| (1-p)e^t | < 1$, c’est une série géométrique :\n$$\n\\sum_{k=0}^{\\infty} r^k = \\frac{1}{1-r}.\n$$\n\nDonc\n$$\nM_Y(t) = \\frac{p e^t}{1 - (1-p) e^t}.\n$$\n\n3. **Loi binomiale $\\mathcal{B}(n,p)$**\n\nSoit $Z \\sim \\mathcal{B}(n,p)$. On peut écrire\n$$\nZ = X_1 + \\cdots + X_n,\n$$\noù les $X_i$ sont indépendantes et suivent toutes une Bernoulli$(p)$.\n\nOn sait que, pour une Bernoulli$(p)$,\n$$\nM_{X_i}(t) = (1-p) + p e^t.\n$$\n\nPar indépendance,\n$$\nM_Z(t) = \\mathbb{E}(e^{tZ})\n= \\mathbb{E}\\big(e^{t(X_1 + \\cdots + X_n)}\\big)\n= \\prod_{i=1}^n \\mathbb{E}(e^{tX_i})\n= \\big((1-p) + p e^t\\big)^n.\n$$\n\n---\n\n### Identification des lois\n\n1. On a\n$$\nM_X(t) = 0.8 e^t + 0.2 = 0.2 + 0.8 e^t.\n$$\n\nEn comparant avec $(1-p) + p e^t$, on obtient $p = 0.8$.\n\n> Donc $X$ suit une loi de Bernoulli de paramètre $0.8$ :\n> $$\n> X \\sim \\text{Bernoulli}(0.8).\n> $$\n\n2. On a\n$$\nM_Y(t) = \\dfrac{0.1 e^t}{1 - 0.9 e^t}.\n$$\n\nEn comparant avec\n$$\nM_Y(t) = \\frac{p e^t}{1 - (1-p) e^t},\n$$\non lit $p = 0.1$ et $1-p = 0.9$.\n\n> Donc $Y$ suit une loi géométrique de paramètre $p = 0.1$ (sur $\\{1,2,\\dots\\}$) :\n> $$\n> Y \\sim \\text{Géométrique}(p = 0.1).\n> $$\n\n3. On a\n$$\nM_Z(t) = (0.3 e^t + 0.7)^{14}\n= \\big( (1 - 0.3) + 0.3 e^t \\big)^{14}.\n$$\n\nEn comparant avec\n$$\nM_Z(t) = \\big( (1-p) + p e^t \\big)^n,\n$$\non obtient $p = 0.3$ et $n = 14$.\n\n> Donc $Z$ suit une loi binomiale $\\mathcal{B}(14, 0.3)$ :\n> $$\n> Z \\sim \\mathcal{B}(n = 14, p = 0.3).\n> $$\n\n## Exercice 4\n\nSoit $X$ une variable aléatoire telle que\n$$\\mathbb{P}(X = k) = \\frac{k}{10} \\quad \\text{pour } k = 1, 2, 3, 4.$$\n\nSoit $Y$ une variable aléatoire indépendante de $X$, et suivant la même distribution.\n\nCalculer la loi de probabilité de $X + Y$.\n\n## Correction — Exercice 4\n\n\nOn a\n$$\n\\mathbb{P}(X = k) = \\frac{k}{10}, \\quad k = 1,2,3,4,\n$$\net $Y$ est indépendante de $X$ et de même loi.\n\n\n|       | Y = 1 | Y = 2 | Y = 3 | Y = 4 |\n| ----- | ----- | ----- | ----- | ----- |\n| X = 1 | 2     | 3     | 4     | 5     |\n| X = 2 | 3     | 4     | 5     | 6     |\n| X = 3 | 4     | 5     | 6     | 7     |\n| X = 4 | 5     | 6     | 7     | 8     |\n\n\nLa variable $S = X + Y$ prend des valeurs entières de $2$ à $8$.\n\n\nLa variable aléatoire $X + Y$ prend ses valeurs dans $\\{2, \\dots, 8\\}$.  \n\nPour $k \\in \\{2, \\dots, 8\\}$, nous avons en utilisant la formule des probabilités totales :\n\n$$\n\\mathbb{P}(X + Y = k)\n= \\sum_{l=1}^{4} \\mathbb{P}(X + Y = k \\mid Y = l)\\,\\mathbb{P}(Y = l)\n$$\n\n$$\n= \\sum_{l=1}^{4} \\mathbb{P}(Y = k - l \\mid Y = l)\\,\\mathbb{P}(Y = l)\n$$\n\n$$\n= \\sum_{l=1}^{4} \\mathbb{P}(X = k - l)\\,\\mathbb{P}(Y = l) \\quad \\text{car $X$ et $Y$ sont indépendantes.}\n$$\n\noù la somme porte sur les $k$ tels que $1 \\le k \\le 4$ et $1 \\le k- l \\le 4$.\n\nOn note\n$$\np_1 = 0.1,\\quad p_2 = 0.2,\\quad p_3 = 0.3,\\quad p_4 = 0.4.\n$$\n\nOn calcule alors, cas par cas :\n\n- Pour $k = 2$  \n  $$(X,Y) = (1,1) \\quad\\Rightarrow\\quad \\mathbb{P}(S=2) = p_1 p_1 = 0.1\\times 0.1 = 0.01.$$\n\n- Pour $k = 3$  \n  $$(X,Y) = (1,2),(2,1)$$\n  $$\n  \\mathbb{P}(S=3) = p_1 p_2 + p_2 p_1 = 0.1\\times 0.2 + 0.2\\times 0.1 = 0.04.\n  $$\n\n- Pour $s = 4$  \n  $$(X,Y) = (1,3),(2,2),(3,1)$$\n  $$\n  \\mathbb{P}(S=4) = p_1 p_3 + p_2 p_2 + p_3 p_1\n  = 0.1\\times 0.3 + 0.2\\times 0.2 + 0.3\\times 0.1\n  = 0.10.\n  $$\n\n- Pour $k = 5$  \n  $$(X,Y) = (1,4),(2,3),(3,2),(4,1)$$\n  $$\n  \\mathbb{P}(S=5) = p_1 p_4 + p_2 p_3 + p_3 p_2 + p_4 p_1\n  = 0.04 + 0.06 + 0.06 + 0.04 = 0.20.\n  $$\n\n- Pour $k = 6$  \n  $$(X,Y) = (2,4),(3,3),(4,2)$$\n  $$\n  \\mathbb{P}(S=6) = p_2 p_4 + p_3 p_3 + p_4 p_2\n  = 0.2\\times 0.4 + 0.3\\times 0.3 + 0.4\\times 0.2\n  = 0.08 + 0.09 + 0.08 = 0.25.\n  $$\n\n- Pour $k = 7$  \n  $$(X,Y) = (3,4),(4,3)$$\n  $$\n  \\mathbb{P}(S=7) = p_3 p_4 + p_4 p_3\n  = 0.3\\times 0.4 + 0.4\\times 0.3\n  = 0.24.\n  $$\n\n- Pour $k = 8$  \n  $$(X,Y) = (4,4) \\quad\\Rightarrow\\quad \\mathbb{P}(S=8) = p_4 p_4 = 0.4\\times 0.4 = 0.16.$$\n\nOn obtient donc la loi de $S = X+Y$ :\n\n$$\n\\begin{array}{c|ccccccc}\ns      & 2    & 3    & 4    & 5    & 6    & 7    & 8    \\\\ \\hline\n\\mathbb{P}(S=s) & 0.01 & 0.04 & 0.10 & 0.20 & 0.25 & 0.24 & 0.16\n\\end{array}\n$$\n\n(On vérifie que la somme vaut bien $1$: $0.01 + 0.04 + 0.10 + 0.20 + 0.25 + 0.24 + 0.16 = 1$.)\n\n## Exercice 5\n\nUn statisticien a modélisé le nombre de mots d’une recherche sur internet en utilisant une loi de Poisson. Supposons que la longueur moyenne est de 3 mots, et soit $X$ le nombre de mots d’une recherche. Comme une recherche ne peut pas être vide, nous utilisons une modélisation par une loi de probabilité restreinte définie par\n\n$$\\mathbb{P}(X = k) = \\mathbb{P}(Y = k \\mid Y \\neq 0) \\quad \\text{où } Y \\sim \\text{Pois}(\\lambda).$$\n\n1) Trouver la loi de $X$.  \n2) Donner la valeur de $\\lambda$ correspondant à une longueur moyenne de 3 mots.  \n3) Quelle est la probabilité d’avoir une recherche de 6 mots ou plus ?\n\n## Correction — Exercice 5\n\n1) Trouver la loi de $X$\n\nPour déterminer la loi d'une variable aléatoire $X$ , il faut d'abord définir son support, c'est-à-dire l'ensemble des valeurs que $X$ peut prendre avec une probabilité non nulle. Dans ce cas, le support de $X$ est l'ensemble des entiers naturels positifs, car une recherche ne peut pas être vide.\n\nNous notons le support de $X$ par $\\Omega_X = \\{1, 2, 3, \\ldots\\}$ = $\\mathbb{N}^*$.\n\nUne fois le support identifié, nous pouvons calculer la probabilité associée à chaque valeur du support de $X$. \n\nRappelons nous que l'ensemble ${X \\in A} = {w \\in \\Omega : X(w) \\in A} = X^-1(A)$.\nPour tout $k \\in \\Omega_X$, nous avons :\n\n${X = k} = {w \\in \\Omega : X(w) = k} = X^{-1}(\\{k\\})$. où $\\Omega$ est l'ensemble des issues possibles de l'expérience aléatoire.\n\nDonc, pour tout $k \\in \\Omega_X$, nous avons :\n\nOn part de la définition de $X$ comme loi de Poisson **conditionnée à être non nulle** :\n\n1. **Définition de $X$ comme loi restreinte :**\n   $$\n   \\mathbb{P}(X = k)\n   = \\mathbb{P}(Y = k \\mid Y \\neq 0).\n   $$\n   C’est donné dans l’énoncé : $X$ est la loi de $Y$ sachant que $Y$ ne vaut pas 0.\n\n2. **Formule de probabilité conditionnelle :**\n   $$\n   \\mathbb{P}(Y = k \\mid Y \\neq 0)\n   = \\frac{\\mathbb{P}(Y = k \\cap Y \\neq 0)}{\\mathbb{P}(Y \\neq 0)}.\n   $$\n   Par définition :  \n   $\\displaystyle \\mathbb{P}(A \\mid B) = \\frac{\\mathbb{P}(A \\cap B)}{\\mathbb{P}(B)}$.\n\n3. **Simplification de l'intersection :**\n   $$\n   \\mathbb{P}(Y = k \\cap Y \\neq 0) = \\mathbb{P}(Y = k)\n   $$\n   car si $k \\ge 1$, alors l'événement « $Y = k$ » implique automatiquement « $Y \\neq 0$ ».  \n   Donc l’intersection ne change rien.\n\n4. **Remplacement du dénominateur :**\n   $$\n   \\mathbb{P}(Y \\neq 0) = 1 - \\mathbb{P}(Y = 0).\n   $$\n   C’est la propriété générale :  \n   $\\displaystyle \\mathbb{P}(B^c) = 1 - \\mathbb{P}(B)$.\n\n5. **Utilisation de la formule de la loi de Poisson :**\n   $$\n   \\mathbb{P}(Y = k) = e^{-\\lambda} \\frac{\\lambda^k}{k!},\n   \\qquad\n   \\mathbb{P}(Y = 0) = e^{-\\lambda}.\n   $$\n\n6. **Substitution dans la formule :**\n   $$\n   \\mathbb{P}(X = k)\n   = \\frac{e^{-\\lambda} \\frac{\\lambda^k}{k!}}{1 - e^{-\\lambda}}.\n   $$\n\n\n\n\n\n###  Résultat final :\n$$\n\\boxed{\\mathbb{P}(X = k) = \\frac{e^{-\\lambda} }{1 - e^{-\\lambda}} \\cdot \\frac{\\lambda^k}{k!}, \\quad k = 1, 2, 3, \\ldots}\n$$\n\n\n### 2) Calcul de l’espérance de $X$\n\nNous avons la loi :\n$$\n\\mathbb{P}(X = k)\n= \\frac{e^{-\\lambda} \\lambda^k}{k!\\,(1 - e^{-\\lambda})}, \\qquad k \\ge 1.\n$$\n\nL’espérance vaut en utilisant le théorème de transfert :\n$$\n\\mathbb{E}(X)\n= \\sum_{k=1}^{+\\infty} k \\, \\mathbb{P}(X = k)\n= \\sum_{k=1}^{\\infty} k \\, \\frac{e^{-\\lambda} \\lambda^k}{k!\\,(1 - e^{-\\lambda})}.\n$$\n\n**On factorise les constantes** :\n$$\n\\mathbb{E}(X)\n= \\frac{e^{-\\lambda}}{1 - e^{-\\lambda}}\n   \\sum_{k=1}^{\\infty} k \\frac{\\lambda^{k}}{k!}.\n$$\n\n---\n\nJustification du passage suivant\n\nOn réécrit :\n$$\nk\\frac{\\lambda^{k}}{k!}\n= \\lambda \\frac{\\lambda^{k-1}}{(k-1)!}.\n$$\n\nEn effet :\n$$\n\\frac{k}{k!} = \\frac{1}{(k-1)!}.\n$$\n\nDonc :\n$$\n\\sum_{k=1}^{\\infty} k \\frac{\\lambda^k}{k!}\n= \\lambda \\sum_{k=1}^{\\infty} \\frac{\\lambda^{k-1}}{(k-1)!}.\n$$\n\nOn effectue le **changement d’indice**  $j = k - 1$ :\n\n- quand $k = 1$, $j = 0$,\n- quand $k \\to \\infty$, $j \\to \\infty$.\n\nD’où :\n$$\n\\sum_{k=1}^{\\infty} \\frac{\\lambda^{k-1}}{(k-1)!}\n= \\sum_{j=0}^{\\infty} \\frac{\\lambda^{j}}{j!}.\n$$\n\nOr cette somme est la **série de Taylor de l’exponentielle** :\n$$\n\\sum_{j=0}^{\\infty} \\frac{\\lambda^{j}}{j!} = e^{\\lambda}.\n$$\n\n\n\nOn revient à l’expression de l’espérance\n\nOn obtient donc :\n$$\n\\mathbb{E}(X)\n= \\frac{e^{-\\lambda}}{1 - e^{-\\lambda}} \\cdot \\lambda e^{\\lambda}.\n$$\nPuis en simplifiant \\( e^{-\\lambda} e^{\\lambda} = 1 \\) :\n$$    \n\\boxed{\n\\mathbb{E}(X) = \\frac{\\lambda}{1 - e^{-\\lambda}}.\n}\n$$    \n\n---\n\n### Détermination de $\\lambda$ pour une espérance de 3\n\nEn résolvant :\n$$\n\\frac{\\lambda}{1 - e^{-\\lambda}} = 3,\n$$\n\n::: {#bfe8341d .cell execution_count=6}\n``` {.python .cell-code}\n#Utilisons Newton-Raphson pour résoudre l'équation\n\nfrom scipy.optimize import newton\nimport numpy as np\n\n# Définir la fonction\ndef equation(lmbda):\n    return lmbda / (1 - np.exp(-lmbda)) - 3\n\n# Résoudre avec Newton-Raphson\nlambda_solution = newton(equation, x0=1.0)\nprint(lambda_solution)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n2.8214393721220787\n```\n:::\n:::\n\n\non obtient numériquement :\n$$\n\\boxed{\\lambda \\approx 2.82}.\n$$\n\n3) Probabilité d’avoir une recherche de 6 mots ou plus\n\nOn cherche :\n$$\n\\mathbb{P}(X \\ge 6).\n$$\n\nOr :\n$$\n\\mathbb{P}(X \\ge 6) = 1 - \\mathbb{P}(X \\le 5).\n$$\n\nEt comme \\(X\\) est la loi de Poisson tronquée :\n$$\n\\mathbb{P}(X \\le 5)\n= \\sum_{k=1}^{5} \\mathbb{P}(X = k)\n= \\sum_{k=1}^{5}\n\\frac{e^{-\\lambda} \\lambda^k}{k!\\,(1 - e^{-\\lambda})}.\n$$\n\nOn factorise pour simplifier :\n$$  \n\\mathbb{P}(X \\le 5)\n= \\frac{e^{-\\lambda}}{1 - e^{-\\lambda}}\n\\sum_{k=1}^{5} \\frac{\\lambda^k}{k!}.\n$$\n\nDonc :\n\n$$\n\\boxed{\n\\mathbb{P}(X \\ge 6)\n= 1 -\n\\frac{e^{-\\lambda}}{1 - e^{-\\lambda}}\n\\sum_{k=1}^{5} \\frac{\\lambda^k}{k!}\n}.\n$$\n\n---\n\n Avec $\\lambda \\approx 2.82$, on calcule :\n\nLe calcul numérique donne :\n$$\n\\boxed{\\mathbb{P}(X \\ge 6) \\approx 0.07}\n$$\n\nsoit **environ 7%**.\n\n## Exercice 6\n\nLa loi de probabilité jointe de deux variables $X$ et $Y$ est définie par\n\n$$\n\\mathbb{P}(X = x, Y = y)\n= \\frac{1}{e^{2} \\, y! \\, (x - y)!}\n\\qquad \\text{pour } x \\in \\mathbb{N} \\text{ et } y = 0, \\ldots, x.\n$$\n\n1) Trouver la loi de probabilité de $X$.  \n2) En déduire la loi de probabilité de $Y$ sachant que $X = x$.\n\n## Correction — Exercice 6\n\nAvant de résoudre l’exercice, rappelons les formules de pascal :\n\n$$\n\\binom{x}{y} = \\frac{x!}{y! (x-y)!},\n$$\net\n$$\n(a + b)^x = \\sum_{y=0}^{x} \\binom{x}{y} a^y b^{x-y}.\n$$\n\nPour a = 1 et b = 1, on obtient :\n$$\n2^x = \\sum_{y=0}^{x} \\binom{x}{y}.\n$$\n\n1) Trouver la loi de probabilité de $X$\n\nLe support de X est $\\mathbb{N}$.\n\nPour tout $x \\in \\mathbb{N}$, on utilise la formule des probabilités totales :\n$$\n\\mathbb{P}(X = x)\n= \\mathbb{P}(X = x, \\Omega)\n= \\mathbb{P}(X = x, \\bigcup_{y=0}^{x} \\{Y = y\\})\n= \\sum_{y=0}^{x} \\mathbb{P}(X = x, Y = y).\n$$\n\n- $\\Omega$ est l’événement certain.\n- Les événements {Y=0}, \\ldots, {Y=x} sont disjoints et forment une partition des valeurs possibles de Y quand X=x.\n- La formule générale de probabilité totale dit :\n  $$\n  \\mathbb{P}(A) = \\sum_i \\mathbb{P}(A \\cap B_i)\n  \\quad \\text{si les } B_i \\text{ forment une partition}.\n  $$\n\n\nEn théorie de probabilité, on écrit souvent $P(A, B)$ pour $P(A \\cap B)$.\n\nOn remplace par la loi jointe donnée :\n$$\n\\mathbb{P}(X = x)\n= \\sum_{y=0}^{x} \\frac{1}{e^{2} \\, y! \\, (x - y)!}.\n$$\n\nOn factorise :\n$$\n\\mathbb{P}(X = x)\n= \\frac{1}{e^{2}} \\sum_{y=0}^{x} \\frac{1}{y! \\, (x - y)!}.\n$$\n\nOn utilise la formule de pascal pour réécrire la somme :\nOn utilise l’identité :\n$$\n\\binom{x}{y} = \\frac{x!}{y! (x-y)!}.\n$$\n\nD’où :\n$$\n\\frac{1}{y!(x-y)!} = \\frac{1}{x!}\\binom{x}{y}.\n$$\n\nAinsi :\n$$\n\\sum_{y=0}^{x} \\frac{1}{y!(x-y)!}\n= \\frac{1}{x!} \\sum_{y=0}^{x} \\binom{x}{y}.\n$$\n\nOr :\n$$\n\\sum_{y=0}^{x} \\binom{x}{y} = 2^x,\n$$\ncar c’est le développement de \\((1+1)^x\\).\n\nOn obtient donc :\nOn obtient :\n$$\n\\mathbb{P}(X = x)\n= \\frac{1}{e^{2}} \\cdot \\frac{2^x}{x!}\n= e^{-2} \\frac{2^x}{x!}.\n$$\n\n\nNous reconnaissons la loi de Poisson de paramètre $\\lambda = 2$.\n\n\n$$\n\\boxed{X \\sim \\text{Poisson}(2)}.\n$$\n\n2) Soient $x, y \\in \\mathbb{N}$. Nous avons\n\n$$\n\\mathbb{P}(Y = y \\mid X = x)\n= \\frac{\\mathbb{P}(X = x, Y = y)}{\\mathbb{P}(X = x)},\n$$\n\net cette probabilité est nulle si $y > x$.  \nSi $y \\le x$, nous avons :\n\n$$\n\\mathbb{P}(Y = y \\mid X = x)\n= \\frac{1}{e^{2} y! (x - y)!} \\times \\frac{e^{2} x!}{2^{x}}\n$$\n\n$$\n= \\binom{x}{y} \\left(\\frac{1}{2}\\right)^{y} \\left(\\frac{1}{2}\\right)^{x - y}.\n$$\n\nNous reconnaissons la densité d’une loi binomiale de paramètres $m = x$ et  \n$p = \\frac{1}{2}$.\n\nDonc :\n$$\n\\boxed{Y \\mid X = x \\sim \\mathcal{B}\\left(x, \\frac{1}{2}\\right)}.\n$$\n\n## Exercice 7\n\nSoient $X_1, \\ldots, X_n$ une suite de variables aléatoires i.i.d. suivant une loi $\\mathcal{L}$.\nNous supposons que les paramètres de la loi $\\mathcal{L}$ sont entièrement caractérisés\npar les moments $\\mu_1 \\equiv \\mathbb{E}(X), \\ldots, \\mu_k \\equiv \\mathbb{E}(X^k)$.\nLa *méthode des moments* consiste à estimer les paramètres de la loi $\\mathcal{L}$\nen remplaçant les moments inconnus par leur estimateur empirique\n\n$$\n\\hat{\\mu}_k = \\frac{1}{n} \\sum_{i=1}^n (X_i)^k.\n$$\n\nNous supposons que les variables aléatoires $X_1, \\ldots, X_n$ suivent une loi de Bernoulli\nde paramètre $p$.\n\n1) Exprimer $p$ en fonction du premier moment de $X$.  \n2) En déduire une estimation de $p$ basée sur l’échantillon.\n\nNous supposons que les variables aléatoires $X_1, \\ldots, X_n$ suivent une loi binomiale\nde paramètres $m$ et $p$.\n\n3) Exprimer $m$ et $p$ en fonction des deux premiers moments de $X$.  \n4) En déduire une estimation de $m$ et de $p$ basée sur l’échantillon.\n\n### Correction Exercice 7\n\n1) Nous avons  \n$$\np = \\mathbb{E}(X) = \\mu_1.\n$$\n\n2) Le paramètre \\(p\\) est donc estimé par\n$$\n\\hat{p} = \\hat{\\mu}_1 = \\frac{1}{n} \\sum_{i=1}^n X_i.\n$$\n\nNotons que dans ce cas précis, il s’agit d’un estimateur sans biais.\n\n---\n\n3) Nous avons, pour une loi binomiale \\(\\mathcal{B}(m,p)\\) :\n$$\n\\mathbb{E}(X) = mp\n\\qquad\\text{et}\\qquad\n\\operatorname{Var}(X) = \\mathbb{E}(X^2) - \\{\\mathbb{E}(X)\\}^2 = mp(1 - p).\n$$\n\nNous en déduisons que\n$$\n1 - p = \\frac{\\mu_2}{\\mu_1} - \\mu_1,\n$$\n\npuis\n$$\np = 1 - \\frac{\\mu_2}{\\mu_1} + \\mu_1,\n$$\n\net\n$$\nm = \\frac{\\mu_1}{1 - \\frac{\\mu_2}{\\mu_1} + \\mu_1}.\n$$\n\n---\n\n4) Nous obtenons les estimateurs par la méthode des moments :\n\n$$\n\\hat{p} = 1 - \\frac{\\hat{\\mu}_2}{\\hat{\\mu}_1} + \\hat{\\mu}_1,\n$$\n\n$$\n\\hat{m} = \n\\frac{\\hat{\\mu}_1}{1 - \\frac{\\hat{\\mu}_2}{\\hat{\\mu}_1} + \\hat{\\mu}_1}.\n$$\n\nÀ noter que ces estimateurs n’ont pas de raison particulière d’être sans biais.\n\n## Exercice 8\n\nSoit $X \\sim \\text{Pois}(\\lambda)$.\n\n1) Donner la fonction génératrice des moments de $X$ et sa fonction caractéristique.  \n\n2) Calculer $\\mathbb{E}(X^3)$.  \n3) Calculer la probabilité que $X$ soit impair.  \n   Utiliser un développement en série entière de $e^\\lambda$ et $e^{-\\lambda}$.  \n\n4) Calculer $\\mathbb{E}(X!)$.\n\n## Correction — Exercice 8\n\nAvant de commencer, rappelons que :\n\n{X impair} qui s'écrit en latex $X \\text{ impair}$ est l'ensemble des issues où la variable aléatoire $X$ prend des valeurs impaires. Cet ensemble peut être représenté comme l'union des événements disjoints {X = 1}, {X = 3}, {X = 5}, etc.\n\nOn peut donc écrire :\n\nL’événement $X$ est impair s’écrit :\n\n$$\n\\{X \\text{ impair}\\}\n= \\{\\omega \\in \\Omega : X(\\omega) \\text{ est impair}\\}\n$$\n\nPar définition d’un entier impair :\n\n$$  \n\\{X \\text{ impair}\\}\n= \\{\\omega : X(\\omega) = 1\\}\n  \\cup \\{\\omega : X(\\omega) = 3\\}\n  \\cup \\{\\omega : X(\\omega) = 5\\}\n  \\cup \\cdots\n$$\n\nC’est-à-dire :\n\n$$\n\\{X \\text{ impair}\\}\n= \\{\\omega \\in \\Omega : \\exists\\, k \\in \\mathbb{N}, \\; X(\\omega) = 2k + 1\\}.\n$$\n\nEn notation abrégée :\n\n$$\n\\{X \\text{ impair}\\}\n= \\{X = 1\\} \\cup \\{X = 3\\} \\cup \\{X = 5\\} \\cup \\cdots\n$$\n\net de manière compacte :\n\n$$\n\\boxed{\n\\{X \\text{ impair}\\}\n= \\bigcup_{k=0}^{\\infty} \\{X = 2k + 1\\}.\n}\n$$\n\n\n1) Fonction génératrice des moments et fonction caractéristique\n\n$\\textbf{1)}$ Soit $t > 0$. Nous avons\n\n$$\nM_X(t)\n= \\mathbb{E}\\{\\exp(tX)\\}\n= e^{-\\lambda} \\sum_{k=0}^{+\\infty} \\exp(tk)\\, \\frac{\\lambda^k}{k!}\n$$\n\n$$ \n= e^{-\\lambda} \\sum_{k=0}^{+\\infty} \\frac{\\{\\lambda \\exp(t)\\}^k}{k!}\n= e^{-\\lambda} \\times \\exp\\{\\lambda \\exp(t)\\}\n$$\n\n$$\n= \\exp\\{\\lambda(\\exp(t) - 1)\\}.\n$$\n\nOn vérifie au passage que la fonction génératrice des moments est ici bien\ndéfinie pour tout $t \\in \\mathbb{R}$.\n\nLa fonction caractéristique se calcule de façon analogue :\n\n$$\n\\varphi_X(t)\n= \\mathbb{E}\\{\\exp(itX)\\}\n= e^{-\\lambda} \\sum_{k=0}^{+\\infty} \\exp(itk)\\, \\frac{\\lambda^k}{k!}\n$$\n\n$$\n= e^{-\\lambda} \\sum_{k=0}^{+\\infty} \\frac{\\{\\lambda \\exp(it)\\}^k}{k!}\n= e^{-\\lambda} \\times \\exp\\{\\lambda \\exp(it)\\}\n$$\n\n$$\n= \\exp\\{\\lambda(\\exp(it) - 1)\\}.\n$$\n\n2) Calcul de $\\mathbb{E}(X^3)$\n\nNous utilisons la fonction génératrice des moments, qui est ici indéfiniment dérivable.\n\nRappel :\n$$\nM_X(t) = \\exp\\{\\lambda(e^t - 1)\\}.\n$$\n\nNous calculons ses dérivées successives.\n\n#### Première dérivée\n\n$$\nM_X'(t)\n= \\lambda e^t \\exp\\{\\lambda(e^t - 1)\\}.\n$$\n#### Deuxième dérivée\n\n$$\nM_X''(t)\n= \\left(\\lambda e^t + \\lambda^2 e^{2t}\\right)\n  \\exp\\{\\lambda(e^t - 1)\\}.\n$$\n#### Troisième dérivée\n\n$$\nM_X^{(3)}(t)\n= \\left(\\lambda e^t + 3\\lambda^2 e^{2t} + \\lambda^3 e^{3t}\\right)\n  \\exp\\{\\lambda(e^t - 1)\\}.\n$$\nNous pouvons maintenant obtenir le troisième moment.\n\n---\n\n### Calcul de  $\\mathbb{E}(X^3)$\n\nPar définition :\n$$\n\\mathbb{E}(X^3) = M_X^{(3)}(0).\n$$\n\nComme $e^{0}=1$, nous obtenons :\n$$\nM_X^{(3)}(0)\n= \\left(\\lambda + 3\\lambda^2 + \\lambda^3\\right)\\exp\\{0\\}\n= \\lambda^3 + 3\\lambda^2 + \\lambda.\n$$\n\n$$\n\\boxed{\\mathbb{E}(X^3)=\\lambda^3 + 3\\lambda^2 + \\lambda.}\n$$\n\n3) Calcul de la probabilité que $X$ soit impair\n\nNous avons\n\n$$\n\\mathbb{P}(X \\text{ impair})\n= \\mathbb{P}\\left(\\bigcup_{k=0}^{\\infty} \\{X = 2k + 1\\}\\right)\n= \\sum_{k=0}^{\\infty} \\mathbb{P}(X = 2k + 1)\n$$\n\n$$\n= \\sum_{k=0}^{\\infty} e^{-\\lambda} \\frac{\\lambda^{2k + 1}}{(2k + 1)!}\n= e^{-\\lambda} \\sum_{k=0}^{\\infty} \\frac{\\lambda^{2k + 1}}{(2k + 1)!}.\n$$ \n\n\nD’autre part,\n$$\ne^\\lambda = \\sum_{k=0}^{+\\infty} \\frac{\\lambda^k}{k!}\n= 1 + \\lambda + \\frac{\\lambda^2}{2} + \\frac{\\lambda^3}{3!} + \\frac{\\lambda^4}{4!} + \\cdots\n$$  \n\net\n$$\ne^{-\\lambda} = \\sum_{k=0}^{+\\infty} \\frac{(-\\lambda)^k}{k!}\n= 1 - \\lambda + \\frac{\\lambda^2}{2} - \\frac{\\lambda^3}{3!} + \\frac{\\lambda^4}{4!} + \\cdots\n$$\n\nDonc,\n$$\ne^\\lambda - e^{-\\lambda}\n= \\sum_{k=0}^{+\\infty} \\frac{\\lambda^k}{k!}\n+ \\sum_{k=0}^{+\\infty} \\frac{(-1)^{k+1}\\lambda^k}{k!}\n= 2 \\sum_{k=0}^{+\\infty} \\frac{\\lambda^{2k+1}}{(2k+1)!}.\n$$\n\nEt finalement,\n$$\n\\mathbb{P}(X \\text{ impair})\n= \\frac{e^{-\\lambda}(e^\\lambda - e^{-\\lambda})}{2}\n= \\frac{1 - e^{-2\\lambda}}{2}.\n$$\n\n4) Nous avons\n\n$$\nE(X!) = \\sum_{k=0}^{+\\infty} e^{-\\lambda} \\frac{\\lambda^k}{k!} \\times k!\n$$\n\n$$\n= e^{-\\lambda} \\sum_{k=0}^{+\\infty} \\lambda^k\n= \n\\begin{cases}\n\\dfrac{e^{-\\lambda}}{1 - \\lambda}, & \\text{si } |\\lambda| < 1, \\\\[6pt]\n+\\infty, & \\text{si } |\\lambda| \\ge 1.\n\\end{cases}\n$$\n\n## Exercice 9\n\nSoient \\(X_1, \\ldots, X_n\\) une suite de variables aléatoires i.i.d. suivant une loi de Bernoulli de paramètre \\(p\\).  \nTrouver la loi de \\(X_1\\) sachant que \\(X_1 + \\cdots + X_n = k\\).\n\n---\n\n## Correction Exercice 9\n\nSoit \\(x \\in \\{0,1\\}\\). Nous avons, en utilisant la formule de Bayes :\n\n$$\n\\mathbb{P}(X_1 = x \\mid X_1 + \\cdots + X_n = k)\n= \\frac{\\mathbb{P}(X_1 + \\cdots + X_n = k \\mid X_1 = x)\\mathbb{P}(X_1 = x)}\n{\\mathbb{P}(X_1 + \\cdots + X_n = k)}.\n$$\n\nOr :\n\n$$\n\\mathbb{P}(X_1 + \\cdots + X_n = k) = \\binom{n}{k} p^k (1-p)^{\\,n-k},\n$$\n\npuisque \\(X_1 + \\cdots + X_n \\sim \\mathcal{B}(n,p)\\).\n\n---\n\nEnsuite :\n\n$$\nX_2 + \\cdots + X_n \\sim \\mathcal{B}(n-1,p),\n$$\n\ndonc\n\n$$\n\\mathbb{P}(X_2 + \\cdots + X_n = k - x)\n= \\binom{n-1}{k-x} p^{\\,k-x} (1-p)^{\\,n-1-k+x}.\n$$\n\nDe plus :\n\n$$\n\\mathbb{P}(X_1 = x) = p^x (1-p)^{\\,1-x}.\n$$\n\n---\n\nNous obtenons finalement :\n\n$$\n\\mathbb{P}(X_1 = x \\mid X_1 + \\cdots + X_n = k)\n= \\frac{\\binom{n-1}{k-x} p^{\\,k-x}(1-p)^{\\,n-1-k+x} \\, p^x (1-p)^{\\,1-x}}\n{\\binom{n}{k} p^k (1-p)^{\\,n-k}}.\n$$\n\nEn simplifiant, on trouve :\n\n$$\n\\mathbb{P}(X_1 = x \\mid X_1 + \\cdots + X_n = k)\n= \\frac{\\binom{n-1}{k-x}}{\\binom{n}{k}}.\n$$\n\nOn calcule les deux cas :\n- Si $x = 0$ :\n  $$\n  \\mathbb{P}(X_1 = 0 \\mid X_1 + \\cdots + X_n = k)\n  = \\frac{\\binom{n-1}{k}}{\\binom{n}{k}}\n  = \\frac{(n-1)! \\, k! \\, (n-k)!}{(n-k)! \\, (n)! \\, k!}\n  = \\frac{n-k}{n}.\n  $$\n- Si $x = 1$ :\n  $$\n  \\mathbb{P}(X_1 = 1 \\mid X_1 + \\cdots + X_n = k)\n  = \\frac{\\binom{n-1}{k-1}}{\\binom{n}{k}}\n  = \\frac{(n-1)! \\, (k-1)! \\, (n-k)!}{(n-k)! \\, (n)! \\, k!}\n  = \\frac{k}{n}.\n  $$\n\nDonc \n\n$$\n\\boxed{X_1 \\mid (X_1 + \\cdots + X_n = k) \\sim \\mathcal{B}\\left(1, \\frac{k}{n}\\right)}.\n$$\n\n# 5 Variables aléatoires continues\n\n## Exercice 1\n\nUne variable aléatoire $X$ possède la fonction de densité\n\n$$\nf(x) = c x \\quad \\text{pour } 0 < x < 1.\n$$\n\n1. Trouver la valeur de $c$.  \n2. Calculer $\\mathbb{P}(X < 0.5)$.  \n3. Calculer $\\mathbb{E}(X)$.  \n4. Calculer la fonction génératrice des moments de $X$.\n\n## Correction — Exercice 1\n\nOn considère la densité\n\n$$\nf(x) = c x \\quad \\text{pour } 0 < x < 1.\n$$\n\n---\n\n1. Détermination de $c$\n\n$f$ est une densité si et seulement si elle vérifie les deux conditions suivantes :\n\n1. $f(x) \\ge 0$ pour tout $x \\in \\mathbb{R}$.  \n   C’est le cas ici si $c \\ge 0$.\n2. L’intégrale de $f$ sur $\\mathbb{R}$ vaut 1.  \n   Ici, cela revient à :\n\n\n$$\n\\int_{-\\infty}^{+\\infty} f(x) \\, dx = 1.\n$$\n\nCalcul :\n\n$$\n\\int_{-\\infty}^{+\\infty} f(x) \\, dx\n= \\int_0^1 c x \\, dx\n= c \\left[ \\frac{x^2}{2} \\right]_0^1\n= \\frac{c}{2}.\n$$\n\nDonc :\n\n$$\n\\frac{c}{2} = 1 \\quad \\Longrightarrow \\quad c = 2.\n$$\n\n---\n\n2. Calcul de $\\mathbb{P}(X < 0.5)$\n\n$$\n\\mathbb{P}(X < 0.5) = \\int_{-\\infty}^{0.5} f(x) \\, dx\n= \\int_0^{0.5} 2x \\, dx.\n$$\n\n$$\n= 2 \\left[ \\frac{x^2}{2} \\right]_0^{0.5}\n= (0.5)^2 = 0.25.\n$$\n\n---\n\n3. Calcul de $\\mathbb{E}(X)$\n\n$$\n\\mathbb{E}(X) = \n= \\int_{-\\infty}^{+\\infty} x \\, f(x) \\, dx\n= \\int_0^1 x \\cdot 2x \\, dx\n= \\int_0^1 2x^2 \\, dx.\n$$\n\n$$\n\\int_0^1 2x^2 \\, dx = 2 \\left[ \\frac{x^3}{3} \\right]_0^1 = \\frac{2}{3}.\n$$\n\nDonc :\n\n$$\n\\mathbb{E}(X) = \\frac{2}{3}.\n$$\n\n---\n\n4. Fonction génératrice des moments (MGF)\n\nLa fonction génératrice des moments est :\n\nsoit $t \\in \\mathbb{R}$,\n$$\nM_X(t) = \\mathbb{E}(e^{tX})\n= \\int_{-\\infty}^{+\\infty} e^{tx} \\, f(x) \\, dx\n= \\int_0^1 2x e^{tx} \\, dx.\n$$\n\nOn calcule l’intégrale par parties.\n\nPosons :\n\n- $u = x$ donc $du = dx$\n- $dv = e^{tx} dx$ donc $v = \\frac{e^{tx}}{t}$\n\nAlors :\n\nSi $t \\neq 0$,\n$$\n\\int_0^1 x e^{tx} dx\n= \\left[ \\frac{x e^{tx}}{t} \\right]_0^1\n- \\int_0^1 \\frac{e^{tx}}{t} dx.\n$$\n\nOn obtient :\n\n$$\n\\int_0^1 x e^{tx} dx\n= \\frac{e^{t}}{t}\n- \\frac{1}{t} \\int_0^1 e^{tx} dx.\n$$\n\nOr :\n\n$$\n\\int_0^1 e^{tx} dx\n= \\left[ \\frac{e^{tx}}{t} \\right]_0^1\n= \\frac{e^{t} - 1}{t}.\n$$\n\nDonc :\n\n$$\n\\int_0^1 x e^{tx} dx\n= \\frac{e^{t}}{t}\n- \\frac{e^{t} - 1}{t^2}.\n$$\n\nDonc si $t \\neq 0$,\n\n$$\nM_X(t) = 2 \\left( \n\\frac{e^{t}}{t}\n- \\frac{e^{t} - 1}{t^2}\n\\right).\n$$\n\nEt si $t = 0$, on a :\n\n$$\nM_X(0) = \\mathbb{E}(e^{0}) = \\mathbb{E}(1) = 1.\n$$\n\n---\n\nDonc la fonction génératrice des moments est donnée par :\n$$\n\\boxed{\nM_X(t) =\n\\begin{cases}\n2 \\left( \n\\frac{e^{t}}{t}\n- \\frac{e^{t} - 1}{t^2}\n\\right), & \\text{si } t \\neq 0, \\\\[6pt]\n1, & \\text{si } t = 0.\n\\end{cases}\n}\n$$\n\n## Exercice 2\n\nSoit la fonction définie par  \n$$f(x) = c\\,x(1 - x)$$  \npour $x \\in [0,1]$, et $0$ sinon.\n\n1. Pour quelle valeur de $c$ est-ce une densité de probabilité ?\n\n2. Déterminer la fonction de répartition de cette loi et sa médiane.\n\n## Exercice 2 — Correction\n\nOn considère la fonction :\n\n$$\nf(x) = c\\,x(1-x), \\quad x \\in [0,1],\n$$\n\net $f(x)=0$ sinon.\n\n---\n\n1) Pour quelle valeur de $c$ est-ce une densité ?\n\nPour que $f$ soit une densité, il faut :\n\n$$\n\\int_0^1 c\\,x(1-x)\\,dx = 1.\n$$\n\nCalculons l’intégrale :\n\n$$\n\\int_0^1 x(1-x)\\,dx\n= \\int_0^1 (x - x^2)\\,dx\n= \\left[\\frac{x^2}{2} - \\frac{x^3}{3}\\right]_0^1\n= \\frac12 - \\frac13\n= \\frac16.\n$$\n\nDonc :\n\n$$\nc \\cdot \\frac{1}{6} = 1\n\\quad \\Longrightarrow \\quad c = 6.\n$$\n\n---\n\n2) Fonction de répartition $F(x)$\n\nPour $x < 0$ :\n$$\nF(x) = 0.\n$$\n\nPour $x \\in [0,1]$ :\n$$\nF(x) = \\int_0^x 6\\,t(1-t)\\,dt.\n$$\n\nCalculons :\n\n$$\n\\int_0^x 6(t - t^2)\\,dt\n= 6\\left[ \\frac{t^2}{2} - \\frac{t^3}{3} \\right]_0^x\n= 6\\left( \\frac{x^2}{2} - \\frac{x^3}{3} \\right).\n$$\n\nDonc :\n\n$$\nF(x) = 3x^2 - 2x^3.\n$$\n\n Pour $x > 1$ :\n$$\nF(x) = 1.\n$$\n\n---\n\nMédiane\n\nLa médiane $m$ vérifie :\n\n$$\nF(m) = 0.5.\n$$\n\nDonc :\n\n$$\n3m^2 - 2m^3 = \\frac12.\n$$\n\nSoit :\n\n$$\n2m^3 - 3m^2 + \\frac12 = 0.\n$$\n\nCeci est équivalent à :\n\n$$\n4m^3 - 6m^2 + 1 = 0.\n$$\n\nC'est une équation polynomiale du troisième degré.\nUne racine m de cette équation se trouve dans l'intervalle [0,1].\n\nEtant donné une telle équation, les racines s'écrivent comme le rapport des diviseurs du terme constant sur les diviseurs du coefficient dominant, qui est 4 dans ce cas.\nLes diviseurs de 1 sont $\\pm 1$, et les diviseurs de 4 sont $\\pm 1, \\pm 2, \\pm 4$.\nDonc les racines rationnelles possibles sont $\\pm 1, \\pm \\frac{1}{2}, \\pm \\frac{1}{4}$.\n\nOn doit choisir parmi ces valeurs celles qui sont dans l'intervalle [0,1], c'est-à-dire $1, \\frac{1}{2}, \\frac{1}{4}$.\n\nEn testant ces valeurs dans l'équation, on trouve que la racine dans l'intervalle [0,1] est :\n$$\nm = \\frac{1}{2}\n$$\n\nDonc la médiane est :\n$$\n\\boxed{m = \\frac{1}{2}}.\n$$\n\nUne autre méthode pour déterminer la médiane consiste à remarquer que la fonction de densité admet la droite d'équation $x = \\frac{1}{2}$ comme axe de symétrie.\n\nPour ce faire, on peut soit tracer la fonction de densité : \n\n::: {#6b8c14b5 .cell execution_count=7}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Define density on R, zero outside [0,1]\ndef f(x):\n    return np.where((x>=0)&(x<=1), 6*x*(1-x), 0)\n\n# Real line range\nx = np.linspace(-5, 5, 800)\ny = f(x)\n\nplt.figure(figsize=(7,4))\nplt.plot(x, y, linewidth=2)\nplt.axvline(0.5, linewidth=2, color='red', linestyle='--', label='x=0.5 (médiane)')\nplt.legend()\nplt.xlabel(\"x\")\nplt.ylabel(\"f(x)\")\nplt.title(\"Density f(x)=6x(1-x) on the whole real line (0 outside [0,1])\")\nplt.grid(True)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_gdr_files/figure-html/cell-8-output-1.png){width=589 height=376}\n:::\n:::\n\n\nLorsque que la droite d'équation $x = a$, est un axe de symétrie pour une fonction $f$, dérivable, alors $f'(a) = 0$.\n\nEt dans notre cas, on trouve bien $f'(\\frac{1}{2}) = 0$.\nEt $a = \\frac{1}{2}$ est la médiane recherchée.\n\n## Exercice 3\n\nSupposons que la fonction de répartition d’une variable aléatoire $X$, correspondant au temps en mois avant décès pour une personne atteinte de cancer, est donnée par\n\n$$\nF(x) =\n\\begin{cases}\n0, & x \\le 0, \\\\\n1 - e^{-0.03 x^{1.2}}, & x > 0.\n\\end{cases}\n$$\n\n1) Vérifier que $F$ est bien une fonction de répartition.  \n\n2) Calculer la probabilité de survivre au moins 12 mois.  \n\n3) Donner une densité de $X$.\n\n## Correction Exercice 3\n\n1) La fonction $F$ est croissante, continue sur $\\mathbb{R}$, avec  \n$\\displaystyle \\lim_{x \\to -\\infty} F(x) = 0$  \net  \n$\\displaystyle \\lim_{x \\to +\\infty} F(x) = 1$.  \n\nC’est donc bien une fonction de répartition.\n\n---\n\n2) Nous avons\n\n$$\n\\mathbb{P}(X \\ge 12)\n= 1 - F(12)\n= 55.3\\%.\n$$\n\n---\n\n3) Une densité, obtenue par dérivation, vaut\n\n$$\nf(x) =\n\\begin{cases}\n0, & x \\le 0, \\\\[6pt]\n0.03\\, x^{0.2} e^{-0.03 x^{1.2}}, & x > 0.\n\\end{cases}\n$$\n\n## Exercice 4\n\nSoit la fonction $F$ définie par $F(x) = 1 - \\exp(-x/2)$ pour $x > 0$, et $0$ sinon.\n\n1) Justifier que $F$ est une fonction de répartition.  \n2) Déterminer les quantiles d’ordres 0.25 et 0.75.  \n3) Soit $X$ une variable aléatoire suivant cette loi, calculer $\\mathbb{P}(1 < X \\le 2)$.\n\n---\n\n## Correction Exercice 4\n\n1)  \nUne fonction de répartition doit être croissante, cadlag (continue à droite, limites à gauche), avec\n\n$$\n\\lim_{x \\to -\\infty} F(x) = 0\n\\quad\\text{et}\\quad\n\\lim_{x \\to +\\infty} F(x) = 1.\n$$\n\nLa fonction $F(\\cdot)$ est nulle sur $]-\\infty, 0]$.  \nPour tout $x \\in ]0, +\\infty[$, $F(\\cdot)$ est dérivable en $x$ avec\n\n$$\nF'(x) = \\frac{1}{2} \\exp\\left(-\\frac{x}{2}\\right) > 0,\n$$\n\ndonc la fonction $F(\\cdot)$ est bien croissante.  \nElle est continue sur $\\mathbb{R}$, donc en particulier continue à droite et avec des limites à gauche.  \n\nNous obtenons également facilement que\n\n$$\n\\lim_{x \\to -\\infty} F(x) = 0\n\\quad\\text{et}\\quad\n\\lim_{x \\to +\\infty} F(x) = 1.\n$$\n\n---\n\n2) Quantiles\n\nOn cherche $x = F^{-1}(0.25)$ :\n\n$$\nF(x) = 1 - \\exp\\left(-\\frac{x}{2}\\right) = \\frac14\n\\;\\Longleftrightarrow\\;\n\\exp\\left(-\\frac{x}{2}\\right) = \\frac34\n\\;\\Longleftrightarrow\\;\nx = -2 \\ln\\left(\\frac34\\right).\n$$\n\nEnsuite, pour $x = F^{-1}(0.75)$ :\n\n$$\nF(x) = 1 - \\exp\\left(-\\frac{x}{2}\\right) = \\frac34\n\\;\\Longleftrightarrow\\;\n\\exp\\left(-\\frac{x}{2}\\right) = \\frac14\n\\;\\Longleftrightarrow\\;\nx = -2 \\ln\\left(\\frac14\\right).\n$$\n\n---\n\n3) Probabilité\n\nPar définition de la fonction de répartition,\n\n$$\n\\mathbb{P}(1 < X \\le 2)\n= F(2) - F(1)\n= \\exp\\left(-\\frac{1}{2}\\right) - \\exp(-1).\n= 0.238.\n$$\n\n## Exercice 5\n\nSoient $Y$ et $Z$ deux variables aléatoires réelles.  \nSoit $U$ une variable aléatoire indépendante de $Y$ et de $Z$, suivant une loi de Bernoulli de paramètre $\\alpha > 0$.\n\nSoit $X$ la variable aléatoire définie par\n\n$$\nX =\n\\begin{cases}\nY & \\text{si } U = 1,\\\\[4pt]\nZ & \\text{si } U = 0.\n\\end{cases}\n$$\n\nAutrement dit, $X = Y$ avec probabilité $\\alpha$ et $X = Z$ avec probabilité $1 - \\alpha$.  \nOn dit que la loi de $X$ est un *mélange de deux distributions*, ce que l’on note\n\n$$\n\\mathbb{P}_X = \\alpha \\mathbb{P}_Y + (1 - \\alpha)\\mathbb{P}_Z.\n$$\n\n---\n\n1) Montrer que\n\n$$\nF_X(x) = \\alpha F_Y(x) + (1 - \\alpha)F_Z(x).\n$$\n\n---\n\nSoit $X$ une variable aléatoire de fonction de répartition $F_X$ définie par\n\n$$\nF_X(x) =\n\\begin{cases}\n0 & \\text{si } x < 0,\\\\[4pt]\n\\dfrac{1}{8} & \\text{si } x \\in [0,1[,\\\\[8pt]\n\\dfrac{x + 1}{4} & \\text{si } x \\in [1,2[,\\\\[8pt]\n\\dfrac{3}{4} & \\text{si } x \\in [2,5[,\\\\[8pt]\n1 & \\text{si } x \\ge 5.\n\\end{cases}\n$$\n\n---\n\n2) Représenter graphiquement la fonction $F_X$.\n\n---\n\n3) Exprimer la loi de $X$ sous la forme\n\n$$\n\\mathbb{P}_X = \\alpha \\mathbb{P}_{\\text{dis}} + (1 - \\alpha)\\mathbb{P}_{\\text{cont}},\n$$\n\noù $\\alpha \\in [0,1]$,  \n$\\mathbb{P}_{\\text{dis}}$ est une loi de probabilité discrète,  \net $\\mathbb{P}_{\\text{cont}}$ est une loi de probabilité continue, qu’on déterminera.\n\n### Correction Exercice 5\n\n1) Montrons que $F_X(x) = \\alpha F_Y(x) + (1 - \\alpha)F_Z(x)$.\n\nNous savons que si $U = 1$, alors $X = Y$, et si $U = 0$, alors $X = Z$. De ce fait, pour exprimer la fonction de répartition de $X$, en fonction de celles de $Y$ et $Z$, nous devons parvenir à conditionner la variable aléatoire $X$ en fonction de la variable aléatoire $U$. \n\nPour cela, nous utilisons la formule des probabilités totales, suivi par la formule de Bayes.\n\n$$\nF_X(x) = \\mathbb{P}(X \\le x)\n= \\mathbb{P}(X \\le x \\cap \\Omega)\n= \\mathbb{P}(X \\le x \\cap \\{U = 1\\}) + \\mathbb{P}(X \\le x \\cap \\{U = 0\\}).\n$$\n\nNous utilisons ensuite la formule de Bayes pour chaque terme :\n\n$$\nF_X(x) = \\mathbb{P}(X \\le x \\mid U = 1) \\mathbb{P}(U = 1)\n+ \\mathbb{P}(X \\le x \\mid U = 0) \\mathbb{P}(U = 0).\n$$\n\nOr, par définition de $X$ :\n$$\n\\mathbb{P}(X \\le x \\mid U = 1) = \\mathbb{P}(Y \\le x) = F_Y(x),\n$$\n\net\n$$\n\\mathbb{P}(X \\le x \\mid U = 0) = \\mathbb{P}(Z \\le x) = F_Z(x).\n$$\n\nCar $U$ est indépendante de $Y$ et de $Z$.\n\nDonc,\n\n$$\nF_X(x) = F_Y(x) \\mathbb{P}(U = 1)\n+ F_Z(x) \\mathbb{P}(U = 0).\n$$\n\nComme $U$ suit une loi de Bernoulli de paramètre $\\alpha$, nous avons :\n\n$$\n\\mathbb{P}(U = 1) = \\alpha\n\\quad\\text{et}\\quad\n\\mathbb{P}(U = 0) = 1 - \\alpha.\n$$  \n\nDonc finalement :\n\n$$\nF_X(x) = \\alpha F_Y(x) + (1 - \\alpha) F_Z(x).\n$$\n\n2) Représentation graphique de la fonction $F_X$.\n\n::: {#c9cd24d7 .cell execution_count=8}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef F(x):\n    x = np.asarray(x)\n    return np.where(\n        x < 0, 0,\n        np.where(\n            x < 1, 1/8,\n            np.where(\n                x < 2, (x + 1)/4,\n                np.where(\n                    x < 5, 3/4,\n                    1\n                )\n            )\n        )\n    )\n\n# Segments de la fonction de répartition\nsegments_horizontaux = [\n    (-10, 0, 0),     # de x=-10 à x=0 : F=0\n    (0, 1, 1/8),     # de x=0 à x=1 : F=1/8\n    (2, 5, 3/4),     # de x=2 à x=5 : F=3/4\n    (5, 10, 1)       # de x=5 à x=10 : F=1\n]\n\nplt.figure(figsize=(12, 5))\n\n# Tracé des segments horizontaux\nfor x_start, x_end, y_val in segments_horizontaux:\n    plt.hlines(y_val, x_start, x_end, colors=\"blue\", linewidth=2)\n\n# Tracé du segment linéaire entre x=1 et x=2 : F(x) = (x+1)/4\nx_linear = np.linspace(1, 2, 100)\ny_linear = (x_linear + 1) / 4\nplt.plot(x_linear, y_linear, 'b-', linewidth=2)\n\n# Points ouverts (limite à gauche non incluse)\nx_open = [0, 1, 5]\ny_open = [0, 1/8, 3/4]\nplt.scatter(x_open, y_open, facecolors=\"none\", edgecolors=\"black\", s=80, zorder=3, linewidths=2)\n\n# Points fermés (valeur incluse)\nx_closed = [0, 1, 2, 5]\ny_closed = [1/8, 2/4, 3/4, 1]\nplt.scatter(x_closed, y_closed, color=\"black\", s=80, zorder=3)\n\n# Format graphique\nplt.xlabel(\"x\", fontsize=12)\nplt.ylabel(\"$F_X(x)$\", fontsize=12)\nplt.title(\"Fonction de répartition $F_X(x)$\", fontsize=14)\nplt.grid(True, linestyle=\"--\", alpha=0.4)\nplt.xlim(-10, 10)\nplt.ylim(-0.05, 1.05)\n\n# Définir les graduations sur l'axe des x\nplt.xticks([0, 1, 2, 3, 4, 5])\n\n# Ajout de repères pour les valeurs importantes\nplt.axhline(y=1/8, color='gray', linestyle=':', alpha=0.3)\nplt.axhline(y=1/2, color='gray', linestyle=':', alpha=0.3)\nplt.axhline(y=3/4, color='gray', linestyle=':', alpha=0.3)\nplt.axvline(x=0, color='gray', linestyle=':', alpha=0.3)\nplt.axvline(x=1, color='gray', linestyle=':', alpha=0.3)\nplt.axvline(x=2, color='gray', linestyle=':', alpha=0.3)\nplt.axvline(x=5, color='gray', linestyle=':', alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_gdr_files/figure-html/cell-9-output-1.png){width=1141 height=469}\n:::\n:::\n\n\nLes points de discontinuité sont représentés par des cercles ouverts (limite à gauche non incluse) et des cercles pleins (valeur incluse) : 0, 1 et 5.\n\n3) Expression de la loi de $X$ sous la forme d’un mélange d'une loi discrète et d’une loi continue.\n\nNous savons que si $P_{cont}$ est une loi de probabilité continue, alors $P_{cont}(X = x) = 0$ pour tout $x \\in \\mathbb{R}$.\n\nDe ce fait, si nous devons écrire la loi de $X$ comme un mélange d'une loi discrète et d’une loi continue, ie \n$$\n\\mathbb{P}_X = \\alpha \\mathbb{P}_{dis} + (1 - \\alpha)\\mathbb{P}_{cont},\n$$\n\nalors aux points de discontinuité de la fonction de répartition $F_X$, la partie continue ne contribue pas à la probabilité. C'est-à-dire que pour tout $x \\in {0,1,5}$, nous avons :\n\n$$\n\\mathbb{P}_X(X = x) = \\alpha \\mathbb{P_{dis}(X = x)} + (1 - \\alpha) \\mathbb{P_{cont}(X = x)}\n= \\alpha \\mathbb{P_{dis}(X = x)} + 0\n= \\alpha \\mathbb{P_{dis}(X = x)}.\n$$\n\nDonc, nous\n\n$$\n\\mathbb{P_X(X = 0)} = \\alpha \\mathbb{P_{dis}(X = 0)}\n$$\n\n$$\n\\mathbb{P_X(X = 1)} = \\alpha \\mathbb{P_{dis}(X = 1)}\n$$\n\n$$\n\\mathbb{P_X(X = 5)} = \\alpha \\mathbb{P_{dis}(X = 5)}.\n$$\n\nC'est trois équations ci-dessous nous permettent de déterminer la loi discrète $\\mathbb{P_{dis}}$ et le paramètre $\\alpha$.\n\nEn effet, en sommant les probabilités aux points de discontinuité, nous avons :\n\n$$\n\\mathbb{P_X(X = 0)} + \\mathbb{P_X(X = 1)} + \\mathbb{P_X(X = 5)}\n= \\alpha \\left( \\mathbb{P_{dis}(X = 0)} + \\mathbb{P_{dis}(X = 1)} + \\mathbb{P_{dis}(X = 5)} \\right).\n$$\n\nDonc si nous définissons la fonction discrète $\\mathbb{P_{dis}}$ comme ayant 0, 1 et 5 comme seuls points de masse, nous avons :\n$$\n\\mathbb{P_{dis}(X = 0)} + \\mathbb{P_{dis}(X = 1)} + \\mathbb{P_{dis}(X = 5)} = 1.\n$$\n\nDans ce cas, nous obtenons :\n\n$$\n\\alpha = \\mathbb{P_X(X = 0)} + \\mathbb{P_X(X = 1)} + \\mathbb{P_X(X = 5)}.\n$$\n\nCalculons les probabilités aux points de discontinuité à l'aide de la fonction de répartition $F_X$.\n\n$$\n\\mathbb{P_X(X = 0)} = F_X(0) - F_X(0^-) = \\frac{1}{8} - 0 = \\frac{1}{8}.\n$$\n\n$$\n\\mathbb{P_X(X = 1)} = F_X(1) - F_X(1^-) = \\frac{2}{4} - \\frac{1}{8} = \\frac{3}{8}.\n$$\n\n$$\n\\mathbb{P_X(X = 5)} = F_X(5) - F_X(5^-) = 1 - \\frac{3}{4} = \\frac{1}{4}.\n$$\n\nEt nous obtenons la loi de $P_{dis}$ :\n\n$$\n\\mathbb{P_{dis}(X = 0)} = \\frac{\\frac{1}{8}}{\\alpha} = \\frac{1}{6},\n\\quad\n\\mathbb{P_{dis}(X = 1)} = \\frac{\\frac{3}{8}}{\\alpha} = \\frac{1}{2},\n\\quad\n\\mathbb{P_{dis}(X = 5)} = \\frac{\\frac{1}{4}}{\\alpha} = \\frac{1}{3}.\n$$\n\nLa fonction de répartition correspondante est donc :\n\n$$F_{dis}(x) =\n\\begin{cases}\n0 & \\text{si } x < 0,\\\\[4pt]\n\\dfrac{1}{6} & \\text{si } x \\in [0,1[,\\\\[8pt]\n\\dfrac{2}{3} & \\text{si } x \\in [1,5[,\\\\[8pt]\n1 & \\text{si } x \\ge 5.\n\\end{cases}\n$$\n\nCeci nous permet de trouver facilement la fonction de répartition de la partie continue.\n\nEn effet, nous avons :\n\n$$\nF_X(x) = \\alpha F_{dis}(x) + (1 - \\alpha) F_{cont}(x).\n$$\n\nDonc,\n\n$$\nF_{cont}(x) = \\frac{F_X(x) - \\alpha F_{dis}(x)}{1 - \\alpha}.\n$$\n\nCe qui nous donne après calcul :\n\n$$\nF_{\\text{cont}}(x) =\n\\begin{cases}\n0 & \\text{si } x < 1,\\\\[4pt]\nx - 1 & \\text{si } x \\in [1,2[,\\\\[8pt]\n1 & \\text{si } x \\ge 2.\n\\end{cases}\n$$\n\nOn peut vérifier que cette fonction est bien une fonction de répartition (croissante, continue, avec des limites 0 et 1 aux extrémités). Elle est dérivable sur $\\mathbb{R}$ avec une densité associée donnée par :\n\n$$\nf_{\\text{cont}}(x) =\n\\begin{cases}\n0 & \\text{si } x < 1,\\\\[4pt]\n1 & \\text{si } x \\in [1,2[,\\\\[8pt]\n0 & \\text{si } x \\ge 2.\n\\end{cases}\n$$\n\nCe qui correspond à la densité d'une variable aléatoire uniformément distribuée sur l'intervalle $[1,2]$.\n\n---\n\n",
    "supporting": [
      "index_gdr_files"
    ],
    "filters": [],
    "includes": {}
  }
}