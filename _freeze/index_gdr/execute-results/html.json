{
  "hash": "b2909f64f4e2081ab1a715ff31d7b1b1",
  "result": {
    "engine": "jupyter",
    "markdown": "---\nformat: \n  html: \n    fontsize: 1.3em\n---\n\n\n\n\n<div class=\"box\">\nVous trouverez ici certains exercices de la fiche de TD, qui vous permettront de mieux comprendre les concepts abordés par le professeur pendant les cours. N'hésitez pas à me contacter si vous avez des questions ou des suggestions. Je mettrai l'accent sur la rédaction des corrections.\n</div>\n\n# 1. Espaces probabilisés\n\n## Exercice 2\n\nSupposons que $\\mathbb{P}(A \\cup B) = 0.6$ et $\\mathbb{P}(A \\cup B^c) = 0.8$.  \nDéterminer $\\mathbb{P}(A)$.\n\n---\n\n## Correction\n\nOn se place sur un espace probabilisé $(\\Omega, \\mathcal{F}, \\mathbb{P})$.  \nSoient $A, B \\in \\mathcal{F}$ deux événements.\n\nOn utilise d’abord les propriétés des complémentaires :\n$$\n(A \\cup B)^c = A^c \\cap B^c,\n\\qquad\n(A \\cup B^c)^c = A^c \\cap B.\n$$\n\nLes deux événements $(A \\cup B)^c$ et $(A \\cup B^c)^c$ sont disjoints, car :\n$$\n(A \\cup B)^c \\cup (A \\cup B^c)^c\n= (A^c \\cap B^c) \\cup (A^c \\cap B)\n= A^c.\n$$\n\nDe plus, pour deux événements disjoints $E$ et $F$,\n$$\n\\mathbb{P}(E \\cup F) = \\mathbb{P}(E) + \\mathbb{P}(F).\n$$\n\nPar conséquent,\n$$\n\\mathbb{P}(A^c)\n= \\mathbb{P}\\big( (A \\cup B)^c \\big)\n+ \\mathbb{P}\\big( (A \\cup B^c)^c \\big).\n$$\n\nOr\n$$\n\\mathbb{P}\\big( (A \\cup B)^c \\big)\n= 1 - \\mathbb{P}(A \\cup B)\n= 1 - 0.6\n= 0.4,\n$$\n$$\n\\mathbb{P}\\big( (A \\cup B^c)^c \\big)\n= 1 - \\mathbb{P}(A \\cup B^c)\n= 1 - 0.8\n= 0.2.\n$$\n\nAinsi,\n$$\n\\mathbb{P}(A^c) = 0.4 + 0.2 = 0.6,\n$$\net donc\n$$\n\\mathbb{P}(A) = 1 - \\mathbb{P}(A^c) = 1 - 0.6 = 0.4.\n$$\n\n\n\n\n---\n\n$$\n\\boxed{\\mathbb{P}(A) = 0.4}\n$$\n\n\n## Exercice 3\n\nDans une ville, 75% de la population a les cheveux bruns, 50% a les yeux marron, et 35% possède à la fois des cheveux bruns et des yeux marron.  \nOn sélectionne une personne au hasard dans la ville. Quelle est la probabilité :\n\n- qu’elle ait les yeux marron **ou** des cheveux bruns ?\n- qu’elle n’ait ni les yeux marron, ni des cheveux bruns ?\n\n---\n\n\n\n\n## Correction\n\nOn se place sur un espace probabilisé $(\\Omega, \\mathcal{F}, \\mathbb{P})$ où $\\Omega = \\{\\text{personnes de la ville}\\}$.\n\nOn définit les événements :\n- $B = \\{\\text{la personne a les cheveux bruns}\\}$,\n- $M = \\{\\text{la personne a les yeux marron}\\}$.\n\nD’après l’énoncé :\n$$\n\\mathbb{P}(B) = 0.75, \\qquad \\mathbb{P}(M) = 0.50, \\qquad \\mathbb{P}(B \\cap M) = 0.35.\n$$\n\n---\n\n### 1 Probabilité que la personne ait les yeux marron **ou** des cheveux bruns\n\nOn utilise la formule d’addition :\n$$\n\\mathbb{P}(B \\cup M) = \\mathbb{P}(B) + \\mathbb{P}(M) - \\mathbb{P}(B \\cap M).\n$$\n\nDonc\n$$\n\\mathbb{P}(B \\cup M) = 0.75 + 0.50 - 0.35 = 0.90.\n$$\n\n---\n\n### 2 Probabilité qu’elle n’ait **ni** les yeux marron **ni** des cheveux bruns\n\nOn veut $\\mathbb{P}(B^c \\cap M^c)$.  \nOr\n$$\nB^c \\cap M^c = (B \\cup M)^c.\n$$\n\nDonc\n$$\n\\mathbb{P}(B^c \\cap M^c) = 1 - \\mathbb{P}(B \\cup M) = 1 - 0.90 = 0.10.\n$$\n\n---\n\n### Résultats\n\n$$\n\\boxed{\\mathbb{P}(\\text{yeux marron ou cheveux bruns}) = 0.90}\n$$\n\n$$\n\\boxed{\\mathbb{P}(\\text{ni yeux marron ni cheveux bruns}) = 0.10}\n$$\n\n## Exercice 4\n\nNous lançons 3 dés équilibrés et indépendants.  \nCalculer la probabilité d’obtenir **au moins un 6** :\n\n- en utilisant la formule d’inclusion–exclusion,\n- en calculant la probabilité de **n’obtenir aucun 6**.\n\n---\n\n## Correction Exercice 4\n\nOn se place sur un espace probabilisé $(\\Omega, \\mathcal{F}, \\mathbb{P})$ où la probabilité $\\mathbb{P}$ est uniforme sur $\\Omega$.\n\nL’univers $\\Omega$ peut s’écrire de deux façons équivalentes :\n\n- sous forme cartésienne :\n$$\n\\Omega = \\{1,2,3,4,5,6\\}^3\n$$\n\n- sous forme explicite :\n$$\n\\Omega = \\{(w_1, w_2, w_3) \\mid w_i \\in \\{1,2,3,4,5,6\\},\\ i = 1,2,3\\}.\n$$\n\nOn définit les événements suivants :\n\n- $A_1 = \\{\\text{le premier dé donne un } 6\\}$,\n- $A_2 = \\{\\text{le deuxième dé donne un } 6\\}$,\n- $A_3 = \\{\\text{le troisième dé donne un } 6\\}$.\n\nNous cherchons\n$$\n\\mathbb{P}(A_1 \\cup A_2 \\cup A_3),\n$$\nla probabilité d’obtenir au moins un 6.\n\n---\n\n### 1 Méthode 1 : formule d’inclusion–exclusion\n\nPour trois événements $A_1, A_2, A_3$, on a\n$$\n\\mathbb{P}(A_1 \\cup A_2 \\cup A_3)\n= \\mathbb{P}(A_1) + \\mathbb{P}(A_2) + \\mathbb{P}(A_3)\n- \\mathbb{P}(A_1 \\cap A_2) - \\mathbb{P}(A_1 \\cap A_3) - \\mathbb{P}(A_2 \\cap A_3)\n+ \\mathbb{P}(A_1 \\cap A_2 \\cap A_3).\n$$\n\nComme les dés sont équilibrés et indépendants :\n$$\n\\mathbb{P}(A_1) = \\mathbb{P}(A_2) = \\mathbb{P}(A_3) = \\frac{1}{6},\n$$\n$$\n\\mathbb{P}(A_i \\cap A_j) = \\frac{1}{6} \\times \\frac{1}{6} = \\frac{1}{36}, \\quad \\text{pour } i \\ne j,\n$$\n$$\n\\mathbb{P}(A_1 \\cap A_2 \\cap A_3) = \\left(\\frac{1}{6}\\right)^3 = \\frac{1}{216}.\n$$\n\nDonc :\n$$\n\\mathbb{P}(A_1 \\cup A_2 \\cup A_3)\n= 3 \\times \\frac{1}{6}\n- 3 \\times \\frac{1}{36}\n+ \\frac{1}{216}\n= \\frac{1}{2} - \\frac{1}{12} + \\frac{1}{216}\n= \\frac{91}{216}\n\\simeq 0.421.\n$$\n\n---\n\n### 2 Méthode 2 : en passant par l’événement complémentaire\n\nL’événement “obtenir au moins un 6” est le complément de “n’obtenir aucun 6”.\n\nL’événement “aucun 6” s’écrit :\n$$\n\\{A_1 \\cup A_2 \\cup A_3\\}^c\n= A_1^c \\cap A_2^c \\cap A_3^c,\n$$\noù $A_i^c$ signifie “le $i$-ème dé ne donne pas 6”.\n\nPour un dé, la probabilité de **ne pas** obtenir 6 est\n$$\n\\mathbb{P}(A_i^c) = \\frac{5}{6}.\n$$\n\nPar indépendance des dés :\n$$\n\\mathbb{P}(A_1^c \\cap A_2^c \\cap A_3^c)\n= \\left(\\frac{5}{6}\\right)^3 = \\frac{125}{216}.\n$$\n\nAinsi,\n$$\n\\mathbb{P}(A_1 \\cup A_2 \\cup A_3)\n= 1 - \\mathbb{P}(A_1^c \\cap A_2^c \\cap A_3^c)\n= 1 - \\frac{125}{216}\n= \\frac{91}{216}\n\\simeq 0.421.\n$$\n\n---\n\n### Résultat\n\nDans les deux cas, on obtient :\n$$\n\\boxed{\\mathbb{P}(\\text{obtenir au moins un 6 en 3 lancers}) = \\frac{91}{216} \\approx 0.421.}\n$$\n\n\n\nJe reviendrai ici sur l'exercice 5 que nous n'avons pas eu la possibilité de mettre oeuvre la simulation monte Carlo.\n\n## Correction de l'exercice 5\n\nNous sélectionnons un entier au hasard parmi les N = 5,000 entiers qui sont dans l'intervalle [0, 4999]. \nQuelle est la probabilité qu’il soit divisible par 4,7 ou 10 ?\n\nModifier le code R vu en cours pour estimer cette probabilité par une simulation\nde Monte-Carlo pour vérifier ce résultat. Vous utiliserez au moins 50,000\nsimulations.\n\nPour résoudre cette exercice, il faut d'abord définir l'espace probabilisé. \nL'espace probabilisé est défini par le triplet (Ω, F, P) où:\n\n\n- Ω est l'ensemble des résultats possibles. Dans notre cas, Ω = {0, 1, 2, ..., 4999}.\n\n- F est la σ-algèbre des événements. Ici, nous pouvons considérer les événements comme les sous-ensembles de Ω.\n\n- P est la mesure de probabilité. Dans notre cas, chaque entier a une probabilité égale d'être sélectionné. On dit encore que la loi de probabilité est uniforme sur Ω.\n\nLe cardinal de l'ensemble Ω est |Ω| = 5000.\nLa probabilité d'un élément spécifique dans Ω est donc $P({x}) = \\frac{1}{5000}$ pour tout x dans Ω.\n\nAprès avoir défini l'espace probabilisé, nous pouvons maintenant définir l'événement A que nous voulons étudier. \nDe ce fait, définissons les événements suivants:\n\n- $A_4$ : l'événement que l'entier sélectionné est divisible par 4.\n- $A_7$ : l'événement que l'entier sélectionné est divisible par 7.\n- $A_{10}$ : l'événement que l'entier sélectionné est divisible par 10.\nL'événement A que nous voulons étudier est l'union de ces trois événements:\n\n$$\nA = A_4 \\cup A_7 \\cup A_{10}\n$$\n\nNous cherchons à calculer la probabilité de l'événement A, c'est-à-dire $P(A)$. Pour cela, nous allons utiliser la formule inclusion-exclusion:\n$$\n\\begin{aligned}\n\\mathbb{P}(A_4 \\cup A_7 \\cup A_{10})\n&= \\mathbb{P}(A_4) + \\mathbb{P}(A_7) + \\mathbb{P}(A_{10}) \\\\\n&\\quad - \\big\\{\\, \\mathbb{P}(A_4 \\cap A_7) + \\mathbb{P}(A_4 \\cap A_{10}) + \\mathbb{P}(A_7 \\cap A_{10}) \\,\\big\\} \\\\\n&\\quad + \\mathbb{P}(A_4 \\cap A_7 \\cap A_{10}).\n\\end{aligned}\n$$\n\nCalculons maintenant chaque terme de cette formule:\nPour calculer chaque probabilité, nous devons compter le nombre d'entiers dans Ω qui satisfont chaque condition $A_i$.\nEt la probabilité de chaque événement est donnée par le rapport du nombre d'entiers satisfaisant la condition sur le cardinal de l'ensemble Ω : \n\n$$\nP(A_i) = \\frac{\\text{nombre d'entiers satisfaisant } A_i}{5000}.\n$$\n\n\n- Pour $A_4$ : Les entiers divisibles par 4 sont 0, 4, 8, ..., 4996. Ainsi le nombre d'entiers divisibles par 4 est de la forme $4k$ et qui vérifient $0 \\leq 4(k+1) \\leq 5000 < 4(k+2)$, donc de cette inégalité, on déduit que $k$ vérifie l'inégalité sous dessous :\n\n$$\nk \\leq \\frac{4999}{4} < k+1.\n$$\n\nDonc le nombre d'entiers divisibles par 4 correspond à la partie entière de $\\frac{4999}{4}$ plus 1 (pour inclure le 0), soit:\n$$\n\\text{nombre d'entiers divisibles par 4} = \\left\\lfloor\\frac{4999}{4}\\right\\rfloor + 1 = 1249 + 1 = 1250.\n$$\n\nDonc, la probabilité que l'entier sélectionné soit divisible par 4 est:\n$$\nP(A_4) = \\frac{1250}{5000} = 0.25.\n$$\n\n- Pour $A_7$ : Les entiers divisibles par 7 sont 0, 7, 14, ..., 4996. En suivant le même raisonnement que pour $A_4$, nous trouvons:\n\n$$\n\\text{nombre d'entiers divisibles par 7} = \\left\\lfloor\\frac{4999}{7}\\right\\rfloor + 1 = 714 + 1 = 715.\n$$\n\nDonc, la probabilité que l'entier sélectionné soit divisible par 7 est:\n$$\nP(A_7) = \\frac{715}{5000} = 0.143.\n$$\n\n- Pour $A_{10}$ : Les entiers divisibles par 10 sont 0, 10, 20, ..., 4990. En suivant le même raisonnement que pour $A_4$, nous trouvons:\n$$\n\\text{nombre d'entiers divisibles par 10} = \\left\\lfloor\\frac{4999}{10}\\right\\rfloor + 1 = 499 + 1 = 500.\n$$\n\nDonc, la probabilité que l'entier sélectionné soit divisible par 10 est:\n$$\nP(A_{10}) = \\frac{500}{5000} = 0.1.\n$$  \n\n- Pour $A_4 \\cap A_7$ : Les entiers qui sont divisibles par 4 et 7 sont ceux qui sont divisibles par 28 qui est le PPCM de 4 et 7. En suivant le même raisonnement que pour $A_4$, nous trouvons:\n$$\n\\text{nombre d'entiers divisibles par 28} = \\left\\lfloor\\frac{4999}{28}\\right\\rfloor + 1 = 178 + 1 = 179.\n$$\n\nDonc, la probabilité que l'entier sélectionné soit divisible par 4 et 7 est:\n\n$$\nP(A_4 \\cap A_7) = \\frac{179}{5000} = 0.0358.\n$$\n\n- Pour $A_4 \\cap A_{10}$ : Les entiers qui sont divisibles par 4 et 10 sont ceux qui sont divisibles par 20 qui est le PPCM de 4 et 10. En suivant le même raisonnement que pour $A_4, nous trouvons:\n$$\n\\text{nombre d'entiers divisibles par 20} = \\left\\lfloor\\frac{4999}{20}\\right\\rfloor + 1 = 249 + 1 = 250.\n$$\n\nDonc, la probabilité que l'entier sélectionné soit divisible par 4 et 10 est:\n$$\nP(A_4 \\cap A_{10}) = \\frac{250}{5000} = 0.05.\n$$\n\n- Pour $A_7 \\cap A_{10}$ : Les entiers qui sont divisibles par 7 et 10 sont ceux qui sont divisibles par 70 qui est le PPCM de 7 et 10. En suivant le même raisonnement que pour $A_4, nous trouvons:\n\n$$\n\\text{nombre d'entiers divisibles par 70} = \\left\\lfloor\\frac{4999}{70}\\right\\rfloor + 1 = 71 + 1 = 72.\n$$\n\nDonc, la probabilité que l'entier sélectionné soit divisible par 7 et 10 est:\n$$\nP(A_7 \\cap A_{10}) = \\frac{72}{5000} = 0.0144.\n$$\n\n- Pour $A_4 \\cap A_7 \\cap A_{10}$ : Les entiers qui sont divisibles par 4, 7 et 10 sont ceux qui sont divisibles par 140 qui est le PPCM de 4, 7 et 10. En suivant le même raisonnement que pour $A_4$, nous trouvons:\n\n$$\n\\text{nombre d'entiers divisibles par 140} = \\left\\lfloor\\frac{4999}{140}\\right\\rfloor + 1 = 35 + 1 = 36.\n$$\n\nDonc, la probabilité que l'entier sélectionné soit divisible par 4, 7 et 10 est:\n$$\nP(A_4 \\cap A_7 \\cap A_{10}) = \\frac{36}{5000} = 0.0072.\n$$  \n\nEn substituant ces valeurs dans la formule d'inclusion-exclusion, nous obtenons:\n$$\n\\begin{aligned}\nP(A)\n&= P(A_4) + P(A_7) + P(A_{10}) \\\\\n&\\quad - \\big\\{\\, P(A_4 \\cap A_7) + P(A_4 \\cap A_{10}) + P(A_7 \\cap A_{10}) \\,\\big\\} \\\\\n&\\quad + P(A_4 \\cap A_7 \\cap A_{10}) \\\\\n&= 0.25 + 0.143 + 0.1 \\\\\n&\\quad - \\big\\{\\, 0.0358 + 0.05 + 0.0144 \\,\\big\\} \\\\\n&\\quad + 0.0072 \\\\\n&= 0.493 - 0.1002 + 0.0072 \\\\\n&= 0.4.\n\\end{aligned}\n$$  \n\nDonc, la probabilité qu'un entier sélectionné au hasard parmi les 5000 entiers soit divisible par 4, 7 ou 10 est de 0.4 ou 40%.\n\nMaintenant, nous allons vérifier ce résultat par une simulation de Monte-Carlo en R avec au moins 50,000 simulations.\nUne simulation de Monte-Carlo est une méthode statistique qui utilise des échantillons aléatoires pour estimer des propriétés mathématiques ou physiques comme des espérances, des intégrales ou des probabilités. Cette méthode fonctionne en générant un grand nombre de scénarios aléatoires et en observant les résultats pour obtenir une estimation statistique :\n\n- On fixe le nombre de simulations, disons n = 50000.\n- On initialise un compteur pour le nombre de succès (entiers divisibles par 4, 7 ou 10).\n- Pour chaque simulation, on génère un entier aléatoire entre 0 et 4999.\n- On vérifie si cet entier est divisible par 4, 7 ou 10. Si c'est le cas, on incrémente le compteur de succès.\n- Après avoir effectué toutes les simulations, on calcule la probabilité estimée comme le ratio du nombre de succès sur le nombre total de simulations.\n\nVoici un exemple de code R pour effectuer cette simulation de Monte-Carlo :\n\n::: {#2bf73255 .cell execution_count=1}\n``` {.python .cell-code}\n# Monte Carlo in Python with running estimate and plot (matplotlib only)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# --- Parameters (mirror your R snippet) ---\nseed = 123\nN = 4999               # sample from 0..N (inclusive)\nM = 50_000             # number of simulations\n\nrng = np.random.default_rng(seed)\nx = rng.integers(low=0, high=N+1, size=M)  # uniform integers in [0, N]\n\nis_div = (x % 4 == 0) | (x % 7 == 0) | (x % 10 == 0)\n\n# Final Monte Carlo estimate (same as R's mean(is_div))\np_hat = is_div.mean()\n\n# Running estimates vs number of simulations\nrunning_est = np.cumsum(is_div) / np.arange(1, M + 1)\n\n# Mean of the running estimates (to draw an horizontal reference line)\nmean_running = running_est.mean()\n\n# --- Plot ---\nplt.figure(figsize=(7.24, 4.07), dpi=100)  # ~724x407 px\n\nplt.plot(np.arange(1, M + 1), running_est, linewidth=2)\n# Add the colored horizontal line at mean_running\nplt.axhline(mean_running, linestyle=\"--\", linewidth=2, color=\"red\")\nplt.xlabel(\"Nombre de simulations\")\nplt.ylabel(\"Estimation de la probabilité\")\nplt.title(\"Convergence de l’estimation Monte-Carlo\", fontsize=14, weight=\"bold\")\nplt.suptitle(\n    \"Courbe de l’estimation cumulée; ligne horizontale = moyenne des estimations\",\n    fontsize=10, color=\"gray\"\n)\n\n# Style cues similar to the provided seaborn example\nax = plt.gca()\nfor spine in [\"top\", \"right\"]:\n    ax.spines[spine].set_visible(False)\nplt.grid(False)\n\nplt.tight_layout()\nplt.show()\n\np_hat\n\n```\n\n::: {.cell-output .cell-output-display}\n![](index_gdr_files/figure-html/cell-2-output-1.png){width=714 height=403}\n:::\n\n::: {.cell-output .cell-output-display execution_count=1}\n```\nnp.float64(0.3965)\n```\n:::\n:::\n\n\n# 2. Probabilité conditionnelle et indépendance\n\n## Exercice 2\n\nUne classe contient un total de 108 étudiants. Parmi ceux-ci, 36 indiquent qu’ils pratiquent un sport de haut niveau, et 21 étudiants parmi ces 36 indiquent qu’ils préfèrent s’entraîner le matin plutôt que l’après-midi.  \nParmi les étudiants qui ne pratiquent pas un sport de haut niveau, 24 indiquent qu’ils préfèrent pratiquer le sport le matin plutôt que l’après-midi.\n\nPour un étudiant pris au hasard dans la classe, calculer les probabilités suivantes :\n\n- probabilité d’être un sportif de haut niveau sachant que l’on préfère s’entraîner le matin ;\n- probabilité de préférer s’entraîner le matin sachant qu’on ne pratique pas un sport de haut niveau ;\n- probabilité de ne pas pratiquer un sport de haut niveau sachant que l’on préfère s’entraîner l’après-midi.\n\n---\n\n## Correction Exercice 2\n\nOn se place sur un espace probabilisé $(\\Omega, \\mathcal{F}, \\mathbb{P})$, où  \n$\\Omega = \\{\\text{ensemble des étudiants de la classe}\\}$,  \n$\\mathcal{F} = \\mathcal{P}(\\Omega)$ (toutes les parties de $\\Omega$)  \net $\\mathbb{P}$ est la probabilité uniforme sur $\\Omega$ (tous les étudiants sont équiprobables).\n\nNous notons :\n\n- $H = \\{\\text{l’étudiant pratique un sport de haut niveau}\\}$,\n- $M = \\{\\text{l’étudiant préfère s’entraîner le matin}\\}$.\n\nD’après l’énoncé, nous avons\n$$\n\\mathbb{P}(H) = \\frac{36}{108} = \\frac{1}{3},\n\\qquad\n\\mathbb{P}(M \\mid H) = \\frac{21}{36} = \\frac{7}{12},\n\\qquad\n\\mathbb{P}(M \\mid H^c) = \\frac{24}{72} = \\frac{1}{3}.\n$$\n\n---\n\n### 1 Probabilité d’être un sportif de haut niveau sachant que l’on préfère s’entraîner le matin\n\nOn cherche $\\mathbb{P}(H \\mid M)$.\n\nEn appliquant la formule de Bayes :\n$$\n\\mathbb{P}(H \\mid M) = \\frac{\\mathbb{P}(M \\mid H)\\mathbb{P}(H)}{\\mathbb{P}(M \\mid H)\\mathbb{P}(H) + \\mathbb{P}(M \\mid H^c)\\mathbb{P}(H^c)}.\n$$\n\nEn remplaçant par les valeurs numériques :\n$$\n\\mathbb{P}(H \\mid M)\n= \\frac{\\frac{7}{12} \\times \\frac{1}{3}}\n{\\frac{7}{12} \\times \\frac{1}{3} + \\frac{1}{3} \\times \\frac{2}{3}}\n= \\frac{\\frac{7}{36}}{\\frac{7}{36} + \\frac{2}{9}}\n= \\frac{7}{15}.\n$$\n\n---\n\n### 2 Probabilité de préférer s’entraîner le matin sachant qu’on ne pratique pas un sport de haut niveau\n\nCette probabilité est directement donnée par l’énoncé :\n$$\n\\mathbb{P}(M \\mid H^c) = \\frac{1}{3}.\n$$\n\n---\n\n### 3 Probabilité de ne pas pratiquer un sport de haut niveau sachant que l’on préfère s’entraîner l’après-midi\n\nOn cherche $\\mathbb{P}(H^c \\mid M^c)$.\n\nOn applique encore une fois la formule de Bayes :\n$$\n\\mathbb{P}(H^c \\mid M^c)\n= \\frac{\\mathbb{P}(M^c \\mid H^c)\\mathbb{P}(H^c)}\n{\\mathbb{P}(M^c \\mid H^c)\\mathbb{P}(H^c) + \\mathbb{P}(M^c \\mid H)\\mathbb{P}(H)}.\n$$\n\nSachant que\n$$\n\\mathbb{P}(M^c \\mid H^c) = \\frac{2}{3},\n\\qquad\n\\mathbb{P}(M^c \\mid H) = \\frac{5}{12},\n\\qquad\n\\mathbb{P}(H^c) = \\frac{2}{3},\n\\qquad\n\\mathbb{P}(H) = \\frac{1}{3},\n$$\n\nnous obtenons\n$$\n\\mathbb{P}(H^c \\mid M^c)\n= \\frac{\\frac{2}{3} \\times \\frac{2}{3}}\n{\\frac{2}{3} \\times \\frac{2}{3} + \\frac{5}{12} \\times \\frac{1}{3}}\n= \\frac{\\frac{4}{9}}{\\frac{4}{9} + \\frac{5}{36}}\n= \\frac{16}{21}.\n$$\n\n---\n\n###  Résumé des résultats\n\n$$\n\\boxed{\\mathbb{P}(H \\mid M) = \\frac{7}{15}}\n$$\n$$\n\\boxed{\\mathbb{P}(M \\mid H^c) = \\frac{1}{3}}\n$$\n$$\n\\boxed{\\mathbb{P}(H^c \\mid M^c) = \\frac{16}{21}}\n$$\n\n## Exercice 3\n\nPour chacune des deux affirmations suivantes, montrer qu’elle est vraie pour tous les événements $A$ et $B$ pour lesquels les probabilités conditionnelles sont calculables, ou donner un contre-exemple.\n\n$\\mathbb{P}(A \\mid B) + \\mathbb{P}(A \\mid B^c) = 1.$\n\n$\\mathbb{P}(A \\mid B) + \\mathbb{P}(A^c \\mid B) = 1.$\n\n## Correction Exercice 3\n\n1. Pour la première affirmation, nous avons en utilisant la formule de Bayes et en supposant que $\\mathbb{P}(B) > 0$ et $\\mathbb{P}(B^c) > 0$ :\n\n$$\n\\mathbb{P}(A \\mid B)  = \\frac{\\mathbb{P}(A \\cap B)}{\\mathbb{P}(B)},\n\\qquad \\text{et} \\qquad\n\\mathbb{P}(A \\mid B^c)  = \\frac{\\mathbb{P}(A \\cap B^c)}{\\mathbb{P}(B^c)}.\n$$\n\nDe ce fait, si nous additionnons les deux probabilités conditionnelles, nous obtenons :\n$$\n\\mathbb{P}(A \\mid B) + \\mathbb{P}(A \\mid B^c)\n= \\frac{\\mathbb{P}(A \\cap B)}{\\mathbb{P}(B)} + \\frac{\\mathbb{P}(A \\cap B^c)}{\\mathbb{P}(B^c)}.\n$$\n\nSi on se met dans une situation où $\\mathbb{P}(B) = \\mathbb{P}(B^c) = 0.5$, et en utilisant le fait que $\\mathbb{P}(A \\cap B) + \\mathbb{P}(A \\cap B^c) = \\mathbb{P}(A)$, nous obtenons :\n$$\n\\mathbb{P}(A \\mid B) + \\mathbb{P}(A \\mid B^c) = 2 \\mathbb{P}(A).\n$$\n\nDonc, cette affirmation est fausse dès que $\\mathbb{P}(A) \\neq 0.5$. Et il est facile de trouver un contre-exemple, c'est-à-dire une expérience aléatoire où cette condition n'est pas satisfaite.\n\nEn effet, si nous considérons une expérience aléatoire consistant à lancer un dé équilibré à six faces, et nous définissons les événements suivants :\n- $A$ : l'événement que le résultat est un 6,\n- $B$ : l'événement que le résultat est un nombre pair.\n\nNous avons :\n$$\n\\mathbb{P}(B) = \\frac{3}{6} = 0.5 = \\mathbb{P}(B^c),\n$$\net \n$$\n\\mathbb{P}(A) = \\frac{1}{6} \\neq 0.5.\n$$\n\nEt dans ce cas l'affirmation ne tient pas.\n\n2. La seconde affirmation est toujours vraie. On peut le démontrer en utilisant la formule de Bayes :\n$$\n\\mathbb{P}(A \\mid B) = \\frac{\\mathbb{P}(A \\cap B)}{\\mathbb{P}(B)},\n\\qquad \\text{et} \\qquad\n\\mathbb{P}(A^c \\mid B) = \\frac{\\mathbb{P}(A^c \\cap B)}{\\mathbb{P}(B)}.\n$$\n\nEn additionnant les deux probabilités conditionnelles, nous obtenons :\n$$\n\\mathbb{P}(A \\mid B) + \\mathbb{P}(A^c \\mid B)\n= \\frac{\\mathbb{P}(A \\cap B)}{\\mathbb{P}(B)} + \\frac{\\mathbb{P}(A^c \\cap B)}{\\mathbb{P}(B)}\n= \\frac{\\mathbb{P}((A \\cap B) \\cup (A^c \\cap B))}{\\mathbb{P}(B)}\n= \\frac{\\mathbb{P}(B)}{\\mathbb{P}(B)} = 1.\n$$\n\nOu tout simplement en remarquant que $P(.|B)$ définit une probabilité sur l'espace probabilisé restreint à l'événement B, et donc la somme des probabilités d'un événement et de son complément est égale à 1.\n\n## Exercice 4\n\nUn jeu de 32 cartes contient une carte manquante. Nous sélectionnons au hasard une carte parmi les 31 cartes restantes.  \nCalculer la probabilité que la carte tirée soit un cœur, en utilisant un conditionnement sur la carte manquante.\n\n---\n\n## Correction Exercice 4\n\nOn se place sur un espace probabilisé $(\\Omega,\\mathcal{F},\\mathbb{P})$ défini ainsi :\n\n- $\\Omega$ est l’ensemble des couples\n  $$\n  \\Omega = \\{(c_{\\text{manq}}, c_{\\text{tirée}})\\mid c_{\\text{manq}}, c_{\\text{tirée}} \\text{ sont des cartes du jeu de 32, } c_{\\text{tirée}}\\neq c_{\\text{manq}}\\}.\n  $$\n  Autrement dit, on choisit d’abord la carte manquante, puis on tire une carte parmi les 31 restantes.\n- Chaque couple possible est supposé équiprobable, donc\n  $$\n  \\mathbb{P}(\\{(c_{\\text{manq}}, c_{\\text{tirée}})\\}) = \\frac{1}{32\\times 31}.\n  $$\n\nOn introduit les événements suivants :\n\n- $M = \\{\\text{la carte manquante est un cœur}\\}$,\n- $T = \\{\\text{la carte tirée est un cœur}\\}$.\n\nDans un jeu de 32 cartes, il y a 8 cœurs au total.\n\nSi vous avez des difficultés pour définir l’espace probabilisé, je vous conseille de définir uniquement les événements $M$ et $T$ et de raisonner en termes de probabilités conditionnelles.\n\n---\n\n### 1 Probabilités conditionnelles\n\n- Si la carte manquante est un cœur (événement $M$), il reste $7$ cœurs parmi les $31$ cartes encore présentes, donc\n  $$\n  \\mathbb{P}(T \\mid M) = \\frac{7}{31}.\n  $$\n\n- Si la carte manquante n’est **pas** un cœur (événement $M^c$), il reste toujours les $8$ cœurs dans les $31$ cartes présentes, donc\n  $$\n  \\mathbb{P}(T \\mid M^c) = \\frac{8}{31}.\n  $$\n\nPar ailleurs, la carte manquante est choisie au hasard parmi les 32 cartes :\n\n- il y a 8 cœurs possibles pour la carte manquante, donc\n  $$\n  \\mathbb{P}(M) = \\frac{8}{32} = \\frac{1}{4},\n  $$\n- et donc\n  $$\n  \\mathbb{P}(M^c) = 1 - \\mathbb{P}(M) = \\frac{3}{4}.\n  $$\n\n---\n\n\n### 2 Application de la formule des probabilités totales\n\nOn veut calculer $\\mathbb{P}(T)$, la probabilité que la carte tirée soit un cœur.  \nOn conditionne par l’événement $M$ (la carte manquante est un cœur ou non) :\n\n$$\n\\mathbb{P}(T)\n= \\mathbb{P}(T \\mid M)\\mathbb{P}(M)\n+ \\mathbb{P}(T \\mid M^c)\\mathbb{P}(M^c).\n$$\n\nEn remplaçant par les valeurs trouvées :\n\n$$\n\\mathbb{P}(T)\n= \\frac{7}{31} \\times \\frac{1}{4}\n+ \\frac{8}{31} \\times \\frac{3}{4}\n= \\frac{7}{124} + \\frac{24}{124}\n= \\frac{31}{124}\n= \\frac{1}{4}.\n$$\n\n---\n\n### Résultat\n\nLa probabilité que la carte tirée soit un cœur vaut\n$$\n\\boxed{\\mathbb{P}(T) = \\frac{1}{4}}.\n$$\n\nOn retrouve d’ailleurs un résultat intuitif : en moyenne, retirer une carte au hasard puis tirer parmi les restantes ne modifie pas la probabilité d’obtenir un cœur, qui reste $8/32 = 1/4$.\n\n\n::: {.byline}\n<span class=\"date\">May 28, 2025</span>\n:::\n\n# Rappels de la séance précédente\n\nL'objectif de la prémière séance de TD était de consolider vos connaissances sur les espaces probabilisés et sur les probabilités conditionnelles.\n\nVous devez actuellement être capable de :\n\n- Définir un espace probabilisé (Ω, F, P).\n\n- Utiliser les propriétés des probabilités pour calculer des probabilités d'événements simples et composés.\n\n- Vous devez maitriser les lois de Morgan :\n\n  - La loi de Morgan pour l'union : $\\overline{A \\cup B} = \\overline{A} \\cap \\overline{B}$\n  - La loi de Morgan pour l'intersection : $\\overline{A \\cap B} = \\overline{A} \\cup \\overline{B}$\n\n- Appliquer la formule d'inclusion-exclusion pour calculer la probabilité de l'union de deux événements :\n\n  $$\n  P(A \\cup B) = P(A) + P(B) - P(A \\cap B)\n  $$\n\n- Appliquer la formule d'inclusion-exclusion pour calculer la probabilité de l'union de plusieurs événements :\n\n  $$\n  \\begin{aligned}\n  P\\left(\\bigcup_{i=1}^{n} A_i\\right) &= \\sum_{i=1}^{n} P(A_i) - \\sum_{1 \\leq i < j \\leq n} P(A_i \\cap A_j) \\\\\n  &\\quad + \\sum_{1 \\leq i < j < k \\leq n} P(A_i \\cap A_j \\cap A_k) - \\ldots + (-1)^{n+1} P(A_1 \\cap A_2 \\cap \\ldots \\cap A_n)\n  \\end{aligned}\n  $$\n- Calculer des probabilités conditionnelles en utilisant la formule de Bayes :\n\n$$\n  P(A|B) = \\frac{P(A \\cap B)}{P(B)}.\n$$\n\nou bien encore :\n\n$$\n  P(A\\cap B) = P(A|B) \\cdot P(B).\n$$\n\n- Appliquer la loi des probabilités totales pour décomposer des probabilités complexes en utilisant des événements disjoints et exhaustifs :\n\nPar exemple, si $C_1, C_2$ sont deux événements disjoints et exhaustifs, c'est-à-dire que $C_1 \\cap C_2 = \\emptyset$ et $C_1 \\cup C_2 = \\Omega$, alors pour tout événement A, on a :\n\n$$\nP(A) = P(A|C_1) \\cdot P(C_1) + P(A|C_2) \\cdot P(C_2).\n$$\n\nEt finalement, cela nous permet de calculer la probabilité de $C_1$ sachant A en utilisant la formule de Bayes :\n\n$$\nP(C_1|A) = \\frac{P(A|C_1) \\cdot P(C_1)}{P(A|C_1) \\cdot P(C_1) + P(A|C_2) \\cdot P(C_2)}.\n$$\n\n\n\n\n\n```{mermaid}\nflowchart LR\n  A[Espaces probabilisés] --> B(Variable aléatoire X)\n  B --> C{type}\n  C --> D[Discrète]\n  C --> E[Continue]\n  D --> F[Caractérisation de la loi de X]\n  E --> G[Caractérisation de la loi de X]\n```\n\n\n\n\n# Variables aléatoires générales\n\n\n## Exercice 1\n\nNous considérons l’expérience aléatoire consistant à lancer 3 fois une pièce de monnaie. Soit $X$ la variable aléatoire donnant le nombre de « Pile » obtenu.\n\n1) Donner la loi de la variable aléatoire $X$.\n2) Donner la fonction de répartition de $X$, et la représenter graphiquement.\n3) Calculer l’espérance, la variance et le coefficient de variation de $X$.\n\n## Correction de l'exercice 1\n\n1) Nous considérons l'espace probabilisé $(\\Omega, \\mathcal{F}, P)$ associé à l'expérience aléatoire de lancer 3 fois une pièce de monnaie. L'ensemble des résultats possibles $\\Omega$ est donné par :\n\n$$\n\\Omega = \\{PPP, PPF, PFP, PFF, FPP, FPF, FFP, FFF\\} = \\{P, F\\}^3\n$$\n\nNous munissons cet espace de la tribu $\\mathcal{F}$ des parties de $\\Omega$ et de la probabilité uniforme $P$.\n\nLa variable aléatoire $X$ est définie comme le nombre de « Pile » obtenus lors des 3 lancers. Ainsi, $X$ peut prendre les valeurs suivantes :\n- $X = 0$ : lorsque le résultat est FFF\n- $X = 1$ : lorsque le résultat est PFF, FPF, ou FFP\n- $X = 2$ : lorsque le résultat est PPF, PFP, ou FPP\n- $X = 3$ : lorsque le résultat est PPP\n\nCette variable est donc définie comme suit :\n\n\n$$\nX : \\Omega^3 \\longrightarrow \\Omega' = \\{0,1,2,3\\}\n$$\n\n$$\n(\\omega_1, \\omega_2, \\omega_3) \\longmapsto\n\\begin{cases}\n0 & \\text{si } (\\omega_1, \\omega_2, \\omega_3) = (FFF),\\\\[4pt]\n1 & \\text{si } (\\omega_1, \\omega_2, \\omega_3) \\in \\{(PFF),(FPF),(FFP)\\},\\\\[4pt]\n2 & \\text{si } (\\omega_1, \\omega_2, \\omega_3) \\in \\{(PPF),(PFP),(FPP)\\},\\\\[4pt]\n3 & \\text{si } (\\omega_1, \\omega_2, \\omega_3) = (PPP).\n\\end{cases}\n$$\n\nComme la variable aléatoire X est **discrète**, il suffit de déterminer la probabilité $\\mathbb{P}_X$ sur les singletons.\n\n$$\n\\begin{aligned}\n\\mathbb{P}_X(0) &= \\frac{1}{8},\\\n\\mathbb{P}_X(1) &= \\frac{3}{8},\\\n\\mathbb{P}_X(2) &= \\frac{3}{8},\\\n\\mathbb{P}_X(3) &= \\frac{1}{8}.\n\\end{aligned}\n$$\n\nAinsi, ( X ) suit une **loi binomiale** :\n$$\nX \\sim \\mathcal{B}(n=3,, p=\\tfrac{1}{2}).\n$$\n\n\n\n2. Fonction de répartition\n\nÀ partir de la définition de la fonction de répartition :\n$$\nF_X(x) = \\mathbb{P}(X \\le x),\n$$\non obtient :\n\n$$\nF_X(x) =\n\\begin{cases}\n0 & \\text{si } x < 0,\\\\[4pt]\n\\frac{1}{8} & \\text{si } 0 \\le x < 1,\\\\[4pt]\n\\frac{4}{8} = \\frac{1}{2} & \\text{si } 1 \\le x < 2,\\\\[4pt]\n\\frac{7}{8} & \\text{si } 2 \\le x < 3,\\\\[4pt]\n1 & \\text{si } 3 \\le x.\n\\end{cases}\n$$\n\nRéprésentation graphique de la fonction de répartition :\n\n::: {#329ef285 .cell execution_count=2}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef F(x):\n    x = np.asarray(x)\n    return np.where(\n        x < 0, 0,\n        np.where(\n            x < 1, 1/8,\n            np.where(\n                x < 2, 4/8,\n                np.where(\n                    x < 3, 7/8,\n                    1\n                )\n            )\n        )\n    )\n\n# Segments horizontaux de la fonction de répartition\nsegments = [\n    (-1, 0, 0),   # de x=-1 à x=0 : F=0\n    (0, 1, 1/8),\n    (1, 2, 4/8),\n    (2, 3, 7/8),\n    (3, 4, 1)\n]\n\nplt.figure(figsize=(6,4))\n\n# Tracé des segments horizontaux uniquement\nfor x_start, x_end, y_val in segments:\n    plt.hlines(y_val, x_start, x_end, colors=\"blue\")\n\n# Points ouverts (limite à gauche non incluse)\nx_open  = [0,   1,   2,   3]\ny_open  = [0, 1/8, 4/8, 7/8]\nplt.scatter(x_open, y_open, facecolors=\"none\", edgecolors=\"black\", s=60, zorder=3)\n\n# Points fermés (valeur incluse)\nx_closed = [0,   1,   2,   3]\ny_closed = [1/8, 4/8, 7/8, 1]\nplt.scatter(x_closed, y_closed, color=\"black\", s=60, zorder=3)\n\n# Format graphique\nplt.xlabel(\"x\")\nplt.ylabel(\"F(x)\")\nplt.title(\"Fonction de répartition \")\nplt.grid(True, linestyle=\"--\", alpha=0.4)\nplt.xlim(-1, 4)\nplt.ylim(-0.05, 1.05)\n\nplt.show()\n\n```\n\n::: {.cell-output .cell-output-display}\n![](index_gdr_files/figure-html/cell-3-output-1.png){width=519 height=377}\n:::\n:::\n\n\n3. Espérance, variance et coefficient de variation\n\nPour calculer l'espérance, la variance et le coefficient de variation de la variable aléatoire $X$, nous allons utiliser la formule de transfert en temps discret :\n\n$$\n\\mathbb{E}[g(X)] = \\sum_{i} g(x_i) \\cdot \\mathbb{P}_X(x_i),\n$$\noù $g$ est une fonction mésurable.\n\nAinsi, pour l'espérance, nous avons :\n$$\n\\mathbb{E}[X] = \\sum_{x=0}^{3} x \\cdot \\mathbb{P}_X(x) = 0 \\cdot \\frac{1}{8} + 1 \\cdot \\frac{3}{8} + 2 \\cdot \\frac{3}{8} + 3 \\cdot \\frac{1}{8} = \\frac{12}{8} = np =1.5.\n$$\n\nPour la variance, nous utilisons la formule :\n\n$$\n\\text{Var}(X) = \\sum_{x=0}^{3} (x - \\mathbb{E}[X])^2 \\cdot \\mathbb{P}_X(x) = (0 - 1.5)^2 \\cdot \\frac{1}{8} + (1 - 1.5)^2 \\cdot \\frac{3}{8} + (2 - 1.5)^2 \\cdot \\frac{3}{8} + (3 - 1.5)^2 \\cdot \\frac{1}{8} = \\frac{6}{8} = np(1-p) = 0.75.\n$$\n\nEnfin, le coefficient de variation est donné par la formule :\n\n$$\nCV(X) = \\frac{\\sigma_X}{\\mathbb{E}[X]} = \\frac{\\sqrt{\\text{Var}(X)}}{\\mathbb{E}[X]} = \\frac{\\sqrt{0.75}}{1.5} = \\frac{\\sqrt{3}}{3} \\approx 0.577.\n$$\n\n# Exercice 2\n\nNous considérons l’expérience aléatoire consistant à jeter un dé jusqu’à obtenir un **« 6 »**. Soit $X$ la variable aléatoire donnant le nombre de jets.\n\n1. Donner la loi de la variable aléatoire $X$.\n2. Donner la fonction de répartition de $X$, et la représenter graphiquement.\n3. Montrer que la fonction génératrice des moments de $X$ vaut :\n\n   $$\n   M_X(t) = \\frac{pe^t}{1 - e^t(1 - p)},\n   $$\n\n   avec $p = \\frac{1}{6}$.\n    \n4. En déduire l’espérance et la variance de $X$, puis son coefficient de variation.\n\n## Correction de l'exercice 2\n\n1) Loi de la variable aléatoire X\n\nNous considérons l'espace probabilisé $(\\Omega^{\\mathbb{N}}, \\mathcal{P}(\\Omega^{\\mathbb{N}}), \\mathbb{P})$ avec $\\Omega = \\{1,2,3,4,5,6\\}$ muni de la tribu produit et de la probabilité uniforme $\\mathbb{P}$.\n\nL'univers $\\Omega^{\\mathbb{N}}$ correspond à l'ensemble des suites infinies de résultats de lancers de dé. Autrement dit on peut l'écrire comme l'ensemble des suites infinies :\n$$\n\\Omega^{\\mathbb{N}} = \\{(\\omega_1, \\omega_2, \\ldots) \\mid \\omega_i \\in \\Omega = \\{1,2,3,4,5,6\\} \\text{ pour tout } i \\in \\mathbb{N}\\}.\n$$\n\n La variable aléatoire étudiée est\n\n$$\nX : \\Omega^{\\mathbb{N}} \\longrightarrow \\Omega' = \\mathbb{N}^*\n$$\n\n$$\n(\\omega_1, \\omega_2, \\ldots) \\longmapsto\n\\begin{cases}\n1 & \\text{si } \\omega_1 = \\{6\\},\\\\[4pt]\n2 & \\text{si } \\omega_1 \\neq \\{6\\} \\text{ et } \\omega_2 = \\{6\\},\\\\[4pt]\n& \\cdots\\\\[4pt]\nk & \\text{si } \\omega_1, \\ldots, \\omega_{k-1} \\neq \\{6\\} \\text{ et } \\omega_k = \\{6\\},\\\\[4pt]\n& \\cdots\n\\end{cases}\n$$\n\nPour tout $k \\in \\mathbb{N}^*$, on a :\n\n$$\n\\mathbb{P}(X = k) = \\mathbb{P}(\\omega_1, \\ldots, \\omega_{k-1} \\neq \\{6\\} \\text{ et } \\omega_k = \\{6\\})\n$$\n\n$$\n= \\prod_{i=1}^{k-1} \\mathbb{P}(\\omega_i \\neq \\{6\\}) \\times \\mathbb{P}(\\omega_k = \\{6\\})\n$$\n\n$$\n= (1 - p)^{k-1}p,\n$$\n\navec $p = \\frac{1}{6}$ la probabilité d'obtenir un \"6\" lors d'un lancer. Il s'agit d'une distribution géométrique de paramètre $p$. \n\n2) Fonction de répartition\n\nSoit $k \\geq 1$. Nous avons :\n\n$$\n\\mathbb{P}(X \\leq k) = \\sum_{j=1}^{k} (1-p)^{j-1}p\n$$\n\n$$\n= p\\sum_{j=0}^{k-1} (1-p)^{j} = p\\frac{1-(1-p)^k}{1-(1-p)} = 1-(1-p)^k.\n$$\n\nA partir de la définition de la fonction de répartition, nous obtenons\n\n$$\nF_X(x) = \n\\begin{cases}\n0 & \\text{si } x < 1,\\\\[4pt]\np & \\text{si } 1 \\leq x < 2,\\\\[4pt]\n& \\cdots,\\\\[4pt]\n1-(1-p)^k & \\text{si } k \\leq x < k+1,\\\\[4pt]\n& \\cdots.\n\\end{cases}\n$$\n\nReprésentation graphique de la fonction de répartition :\n\n::: {#cd9129f4 .cell execution_count=3}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef plot_cdf_geometrique(p=0.3, k_max=8):\n    \"\"\"\n    Trace la fonction de répartition\n        F_X(x) = 0                si x < 1\n               = 1 - (1-p)^k      si k <= x < k+1   pour k = 1,2,...\n    avec une troncature à k_max.\n    \"\"\"\n    if not (0 < p < 1):\n        raise ValueError(\"p doit être dans l'intervalle (0, 1).\")\n\n    plt.figure(figsize=(6, 4))\n\n    # Segment avant 1 : F(x) = 0 pour 0 <= x < 1\n    plt.hlines(0, 0, 1, linewidth=2)\n\n    # Segments horizontaux pour k = 1,...,k_max\n    for k in range(1, k_max + 1):\n        level = 1 - (1 - p)**k\n        plt.hlines(level, k, k + 1, linewidth=2)\n\n    # Points ouverts (limite à gauche, non incluse)\n    x_open, y_open = [], []\n    # Points fermés (valeur de la FDR)\n    x_closed, y_closed = [], []\n\n    for k in range(1, k_max + 1):\n        # Limite à gauche en k :\n        if k == 1:\n            left_limit = 0\n        else:\n            left_limit = 1 - (1 - p)**(k - 1)\n\n        value_at_k = 1 - (1 - p)**k\n\n        x_open.append(k)\n        y_open.append(left_limit)\n\n        x_closed.append(k)\n        y_closed.append(value_at_k)\n\n    # Cercles ouverts (non remplis) : limite à gauche\n    plt.scatter(\n        x_open, y_open,\n        facecolors=\"none\",\n        edgecolors=\"black\",\n        s=60,\n        zorder=3\n    )\n\n    # Cercles fermés (remplis) : valeur de F_X en k\n    plt.scatter(\n        x_closed, y_closed,\n        color=\"black\",\n        s=60,\n        zorder=3\n    )\n\n    # Mise en forme\n    plt.xlabel(\"x\")\n    plt.ylabel(\"F(x)\")\n    plt.title(f\"Fonction de répartition géométrique (p = {p})\")\n    plt.xlim(0, k_max + 1.5)\n    plt.ylim(-0.05, 1.05)\n    plt.grid(True, linestyle=\"--\", alpha=0.4)\n\n    plt.show()\n\n# Exemple d'appel\nplot_cdf_geometrique(p=0.25, k_max=8)\n```\n\n::: {.cell-output .cell-output-display}\n![](index_gdr_files/figure-html/cell-4-output-1.png){width=514 height=377}\n:::\n:::\n\n\n3) Fonction génératrice des moments\n\nEn utilisant la formule de transfert en temps discret, nous avons :\n$$\nM_X(t) = \\int e^{tX} d\\mathbb{P} = \\sum_{x=1}^{\\infty} e^{tx}\\mathbb{P}_X(x)\n$$\n\n$$\n= p\\sum_{j=1}^{\\infty} e^{tj}(1-p)^{j-1}\n$$\n\n$$\n= pe^t\\sum_{j=0}^{\\infty} e^{tj}(1-p)^j\n$$\n\n$$\n= pe^t\\sum_{j=0}^{\\infty} q^j \\text{ avec } q = e^t(1-p)\n$$\n\n$$\n= \\frac{pe^t}{1-q} = \\frac{pe^t}{1-e^t(1-p)}.\n$$\n\nLa série converge si et seulement si $|q| < 1$, c'est-à-dire si $|e^t(1-p)| < 1$, soit $e^t < \\frac{1}{1-p}$, ce qui donne $t < \\ln\\left(\\frac{1}{1-p}\\right) = -\\ln(1-p)$.\n\n4) Espérance, variance et coefficient de variation\n\nNous savons que la fonction $M_X$ est indéfiniment dérivable dans un voisinage de 0, et que $M'_X(0) = E(X)$ et $M''_X(0) = E(X^2)$. Nous avons après calcul\n\n$$\nM'_x(t) = \\frac{pe^t}{\\{1-e^t(1-p)\\}^2},\n$$\n\n$$\nM''_x(t) = \\frac{pe^t\\{1-(1-p)^2e^{2t}\\}}{\\{1-e^t(1-p)\\}^4},\n$$\n\ndont nous déduisons que\n\n$$\nM'_x(0) = \\frac{1}{p} \\equiv E(X),\n$$\n\n$$\nM''_x(0) = \\frac{p\\{1-(1-p)^2\\}}{\\{1-(1-p)\\}^4} = \\frac{2-p}{p^2} \\equiv E(X^2),\n$$\n\npuis\n\n$$\nV(X) = \\frac{1-p}{p^2},\n$$\n\n$$\nCV(X) = \\sqrt{1-p}.\n$$\n\nNumériquement, nous obtenons\n\n$$\nE(X) = 6, \\quad V(X) = 30, \\quad CV(X) = 0.91.\n$$\n\n\n## Exercice 3\n\nSoit $X$ une variable aléatoire admettant un moment d'ordre 2. Nous notons $m = E(X)$ et $\\sigma^2 = V(X)$. Soient $X_1, \\ldots, X_n$ des variables aléatoires indépendantes et de même loi que $X$. Nous notons\n\n$$\n\\overline{X}_n = \\frac{1}{n}\\sum_{i=1}^{n} X_i\n$$\n\nla moyenne empirique des $X_i$, $i = 1, \\ldots, n$, et\n\n$$\ns_X^2 = \\frac{1}{n-1}\\sum_{i=1}^{n} (X_i - \\overline{X}_n)^2\n$$\n\nla dispersion des $X_i$, $i = 1, \\ldots, n$.\n\n1) Montrer que $E(\\overline{X}_n) = m$.\n\n2) Montrer que $E(s_X^2) = \\sigma^2$. Vous pourrez utiliser l'identité\n\n$$\ns_X^2 = \\frac{1}{2n(n-1)} \\sum_{i=1}^{n} \\sum_{\\substack{j=1\\\\j \\neq i}}^{n} (X_i - X_j)^2.\n$$\n\n3) Utiliser ces résultats pour obtenir une approximation de Monte-Carlo de l'espérance, la variance et le coefficient de variation de la variable aléatoire $X$ de l'exercice 2.\n\n- Démonstration de l'équivalence des formules de variance\n\nNous voulons démontrer que :\n\n$$\\frac{1}{n-1}\\sum_{i=1}^{n}(X_i - \\bar{X}_n)^2 = \\frac{1}{2n(n-1)}\\sum_{i=1}^{n}\\sum_{\\substack{j=1\\\\j\\neq i}}^{n}(X_i - X_j)^2$$\n\n- Étape 1 : Développement de la somme double\n\nCommençons par développer le membre de droite :\n\n$$\\sum_{i=1}^{n}\\sum_{\\substack{j=1\\\\j\\neq i}}^{n}(X_i - X_j)^2$$\n\nDéveloppons $(X_i - X_j)^2$ :\n\n$$= \\sum_{i=1}^{n}\\sum_{\\substack{j=1\\\\j\\neq i}}^{n}(X_i^2 - 2X_iX_j + X_j^2)$$\n\n$$= \\sum_{i=1}^{n}\\sum_{\\substack{j=1\\\\j\\neq i}}^{n}X_i^2 - 2\\sum_{i=1}^{n}\\sum_{\\substack{j=1\\\\j\\neq i}}^{n}X_iX_j + \\sum_{i=1}^{n}\\sum_{\\substack{j=1\\\\j\\neq i}}^{n}X_j^2$$\n\n- Étape 2 : Simplification de chaque terme\n\nPremier terme :\n$$\\sum_{i=1}^{n}\\sum_{\\substack{j=1\\\\j\\neq i}}^{n}X_i^2 = \\sum_{i=1}^{n}X_i^2 \\cdot (n-1) = (n-1)\\sum_{i=1}^{n}X_i^2$$\n\nCar pour chaque $i$ fixé, $X_i^2$ est sommé $(n-1)$ fois (pour tous les $j \\neq i$).\n\nTroisième terme :\nPar symétrie, le troisième terme donne le même résultat :\n$$\\sum_{i=1}^{n}\\sum_{\\substack{j=1\\\\j\\neq i}}^{n}X_j^2 = (n-1)\\sum_{j=1}^{n}X_j^2 = (n-1)\\sum_{i=1}^{n}X_i^2$$\n\nDeuxième terme :\n$$\\sum_{i=1}^{n}\\sum_{\\substack{j=1\\\\j\\neq i}}^{n}X_iX_j$$\n\nPour chaque paire $(i,j)$ avec $i \\neq j$, le produit $X_iX_j$ apparaît dans cette double somme. On peut réécrire :\n\n$$= \\sum_{i=1}^{n}X_i\\sum_{\\substack{j=1\\\\j\\neq i}}^{n}X_j = \\sum_{i=1}^{n}X_i\\left(\\sum_{j=1}^{n}X_j - X_i\\right)$$\n\n$$= \\sum_{i=1}^{n}X_i \\cdot n\\bar{X}_n - \\sum_{i=1}^{n}X_i^2$$\n\n$$= n\\bar{X}_n \\cdot n\\bar{X}_n - \\sum_{i=1}^{n}X_i^2 = n^2\\bar{X}_n^2 - \\sum_{i=1}^{n}X_i^2$$\n\n- Étape 3 : Assemblage\n\nEn rassemblant les trois termes :\n\n$$\\sum_{i=1}^{n}\\sum_{\\substack{j=1\\\\j\\neq i}}^{n}(X_i - X_j)^2 = (n-1)\\sum_{i=1}^{n}X_i^2 - 2\\left(n^2\\bar{X}_n^2 - \\sum_{i=1}^{n}X_i^2\\right) + (n-1)\\sum_{i=1}^{n}X_i^2$$\n\n$$= (n-1)\\sum_{i=1}^{n}X_i^2 - 2n^2\\bar{X}_n^2 + 2\\sum_{i=1}^{n}X_i^2 + (n-1)\\sum_{i=1}^{n}X_i^2$$\n\n$$= 2(n-1)\\sum_{i=1}^{n}X_i^2 + 2\\sum_{i=1}^{n}X_i^2 - 2n^2\\bar{X}_n^2$$\n\n$$= 2n\\sum_{i=1}^{n}X_i^2 - 2n^2\\bar{X}_n^2$$\n\n$$= 2n\\left(\\sum_{i=1}^{n}X_i^2 - n\\bar{X}_n^2\\right)$$\n\n- Étape 4 : Lien avec la variance classique\n\nRappelons la formule classique de la variance :\n\n$$\\sum_{i=1}^{n}(X_i - \\bar{X}_n)^2 = \\sum_{i=1}^{n}(X_i^2 - 2X_i\\bar{X}_n + \\bar{X}_n^2)$$\n\n$$= \\sum_{i=1}^{n}X_i^2 - 2\\bar{X}_n\\sum_{i=1}^{n}X_i + n\\bar{X}_n^2$$\n\n$$= \\sum_{i=1}^{n}X_i^2 - 2\\bar{X}_n \\cdot n\\bar{X}_n + n\\bar{X}_n^2$$\n\n$$= \\sum_{i=1}^{n}X_i^2 - n\\bar{X}_n^2$$\n\n- Étape 5 : Conclusion\n\nD'après l'étape 3 :\n$$\n\\sum_{i=1}^{n}\\sum_{\\substack{j=1\\\\j\\neq i}}^{n}(X_i - X_j)^2 = 2n\\left(\\sum_{i=1}^{n}X_i^2 - n\\bar{X}_n^2\\right)\n$$\n\n\nD'après l'étape 4 :\n$$\n\\sum_{i=1}^{n}(X_i - \\bar{X}_n)^2 = \\sum_{i=1}^{n}X_i^2 - n\\bar{X}_n^2\n$$\n\nDonc :\n$$\n\\sum_{i=1}^{n}\\sum_{\\substack{j=1\\\\j\\neq i}}^{n}(X_i - X_j)^2 = 2n\\sum_{i=1}^{n}(X_i - \\bar{X}_n)^2\n$$\n\nEn divisant par $2n(n-1)$ :\n\n$$\n\\frac{1}{2n(n-1)}\\sum_{i=1}^{n}\\sum_{\\substack{j=1\\\\j\\neq i}}^{n}(X_i - X_j)^2 = \\frac{1}{n-1}\\sum_{i=1}^{n}(X_i - \\bar{X}_n)^2\n$$\n\n## Correction de l'exercice 3\n\n1) Calcul de $E(\\overline{X}_n)$\n\nPar linéarité de l'espérance, nous avons :\n\n$$\nE(\\overline{X}_n) = E\\left(\\frac{1}{n}\\sum_{i=1}^{n} X_i\\right) = \\frac{1}{n}\\sum_{i=1}^{n} E(X_i) = \\frac{1}{n} \\cdot n \\cdot m = m.\n$$\n\n\n\n2) Calcul de $E(s_X^2)$\n\nEn utilisant l'identité donnée, nous avons en utilisant la linéarité de l'espérance :\n\n$$\nE(s_X^2) = E\\left(\\frac{1}{2n(n-1)} \\sum_{i=1}^{n} \\sum_{\\substack{j=1\\\\j \\neq i}}^{n} (X_i - X_j)^2\\right)\n$$\n\n$$\n= \\frac{1}{2n(n-1)} \\sum_{i=1}^{n} \\sum_{\\substack{j=1\\\\j \\neq i}}^{n} E\\left((X_i - X_j)^2\\right).\n$$\n\nIci on peut utiliser deux méthodes de calcul qui sont équivalentes.\n\nLa première consiste à remarquer que pour $i \\neq j$ :\n\n$$\nE\\left((X_i - X_j)^2\\right) = E\\left(X_i^2\\right) - 2E(X_i X_j) + E\\left(X_j^2\\right)\n$$\n\nComme les $X_i$ sont indépendantes et de même loi, nous avons :\n\n$$\nE\\left(X_i^2\\right) = E\\left(X_1^2\\right) \\quad \\text{et} \\quad E(X_i X_j) = E(X_1)E(X_2) \\quad \\text{pour } i \\neq j.\n$$\n\nDe ce fait, nous obtenons :\n\n$$\nE\\left((X_i - X_j)^2\\right) = E\\left(X_1^2\\right) - 2E(X_1)E(X_2) + E\\left(X_2^2\\right)\n$$\n\n$$\n= E\\left(X_1^2\\right) - 2E(X_1X_2) + E\\left(X_2^2\\right)\n$$\n\nCar $X_1$ et $X_2$ sont indépendantes et de même loi.\n\nDonc finalement :\n\n$$\nE\\left((X_i - X_j)^2\\right) = E\\left((X_1 - X_2)^2\\right).\n$$\n\nUne autre méthode consiste à utiliser la formule de Koenig-Huygens pour la variance. Nous avons :\n\n$$\nE\\left((X_i - X_j)^2\\right) = E\\left(X_1^2\\right) - 2E(X_1)E(X_2) + E\\left(X_2^2\\right)\n$$\n\n$$\n= 2\\left(E\\left(X_1^2\\right) - E(X_1)^2\\right) = 2V(X) = 2\\sigma^2.\n$$\n\nLorsque nous utilisons la première méthode, nous devons calculer $E\\left((X_1 - X_2)^2\\right)$ en développant :\n\n$$\nE\\left((X_1 - X_2)^2\\right) = E[{(X_1 - m) - (X_2 - m)}^2]\n$$\n\n$$\n= E\\left((X_1 - m)^2\\right) + E\\left((X_2 - m)^2\\right) - 2E\\left((X_1 - m)(X_2 - m)\\right)\n$$\n\n$$\n= E\\left((X_1 - m)^2\\right) + E\\left((X_2 - m)^2\\right) - 2E(X_1 - m)\\times E(X_2 - m)\n$$\nCar $X_1$ et $X_2$ sont indépendantes.\n\n$$\n= \\sigma^2 + \\sigma^2 - 2 \\cdot 0 \\cdot 0 = 2\\sigma^2.\n$$\n\nDonc, nous obtenons finalement :\n$$\nE(s_X^2) = \\frac{1}{2} \\cdot 2\\sigma^2 = \\sigma^2.\n$$\n\n3) Approximation de Monte-Carlo\n\nCe que nous avons montré aux points précédents, est que nous pouvons estimer l'espérance et la variance de la variable aléatoire $X$ en utilisant les statistiques d'échantillon $\\overline{X}_n$ et $s_X^2$. C'est-à-dire en générant un grand nombre de réalisations indépendantes de $X$ et en calculant la moyenne empirique et la dispersion de ces réalisations.\n\nVoici un exemple de code R pour effectuer cette approximation de Monte-Carlo pour la variable aléatoire $X$ de l'exercice 2 :\n\n\n```default\n# Monte Carlo approximation in R\n\n# Répétition de l'expérience aléatoire\n# Simulation du nombre de lancers nécessaires pour obtenir un 6\n\nset.seed(360)\nn <- 50000\nctr <- 0\nsimlist <- numeric(n)\n\nwhile (ctr < n) {\n  ctr <- ctr + 1\n  \n  # Valeur du lancer (initialisation)\n  trial <- 0\n  \n  # Nombre de tentatives (initialisation)\n  nb_tent <- 0\n  \n  while (trial != 6) {\n    nb_tent <- nb_tent + 1\n    trial <- sample(1:6, 1)\n  }\n  \n  simlist[ctr] <- nb_tent\n}\n\n# Affichage des résultats\nmean(simlist)\nvar(simlist)\n\n# Statistiques supplémentaires\ncat(\"\\n=== Résultats de la simulation ===\\n\")\ncat(\"Nombre de simulations:\", n, \"\\n\")\ncat(\"Moyenne du nombre de lancers:\", mean(simlist), \"\\n\")\ncat(\"Variance:\", var(simlist), \"\\n\")\ncat(\"Écart-type:\", sd(simlist), \"\\n\")\ncat(\"Minimum:\", min(simlist), \"\\n\")\ncat(\"Maximum:\", max(simlist), \"\\n\")\ncat(\"Médiane:\", median(simlist), \"\\n\")\n\n# Histogramme\nhist(simlist, \n     breaks = 30, \n     col = \"lightblue\", \n     border = \"white\",\n     main = \"Distribution du nombre de lancers pour obtenir un 6\",\n     xlab = \"Nombre de lancers\",\n     ylab = \"Fréquence\",\n     xlim = c(0, max(simlist)))\n\n# Ajout de la moyenne théorique\nabline(v = 6, col = \"red\", lwd = 2, lty = 2)\nlegend(\"topright\", \n       legend = c(\"Moyenne observée\", \"Moyenne théorique = 6\"),\n       col = c(\"blue\", \"red\"), \n       lty = c(1, 2), \n       lwd = 2)\n```\n\n::: {#455aee26 .cell execution_count=4}\n``` {.python .cell-code}\nimport random\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Répétition de l'expérience\nrandom.seed(360)\nn = 50000\nsimlist = []\n\nfor _ in range(n):\n    trial = 0        # valeur du lancer\n    nb_tent = 0      # nombre de tentatives\n    \n    while trial != 6:\n        nb_tent += 1\n        trial = random.randint(1, 6)  # équivalent de sample(1:6,1)\n    \n    simlist.append(nb_tent)\n\nsimlist = np.array(simlist)\n\n# Résultats\nprint(\"\\n=== Résultats de la simulation ===\")\nprint(\"Nombre de simulations:\", n)\nprint(\"Moyenne du nombre de lancers:\", simlist.mean())\nprint(\"Variance:\", simlist.var(ddof=1))\nprint(\"Écart-type:\", simlist.std(ddof=1))\nprint(\"Minimum:\", simlist.min())\nprint(\"Maximum:\", simlist.max())\nprint(\"Médiane:\", np.median(simlist))\n\n# Calcul des moyennes cumulées\nmean_values = np.cumsum(simlist) / np.arange(1, n + 1)\n\n# Tracé\nplt.figure(figsize=(7, 4))\nplt.plot(mean_values, label=\"Moyenne empirique\")\nplt.axhline(6, color=\"red\", linestyle=\"--\", label=\"Espérance théorique = 6\")\n\nplt.xlabel(\"n (nombre d'expériences)\")\nplt.ylabel(\"Moyenne empirique\")\nplt.title(\"Convergence de la moyenne empirique vers l'espérance théorique\")\nplt.legend()\nplt.grid(alpha=0.4)\nplt.show() \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n=== Résultats de la simulation ===\nNombre de simulations: 50000\nMoyenne du nombre de lancers: 6.00114\nVariance: 30.1522217448349\nÉcart-type: 5.4911038730691395\nMinimum: 1\nMaximum: 65\nMédiane: 4.0\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_gdr_files/figure-html/cell-5-output-2.png){width=583 height=378}\n:::\n:::\n\n\n```default\n# Utilisation de la distribution géométrique\n# Alternative plus efficace à la simulation par boucle\n\nset.seed(360)\nn <- 50000\np <- 1/6\nctr <- 0\nsimlist <- numeric(n)\n\nwhile (ctr < n) {\n  ctr <- ctr + 1\n  \n  # Génération d'une réalisation d'une loi géométrique\n  # Attention, rgeom(n,p) donne le nombre d'échecs avant\n  # le premier succès\n  simlist[ctr] <- rgeom(1, p) + 1\n}\n\n# Affichage des résultats\nmean(simlist)\nvar(simlist)\n\n# Statistiques détaillées\ncat(\"\\n=== Résultats de la simulation (méthode géométrique) ===\\n\")\ncat(\"Nombre de simulations:\", n, \"\\n\")\ncat(\"Probabilité de succès (p):\", p, \"\\n\")\ncat(\"Moyenne observée:\", mean(simlist), \"\\n\")\ncat(\"Moyenne théorique:\", 1/p, \"\\n\")\ncat(\"Variance observée:\", var(simlist), \"\\n\")\ncat(\"Variance théorique:\", (1-p)/p^2, \"\\n\")\ncat(\"Écart-type observé:\", sd(simlist), \"\\n\")\ncat(\"Médiane:\", median(simlist), \"\\n\")\n\n# Comparaison graphique\npar(mfrow = c(1, 2))\n\n# Histogramme\nhist(simlist, \n     breaks = 50, \n     col = \"lightgreen\", \n     border = \"white\",\n     main = \"Distribution du nombre de lancers\\n(méthode rgeom)\",\n     xlab = \"Nombre de lancers\",\n     ylab = \"Fréquence\",\n     probability = TRUE)\n\n# Ajout de la moyenne\nabline(v = mean(simlist), col = \"blue\", lwd = 2)\nabline(v = 1/p, col = \"red\", lwd = 2, lty = 2)\nlegend(\"topright\", \n       legend = c(\"Moyenne obs.\", \"Moyenne théo.\"),\n       col = c(\"blue\", \"red\"), \n       lty = c(1, 2), \n       lwd = 2,\n       cex = 0.8)\n\n# QQ-plot pour vérifier la distribution\nqqplot(qgeom(ppoints(n), p) + 1, simlist,\n       main = \"QQ-Plot : Théorique vs Observé\",\n       xlab = \"Quantiles théoriques (Géométrique)\",\n       ylab = \"Quantiles observés\",\n       col = \"darkgreen\",\n       pch = 20,\n       cex = 0.5)\nabline(0, 1, col = \"red\", lwd = 2)\n\npar(mfrow = c(1, 1))\n\n# Méthode encore plus efficace (vectorisée)\ncat(\"\\n=== Méthode vectorisée (plus rapide) ===\\n\")\nset.seed(360)\nsimlist_vec <- rgeom(n, p) + 1\ncat(\"Moyenne:\", mean(simlist_vec), \"\\n\")\ncat(\"Variance:\", var(simlist_vec), \"\\n\")\n```\n\n::: {#697aa9a3 .cell execution_count=5}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport scipy.stats as st\n\n# Paramètres\nnp.random.seed(360)\nn = 50000\np = 1/6\n\n# La loi géométrique de R donne \"nombre d'échecs avant succès\"\n# => équivalent python : st.geom(p).rvs() renvoie déjà nb_tentatives\n# MAIS Python compte à partir de 1, donc c'est ce qu’on veut !\nsimlist = st.geom(p).rvs(size=n)\n\n# Affichage des résultats\nprint(\"\\n=== Résultats de la simulation (méthode géométrique) ===\")\nprint(\"Nombre de simulations:\", n)\nprint(\"Probabilité de succès p:\", p)\nprint(\"Moyenne observée:\", simlist.mean())\nprint(\"Moyenne théorique:\", 1/p)\nprint(\"Variance observée:\", simlist.var(ddof=1))\nprint(\"Variance théorique:\", (1 - p) / p**2)\nprint(\"Écart-type observé:\", simlist.std(ddof=1))\nprint(\"Médiane:\", np.median(simlist))\n\n\n# Moyennes cumulées\nmean_values = np.cumsum(simlist) / np.arange(1, n + 1)\n\n# Tracé\nplt.figure(figsize=(7, 4))\nplt.plot(mean_values, label=\"Moyenne empirique\")\nplt.axhline(1/p, color=\"red\", linestyle=\"--\", label=f\"Espérance théorique = {1/p:.0f}\")\n\nplt.xlabel(\"n (nombre d'expériences)\")\nplt.ylabel(\"Moyenne empirique\")\nplt.title(\"Convergence de la moyenne empirique de la loi géométrique\")\nplt.legend()\nplt.grid(alpha=0.4)\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n=== Résultats de la simulation (méthode géométrique) ===\nNombre de simulations: 50000\nProbabilité de succès p: 0.16666666666666666\nMoyenne observée: 6.04552\nMoyenne théorique: 6.0\nVariance observée: 29.974807425748512\nVariance théorique: 30.000000000000004\nÉcart-type observé: 5.474925335175678\nMédiane: 4.0\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_gdr_files/figure-html/cell-6-output-2.png){width=585 height=378}\n:::\n:::\n\n\n## Exercice 4\n\nSoient X et Y deux variables aléatoires de carré $\\mathbb{P}$-intégrable.\n\n1. Soit $a \\in \\mathbb{R}$.  \n   Écrire  \n   $$\n   E\\big((|X| - a|Y|)^2\\big)\n   $$\n   sous la forme d’un polynôme en a, et calculer son discriminant $\\Delta$.\n\n2. Expliquer pourquoi $\\Delta \\le 0$, et en déduire l’inégalité de Hölder :\n   $$\n   E(|XY|) \\le \\sqrt{E(X^2)} \\, \\sqrt{E(Y^2)}.\n   $$\n\n3. En appliquant l’inégalité précédente à des variables bien choisies, en déduire que :\n   $$\n   |\\mathrm{Cov}(X,Y)| \\le \\sqrt{V(X)} \\, \\sqrt{V(Y)}.\n   $$\n\nL’inégalité de Hölder se généralise sous la forme suivante (admise dans la suite).  \nSoient deux nombres réels $p, q \\ge 1$ conjugués, c’est-à-dire tels que :\n$$\n\\frac{1}{p} + \\frac{1}{q} = 1.\n$$\n\nSoient X et Y deux variables aléatoires telles que  \n$\\int |X|^p \\, d\\mathbb{P} < \\infty$ et $\\int |Y|^q \\, d\\mathbb{P} < \\infty$.  \nAlors :\n$$\nE(|XY|) \\le \\big(E(|X|^p)\\big)^{1/p} \\, \\big(E(|Y|^q)\\big)^{1/q}.\n$$\n\nSoient maintenant deux réels $r$ et $s$ tels que $1 < r < s < \\infty$.  \nSoit $Z$ une variable aléatoire telle que $\\int |Z|^s \\, d\\mathbb{P} < \\infty$.\n\n4. Vérifier que les réels $\\frac{s}{r}$ et $\\frac{s}{s-r}$ sont conjugués.  \n   En appliquant l’inégalité de Hölder avec des variables aléatoires $X$ et $Y$ bien choisies, montrer que :\n   $$\n   E(|Z|^r) \\le \\big(E(|Z|^s)\\big)^{r/s}.\n   $$\n\n5. En déduire que si une variable aléatoire $Z$ admet un moment d’ordre $s > 1$, alors elle admet un moment d’ordre $r$ pour tous les réels $1 < r < s$.\n\n## Correction de l'exercice 4\n\n1) Calcul de $E\\big((|X| - a|Y|)^2\\big)$\n\nNous avons en utilisant la linéarité de l'espérance :\n\n$$\nE\\big((|X| - a|Y|)^2\\big) = E(|X|^2) - 2aE(|X||Y|) + a^2E(|Y|^2).\n$$\n\nest sa forme de polynôme en $a$.\n\nSon discriminant est donc donné par :\n\n$$\n\\Delta = (-2E(|X||Y|))^2 - 4E(|X|^2)E(|Y|^2) = 4\\big(E(|X||Y|)^2 - E(|X|^2)E(|Y|^2)\\big).\n$$\n\nComme $E\\big((|X| - a|Y|)^2\\big) \\geq 0$ pour tout $a \\in \\mathbb{R}$, le polynôme ne peut pas avoir deux racines réelles distinctes, donc $\\Delta \\leq 0$.\n\n3) En appliquant l’inégalité précédente en remplaçant $X$ par $X - E(X)$, et\n$Y$ par $Y - E(Y)$, nous obtenons :\n\n$$\nE \\big( \\{ X - E(X) \\} \\{ Y - E(Y) \\} \\big)\n\\le \\sqrt{E \\big( \\{ X - E(X) \\}^2 \\big)} \\, \\sqrt{E \\big( \\{ X - E(X) \\}^2 \\big)}.\n$$\n\nNous avons ensuite\n\n$$\n|\\mathrm{Cov}(X,Y)|\n  = \\big| E \\big( \\{ X - E(X) \\} \\{ Y - E(Y) \\} \\big) \\big|\n  \\le E \\big( \\{ X - E(X) \\} \\{ Y - E(Y) \\} \\big),\n$$\n\nce qui donne le résultat voulu.\n\n---\n\n4) Nous avons bien\n\n$$\n\\frac{r}{s} + \\frac{s - r}{s} = 1.\n$$\n\nEn appliquant l’inégalité de Hölder générale aux variables aléatoires $X = Z^r$\net $Y = 1$, et avec les nombres conjugués $\\frac{s}{r}$ et $\\frac{s}{s-r}$,\nnous obtenons\n\n$$\nE \\big( |Z^r \\times 1| \\big)\n\\le \\Big( E \\big( |Z^r|^{\\frac{s}{r}} \\big) \\Big)^{\\frac{r}{s}}\n     \\Big( E \\big( |1|^{\\frac{s}{s-r}} \\big) \\Big)^{\\frac{s-r}{s}}\n     = \\Big( E(|Z|^s) \\Big)^{\\frac{r}{s}}.\n$$\n\n$$\n\\Rightarrow \\quad\nE(|Z|^r) \\le \\big( E(|Z|^s) \\big)^{\\frac{r}{s}}.\n$$\n\n\n\n5) Cela découle de l’inégalité précédente.\n\n",
    "supporting": [
      "index_gdr_files"
    ],
    "filters": [],
    "includes": {}
  }
}