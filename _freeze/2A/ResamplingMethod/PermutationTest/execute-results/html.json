{
  "hash": "7323da59706e6cc3cb86a10bb34d00e6",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \" Resampling methods : Permutation Test\"\nsidebar: auto\nauthor:\n  - Jumbong Junior \ncategories: []\ntags: [Resampling methods, Permutation test, wilcoxon test,Monte Carlo ]\n\ntitle-block-banner: false\nformat: \n  html: \n    mainfont: Times New Roman\n    fontsize: 16pt\n\njupyter: python3\n        \n---\n\n\n\n\n# Introduction\n\nResampling methods are a key part of statistical inference. It is important to understand them in order to have accurate estimates, validate models, and dealt with the uncertainty of the data. Indeed, **the resampling methods mesure the variability of statistics by using subsamples from the original data**. There are least four resampling methods: **bootstrap, jackknife, permutation test, and cross-validation** :\n\n| Resampling method | Applications | Type of resampling |\n|-------------------|--------------|--------------------|\n| Efron's bootstrap | Bias, variance, confidence intervals, hypothesis testing | With replacement |\n| Permutation test | Hypothesis testing | Without replacement |\n| Jackknife | Bias, variance, confidence intervals | Leave-one-out |\n| Cross-validation | Model selection, validation | Leave-p-out |\n\nThis document will focus on the permutation test. \n\n# General framework of the hypothesis test\n\nThe goal of statistics is to learn about a population by studying a sample. Often, we don't know the process that produced the sample. Mathematically, the sample is a random variable. \n\nLet $X = (X_1, X_2, \\ldots, X_n)$ be a sample of size $n$ drawn from a population or a probability distribution $P$. \n\nA hypothesis is a declarative statement about the population or the probability distribution. A hypothesis test involves two complementary hypotheses :\n\n- The null hypothesis $H_0$ : \"$P \\in \\mathcal{P}_0$\" where $\\mathcal{P}_0$ is a set of probability distributions.\n- The alternative hypothesis $H_1$ : \"$P \\notin \\mathcal{P}_0$\". \n\nA test $\\phi$ is a decision procedure between $H_0$ and $H_1$, given the sample $X$. \n$$\n\\phi(x) =\n\\begin{cases}\n    1 & \\text{reject} H_0 \\\\\n    0 & \\text{It is not reject} H_0\n\\end{cases}\n$$\n\n\nA test $\\phi$ is defined using a real-valued test statistic $T(X)$ :\n\n- $\\phi(x) = \\mathbb{1}_{\\{T(x) > t_{\\alpha}\\}}$ where $t_{\\alpha}$ is the critical value of the test at level $\\alpha$. $H_0$ is rejected when $T(x) > t_{\\alpha}$.\n- $\\sup_{P \\in \\mathcal{P}_0} \\mathbb{E}_P \\{\\phi\\} \\leq \\alpha : \\text{ The law of } T \\text{ under } H_0 \\text{ is known}$.\n\nThe p-value of a test is the smallest level $\\alpha$ at which the null hypothesis is rejected :\n$$\np(x) = \\inf \\{ \\alpha : T(x) > t_{\\alpha} \\}\n$$\n\nTo construct a test, it is necessary to know the distribution or the law of the test statistic under the null hypothesis in other to compute :\n\n- The critical value $t_{\\alpha}$.\n- The p-value $p(x)$.\n\n# Motivation of the permutation test\n\nIn pratice, the law of the test statistic T is often approximated asymptotically, or requires verifying parametric assumptions that are hard to justify (eg. normality, homoscedasticity, etc.).\n\nThe permutation test is a non-parametric test. It can be used :\n\n- When the size of the sample is small to approximate the law of the test statistic.\n\n- When parametric test is not valid.\n\n# General framework of the permutation test\n\n**Principle** : Working on the observed data to avoid having to assume a specific distribution for the observed sample.\n\n## General Principle \n\n**The statistical Model** : $X \\sim P, P \\in \\mathcal{P}$.\n$P$ is a probability defined on $\\mathbb{R}^N$ and $\\mathcal{P}$ is a set of probability distributions on $\\mathbb{R}^N$.\n\n**The hypothesis test** : $H_0 : P \\in \\mathcal{P}_0$ vs $H_1 : P \\notin \\mathcal{P}_0$. $\\mathcal{P}_0$ is a subset of $\\mathcal{P}$.\n\n**Group of Transformations** : $G$ has the size $|G| = M$.\n$G$ has an idendity element $e$.\nThe operation inside $G$ is associative : $g_1, g_2 \\in G \\Rightarrow g_1 \\cdot g_2 \\in G$.\nFor each $g \\in G$, there is an inverse element $g^{-1}$ such that $g \\cdot g^{-1} = g^{-1} \\cdot g = e$.\n\n$G$ acts on $\\mathbb{R}^N$ : $g \\in G, x \\in \\mathbb{R}^N \\Rightarrow g \\cdot x \\in \\mathbb{R}^N$.\nwhere $e \\cdot x = x$ and $g_1 \\cdot (g_2 \\cdot x) = (g_1 g_2) \\cdot x$\n\n::: {.callout-note icon=false}\n\n## Echangeability Assumption\n\nUnder $H_0$ ($P \\in \\mathcal{P}_0$): for every $g \\in G$, $\\mathcal{L}_P(g \\cdot X) = \\mathcal{L}_P(X)$.\n:::\n\n**Permutation Test** : The permutation test is a test that uses the exchangeability assumption to construct a test.\n\nLet's define the decision function $\\phi$  and the p-value $p$ of the permutation test :\n\n::: {.callout-note icon=false}\n\n## Permutation Test with Level $\\leq \\alpha$\n\n$$\n\\phi(X) = \\mathbb{1}_{\\{T(X) >T^{(\\lceil M(1-\\alpha) \\rceil)}\\}}\n$$\nwhere $T^{(\\lceil M(1-\\alpha) \\rceil)}$ is the $\\lceil M(1-\\alpha) \\rceil$-th order statistic of the test statistic $T(g \\cdot X)$ for $g \\in G$.\n\nThe p-value of the permutation test is :\n$$\np(X) = \\frac{1}{M} \\sum_{g \\in G} \\mathbb{1}_{\\{T(g \\cdot X) > T(X)\\}}\n$$\n\n:::\n\n# Application of the permutation test\n\nThe permutation test has many applications in statistics :\n\n- **Symmetry test** : The permutation test can be used to test the symmetry of a distribution. An example is the Wilcoxon sign-rank test for paired samples.\n\n- **Independence test** : which help assess the effect of one variable on another. An example is the Spearman test.\n\n- **Comparison of two samples** : The permutation test can be used to compare the distribution of two samples. For example to assess if two samples have the same law.\n\n# Symmetry test \n\n**Statistical Model** : $X = (X_1, X_2, \\ldots, X_n)\\overset{i.i.d}{\\sim} F$ where $F$ is a Cumulative Distribution Function (CDF) defined on $\\mathbb{R}$.\n$X = (X_1, \\dots, X_n) \\in \\mathbb{R}^n$\n\n**The hypothesis test** : $H_0 : F \\text{ is symmetric around } 0$ vs $H_1 : F \\text{ is not symmetric around } 0$.\n\n**The transformation group** : $G = \\{-1, 1\\}^n$. with cardinality $|G| = 2^n$.\n\n::: {.callout-note icon=false}\n\n## Exchangeability Assumption\n\nUnder $H_0$ ($F$ is symmetric around 0) : for every $g \\in G$, $\\mathcal{L}_F(g \\cdot X) = \\mathcal{L}_F(X)$.\n:::\n\n**Test statistic** :  The test statistic can be defined by :\n\n- T(X) = |$\\overline{X}$| where $\\overline{X} = \\frac{1}{n} \\sum_{i=1}^n X_i$.\n\n- $T(X) = \\sum_{i=1}^n R_i sign(X_i)$ where $R_i$ is the rank of $|X_i|$ in $|X_1|, |X_2|, \\ldots, |X_n|$.\n\n::: {.callout-tip icon=false}\n\n## Algo : Symmetry test by permutation test (Monte Carlo)\n\n**Variable** : \n\n   - B : 999,999 such that (B+1)$\\alpha$ is an integer.\n\n**Begin** :\n\nfor b = 1 to B do:\n\n   - Generate $g^b \\sim \\mathcal{U}(G)$\n\n   - Compute $T^b = T(g^b \\cdot X)$.  \n   \n**end for.**\n\nReturn the p-value $p(X) = \\frac{1}{B} \\sum_{b=1}^B \\mathbb{1}_{\\{T^b > T(X)\\}}$\n\nend\n\n:::\n\nThe code below is an implementation of the permutation test for the symmetry test.\n\n::: {#77a441f9 .cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\n\n\n# Inputs : x (array) or (list) : sample.\n#func : function : statistic function.\n#nsamp : int : number of samples.\n# Outputs : dictionnary : statistic, sample_perm, p-value.\n\ndef symmetry_perm(x, func=np.mean, nsamp =9999):\n    x = np.asarray(x)\n    n = x.shape[0]\n\n    tobs = func(x)\n    # Generate nsamp samples sign permutation\n    eps = np.random.choice([-1, 1], size=(nsamp, n), replace=True)\n    # Compute the test statistic for each sample\n    permuted = eps * x\n\n    try:\n        tperm = func(permuted, axis=1)\n    except TypeError:\n        tperm = np.apply_along_axis(func, 1, permuted)\n    # Compute the p-value\n\n    count = np.sum(np.abs(tperm) >= np.abs(tobs))\n    p = (count + 1) / (nsamp + 1)\n    return {\"statistic\": tobs, \"sample_perm\": tperm, \"p-value\": p}\n```\n:::\n\n\nLet's apply the symmetry test to a central normal distribution.\n\n::: {#135eebc1 .cell execution_count=2}\n``` {.python .cell-code}\n#np.random.seed(42)\n\nn = 100\nx = np.random.normal(0, 1, n)\n\nres = symmetry_perm(x, func=np.mean, nsamp=9999)\n\nprint(f\"Statistic : {res['statistic']},\" \n      f\" p-value : {res['p-value']}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nStatistic : 0.016915530800782546, p-value : 0.8739\n```\n:::\n:::\n\n\n**Distribution of the mean and the ergodic mean p-values.**\n\n::: {#5d62a728 .cell execution_count=3}\n``` {.python .cell-code}\nB = 6000\n# P value for a range of nsamp from 1 to B\np_values = [symmetry_perm(x, func=np.mean, nsamp=i)[\"p-value\"] for i in np.arange(1, B)]\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns   \n\nfig, axes = plt.subplots(1, 2, figsize=(8, 3))\n\n## ðŸŸ¢ Histogram: Distribution of Test Statistic\nsns.histplot(res['sample_perm'], ax=axes[0], kde=True, color='#1f77b4', alpha=0.7)\naxes[0].axvline(res['statistic'], color='red', linestyle='--', linewidth=2, label='Observed Statistic')\naxes[0].set_title('Distribution of the Test Statistic', fontsize=14, fontweight='bold')\naxes[0].set_xlabel('Test Statistic', fontsize=12)\naxes[0].set_ylabel('Frequency', fontsize=12)\naxes[0].legend(fontsize=12)\n# No grid for better readability\naxes[0].grid(False)\n\n## ðŸ”µ Line Plot: Ergodic Mean of the P-value\naxes[1].plot(np.arange(1, B), p_values, color='#ff7f0e', linewidth=2)\naxes[1].axhline(np.mean(p_values), color='black', linestyle='--', linewidth=2, label='Mean p-value')\naxes[1].set_title('Ergodic Mean of the P-value', fontsize=14, fontweight='bold')\naxes[1].set_xlabel('Iterations', fontsize=12)\naxes[1].set_ylabel('P-value', fontsize=12)\naxes[1].legend(fontsize=12)\naxes[1].grid(False)\n# Annotation for Mean P-value\naxes[1].text(0.6 * B, np.mean(p_values), f\"Mean p-value: {np.mean(p_values):.4f}\",\n             fontsize=12, color='black', fontweight='bold')\n\n# Improve layout\nplt.tight_layout()\n\n# Show the plot\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](PermutationTest_files/figure-html/cell-4-output-1.png){width=760 height=278}\n:::\n:::\n\n\n## Symmetry test : Wilcoxon sign-rank test\n\nOne of the most popular permutation tests is the Wilcoxon sign-rank test. It allows to test for example if two paired samples come from the same distribution. To accomplish this, the Wilcoxon sign-rank test assess if the difference between the two samples is symmetric around 0.\n\nThe test statistic is defined by :\n\n$$\nT(X) = \\sum_{i=1}^n R_i sign(X_i)\n$$\n\nwhere $R_i$ is the rank of $|X_i|$ in $|X_1|, |X_2|, \\ldots, |X_n|$.\n\nFor the application of the Wilcoxon sign-rank test, for 10 patients, we have data on the number of hours of sleep after taking a drug and after taking a placebo. The goal is to test if the number of hours of sleep is the same after taking the drug and the placebo.\n\n::: {#1e066e3f .cell execution_count=4}\n``` {.python .cell-code}\nfrom scipy.stats import wilcoxon, PermutationMethod\nimport pandas as pd\n\ntreatment = np.array([6.1,7.0,8.2,7.6,6.5,8.4,6.9,6.7,7.4,5.8])\nplacebo  = np.array([5.2,7.9,3.9,4.7,5.3,5.4,4.2,6.1,3.8,6.3])\nz = treatment - placebo\n# Stat test \nstatW = lambda z: (pd.Series(np.abs(z)).rank() * np.sign(z)).sum()\n\n# Test per permutation\nres = symmetry_perm(z, func=statW, nsamp=59999)\n\nprint(f\"Statistic : {res['statistic']},\" \n      f\" p-value : {res['p-value']}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nStatistic : 45.0, p-value : 0.0199\n```\n:::\n:::\n\n\nThe p-value is 0.01985 less than 0.05, we reject the null hypothesis. The number of hours of sleep is not the same after taking the drug and the placebo.\n\n# Independence test\n\nLet's define the framework of the independence test :\n\n**Statistical Model** : $(Y_1,Z_1), (Y_2,Z_2), \\ldots, (Y_n,Z_n) \\overset{i.i.d}{\\sim} F$ where $F$ is a joint distribution defined on $\\mathbb{R}^2$.\nX = $(Y, Z) = (Y_1, Z_1), (Y_2, Z_2), \\ldots, (Y_n, Z_n)$, matrix of size $n \\times 2$.\n\n**The hypothesis test** : $H_0 : F(y,z) = F_Y(y)F_Z(z)$ vs $H_1 : F(y,z) \\neq F_Y(y)F_Z(z)$. where $F_Y$ and $F_Z$ are the marginal distributions of $Y$ and $Z$.\n\n**The transformation group** : G is the permutation group of size $|G| = n!$. G acts on $\\mathbb{\\textbf{M}}^{2 \\times n}$ by :\n$$\n\\sigma \\cdot X = \\bigl((y_1, z_{\\sigma(1)}), (y_2, z_{\\sigma(2)}), \\ldots, (y_n, z_{\\sigma(n)})\\bigr)\n$$\nFor $\\sigma \\in G$ and $X = (Y, Z)$.\n\nThe test statistic (association measure or correlation) is defined by :\n\n- T(X) = $\\sum_{i=1}^n (Y_i - \\overline{Y})(Z_i - \\overline{Z})$, pearson correlation.\n\n- T(X) = $\\sum_{i=1}^n R_i(Y) R_i(Z) - \\frac{4(n+1)^2}{n}$, Spearman correlation.\n\n::: {.callout-note icon=false}\n\n# Algo : Independence test by permutation test (Monte Carlo)\n\n**Variable** : \n\n   - B : 999,999 such that (B+1)$\\alpha$ is an integer.\n\n**Begin** :\n\nfor b = 1 to B do:\n\n   - Generate $\\sigma^b \\sim \\mathcal{U}(G)$ (Transformation group)\n\n   - Compute $T^b = T(\\sigma^b \\cdot X)$. (law of permutation)\n\n**end for.**\n\nReturn the p-value $p(X) = \\frac{1}{B+1} (\\sum_{b=1}^B (\\mathbb{1}_{\\{T^b > T(X)\\}}) + 1)$\n\nend\n\n:::\n\nFor example, We want to study the effect of the molar ratio of sebacic acid to the intrinsic viscosity of copolyster. The data is given by :\n\n::: {#41b4ded2 .cell execution_count=5}\n``` {.python .cell-code}\nmolar_ratio = np.array([1, 0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3])\nviscosity = np.array([0.45, 0.2, 0.34, 0.58, 0.7, 0.57, 0.55, 0.44 ])\n\n# Permutation procedure\n\ndef indep_perm(x, y, func, nsamp =9999):\n    x = np.asarray(x)\n    y = np.asarray(y)\n    n = x.shape[0]\n\n    tobs = func(x, y)\n    \n    perms = np.array([np.random.permutation(n) for _ in range(nsamp)])\n\n    tperm = np.array([func(x, y[perm]) for perm in perms])\n\n    # Compute the p-value\n    count = np.sum(np.abs(tperm) >= np.abs(tobs))\n    p = (count + 1) / (nsamp + 1)\n    return {\"statistic\": tobs, \"sample_perm\": tperm, \"p-value\": p}\n\n## Pearson correlation\nstatPearson = lambda x, y: np.corrcoef(x, y)[0, 1]\n\nres = indep_perm(molar_ratio, viscosity, func=statPearson, nsamp=9999)\n\nprint(f\"Statistic : {res['statistic']},\" \n      f\" p-value : {res['p-value']}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nStatistic : -0.4633642874743305, p-value : 0.2446\n```\n:::\n:::\n\n\nIt can be compared to the Pearson correlation test.\n\n::: {#de57d6a2 .cell execution_count=6}\n``` {.python .cell-code}\nfrom scipy.stats import pearsonr\n\nstat, p = pearsonr(molar_ratio, viscosity)\nprint(f\"Statistic : {stat}, p-value : {p}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nStatistic : -0.46336428747433034, p-value : 0.24754088204924157\n```\n:::\n:::\n\n\n# Comparison of two samples\n\nThe general framework of the comparison of two samples is :\n\n**Statistical Model** : Two independent samples $X = (X_1, X_2, \\ldots, X_n) \\overset{i.i.d}{\\sim} F$ and $Y = (Y_1, Y_2, \\ldots, Y_m) \\overset{i.i.d}{\\sim} G$ where $F$ and $G$ are two distributions defined on $\\mathbb{R}$.\n\n**The hypothesis test** : $H_0 : F = G$ versus\n$H_1^{less} : F \\leq G$ or $H_1^{greater} : F \\geq G$ or $H_1^{diff} : F \\neq G$.\n\n**The transformation group** : $G$ is the permutation group of size $|G| = n! \\times m!$. $G$ acts on $\\mathbb{R}^{n+m}$ by :\n$$\n\\sigma \\cdot (X, Y) = (X_{\\sigma(1)}, X_{\\sigma(2)}, \\ldots, X_{\\sigma(n)}, Y_{\\sigma(n+1)}, Y_{\\sigma(n+2)}, \\ldots, Y_{\\sigma(n+m)})\n$$\n\nThe test statistic is defined by :\n\n- T(X) = |$\\overline{X} - \\overline{Y}$|, difference of means.\n\n- $T(X) = \\left| Y\\left(\\left\\lfloor \\frac{n}{2} \\right\\rfloor\\right) - Z\\left(\\left\\lfloor \\frac{n}{2} \\right\\rfloor\\right) \\right|$, difference of medians.\n\n::: {.callout-note icon=false}\n\n# Algo : Comparison of two samples by permutation test (Monte Carlo)\nG is the permutation group of size $|G| = (n+m)!$.\n**Variable** : \n\n   - B : 999,999 such that (B+1)$\\alpha$ is an integer.\n\n**Begin** :\n\nfor b = 1 to B do:\n\n   - Generate $\\sigma^b \\sim \\mathcal{U}(G)$ (Permutation group)\n\n   - Compute $T^b = T(\\sigma^b \\cdot (X, Y))$. (law of permutation)\n\n**end for.**\n\nReturn the p-value $p(X) = \\frac{1}{B+1} (\\sum_{b=1}^B (\\mathbb{1}_{\\{T^b > T(X)\\}}) + 1)$\n\nend\n\n:::\n\nThe code below is an implementation of the comparison of two samples by permutation test.\n\n::: {#3145fab2 .cell execution_count=7}\n``` {.python .cell-code}\nfrom scipy.stats import rankdata\n\ndef twosamp_perm(y,z, func, nsamp =9999, alternative=\"two-sided\"):\n    y = np.asarray(y)\n    z = np.asarray(z)\n    \n    m = len(y)\n    n = len(z)\n    N = m + n\n\n    # Concatenate the two samples into one array\n    x = np.concatenate([y, z])\n    \n    # Compute the observed test statistic\n    tobs = func(y, z)\n    \n    # Generate nsamp permutations: for each permutation, randomly select m indices from 0 to N-1\n    # (Note: Python indices are 0-based, unlike R which is 1-based.)\n    sig = np.array([np.random.choice(N, size=m, replace=False) for _ in range(nsamp)])\n    \n    # Compute test statistic for each permutation\n    tperm = np.empty(nsamp)\n    for i, indices in enumerate(sig):\n        # The complement of the chosen indices: all indices not in 'indices'\n        complement = np.setdiff1d(np.arange(N), indices)\n        tperm[i] = func(x[indices], x[complement])\n    \n    # Calculate the p-value according to the specified alternative hypothesis\n    if alternative == \"two.sided\":\n        p = np.sum(np.abs(tperm) >= np.abs(tobs)) + 1\n    elif alternative == \"greater\":\n        p = np.sum(tperm >= tobs) + 1\n    elif alternative == \"less\":\n        p = np.sum(tperm <= tobs) + 1\n    else:\n        raise ValueError(\"alternative must be one of 'two.sided', 'greater', or 'less'\")\n    \n    p_value = p / (nsamp + 1)\n    \n    # Return results in a dictionary\n    return {\n        \"statistic\": tobs,\n        \"sample_perm\": tperm,\n        \"p-value\": p_value\n    }\n\n# Simulate data\n\ny = np.random.normal(1, 1, 20)\nz = np.random.normal(0, 1, 10)\n\nmean_diff = lambda y, z: np.mean(y) - np.mean(z)\n\nres = twosamp_perm(y, z, func=mean_diff, nsamp=9999, alternative=\"greater\")\n\nprint(f\"Statistic : {res['statistic']},\" \n      f\" p-value : {res['p-value']}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nStatistic : 0.9476770542185005, p-value : 0.0239\n```\n:::\n:::\n\n\nExample : \n\n- yi: placental membrane permeability at term.\n\n- zj: permeability measured between 12 and 26 weeks of gestation.\n\nThe statistic test is given by the wilcoxon statistic rank :\n\n$$\nT(X) = \\sum_{i=1}^n R_i(Y)  -n(N+1)/2\n$$\n\nThe alternative of interest is that the placental membrane is more permeable during full-term pregnancy.\n\n::: {#3e0a1985 .cell execution_count=8}\n``` {.python .cell-code}\ny = np.array([0.80, 0.83, 1.89, 1.04, 1.45, 1.38, 1.91, 1.64, 0.73, 1.46])\nz = np.array([1.15, 0.88, 0.90, 0.74, 1.21])\n\n# Wilcoxon statistic rank\n\nrankwilcoxon = lambda y, z: np.sum(\n    rankdata(np.concatenate([np.asarray(y), np.asarray(z)]))[:len(y)]\n) - len(y) * (len(y) + len(z) + 1) / 2\n\nres = twosamp_perm(y, z, func=rankwilcoxon, nsamp=9999, alternative=\"greater\")\n\nprint(f\"Statistic : {res['statistic']},\" \n      f\" p-value : {res['p-value']}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nStatistic : 10.0, p-value : 0.1265\n```\n:::\n:::\n\n\n::: {#89aa6ed6 .cell execution_count=9}\n``` {.python .cell-code}\n #--- Create Figure ---\nfig, axes = plt.subplots(1, 2, figsize=(7, 3))\n\n# --- Left Plot: Boxplot of Data ---\nsns.boxplot(data=[y, z], ax=axes[0], width=0.5, palette=\"Blues\", fliersize=3)\naxes[0].set_xticks([0, 1])\naxes[0].set_xticklabels([\"y\", \"z\"])\naxes[0].set_title(\"Les donnÃ©es\", fontsize=14)\naxes[0].set_xlabel(\"Group\")\naxes[0].set_ylabel(\"Value\")\n\n# --- Right Plot: Permutation Distribution ---\nsns.histplot(res['sample_perm'], bins=40, color=\"black\", alpha=0.8, ax=axes[1])\naxes[1].axvline(res['statistic'], color='red', linestyle='-', linewidth=2, label=\"Observed Statistic\")\naxes[1].set_title(r\"Distribution de permutation de $T(X)$\", fontsize=14)\naxes[1].set_xlabel(\"x\")\naxes[1].set_ylabel(\"Frequency\")\naxes[1].legend()\n\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](PermutationTest_files/figure-html/cell-10-output-1.png){width=697 height=277}\n:::\n:::\n\n\n",
    "supporting": [
      "PermutationTest_files/figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}