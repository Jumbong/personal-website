{
  "hash": "385e8308cf7ba59176400de563221a1d",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \" Bias, Variance and Mean Squared Error (MSE) of an Estimator using Bootstrap Method in python\"\nsidebar: auto\nnumber-sections: false\ntoc: true\nauthor:\n  - Jumbong Junior \ncategories: []\ntags: [Bootstrap, Confidence Interval, Bias, Variance, Mean Squared Error,Monte Carlo Simulation]\ntitle-block-banner: false\nbibliography: references.bib\nformat: \n  html: \n    mainfont: Times New Roman\n    fontsize: 16pt\n\njupyter: python3\nnotice: |\n    @wasserman2013all\n---\n\n\n\n\n\n# Definition\n\nGiven a sample $\\mathcal{X} = \\{X_1, X_2, \\ldots, X_n\\}$ of size $n$ from a distribution $P$.\n\nLet $\\theta = \\theta(P)$ $\\in \\Theta$ a parameter of interest.\n\nLet $T(\\mathcal{X})$ be a statistic that estimates $\\theta$.\n\nLet $\\theta(P_n)$ be a plug-in estimator of $\\theta$.\n\nIn this post, the **bootstrap method** will be used to estimate the **bias**, **variance**, and **mean squared error (MSE)** of the estimator $T(\\mathcal{X})$ based on the sample $\\mathcal{X}$ :\n\n- The **bias** of $T(\\mathcal{X})$ := $\\mathbb{E_P}[T(\\mathcal{X})] - \\theta$.\n\n- The **variance** of $T(\\mathcal{X})$ := $\\mathbb{E_P}[(T(\\mathcal{X}) - \\mathbb{E_P}[T(\\mathcal{X})])^2]$.\n\n- The **mean squared error (MSE)** of $T(\\mathcal{X})$ := $\\mathbb{E_P}[(T(\\mathcal{X}) - \\theta)^2]$.\n\nThe **bootstrap method** has two steps:\n\n- Estimation using plug-in estimator $\\theta(P_n)$.\n\n- Approximation using Monte Carlo simulation.\n\n# Bootstrap Estimation of the bias.\n\n## Estimation using plug-in estimator \n\n- Real world : $\\mathbb{E_P[R_n(\\mathcal{X},P)]}$ with $R_n(\\mathcal{X},P) = T(\\mathcal{X}) - \\theta(P_n)$.\n\n- Bootstrap world : $R_n^*$ = $R_n(\\mathcal{X}^*,P_n) = T(\\mathcal{X}^*) - \\theta(P_n)$.\n\n## Approximation using Monte Carlo simulation\n\nFor a large positive integer $B$ and by the law of large numbers, \n$$\n\\frac{1}{B} \\sum_{b=1}^{B} T(\\mathcal{X}^*_b, P_n) - \\theta(P_n)\\overset{\\mathbb{P}}{\\longrightarrow} \\mathbb{E_P}[R_n(\\mathcal{X},P)] - \\theta(P).\n$$\n\n::: {.callout-note icon=\"false\"}\n\n### Bootstrap Estimation of the Bias\n\n**Variable**:\n\n- $B$ : A large number of bootstrap samples.\n\n**Begin**:\n\nFor $b = 1, 2, \\ldots, B$:\n  \n  - Generate a bootstrap sample $\\mathcal{X}^*_b$ from $\\mathcal{X}$.\n  - Compute $T(\\mathcal{X}^*_b)$, the bootstrap replication of $T(\\mathcal{X})$.\n\nEnd.\n\n**Return**: \n\n$$\n\\frac{1}{B} \\sum_{b=1}^{B} T(\\mathcal{X}^*_b) - \\theta(P_n)\n$$\n\n:::\n\n# Bootstrap Estimation of the Variance.\n\nGiven $T(\\mathcal{X})$ an estimator of $\\theta = \\theta(P)$.\n\n- Real world : $\\mathbb{E_P}[R_n(\\mathcal{X},P)^2]$ with $R_n(\\mathcal{X},P) = T(\\mathcal{X}) - E_P[T(\\mathcal{X})]$.\n\n- Bootstrap world : $R_n^*$ = $R_n(\\mathcal{X}^*,P_n) = T(\\mathcal{X}^*) - E(T(\\mathcal{X}^*)|\\mathcal{X})$.\n\n## Approximation using Monte Carlo simulation\n\nFor a large positive integer $B$ and by the law of large numbers,\n$$\n\\begin{align}\n    \\frac{1}{B} \\sum_{b=1}^{B} \n    &\\left(T(\\mathcal{X}^*_b) - \\frac{1}{B} \\sum_{b=1}^{B} T(\\mathcal{X}^*_b) \\right)^2  \\notag \\\\\n    &= \\frac{1}{B} \\sum_{b=1}^{B} T(\\mathcal{X}^*_b)^2 \n    - \\left(\\frac{1}{B} \\sum_{b=1}^{B} T(\\mathcal{X}^*_b)\\right)^2  \\notag \\\\\n    &\\overset{\\mathbb{P}}{\\longrightarrow} \n    \\mathbb{E_P}[R_n(\\mathcal{X},P)^2] \n    - \\left(\\mathbb{E_P}[R_n(\\mathcal{X},P)]\\right)^2.\n\\end{align}\n$$\n\n::: {.callout-note icon=\"false\"}\n\n\n### Bootstrap Estimation of the Variance\n\n**Variable**:\n\n- $B$ : A large number of bootstrap samples.\n\n**Begin**:\n\nFor $b = 1, 2, \\ldots, B$:\n  \n  - Generate a bootstrap sample $\\mathcal{X}^*_b$ from $\\mathcal{X}$.\n  - Compute $T(\\mathcal{X}^*_b)$, the bootstrap replication of $T(\\mathcal{X})$.\n\nEnd.\n\n**Return**:\n\n$$\n\\frac{1}{B} \\sum_{b=1}^{B} T(\\mathcal{X}^*_b)^2 - \\left(\\frac{1}{B} \\sum_{b=1}^{B} T(\\mathcal{X}^*_b)\\right)^2\n$$\n\n:::\n\n# Bootstrap Estimation of the Mean Squared Error (MSE).\n\nLet $T(\\mathcal{X})$ be an estimator of $\\theta = \\theta(P)$.\n\nLet $\\theta(P_n)$ be a plug-in estimator of $\\theta$.\n\n- Real world : $\\mathbb{E_P}[R_n(\\mathcal{X},P)^2]$ with $R_n(\\mathcal{X},P) = T(\\mathcal{X}) - \\theta(P)$.\n\n- Bootstrap world : $R_n^*$ = $R_n(\\mathcal{X}^*,P_n) = T(\\mathcal{X}^*) - \\theta(P_n)$.\n\n## Approximation using Monte Carlo simulation\n\nFor a large positive integer $B$ and by the law of large numbers,\n\n$$\n\\begin{align}\n    \\frac{1}{B} \\sum_{b=1}^{B} \n    &\\left(T(\\mathcal{X}^*_b) - \\theta(P_n) \\right)^2  \\notag \\\\\n    &= \\frac{1}{B} \\sum_{b=1}^{B} T(\\mathcal{X}^*_b)^2 \n    - 2 \\theta(P_n) \\frac{1}{B} \\sum_{b=1}^{B} T(\\mathcal{X}^*_b) \n    + \\theta(P_n)^2  \\notag \\\\\n    &\\overset{\\mathbb{P}}{\\longrightarrow} \n    \\mathbb{E_P}[T(\\mathcal{X})^2] \n    - 2 \\theta(P) \\mathbb{E_P}[T(\\mathcal{X})] \n    + \\theta(P)^2\\\\\n    &= \\mathbb{E_P}[R_n(\\mathcal{X},P)^2]\n\\end{align}   \n$$\n\n::: {.callout-note icon=\"false\"}\n\n### Bootstrap Estimation of the Mean Squared Error (MSE)\n\n**Variable**:\n\n- $B$ : A large number of bootstrap samples.\n\n**Begin**:\n\nFor $b = 1, 2, \\ldots, B$:\n  \n  - Generate a bootstrap sample $\\mathcal{X}^*_b$ from $\\mathcal{X}$.\n  - Compute $T(\\mathcal{X}^*_b)$, the bootstrap replication of $T(\\mathcal{X})$.\n\nEnd.\n\n**Return**:\n\n$$\n\\frac{1}{B} \\sum_{b=1}^{B} (T(\\mathcal{X}^*_b) - \\theta(P_n))^2 \n$$\n\n:::\n\n## Choosing B\n\nFactors to consider when choosing the number of bootstrap samples $B$:\n\n- The computational cost of the bootstrap method.\n\n- The size of the sample $\\mathcal{X}$.\n\n- Model complexity.\n\nThe most important aspect is to ensure that the algorithm converges.So, continue to increase the number of bootstrap samples until the algorithm stabilizes.\n\n# Python Implementation\n\nGiven a sample $\\mathcal{X} = \\{X_1, X_2, \\ldots, X_20\\}$ of size $n = 20$ from a centered normal distribution $P$ with mean $\\mu = 0$ and variance $\\sigma^2 = 1$.\n\nThe parameters of interest are the the variance $\\theta = \\theta(P) = \\sigma^2$. \n\nThe estimator $T(\\mathcal{X})$ is the sample variance :\n\n$$\nT(\\mathcal{X}) = \\frac{1}{n-1} \\sum_{i=1}^{n} (X_i - \\bar{X})^2\n$$\n\nwhere $\\bar{X} = \\frac{1}{n} \\sum_{i=1}^{n} X_i$.\n\n::: {#8c51fb90 .cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# -----------------------------------------------------\n# Generate a $X1,...X_20$ sample from a normal distribution, n = 20 \n# -----------------------------------------------------\nnp.random.seed(42)\n\nx = np.random.normal(0, 1, 20)\n\n# -----------------------------------------------------\n# Estimator of the variance\n# -----------------------------------------------------\ndef sample_variance(x):\n    n = len(x)\n    mean = np.mean(x)\n    return np.sum((x - mean)**2) / (n - 1)\n\n# -----------------------------------------------------\n# Fonction to compute the bootstrap bias.\n# -----------------------------------------------------\n\ndef bias_boot(x, fun, nsamp=9999):\n    x = np.array(x)\n    n = len(x)\n    # Compute the plug-in estimator\n    tobs = fun(x)\n\n    tboot = np.empty(nsamp)\n\n    for i in range(nsamp):\n        indices = np.random.choice(n, n, replace=True)\n        tboot[i] = fun(x[indices])\n    bias = np.mean(tboot) - tobs\n    return {'statistic': tobs, 'sample_boot': tboot, 'bias': bias}\n```\n:::\n\n\nLet represent the distribution of $\\hat{\\sigma}^2$ and the ergotic mean of the bias.\n\n::: {#be24d4bc .cell execution_count=2}\n``` {.python .cell-code}\nimport seaborn as sns\nB = 10_000\n\n# -----------------------------------------------------\n# Compute the bootstrap bias of nsamp from 1 to B\n# -----------------------------------------------------\n\nboot_bias =  [bias_boot(x, sample_variance, nsamp=i)for i in np.linspace(1, B,350, dtype=int)]\n\n# -----------------------------------------------------\n# Extract the sample variance and the ergotic mean of the bias\n# -----------------------------------------------------\n\nsigma_hat_2 = boot_bias[-1]['sample_boot']\n\n# -----------------------------------------------------\n# Compute the p-value\n# -----------------------------------------------------\n\np_values = np.mean(sigma_hat_2 > boot_bias[-1]['statistic'])\n\nfig, axes = plt.subplots(1, 2, figsize=(8, 4))\n\n\n\n# -----------------------------------------------------\n# Plot the distribution of the sample variance\n# -----------------------------------------------------\n\nsns.histplot(sigma_hat_2, ax=axes[0], kde=True)\naxes[0].set_title(r'Distribution of the sample variance $\\hat{\\sigma}^2$')\naxes[0].axvline(boot_bias[-1]['statistic'], color='red', linestyle='--')\n\n# -----------------------------------------------------\n# Plot the ergotic mean of the bias\n# -----------------------------------------------------\n\naxes[1].plot(np.linspace(1, B,350, dtype=int),[b['bias'] for b in boot_bias])\naxes[1].axhline(np.mean([b['bias'] for b in boot_bias]), color='red', linestyle='--')\naxes[1].set_title('Ergotic mean of the bias')\n\nplt.show()\n\n```\n\n::: {.cell-output .cell-output-display}\n![](BiasVarianceQuadraticError_files/figure-html/cell-3-output-1.png){width=676 height=361}\n:::\n:::\n\n\nThe distribution of the sample variance $\\hat{\\sigma}^2$ is centered around the plug-in estimator $\\hat{\\sigma}^2$. For B >4000, the ergotic mean of the bias stabilizes.\n\n### References\n\n::: {#refs}\n\n:::\n\n",
    "supporting": [
      "BiasVarianceQuadraticError_files/figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}