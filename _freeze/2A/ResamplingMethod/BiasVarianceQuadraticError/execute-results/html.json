{
  "hash": "754b36714d757f9755b09ec964e66da4",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \" Bias, Variance and Mean Squared Error (MSE) of an Estimator using Bootstrap Method in python\"\nsidebar: auto\nnumber-sections: false\ntoc: true\nauthor:\n  - Jumbong Junior \ncategories: []\ntags: [Bootstrap, Confidence Interval, Bias, Variance, Mean Squared Error,Monte Carlo Simulation]\ntitle-block-banner: false\nbibliography: references.bib\nformat: \n  html: \n    mainfont: Times New Roman\n    fontsize: 16pt\n\njupyter: python3\nnotice: |\n    @wasserman2013all\n---\n\n\n# Definition\n\nGiven a sample $\\mathcal{X} = \\{X_1, X_2, \\ldots, X_n\\}$ of size $n$ from a distribution $P$.\n\nLet $\\theta = \\theta(P)$ $\\in \\Theta$ a parameter of interest.\n\nLet $T(\\mathcal{X})$ be a statistic that estimates $\\theta$.\n\nLet $\\theta(P_n)$ be a plug-in estimator of $\\theta$.\n\nIn this post, the **bootstrap method** will be used to estimate the **bias**, **variance**, and **mean squared error (MSE)** of the estimator $T(\\mathcal{X})$ based on the sample $\\mathcal{X}$ :\n\n- The **bias** of $T(\\mathcal{X})$ := $\\mathbb{E_P}[T(\\mathcal{X})] - \\theta$.\n\n- The **variance** of $T(\\mathcal{X})$ := $\\mathbb{E_P}[(T(\\mathcal{X}) - \\mathbb{E_P}[T(\\mathcal{X})])^2]$.\n\n- The **mean squared error (MSE)** of $T(\\mathcal{X})$ := $\\mathbb{E_P}[(T(\\mathcal{X}) - \\theta)^2]$.\n\nThe **bootstrap method** has two steps:\n\n- Estimation using plug-in estimator $\\theta(P_n)$.\n\n- Approximation using Monte Carlo simulation.\n\n# Bootstrap Estimation of the bias.\n\n## Estimation using plug-in estimator \n\n- Real world : $\\mathbb{E_P[R_n(\\mathcal{X},P)]}$ with $R_n(\\mathcal{X},P) = T(\\mathcal{X}) - \\theta(P_n)$.\n\n- Bootstrap world : $R_n^*$ = $R_n(\\mathcal{X}^*,P_n) = T(\\mathcal{X}^*) - \\theta(P_n)$.\n\n## Approximation using Monte Carlo simulation\n\nFor a large positive integer $B$ and by the law of large numbers, \n$$\n\\frac{1}{B} \\sum_{b=1}^{B} T(\\mathcal{X}^*_b, P_n) - \\theta(P_n)\\overset{\\mathbb{P}}{\\longrightarrow} \\mathbb{E_P}[R_n(\\mathcal{X},P)] - \\theta(P).\n$$\n\n::: {.callout-note icon=\"false\"}\n\n### Bootstrap Estimation of the Bias\n\n**Variable**:\n\n- $B$ : A large number of bootstrap samples.\n\n**Begin**:\n\nFor $b = 1, 2, \\ldots, B$:\n  \n  - Generate a bootstrap sample $\\mathcal{X}^*_b$ from $\\mathcal{X}$.\n  - Compute $T(\\mathcal{X}^*_b)$, the bootstrap replication of $T(\\mathcal{X})$.\n\nEnd.\n\n**Return**: \n\n$$\n\\frac{1}{B} \\sum_{b=1}^{B} T(\\mathcal{X}^*_b) - \\theta(P_n)\n$$\n\n:::\n\n# Bootstrap Estimation of the Variance.\n\nGiven $T(\\mathcal{X})$ an estimator of $\\theta = \\theta(P)$.\n\n- Real world : $\\mathbb{E_P}[R_n(\\mathcal{X},P)^2]$ with $R_n(\\mathcal{X},P) = T(\\mathcal{X}) - E_P[T(\\mathcal{X})]$.\n\n- Bootstrap world : $R_n^*$ = $R_n(\\mathcal{X}^*,P_n) = T(\\mathcal{X}^*) - E(T(\\mathcal{X}^*)|\\mathcal{X})$.\n\n## Approximation using Monte Carlo simulation\n\nFor a large positive integer $B$ and by the law of large numbers,\n$$\n\\begin{align}\n    \\frac{1}{B} \\sum_{b=1}^{B} \n    &\\left(T(\\mathcal{X}^*_b) - \\frac{1}{B} \\sum_{b=1}^{B} T(\\mathcal{X}^*_b) \\right)^2  \\notag \\\\\n    &= \\frac{1}{B} \\sum_{b=1}^{B} T(\\mathcal{X}^*_b)^2 \n    - \\left(\\frac{1}{B} \\sum_{b=1}^{B} T(\\mathcal{X}^*_b)\\right)^2  \\notag \\\\\n    &\\overset{\\mathbb{P}}{\\longrightarrow} \n    \\mathbb{E_P}[R_n(\\mathcal{X},P)^2] \n    - \\left(\\mathbb{E_P}[R_n(\\mathcal{X},P)]\\right)^2.\n\\end{align}\n$$\n\n::: {.callout-note icon=\"false\"}\n\n\n### Bootstrap Estimation of the Variance\n\n**Variable**:\n\n- $B$ : A large number of bootstrap samples.\n\n**Begin**:\n\nFor $b = 1, 2, \\ldots, B$:\n  \n  - Generate a bootstrap sample $\\mathcal{X}^*_b$ from $\\mathcal{X}$.\n  - Compute $T(\\mathcal{X}^*_b)$, the bootstrap replication of $T(\\mathcal{X})$.\n\nEnd.\n\n**Return**:\n\n$$\n\\frac{1}{B} \\sum_{b=1}^{B} T(\\mathcal{X}^*_b)^2 - \\left(\\frac{1}{B} \\sum_{b=1}^{B} T(\\mathcal{X}^*_b)\\right)^2\n$$\n\n:::\n\n# Bootstrap Estimation of the Mean Squared Error (MSE).\n\nLet $T(\\mathcal{X})$ be an estimator of $\\theta = \\theta(P)$.\n\nLet $\\theta(P_n)$ be a plug-in estimator of $\\theta$.\n\n- Real world : $\\mathbb{E_P}[R_n(\\mathcal{X},P)^2]$ with $R_n(\\mathcal{X},P) = T(\\mathcal{X}) - \\theta(P)$.\n\n- Bootstrap world : $R_n^*$ = $R_n(\\mathcal{X}^*,P_n) = T(\\mathcal{X}^*) - \\theta(P_n)$.\n\n## Approximation using Monte Carlo simulation\n\nFor a large positive integer $B$ and by the law of large numbers,\n\n$$\n\\begin{align}\n    \\frac{1}{B} \\sum_{b=1}^{B} \n    &\\left(T(\\mathcal{X}^*_b) - \\theta(P_n) \\right)^2  \\notag \\\\\n    &= \\frac{1}{B} \\sum_{b=1}^{B} T(\\mathcal{X}^*_b)^2 \n    - 2 \\theta(P_n) \\frac{1}{B} \\sum_{b=1}^{B} T(\\mathcal{X}^*_b) \n    + \\theta(P_n)^2  \\notag \\\\\n    &\\overset{\\mathbb{P}}{\\longrightarrow} \n    \\mathbb{E_P}[T(\\mathcal{X})^2] \n    - 2 \\theta(P) \\mathbb{E_P}[T(\\mathcal{X})] \n    + \\theta(P)^2\\\\\n    &= \\mathbb{E_P}[R_n(\\mathcal{X},P)^2]\n\\end{align}   \n$$\n\n::: {.callout-note icon=\"false\"}\n\n### Bootstrap Estimation of the Mean Squared Error (MSE)\n\n**Variable**:\n\n- $B$ : A large number of bootstrap samples.\n\n**Begin**:\n\nFor $b = 1, 2, \\ldots, B$:\n  \n  - Generate a bootstrap sample $\\mathcal{X}^*_b$ from $\\mathcal{X}$.\n  - Compute $T(\\mathcal{X}^*_b)$, the bootstrap replication of $T(\\mathcal{X})$.\n\nEnd.\n\n**Return**:\n\n$$\n\\frac{1}{B} \\sum_{b=1}^{B} (T(\\mathcal{X}^*_b) - \\theta(P_n))^2 \n$$\n\n:::\n\n\n### References\n\n::: {#refs}\n\n:::\n\n",
    "supporting": [
      "BiasVarianceQuadraticError_files"
    ],
    "filters": [],
    "includes": {}
  }
}