{
  "hash": "4bd116a441071bdb71f49599ce10f319",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \" Bootstrap Efron\"\nsidebar: auto\nauthor:\n  - Jumbong Junior \ncategories: []\ntags: [Bootstrap, Monte Carlo, Resampling,distribution, empirical distribution, statistical functional, bootstrap sample, bootstrap distribution, simulation]\n\ntitle-block-banner: false\nformat: \n  html: \n    mainfont: Times New Roman\n    fontsize: 16pt\n\njupyter: python3\n        \n---\n\n\n# Introduction\n\nThe bootstrap method is a resampling technique proposed by Efron in 1980. First, it was introduced in order to estimate the variance of a statistic. \n\n## Motivation \n\nGiven a sample $\\mathcal{X_n} = \\{X_1, X_2, \\ldots, X_n\\}$, consisting of i.i.d. random variables drawn for an unknown distribution $P$ , our goal is to estimate the distribution of the R($\\mathbfcal{X_n}$,$P$). \n\nIn other words, given a sample $\\mathcal{X_n}$ from an unknown distribution $P$, how can we estimate the distribution of a variable that depends on both sample and underlying law $P$ ?\n\n\n## Objective \n\nEstimating the law of  R($\\mathbfcal{X_n}$,$P$) without making parametric assumptions about the distribution $P$.\n\n## Principe du bootstrap \n\nGenerating multiple new sample by randomly selecting values with replacement from the original sample $\\mathcal{X_n} = \\{X_1, X_2, \\ldots, X_n\\}$. \n\nEach new sample, known as **bootstrap sample**, has the sample size as the original sample. By repeating this process many times, we can create an empirical distribution  $P_n$ which serves as an approximation of the true distribution $P$.\n\nThis empirical distribution allows us to estimate the distribution of the variable R($\\mathbfcal{X_n}$,$P$), even if we do not know the true distribution $P$.\n\n# Background\n\n## Estimation by injection : empirical distribution\n\nWe consider a probability space $(\\Omega, \\mathcal{F}, P)$. \n\nLet $\\mathcal{X} = \\{X_1, X_2, \\ldots, X_n\\}$  be a sample generated from an unknown distribution $P$,\n\nthe cumulative distribution function (CDF) of P is defined as :\n\n$$\nF(x) = P(X_1 \\leq x)\n$$\n\nOne way to estimate the CDF of P is to use the empirical distribution function, which is defined as : \n$$\nF_n(x) = \\frac{1}{n} \\sum_{i=1}^{n} \\mathbb{1}_{X_i \\leq x}\n$$\n\nwhere $\\mathbb{1}_{X_i \\leq x}$ is the indicator function that takes the value 1 if $X_i \\leq x$ and 0 otherwise. In other words, given the sample $\\mathcal{X} = \\{X_1, X_2, \\ldots, X_n\\}$, to estimate the CDF of P, we count the number of observations that are less than or equal to x and divide by the sample size n.\n\nIt becomes simple to understand the empirical measure associated with $F_n$. It assigns an equal weight of 1/n of each sample observation :\n\n$$\n\\hat{P_n} = \\frac{1}{n} \\sum_{i=1}^{n} \\delta_{X_i}\n$$\n\nwhere $\\delta_{X_i}$ is the Dirac measure at $X_i$.\n\nIt possible to define the estimation by injection. First, let's define a statistical functional.\n\n## Statistical functional\n\nA statistical functional is a parameter expressed as a function of the distribution of sample :\n$\\theta = \\theta(P)$ or $\\theta = \\theta(F)$.\n\nExamples :\n\n- Mean : $\\theta(P) = \\int x dP(x)$\n- Variance : $\\theta(P) = \\int (x - \\mu)^2 dP(x)$\n- Median : $\\theta(P) = F^{-1}(1/2)$\n\nAn estimator by injection consists of replacing the unknown distribution P by the empirical distribution $P_n$ in the statistical functional $\\theta(P)$ : \n\n$\\hat{\\theta_n} = \\theta(P_n)$ or $\\hat{\\theta_n} = \\theta(F_n)$. \n\nExamples :\n\n- Mean : $\\hat{\\mu_n} = \\frac{1}{n} \\sum_{i=1}^{n} X_i$\n- Variance : $\\hat{\\sigma^2_n} = \\frac{1}{n} \\sum_{i=1}^{n} (X_i - \\hat{\\mu_n})^2$\n- Median : $m_e(P_n) = X_(n/2)$ or $X_(n/2 + 1)$ i.e $X_(\\left \\lceil{n/2}\\right \\rceil)$.  \n\nThis is called the plug-in principle. \n\n## Bootstrap principle\n\nExtend the plug-in principle to create several bootstrap samples by resampling with replacement from the original sample $\\mathcal{X} = \\{X_1, X_2, \\ldots, X_n\\}$. In each bootstrap sample, the statistical functional is estimated by injection using the empirical distribution $P_n$. \n\n![Bootstrap principle](image.png)\n\nIn nutshell, the bootstrap method can be summarized as follows : \n\n::: {.callout-note icon=\"false\"}\n\n## **Bootstrap Estimator of the law of R($\\mathbfcal{X_n}$,$P$)**.\n\nThe distribution of R($\\mathbfcal{X_n}$,$P$) is estimated using the **conditional law** of :\n\n$$\nR_n^* = R(\\mathcal{X}_n^*, P_n) \\quad \\text{given} \\quad \\mathcal{X}_n.\n$$\n\n:::\n\n::: {.callout-note icon=\"false\"}\n\n## **Monte Carlo approximation and Resampling**\n\nGiven a sample $\\mathcal{X_n} = \\{X_1, X_2, \\ldots, X_n\\}$, a **bootstrap sample** $\\mathcal{X_n}^* = \\{X_1^*, X_2^*, \\ldots, X_n^*\\}$ is generated by resampling with replacement from the original sample $\\mathcal{X_n}$.\n\nThe **conditional law** of $R_n^*$ given $\\mathcal{X}$ can be approximated using a **Monte Carlo method**.\n\n:::\n\nThis process is called **resampling**.\n\n# Application \n\nIn Python, to generate a bootstrap sample, we can use the `numpy.random.choice` function with the `replace=True` argument.\n\nHere is an example of generating a bootstrap sample from an original sample `data`:\n\n::: {#dba19e64 .cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\n\ndata = np.array([1, 2, 3, 4, 5])\nbootstrap_sample = np.random.choice(data, size=len(data), replace=True)\nprint(f\"The bootstrap sample is: {bootstrap_sample}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nThe bootstrap sample is: [3 4 5 2 1]\n```\n:::\n:::\n\n\n## Bootstrap distribution of the sample mean $R(\\mathcal{X_n}, P) = \\bar{X}$\n\n\nFor application, the unknown distribution $P$ is assumed to be normal with mean $\\mu = 0$ and standard deviation $\\sigma = 1$. We generate a sample of size $n = 20$ from this distribution. The distribution of $\\bar{X}$ will be estimated using the unknown distribution $P$ or non-asymptotic, the asymptotic distribution, and the bootstrap distribution.\n\n$X_1, X_2, \\ldots, X_{20} \\sim \\mathcal{N}(0, 1)$ or $P$\n\n$R(\\mathcal{X_n}, P) = \\bar{X}$\n\n- $\\mathcal{L}_(\\overline{X})$ : unknown distribution $P$ in green\n\n- $N\\big(\\mu, \\frac{\\sigma}{\\sqrt{n}}\\big)$ : asymptotic distribution in red\n\n- $\\mathcal{L}_P\\bigl(\\overline{X}^{*} \\mid X_n\\bigr)\\quad \\text{(bootstrap estimator)}$ in black\n\n::: {#3a636390 .cell execution_count=2}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\nfrom statsmodels.distributions.empirical_distribution import ECDF\n\n# -------------------------------------------------------\n#  Generate or load your data\n#    Here, we simulate data from a normal distribution\n# -------------------------------------------------------\nnp.random.seed(42)      # For reproducibility\nn = 20                  # Sample size\nmu_true = 0         # True mean of the distribution\nsigma_true = 1     # True std dev of the distribution\n\n# Simulate one sample of size n (the 'observed' data)\ndata = np.random.normal(mu_true, sigma_true, n)\n\n# -------------------------------------------------------\n#  Basic statistics from the sample\n# -------------------------------------------------------\nxbar = np.mean(data)           # Sample mean\ns = np.std(data, ddof=1)       # Sample standard deviation (unbiased)\n\n\n# -------------------------------------------------------\n#  Non-asymptotic distribution\n#    (Approximation by simulating from the known true distribution)\n# -------------------------------------------------------\nM = 500  # Number of fresh samples to approximate the true distribution\ntrue_means = np.empty(M)\n\nfor i in range(M):\n    # Generate a new sample of size n from the TRUE distribution\n    new_sample = np.random.normal(mu_true, sigma_true, n)\n    true_means[i] = np.mean(new_sample)\n\n# -------------------------------------------------------\n# 2) Asymptotic (normal) distribution\n#    Based on CLT:  ~ Normal(mean = xbar, sd = s / sqrt(n))\n# -------------------------------------------------------\n# We'll generate a range of x-values for plotting the CDF\nx_values = np.linspace(xbar - 4*s, xbar + 4*s, 300)\ncdf_asymptotic = norm.cdf(x_values, loc=xbar, scale=s / np.sqrt(n))\n\n\n\n# -------------------------------------------------------\n# 3) Bootstrap distribution of the sample mean\n# -------------------------------------------------------\nB = 500  # Number of bootstrap replicates\nboot_means = np.empty(B)\n\nfor i in range(B):\n    # Resample 'data' with replacement\n    sample_boot = np.random.choice(data, size=n, replace=True)\n    # Compute mean of bootstrap sample\n    boot_means[i] = np.mean(sample_boot)\n\n\n# -------------------------------------------------------\n# 4) Empirical CDFs\n# -------------------------------------------------------\necdf_boot = ECDF(boot_means)\necdf_true = ECDF(true_means)\n\n# -------------------------------------------------------\n# 6) Plot the three distributions (CDFs)\n# -------------------------------------------------------\nplt.figure(figsize=(8, 5))\n\n# Non-asymptotic (true) empirical CDF\nplt.plot(x_values, ecdf_true(x_values), 'g-', label='Non-asymptotic (Simulated)')\n\n# Asymptotic (normal) CDF\nplt.plot(x_values, cdf_asymptotic, 'r-', label='Asymptotic (Normal Approx)')\n\n# Bootstrap empirical CDF\nplt.plot(x_values, ecdf_boot(x_values), 'k-', label='Bootstrap')\n\n\n\nplt.title(\"Comparison of Asymptotic, Bootstrap, and Non-Asymptotic CDFs\")\nplt.xlabel(\"Sample Mean\")\nplt.xlim(-1, 1)\nplt.ylabel(\"CDF\")\nplt.legend()\nplt.grid(False)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](boostrappEfron_files/figure-html/cell-3-output-1.png){width=678 height=449}\n:::\n:::\n\n\n## Bootstrap distribution of the sample mean $R(\\mathcal{X_n}, P) = \\sqrt{n}(\\bar{X} - \\mu)$\n\nIn this example, we consider the same normal distribution as before, but we estimate the distribution of the standardized sample mean $R_n = \\sqrt{n}(\\bar{X} - \\mu)$.\n\n::: {#8dc55d71 .cell execution_count=3}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom statsmodels.distributions.empirical_distribution import ECDF\nfrom scipy.stats import norm\n\n# ------------------------------------------------------------------\n# 1) Paramètres et génération d'un échantillon initial\n# ------------------------------------------------------------------\nnp.random.seed(42)\nn = 20\nmu_true = 5.0\nsigma_true = 2.0\n\n# Échantillon observé (simulé) de taille n\ndata = np.random.normal(mu_true, sigma_true, n)\n\n# Moyenne empirique et écart-type empirique\nxbar = np.mean(data)\ns = np.std(data, ddof=1)\n\n# ------------------------------------------------------------------\n# 2) Calcul de R_n pour l'échantillon initial (simple curiosité)\n# ------------------------------------------------------------------\nRn_observe = np.sqrt(n) * (xbar - mu_true)\nprint(\"R_n observé sur l'échantillon :\", Rn_observe)\n\n# ------------------------------------------------------------------\n# 1) Distribution non-asymptotique \"exacte\"\n#    En supposant connaître la loi d'origine\n# ------------------------------------------------------------------\nM = 500\nRn_exact = np.empty(M)\n\nfor i in range(M):\n    new_sample = np.random.normal(mu_true, sigma_true, n)\n    xbar_new = np.mean(new_sample)\n    Rn_exact[i] = np.sqrt(n)*(xbar_new - mu_true)\n\necdf_exact = ECDF(Rn_exact)\n\n# ------------------------------------------------------------------\n# 2) Distribution asymptotique de R_n (approx normale)\n#    Sous H0 : R_n ~ Normal(0, sigma^2) si on suppose sigma_true connu\n# ------------------------------------------------------------------\n# On crée un vecteur de x\nx_vals = np.linspace(-4*s*np.sqrt(n), 4*s*np.sqrt(n), 500)\n\n# Par le TCL, R_n ~ N(0, sigma^2)\n# => On utilise la \"vraie\" sigma si on la connaît\ncdf_asympt = norm.cdf(x_vals, loc=0, scale=sigma_true)\n\n\n\n# ------------------------------------------------------------------\n# 3) Distribution bootstrap de R_n\n# ------------------------------------------------------------------\nB = 500\nRn_boot = np.empty(B)\n\nfor i in range(B):\n    sample_boot = np.random.choice(data, size=n, replace=True)\n    xbar_boot = np.mean(sample_boot)\n    # On connaît mu_true dans cet exemple, sinon on remplacerait mu_true\n    # par la moyenne empirique globale si on voulait un pivot, etc.\n    Rn_boot[i] = np.sqrt(n) * (xbar_boot - mu_true)\n\necdf_boot = ECDF(Rn_boot)\n\n\n\n# ------------------------------------------------------------------\n# 4) Tracé des trois distributions (CDF)\n# ------------------------------------------------------------------\nplt.figure(figsize=(8,5))\n\n# Non-asymptotique\nplt.plot(x_vals, ecdf_exact(x_vals), 'g-', label='non-asympt')\n\n# Asymptotique\nplt.plot(x_vals, cdf_asympt, 'r-', label=r'asympt (N(0, sigma^2))')\n\n# Bootstrap\nplt.plot(x_vals, ecdf_boot(x_vals), 'k-', label='boot')\n\n\n\nplt.title(r\"Distribution de $R_n = \\sqrt{n}(\\bar{X} - \\mu)$\")\nplt.xlim(-10, 10)\nplt.xlabel(r\"$R_n$\")\nplt.ylabel(r\"F($R_n$)\")\nplt.legend()\nplt.grid(False)\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nR_n observé sur l'échantillon : -1.532140911327418\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](boostrappEfron_files/figure-html/cell-4-output-2.png){width=679 height=454}\n:::\n:::\n\n\n",
    "supporting": [
      "boostrappEfron_files"
    ],
    "filters": [],
    "includes": {}
  }
}