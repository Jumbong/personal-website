{
  "hash": "75cf77a474579e80d899fcf4032c1d07",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \" Sampling Bias and Class Imbalance (Oversampling and Undersampling) in Maximum-likelihood Logistic Regression\"\nsidebar: auto\nauthor:\n  - Jumbong Junior \ncategories: []\ntags: []\n\ntitle-block-banner: false\nhtml:\n    code-fold : true\njupyter: python3\nformat: \n    pdf: \n      fontsize: 12pt\n        \n---\n\n\n# Introduction \n\nIn this article, the impact of sampling bias and class imbalance on logistic regression models is explored. The article covers two topics :\n\n- First, we hypothesize that the predictive performance of a logistic regression model is related to the sampling bias associated with the data and it has a perrformance advantage when the data is balanced. The hypothesis is testing with two simulated datasets : a balanced dataset (50:50) and an imbalanced dataset (80:20).  Each dataset will be sampled to produce samples with the following distribution : 50:50, 60:40, 70:30, 80:20, 90:10, 95:5, 99:1. \n\n- Second, in the case of class imbalance data, if the oversampling technique is used the intercept of the logistic regression model after oversampling need to be adjusted. In other words, let denote $\\hat{\\beta_0}$ the intercept of the logistic regression model after oversampling, then the following correction needs to be performed on it : \n\n$$\n\\hat{\\beta}_0 - \\ln\\left[\\left(\\frac{1-\\tau}{\\tau}\\right)\\left(\\frac{\\bar{y}}{1-\\bar{y}}\\right)\\right]\n$$\n\nwhere $\\hat{\\beta_0}$ is the intercept of the logistic regression model after oversampling, $\\tau$ is the proportion of the minority class in the original dataset (or in population), and $\\bar{y}$ is the proportion of the minority class in the oversampled dataset (or in sample).\n\n# 1. Simulated Data Generation\n\nMany authors document that, for logistic regressionthe , the probability distribution of the dependent variable is assumed to be Bernoulli and the mass function f is given by :\n\n$$\nf(y, x, \\alpha, \\beta) = p(x, \\alpha, \\beta)^y(1-p(x, \\alpha, \\beta))^{1-y}\n$$\n\nwhere \n$$\np(x, \\alpha, \\beta) = \\frac{\\exp(\\alpha + \\beta x)}{1 + \\exp(\\alpha + \\beta x)} \n$$\n\nand where y is the dependent variable, x is the independent variable, $\\alpha$ and $\\beta$ are the parameters to be estimated using the maximum likelihood method (MLE).\n\nFor generating the the bernouilli trial y using for a fixed parameter P, we use the following equation :\n\n$$\ny(p) = \n\\begin{cases}\n\\text{dummy} \\leftarrow \\mathrm{rnd}(1), \\\\\n0, & \\text{if dummy} < p, \\\\\n1, & \\text{otherwise}.\n\\end{cases}\n$$\n\nwhere rnd(1) is a random number generator that generates a random number between 0 and 1.\n\nThe conditional bernouilli trials y are then generated by substituting of $p(x, \\alpha, \\beta)$ :\n\n$$\ny(x, \\alpha, \\beta) =\n\\begin{cases}\n0, & \\text{if rnd(1)} < p(x, \\alpha, \\beta), \\\\\n1, & \\text{otherwise}.\n\\end{cases}\n$$\n\n\nIn order to generate the data, the following steps will be respected : \n- Generate x the predictor variable from a uniform distribution, which ranges from 0 to 10.\n- Choose the parameters $\\alpha$ and $\\beta$, which will help to genererate the distribution of the dependent variable y.\n- Generate the dependent variable y using the logistic function $p(x, \\alpha, \\beta)$.\n\n\n## 1.1 Numerical Approach to determine the value of $\\beta$\n\nThe numerical approach consists to determine, for a given value of $\\alpha =-10$, the value of $\\beta$ that will allow to have a proportion of y=1 equal to 0.5 in the case of a balanced dataset and 0.2 in the case of an imbalanced dataset.\n\nThe optimization problem can be formulated as follows :\n\n\n$$\n\\min_{\\beta} \\left( \\text{prop} - \\frac{1}{n}\\sum_{i=1}^{n} \\frac{\\exp(\\alpha + \\beta x_i)}{1 + \\exp(\\alpha + \\beta x_i)} \\right)^2 \n$$\n\nwhere $\\text{prop}$ is the proportion of y=1 in the dataset, $x_i$ is the predictor variable, and $n$ is the number of observations.\n\nThe optimization problem can be solved using the `scipy.optimize.minimize` function with the Nelder-Mead method.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\nfrom scipy.optimize import minimize\n\n# Define the logistic function\ndef logistic_function(x, alpha, beta):\n    return 1 / (1 + np.exp(-(alpha + beta * x)))\n\n# Objective function: minimize the squared difference between mean(pi) and 0.2\ndef objective(alpha, prop, beta, n):\n    x = np.random.uniform(0, 10, n)  # Simulate x values\n    pi = logistic_function(x, alpha, beta)\n    return (np.mean(pi) - 0.2)**2  # Target mean(pi) = 0.2\n\n# Initial guesses for alpha and beta\ninitial_params = [0]\n\n# Optimize alpha and beta\nresult = minimize(lambda params: objective(-10, 0.2, params, 50000), initial_params, method='Nelder-Mead')\n\n# Get optimized alpha and beta\nbeta_opt = result.x\nprint(f\"Optimized alpha: {-10}, beta: {beta_opt}\")\n\n# Generate x and simulate y\nx = np.random.uniform(0, 10, 1000)\npi = logistic_function(x, -10, beta_opt)\ny = (np.random.uniform(0, 1, 1000) < pi).astype(int)\n\n# Verify proportions\ny_mean = np.mean(y)\nprint(f\"Proportion of y=1: {y_mean:.2f}, y=0: {1-y_mean:.2f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nOptimized alpha: -10, beta: [1.23975]\nProportion of y=1: 0.21, y=0: 0.79\n```\n:::\n:::\n\n\n## Simulated Data Generation with $\\alpha = -10$ \n\nThe graphical approach consists to fix the value of $\\alpha$ for example $\\alpha = -10$ and to vary the value of $\\beta$ in order to see the suitable distribution of the dependent variable y.\n\nLet's consider two cases :\n\n  - Case A : A balanced dataset with 50:50 distribution of y=0 and y=1.\n  - Case B : An imbalanced dataset with 80:20 distribution of y=0 and y=1.\n\nThe code below generates the data for the two cases and plots the proportion of y=1 as a function of beta.\n\nThe graph @fig-simulate_case_data, both the datasets have a total of 50,000 events, with the Case A dataset having a class distribution of about 50:50 and Case B dataset having a class distribution of about 80:20.\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Inputs : n_events, alpha, beta, random_state\n# Outputs : x, y, prop_y0, prop_y1\n# Objective : Simulate data from a logistic model with given alpha, beta.\n\ndef simulate_case_data(n_events, alpha, beta, random_state=42):\n    \"\"\"\n    Simulate data from a logistic model with given alpha, beta.\n    \n    x ~ Uniform(0, 10), y ~ Bernoulli(pi(x)), \n    where pi(x) = exp(alpha + beta*x) / (1 + exp(alpha + beta*x)).\n\n    Parameters\n    ----------\n    n_events : int\n        Number of observations (events) to generate.\n    alpha : float\n        Intercept (alpha) for the logistic function.\n    beta : float\n        Slope (beta) for the logistic function.\n    random_state : int\n        Seed for reproducibility.\n\n    Returns\n    -------\n    x : np.ndarray of shape (n_events,)\n        Predictor values sampled from Uniform(0,10).\n    y : np.ndarray of shape (n_events,)\n        Binary outcomes (0 or 1) from Bernoulli trials.\n    prop_y0 : float\n        Proportion of y==0 in the dataset.\n    prop_y1 : float\n        Proportion of y==1 in the dataset.\n    \"\"\"\n    np.random.seed(random_state)\n    \n    # 1) Draw x from Uniform(0,10)\n    x = np.random.uniform(0, 10, size=n_events)\n    \n    # 2) Compute pi(x, alpha, beta)\n    logit = alpha + beta*x\n    pi = np.exp(logit) / (1.0 + np.exp(logit))\n    \n    # 3) Generate y via Bernoulli(pi)\n    dummy = np.random.rand(n_events)\n    y = (dummy < pi).astype(int)\n    \n    # 4) Calculate proportions of 0 and 1\n    prop_y0 = np.mean(y == 0)\n    prop_y1 = np.mean(y == 1)\n    \n    return x, y, prop_y0, prop_y1\n\n# ---------------- Example usage ----------------\n\n\n# Case A: alpha=-10, beta=2 --> expected ~50:50 distribution\nxA, yA, p0_A, p1_A = simulate_case_data(\n    n_events=50000,\n    alpha=-10,\n    beta=2,\n    random_state=42\n)\n\n# Case B: alpha=-10, beta=3.85 --> expected ~80:20 distribution\nxB, yB, p0_B, p1_B = simulate_case_data(\n    n_events=50000,\n    alpha=-10,\n    beta=beta_opt,\n    random_state=42\n)\n\n# Verify proportions\n\n# Suppose p0_A, p1_A, p0_B, p1_B are already defined\n# e.g., p0_A = 0.50; p1_A = 0.50; p0_B = 0.80; p1_B = 0.20\n\nfig, axes = plt.subplots(1, 2, figsize=(10, 4))  # 1 row, 2 columns\n\n# -------- LEFT SUBPLOT: Case A -----------\nax1 = axes[0]\nbar_container_A = ax1.bar(['y=0', 'y=1'], [p0_A, p1_A], color=['steelblue', 'orange'])\nax1.set_title('Proportion of y=0 and y=1 in Case A')\nax1.set_xlabel('y')\nax1.set_ylabel('Proportion')\nax1.bar_label(bar_container_A, fmt='%.2f')\n\n# -------- RIGHT SUBPLOT: Case B -----------\nax2 = axes[1]\nbar_container_B = ax2.bar(['y=0', 'y=1'], [p0_B, p1_B], color=['steelblue', 'orange'])\nax2.set_title('Proportion of y=0 and y=1 in Case B')\nax2.set_xlabel('y')\nax2.set_ylabel('Proportion')\nax2.bar_label(bar_container_B, fmt='%.2f')\n\nplt.tight_layout()  # improves spacing\nplt.show()\n\n```\n\n::: {.cell-output .cell-output-display}\n![Simulated data from logistic model with alpha=-10, beta=2 and alpha=-10, beta=beta_opt](bias_sample_files/figure-pdf/fig-simulate_case_data-output-1.pdf){#fig-simulate_case_data fig-pos='H'}\n:::\n:::\n\n\nThe theoretical properties of the simulated datasets are presented in @fig-plot_logit_vs_x. The left subplot shows the probability of y=1 as a function of x for Case A and Case B. The right subplot shows the logit function as a function of x for Case A and Case B. The logit function is given by $\\alpha + \\beta x$.\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef logistic(x, alpha, beta):\n    return np.exp(alpha + beta*x) / (1.0 + np.exp(alpha + beta*x))\n\nx_values = np.linspace(0, 10, 50000)\n\n# Case A\nalpha_A, beta_A = -10, 2\npi_A = logistic(x_values, alpha_A, beta_A)\nlogit_A = alpha_A + beta_A * x_values\n\n# Case B\nalpha_B, beta_B = -10, beta_opt\npi_B = logistic(x_values, alpha_B, beta_B)\nlogit_B = alpha_B + beta_B * x_values\n\nfig, axes = plt.subplots(2, 2, figsize=(10, 8))\n\n# (a) Probability vs. x for Case A\naxes[0, 0].plot(x_values, pi_A, 'r')\naxes[0, 0].set_title('Case A: Probability vs. x')\naxes[0, 0].set_xlabel('x')\naxes[0, 0].set_ylabel(r'$\\pi(x,\\alpha,\\beta)$')  # <-- raw string\n# Add horizontal line at y=0.5 and vertical line at x= 5\naxes[0, 0].axhline(y=0.5, color='k', linestyle='--')\naxes[0, 0].axvline(x=5, color='k', linestyle='--')\n# (b) Logit vs. x for Case A\naxes[0, 1].plot(x_values, logit_A, 'r')\naxes[0, 1].set_title('Case A: Logit vs. x')\naxes[0, 1].set_xlabel('x')\naxes[0, 1].set_ylabel(r'$\\alpha + \\beta x$')      # <-- raw string\n\n# (c) Probability vs. x for Case B\naxes[1, 0].plot(x_values, pi_B, 'r')\naxes[1, 0].set_title('Case B: Probability vs. x')\naxes[1, 0].set_xlabel('x')\naxes[1, 0].set_ylabel(r'$\\pi(x,\\alpha,\\beta)$')  # <-- raw string\n# Add horizontal line at y=0.5 and vertical line at x= 5\naxes[1, 0].axhline(y=0.2, color='k', linestyle='--')\naxes[1, 0].axvline(x=5, color='k', linestyle='--')\n\n# (d) Logit vs. x for Case B\naxes[1, 1].plot(x_values, logit_B, 'r')\naxes[1, 1].set_title('Case B: Logit vs. x')\naxes[1, 1].set_xlabel('x')\naxes[1, 1].set_ylabel(r'$\\alpha + \\beta x$')      # <-- raw string\n\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Logit vs. x for Case A and Case B](bias_sample_files/figure-pdf/fig-plot_logit_vs_x-output-1.pdf){#fig-plot_logit_vs_x fig-pos='H'}\n:::\n:::\n\n\n# Methodology\n\nTo test the hypothesis that sampling bias controls the optimal class balance required for the best predictive performance of maximum-likelihood logistic regression, samples will be drawn from the two datasets (Case A and Case B) with varying class distributions. The class distributions will be as follows: 50:50, 60:40, 70:30, 80:20, 90:10, 95:5, and 99:1, respectively.\n\nFor each \n\n",
    "supporting": [
      "bias_sample_files"
    ],
    "filters": []
  }
}